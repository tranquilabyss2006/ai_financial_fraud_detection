
=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\.gitignore ===
environment.yml


=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\all_content.txt ===

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\.gitignore ===
environment.yml


=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\all_content.txt ===

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\environment.yml ===

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\README.md ===
# Financial Fraud Detection System

A comprehensive, world-class fraud detection system that uses advanced machine learning, statistical analysis, and AI techniques to identify potentially fraudulent transactions.

## Features

### üõ°Ô∏è Multi-Layer Detection
- **Unsupervised Learning**: Isolation Forest, Local Outlier Factor, Autoencoders, One-Class SVM
- **Supervised Learning**: Random Forest, XGBoost, LightGBM, Neural Networks
- **Rule-Based Detection**: Configurable rules with weights for known fraud patterns

### üîç Advanced Feature Engineering
- **Statistical Features**: Benford's Law, Z-score, MAD, percentile analysis
- **Graph Features**: Centrality measures, clustering coefficients, community detection
- **NLP Features**: Sentiment analysis, keyword detection, pattern matching
- **Time Series Features**: Burstiness analysis, gap analysis, temporal patterns

### üìä Intelligent Analysis
- **Risk Scoring**: Weighted combination of algorithm outputs
- **Explainable AI**: SHAP values and natural language explanations
- **Dynamic Thresholds**: Percentile-based and adaptive thresholds
- **Real-time Monitoring**: Continuous learning from new data

### üìà Professional Reporting
- **Executive Summary**: High-level overview for stakeholders
- **Detailed Analysis**: Comprehensive forensic audit reports
- **Technical Reports**: Model performance and system metrics
- **Custom Reports**: User-configurable report generation

### üåê User-Friendly Interface
- **Streamlit Dashboard**: Interactive web-based interface
- **Data Upload**: Support for CSV, Excel files up to 10GB
- **Column Mapping**: AI-powered column matching with confirmation
- **Visualization**: Interactive charts and graphs

## Installation

### Option 1: Using Conda (Recommended)

1. Clone the repository:
```bash
git clone https://github.com/your-username/fraud-detection-system.git
cd fraud-detection-system

2. Create and activate the conda environment:
conda env create -f environment.yml
conda activate fraud-detection

3.Install additional packages:
pip install -r requirements.txt



Usage
Running the Application

1. Start the Streamlit application:
streamlit run app/main_dashboard.py

2. Open your web browser and navigate to the provided URL (usually http://localhost:8501)


Expected Data Format
The system expects transaction data with the following columns:

transaction_id,timestamp,amount,currency,sender_id,receiver_id,sender_account_type,receiver_account_type,sender_bank,receiver_bank,sender_location,receiver_location,transaction_type,transaction_category,merchant_id,merchant_category,ip_address,device_id,description,notes,authorization_status,chargeback_flag,fraud_flag



Data Types:
transaction_id: string/integer (unique identifier)
timestamp: datetime (ISO format: YYYY-MM-DD HH:MM:SS)
amount: float (transaction value)
currency: string (3-letter ISO code)
sender_id: string (entity initiating transaction)
receiver_id: string (entity receiving transaction)
sender_account_type: string (personal, business, corporate, etc.)
receiver_account_type: string (personal, business, corporate, etc.)
sender_bank: string (name of sender's bank)
receiver_bank: string (name of receiver's bank)
sender_location: string (country, state, city)
receiver_location: string (country, state, city)
transaction_type: string (transfer, payment, withdrawal, deposit)
transaction_category: string (retail, services, gambling, etc.)
merchant_id: string (if applicable)
merchant_category: string (if applicable)
ip_address: string (IPv4 or IPv6)
device_id: string (unique device identifier)
description: string (transaction description)
notes: string (additional notes)
authorization_status: string (approved, declined, pending)
chargeback_flag: boolean (True/False)
fraud_flag: boolean (True/False, for supervised learning)

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\requirements.txt ===
# Core Data Processing
pandas>=1.5.0
numpy>=1.21.0
scipy>=1.9.0

# Machine Learning
scikit-learn>=1.1.0
xgboost>=1.6.0
lightgbm>=3.3.0
imbalanced-learn>=0.9.0

plotly==5.22.0

# Deep Learning
tensorflow>=2.10.0
keras>=2.10.0

# Graph Analysis
networkx>=2.8.0
python-louvain>=0.16

# Natural Language Processing
nltk>=3.7
textblob>=0.17.1
gensim>=4.2.0

# Visualization
matplotlib>=3.5.0
seaborn>=0.11.0
plotly>=5.10.0
graphviz>=0.20.0

# Explainability
shap>=0.41.0

# Large Data Processing
dask>=2022.8.0

# Web Interface
streamlit>=1.12.0

# Report Generation
reportlab>=3.6.0
fpdf2>=2.6.0

# Configuration
pyyaml>=6.0

# API and Web
requests>=2.28.0

# Utilities
tqdm>=4.64.0
joblib>=1.1.0
python-dateutil>=2.8.0
pytz>=2022.1

# Testing
pytest>=7.1.0
pytest-cov>=3.0.0

# Development
jupyter>=1.0.0
ipykernel>=6.15.0

xlrd

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\.devcontainer\devcontainer.json ===
{
  "name": "Python 3",
  // Or use a Dockerfile or Docker Compose file. More info: https://containers.dev/guide/dockerfile
  "image": "mcr.microsoft.com/devcontainers/python:1-3.11-bullseye",
  "customizations": {
    "codespaces": {
      "openFiles": [
        "README.md",
        "app/main_dashboard.py"
      ]
    },
    "vscode": {
      "settings": {},
      "extensions": [
        "ms-python.python",
        "ms-python.vscode-pylance"
      ]
    }
  },
  "updateContentCommand": "[ -f packages.txt ] && sudo apt update && sudo apt upgrade -y && sudo xargs apt install -y <packages.txt; [ -f requirements.txt ] && pip3 install --user -r requirements.txt; pip3 install --user streamlit; echo '‚úÖ Packages installed and Requirements met'",
  "postAttachCommand": {
    "server": "streamlit run app/main_dashboard.py --server.enableCORS false --server.enableXsrfProtection false"
  },
  "portsAttributes": {
    "8501": {
      "label": "Application",
      "onAutoForward": "openPreview"
    }
  },
  "forwardPorts": [
    8501
  ]
}

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\.ipynb_checkpoints\all_content-checkpoint.txt ===
 
=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\tests\.ipynb_checkpoints\test_models-checkpoint.py === 
"""
Test cases for models module
"""

import pytest
import pandas as pd
import numpy as np
import os
import sys

# Add src to path
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'src'))

from fraud_detection_engine.models.unsupervised import UnsupervisedModels
from fraud_detection_engine.models.supervised import SupervisedModels
from fraud_detection_engine.models.rule_based import RuleEngine

class TestUnsupervisedModels:
    """Test cases for UnsupervisedModels class"""
    
    def setup_method(self):
        """Setup test data"""
        self.models = UnsupervisedModels(contamination=0.1)
        
        # Create test data
        np.random.seed(42)
        self.test_data = pd.DataFrame({
            'feature1': np.random.normal(0, 1, 100),
            'feature2': np.random.normal(0, 1, 100),
            'feature3': np.random.normal(0, 1, 100),
            'feature4': np.random.normal(0, 1, 100),
            'feature5': np.random.normal(0, 1, 100)
        })
        
        # Add some outliers
        self.test_data.loc[0:4, 'feature1'] = 10  # Outliers
        self.test_data.loc[5:9, 'feature2'] = -10  # Outliers
    
    def test_run_models(self):
        """Test running models"""
        results = self.models.run_models(self.test_data)
        
        # Check that results are returned
        assert isinstance(results, dict)
        assert len(results) > 0
        
        # Check that each model has required keys
        for model_name, model_results in results.items():
            assert 'predictions' in model_results
            assert 'scores' in model_results
            assert 'model' in model_results
            assert 'feature_names' in model_results
    
    def test_isolation_forest(self):
        """Test Isolation Forest model"""
        results = self.models.run_models(self.test_data)
        
        if 'isolation_forest' in results:
            model_results = results['isolation_forest']
            
            # Check predictions
            predictions = model_results['predictions']
            assert len(predictions) == len(self.test_data)
            assert set(predictions) == {-1, 1}  # -1 for outliers, 1 for inliers
            
            # Check scores
            scores = model_results['scores']
            assert len(scores) == len(self.test_data)
            assert all(0 <= score <= 1 for score in scores)  # Normalized scores
    
    def test_local_outlier_factor(self):
        """Test Local Outlier Factor model"""
        results = self.models.run_models(self.test_data)
        
        if 'local_outlier_factor' in results:
            model_results = results['local_outlier_factor']
            
            # Check predictions
            predictions = model_results['predictions']
            assert len(predictions) == len(self.test_data)
            assert set(predictions) == {-1, 1}  # -1 for outliers, 1 for inliers
            
            # Check scores
            scores = model_results['scores']
            assert len(scores) == len(self.test_data)
            assert all(0 <= score <= 1 for score in scores)  # Normalized scores
    
    def test_autoencoder(self):
        """Test Autoencoder model"""
        results = self.models.run_models(self.test_data)
        
        if 'autoencoder' in results:
            model_results = results['autoencoder']
            
            # Check predictions
            predictions = model_results['predictions']
            assert len(predictions) == len(self.test_data)
            assert set(predictions) == {-1, 1}  # -1 for outliers, 1 for inliers
            
            # Check scores
            scores = model_results['scores']
            assert len(scores) == len(self.test_data)
            assert all(0 <= score <= 1 for score in scores)  # Normalized scores
    
    def test_predict(self):
        """Test making predictions"""
        # First fit models
        self.models.run_models(self.test_data)
        
        # Create new data
        new_data = pd.DataFrame({
            'feature1': np.random.normal(0, 1, 10),
            'feature2': np.random.normal(0, 1, 10),
            'feature3': np.random.normal(0, 1, 10),
            'feature4': np.random.normal(0, 1, 10),
            'feature5': np.random.normal(0, 1, 10)
        })
        
        # Test prediction with Isolation Forest
        if 'isolation_forest' in self.models.models:
            result = self.models.predict(new_data, 'isolation_forest')
            
            assert 'predictions' in result
            assert 'scores' in result
            assert len(result['predictions']) == len(new_data)
            assert len(result['scores']) == len(new_data)
    
    def test_get_feature_importance(self):
        """Test getting feature importance"""
        # First fit models
        self.models.run_models(self.test_data)
        
        # Test with Isolation Forest
        if 'isolation_forest' in self.models.models:
            importance = self.models.get_feature_importance('isolation_forest')
            
            assert isinstance(importance, pd.DataFrame)
            assert 'feature' in importance.columns
            assert 'importance' in importance.columns
            assert len(importance) == len(self.test_data.columns)

class TestSupervisedModels:
    """Test cases for SupervisedModels class"""
    
    def setup_method(self):
        """Setup test data"""
        self.models = SupervisedModels(test_size=0.3, random_state=42)
        
        # Create test data
        np.random.seed(42)
        self.test_data = pd.DataFrame({
            'feature1': np.random.normal(0, 1, 100),
            'feature2': np.random.normal(0, 1, 100),
            'feature3': np.random.normal(0, 1, 100),
            'feature4': np.random.normal(0, 1, 100),
            'feature5': np.random.normal(0, 1, 100),
            'fraud_flag': np.random.choice([0, 1], 100, p=[0.9, 0.1])  # Imbalanced classes
        })
    
    def test_run_models(self):
        """Test running models"""
        results = self.models.run_models(self.test_data)
        
        # Check that results are returned
        assert isinstance(results, dict)
        assert len(results) > 0
        
        # Check that each model has required keys
        for model_name, model_results in results.items():
            assert 'model' in model_results
            assert 'performance' in model_results
            assert 'feature_importance' in model_results
            assert 'feature_names' in model_results
    
    def test_random_forest(self):
        """Test Random Forest model"""
        results = self.models.run_models(self.test_data)
        
        if 'random_forest' in results:
            model_results = results['random_forest']
            
            # Check model
            model = model_results['model']
            assert model is not None
            
            # Check performance
            performance = model_results['performance']
            assert 'accuracy' in performance
            assert 'precision' in performance
            assert 'recall' in performance
            assert 'f1' in performance
            assert 'roc_auc' in performance
            
            # Check feature importance
            importance = model_results['feature_importance']
            assert isinstance(importance, pd.DataFrame)
            assert 'feature' in importance.columns
            assert 'importance' in importance.columns
    
    def test_xgboost(self):
        """Test XGBoost model"""
        results = self.models.run_models(self.test_data)
        
        if 'xgboost' in results:
            model_results = results['xgboost']
            
            # Check model
            model = model_results['model']
            assert model is not None
            
            # Check performance
            performance = model_results['performance']
            assert 'accuracy' in performance
            assert 'precision' in performance
            assert 'recall' in performance
            assert 'f1' in performance
            assert 'roc_auc' in performance
            
            # Check feature importance
            importance = model_results['feature_importance']
            assert isinstance(importance, pd.DataFrame)
            assert 'feature' in importance.columns
            assert 'importance' in importance.columns
    
    def test_predict(self):
        """Test making predictions"""
        # First fit models
        self.models.run_models(self.test_data)
        
        # Create new data
        new_data = pd.DataFrame({
            'feature1': np.random.normal(0, 1, 10),
            'feature2': np.random.normal(0, 1, 10),
            'feature3': np.random.normal(0, 1, 10),
            'feature4': np.random.normal(0, 1, 10),
            'feature5': np.random.normal(0, 1, 10)
        })
        
        # Test prediction with Random Forest
        if 'random_forest' in self.models.models:
            result = self.models.predict(new_data, 'random_forest')
            
            assert 'predictions' in result
            assert 'probabilities' in result
            assert len(result['predictions']) == len(new_data)
            assert len(result['probabilities']) == len(new_data)
    
    def test_get_feature_importance(self):
        """Test getting feature importance"""
        # First fit models
        self.models.run_models(self.test_data)
        
        # Test with Random Forest
        if 'random_forest' in self.models.models:
            importance = self.models.get_feature_importance('random_forest')
            
            assert isinstance(importance, pd.DataFrame)
            assert 'feature' in importance.columns
            assert 'importance' in importance.columns
    
    def test_get_performance(self):
        """Test getting performance metrics"""
        # First fit models
        self.models.run_models(self.test_data)
        
        # Test with Random Forest
        if 'random_forest' in self.models.models:
            performance = self.models.get_performance('random_forest')
            
            assert isinstance(performance, dict)
            assert 'accuracy' in performance
            assert 'precision' in performance
            assert 'recall' in performance
            assert 'f1' in performance
            assert 'roc_auc' in performance

class TestRuleEngine:
    """Test cases for RuleEngine class"""
    
    def setup_method(self):
        """Setup test data"""
        self.rule_engine = RuleEngine(threshold=0.5)
        
        # Create test data
        self.test_data = pd.DataFrame({
            'transaction_id': range(1, 11),
            'timestamp': pd.date_range('2023-01-01', periods=10, freq='D'),
            'amount': [100, 200, 10000, 5000, 15000, 300, 400, 12000, 8000, 20000],
            'sender_id': [f'sender_{i}' for i in range(10)],
            'receiver_id': [f'receiver_{i}' for i in range(10)],
            'sender_location': ['USA'] * 5 + ['Canada'] * 5,
            'receiver_location': ['USA'] * 8 + ['North Korea'] * 2,
            'description': [
                'Normal payment',
                'Regular transfer',
                'Large amount',
                'Big transaction',
                'Huge payment',
                'Small amount',
                'Normal transfer',
                'Large transaction',
                'Big payment',
                'Massive amount'
            ]
        })
    
    def test_apply_rules(self):
        """Test applying rules"""
        results = self.rule_engine.apply_rules(self.test_data)
        
        # Check that results are returned
        assert isinstance(results, dict)
        
        # Check required keys
        assert 'rule_results' in results
        assert 'rule_scores' in results
        assert 'total_scores' in results
        assert 'normalized_scores' in results
        assert 'rule_violations' in results
        assert 'violated_rule_names' in results
        assert 'rules' in results
        assert 'rule_weights' in results
        assert 'rule_descriptions' in results
    
    def test_high_amount_rule(self):
        """Test high amount rule"""
        # Apply rules
        results = self.rule_engine.apply_rules(self.test_data)
        
        # Check rule results
        if 'high_amount' in results['rule_results']:
            rule_results = results['rule_results']['high_amount']
            
            # Should flag transactions with amount > 10000
            expected_flags = [amount > 10000 for amount in self.test_data['amount']]
            assert rule_results == expected_flags
    
    def test_cross_border_rule(self):
        """Test cross border rule"""
        # Apply rules
        results = self.rule_engine.apply_rules(self.test_data)
        
        # Check rule results
        if 'cross_border' in results['rule_results']:
            rule_results = results['rule_results']['cross_border']
            
            # Should flag transactions with different sender and receiver countries
            expected_flags = []
            for i in range(len(self.test_data)):
                sender_country = self.test_data.loc[i, 'sender_location'].split(',')[0]
                receiver_country = self.test_data.loc[i, 'receiver_location'].split(',')[0]
                expected_flags.append(sender_country != receiver_country)
            
            assert rule_results == expected_flags
    
    def test_high_risk_country_rule(self):
        """Test high risk country rule"""
        # Apply rules
        results = self.rule_engine.apply_rules(self.test_data)
        
        # Check rule results
        if 'high_risk_country' in results['rule_results']:
            rule_results = results['rule_results']['high_risk_country']
            
            # Should flag transactions involving North Korea
            expected_flags = ['North Korea' in location for location in self.test_data['receiver_location']]
            assert rule_results == expected_flags
    
    def test_rule_violations(self):
        """Test rule violations"""
        # Apply rules
        results = self.rule_engine.apply_rules(self.test_data)
        
        # Check that violations are detected
        violations = results['rule_violations']
        assert len(violations) == len(self.test_data)
        
        # Check that some transactions are flagged
        assert any(violations)  # At least one transaction should be flagged
    
    def test_add_rule(self):
        """Test adding custom rule"""
        # Add custom rule
        def custom_rule(row):
            return row.get('amount', 0) > 5000
        
        self.rule_engine.add_rule('custom_rule', custom_rule, weight=0.5, description='Custom rule')
        
        # Check that rule is added
        assert 'custom_rule' in self.rule_engine.rules
        assert self.rule_engine.rule_weights['custom_rule'] == 0.5
        assert self.rule_engine.rule_descriptions['custom_rule'] == 'Custom rule'
    
    def test_remove_rule(self):
        """Test removing rule"""
        # Remove a rule
        if 'high_amount' in self.rule_engine.rules:
            self.rule_engine.remove_rule('high_amount')
            
            # Check that rule is removed
            assert 'high_amount' not in self.rule_engine.rules
            assert 'high_amount' not in self.rule_engine.rule_weights
            assert 'high_amount' not in self.rule_engine.rule_descriptions
    
    def test_update_rule_weight(self):
        """Test updating rule weight"""
        # Update rule weight
        self.rule_engine.update_rule_weight('high_amount', 0.5)
        
        # Check that weight is updated
        assert self.rule_engine.rule_weights['high_amount'] == 0.5
    
    def test_get_rules(self):
        """Test getting rules"""
        rules_info = self.rule_engine.get_rules()
        
        # Check structure
        assert 'rules' in rules_info
        assert 'weights' in rules_info
        assert 'descriptions' in rules_info
        
        # Check that rules are returned
        assert len(rules_info['rules']) > 0
        assert len(rules_info['weights']) > 0
        assert len(rules_info['descriptions']) > 0

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\.ipynb_checkpoints\environment-checkpoint.yml ===
name: fraud-detection
channels:
  - conda-forge
  - defaults
dependencies:
  - python=3.9
  - pip>=22.0.0
  - voila
  
  # Core Data Processing
  - pandas>=1.5.0
  - numpy>=1.21.0
  - scipy>=1.9.0
  
  # Machine Learning
  - scikit-learn>=1.1.0
  - xgboost>=1.6.0
  - lightgbm>=3.3.0
  - imbalanced-learn>=0.9.0
  
  # Deep Learning
  - tensorflow>=2.10.0
  - keras>=2.10.0
  
  # Graph Analysis
  - networkx>=2.8.0
  - python-louvain>=0.16
  
  # Natural Language Processing
  - nltk>=3.7
  - textblob>=0.17.1
  - gensim>=4.2.0
  
  # Visualization
  - matplotlib>=3.5.0
  - seaborn>=0.11.0
  - plotly>=5.10.0
  - python-graphviz>=0.20.0
  
  # Explainability
  - shap>=0.41.0
  
  # Large Data Processing
  - dask>=2022.8.0
  
  # Web Interface
  - streamlit>=1.12.0
  
  # Report Generation
  - reportlab>=3.6.0
  
  # Configuration
  - pyyaml>=6.0
  
  # API and Web
  - requests>=2.28.0
  
  # Utilities
  - tqdm>=4.64.0
  - joblib>=1.1.0
  - python-dateutil>=2.8.0
  - pytz>=2022.1
  
  # Testing
  - pytest>=7.1.0
  - pytest-cov>=3.0.0
  
  # Development
  - jupyter>=1.0.0
  - ipykernel>=6.15.0
  
  # Additional packages via pip
  - pip:
    - fpdf2>=2.6.0

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\.ipynb_checkpoints\README-checkpoint.md ===

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\.ipynb_checkpoints\requirements-checkpoint.txt ===
# Core Data Processing
pandas>=1.5.0
numpy>=1.21.0
scipy>=1.9.0

# Machine Learning
scikit-learn>=1.1.0
xgboost>=1.6.0
lightgbm>=3.3.0
imbalanced-learn>=0.9.0

plotly==5.22.0

# Deep Learning
tensorflow>=2.10.0
keras>=2.10.0

# Graph Analysis
networkx>=2.8.0
python-louvain>=0.16

# Natural Language Processing
nltk>=3.7
textblob>=0.17.1
gensim>=4.2.0

# Visualization
matplotlib>=3.5.0
seaborn>=0.11.0
plotly>=5.10.0
graphviz>=0.20.0

# Explainability
shap>=0.41.0

# Large Data Processing
dask>=2022.8.0

# Web Interface
streamlit>=1.12.0

# Report Generation
reportlab>=3.6.0
fpdf2>=2.6.0

# Configuration
pyyaml>=6.0

# API and Web
requests>=2.28.0

# Utilities
tqdm>=4.64.0
joblib>=1.1.0
python-dateutil>=2.8.0
pytz>=2022.1

# Testing
pytest>=7.1.0
pytest-cov>=3.0.0

# Development
jupyter>=1.0.0
ipykernel>=6.15.0

xlrd

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\.streamlit\config.toml ===
[runtime]
python = "3.9"


=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\app\main_dashboard.py ===
"""
Main Dashboard for Financial Fraud Detection System
Streamlit-based user interface for the fraud detection platform
"""
import streamlit as st
import pandas as pd
import numpy as np
import os
import sys
import time
import json
import plotly.express as px
import plotly.graph_objects as go
from datetime import datetime, timedelta
import yaml
from io import BytesIO, StringIO
import logging
import tempfile
import chardet
import traceback
from typing import Optional
# Add src to path to import our modules
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'src'))
from fraud_detection_engine.ingestion.data_loader import DataLoader
from fraud_detection_engine.ingestion.column_mapper import ColumnMapper
from fraud_detection_engine.features.statistical_features import StatisticalFeatures
from fraud_detection_engine.features.graph_features import GraphFeatures
from fraud_detection_engine.features.nlp_features import NLPFeatures
from fraud_detection_engine.features.timeseries_features import TimeSeriesFeatures
from fraud_detection_engine.models.unsupervised import UnsupervisedModels
from fraud_detection_engine.models.supervised import SupervisedModels
from fraud_detection_engine.models.rule_based import RuleEngine
from fraud_detection_engine.analysis.risk_scorer import RiskScorer
from fraud_detection_engine.analysis.explainability import Explainability
from fraud_detection_engine.reporting.pdf_generator import PDFGenerator
from fraud_detection_engine.utils.api_utils import is_api_available
# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)
# Set page configuration
st.set_page_config(
    page_title="Financial Fraud Detection System",
    page_icon="üõ°Ô∏è",
    layout="wide",
    initial_sidebar_state="expanded"
)
# Load configuration
@st.cache_resource
def load_config():
    """Load configuration from YAML files"""
    config = {}
    try:
        # Get the directory where this script is located
        script_dir = os.path.dirname(os.path.abspath(__file__))
        
        # Construct paths to config files relative to script location
        model_params_path = os.path.join(script_dir, '..', 'config', 'model_params.yml')
        rule_engine_path = os.path.join(script_dir, '..', 'config', 'rule_engine_config.yml')
        api_keys_path = os.path.join(script_dir, '..', 'config', 'api_keys.yml')
        
        with open(model_params_path, 'r') as f:
            config['model_params'] = yaml.safe_load(f)
        with open(rule_engine_path, 'r') as f:
            config['rule_engine'] = yaml.safe_load(f)
        with open(api_keys_path, 'r') as f:
            config['api_keys'] = yaml.safe_load(f)
    except Exception as e:
        st.error(f"Error loading configuration: {str(e)}")
        config = {
            'model_params': {},
            'rule_engine': {},
            'api_keys': {}
        }
    return config
config = load_config()
# Initialize session state variables
if 'data' not in st.session_state:
    st.session_state.data = None
if 'processed_data' not in st.session_state:
    st.session_state.processed_data = None
if 'risk_scores' not in st.session_state:
    st.session_state.risk_scores = None
if 'column_mapping' not in st.session_state:
    st.session_state.column_mapping = None
if 'model_results' not in st.session_state:
    st.session_state.model_results = {}
if 'explanations' not in st.session_state:
    st.session_state.explanations = None
if 'processing_complete' not in st.session_state:
    st.session_state.processing_complete = False
if 'original_data' not in st.session_state:
    st.session_state.original_data = None
if 'features_df' not in st.session_state:
    st.session_state.features_df = None
# Sidebar
def render_sidebar():
    """Render the sidebar with navigation and controls"""
    st.sidebar.title("üõ°Ô∏è Fraud Detection System")
    st.sidebar.image("https://www.streamlit.io/images/brand/streamlit-logo-secondary-colormark-darktext.png", width=200)
    
    # Navigation
    st.sidebar.subheader("Navigation")
    page = st.sidebar.radio("Go to", [
        "Data Upload", 
        "Column Mapping", 
        "Analysis Settings", 
        "Run Detection", 
        "Results Dashboard", 
        "Explainability", 
        "Reports"
    ])
    
    # System status
    st.sidebar.subheader("System Status")
    if st.session_state.data is not None:
        st.sidebar.success(f"Data loaded: {len(st.session_state.data)} transactions")
    else:
        st.sidebar.warning("No data loaded")
    
    if st.session_state.processing_complete:
        st.sidebar.success("Analysis complete")
    else:
        st.sidebar.info("Analysis pending")
    
    # API Status
    st.sidebar.subheader("API Status")
    api_status = {
        'Gemini': is_api_available('gemini'),
        'OpenAI': is_api_available('openai'),
        'News API': is_api_available('news_api'),
        'Geolocation': is_api_available('geolocation'),
        'Sanctions': is_api_available('sanctions'),
        'Tax Compliance': is_api_available('tax_compliance'),
        'Bank Verification': is_api_available('bank_verification'),
        'Identity Verification': is_api_available('identity_verification')
    }
    
    for api, status in api_status.items():
        if status:
            st.sidebar.success(f"{api}: ‚úÖ Available")
        else:
            st.sidebar.warning(f"{api}: ‚ùå Not Available")
    
    # Configuration
    st.sidebar.subheader("Configuration")
    if st.sidebar.button("Reload Config"):
        config = load_config()
        st.sidebar.success("Configuration reloaded")
    
    # About section
    st.sidebar.subheader("About")
    st.sidebar.info("""
    This is a comprehensive financial fraud detection system that uses advanced machine learning, statistical analysis, and AI techniques to identify potentially fraudulent transactions.
    """)
    
    return page
# Helper function to detect file encoding
def detect_file_encoding(file_content):
    """Detect the encoding of file content"""
    try:
        result = chardet.detect(file_content)
        return result['encoding']
    except Exception as e:
        logger.error(f"Error detecting file encoding: {str(e)}")
        return None
# Helper function to read CSV with multiple encoding attempts
def read_csv_with_encodings(file_content, filename):
    """Read CSV file trying multiple encodings"""
    encodings = ['utf-8', 'latin-1', 'cp1252', 'iso-8859-1', 'utf-16']
    
    # First try with detected encoding
    detected_encoding = detect_file_encoding(file_content)
    if detected_encoding:
        try:
            st.write(f"Trying detected encoding: {detected_encoding}")
            return pd.read_csv(StringIO(file_content.decode(detected_encoding)))
        except Exception as e:
            st.warning(f"Failed with detected encoding {detected_encoding}: {str(e)}")
    
    # Try common encodings
    for encoding in encodings:
        try:
            st.write(f"Trying encoding: {encoding}")
            return pd.read_csv(StringIO(file_content.decode(encoding)))
        except UnicodeDecodeError:
            st.warning(f"Failed with {encoding} encoding: Unicode decode error")
        except Exception as e:
            st.warning(f"Failed with {encoding} encoding: {str(e)}")
    
    return None
# Helper function to read Excel with multiple engines
def read_excel_with_engines(file_content, filename):
    """Read Excel file trying multiple engines"""
    # Try default engine first
    try:
        st.write("Trying default Excel engine")
        return pd.read_excel(BytesIO(file_content))
    except Exception as e:
        st.warning(f"Failed with default engine: {str(e)}")
    
    # Try specific engines based on file extension
    if filename.endswith('.xlsx'):
        try:
            st.write("Trying openpyxl engine")
            return pd.read_excel(BytesIO(file_content), engine='openpyxl')
        except Exception as e:
            st.warning(f"Failed with openpyxl: {str(e)}")
    elif filename.endswith('.xls'):
        try:
            st.write("Trying xlrd engine")
            return pd.read_excel(BytesIO(file_content), engine='xlrd')
        except Exception as e:
            st.warning(f"Failed with xlrd: {str(e)}")
    
    return None
# Data Upload Page
def render_data_upload():
    """Render the data upload page"""
    st.title("üìÅ Data Upload")
    st.markdown("""
    Upload your transaction data for analysis. The system supports CSV and Excel files.
    """)
    
    # Debug information
    st.write("### Debug Information")
    st.write(f"Current working directory: {os.getcwd()}")
    st.write(f"Script directory: {os.path.dirname(os.path.abspath(__file__))}")
    
    # File upload with enhanced error handling
    uploaded_file = st.file_uploader(
        "Choose a CSV or Excel file",
        type=['csv', 'xlsx', 'xls'],
        help="Upload a file containing transaction data",
        key="file_uploader"
    )
    
    # Debug: Show if file was uploaded
    if uploaded_file is not None:
        st.write("### File Upload Details")
        st.write(f"File name: {uploaded_file.name}")
        st.write(f"File type: {uploaded_file.type}")
        st.write(f"File size: {uploaded_file.size} bytes")
    
    if uploaded_file is not None:
        try:
            # Read file content directly
            file_content = uploaded_file.read()
            
            # Read the file based on its type
            df = None
            
            if uploaded_file.name.endswith('.csv'):
                st.write("### Processing CSV file")
                df = read_csv_with_encodings(file_content, uploaded_file.name)
                
                if df is None:
                    st.error("Could not read CSV file with any encoding")
                    
                    # Show file content for debugging
                    st.write("### File Content (first 1000 bytes)")
                    try:
                        st.write(file_content[:1000])
                    except Exception as e:
                        st.error(f"Could not display file content: {str(e)}")
                    
                    # Try to create a simple CSV from the content
                    st.write("### Attempting to create CSV from content")
                    try:
                        # Try different encodings directly
                        for encoding in ['utf-8', 'latin-1', 'cp1252']:
                            try:
                                content_str = file_content.decode(encoding, errors='ignore')
                                df = pd.read_csv(StringIO(content_str))
                                st.success(f"Successfully read CSV with {encoding} encoding")
                                break
                            except:
                                continue
                    except Exception as e:
                        st.error(f"Failed to create CSV from content: {str(e)}")
                
            elif uploaded_file.name.endswith(('.xlsx', '.xls')):
                st.write("### Processing Excel file")
                df = read_excel_with_engines(file_content, uploaded_file.name)
                
                if df is None:
                    st.error("Could not read Excel file with any engine")
            else:
                st.error(f"Unsupported file type: {uploaded_file.name}")
            
            # Validate the dataframe
            if df is None:
                st.error("Failed to read the uploaded file")
                return
            
            if df.empty:
                st.error("The uploaded file appears to be empty")
                return
            
            # Display data info
            st.success(f"File uploaded successfully! Shape: {df.shape}")
            st.write("### Data Preview")
            st.dataframe(df.head(10))
            
            # Store in session state
            st.session_state.data = df
            st.session_state.original_data = df.copy()
            st.write("Data stored in session state")
            
            # Show column information
            st.write("### Column Information")
            col_info = pd.DataFrame({
                'Column': df.columns,
                'Data Type': df.dtypes,
                'Non-Null Count': df.count(),
                'Null Count': df.isnull().sum(),
                'Unique Values': df.nunique()
            })
            st.dataframe(col_info)
            
            # Basic statistics
            st.write("### Basic Statistics")
            numeric_cols = df.select_dtypes(include=[np.number]).columns
            if len(numeric_cols) > 0:
                st.dataframe(df[numeric_cols].describe())
            else:
                st.info("No numeric columns found for statistics")
                
            # Check file structure
            st.write("### File Structure Check")
            if len(df.columns) > 0:
                st.write(f"Found {len(df.columns)} columns")
                st.write("Columns:", list(df.columns))
            else:
                st.error("No columns found in the file")
                
        except Exception as e:
            st.error(f"Error processing file: {str(e)}")
            st.write("### Error Details")
            st.text(traceback.format_exc())
    
    # Sample data option
    st.markdown("---")
    st.subheader("Or Use Sample Data")
    
    if st.button("Load Sample Data"):
        try:
            # Look for sample data in multiple locations
            possible_paths = [
                os.path.join('..', 'data', 'sample_transactions.csv'),
                os.path.join('data', 'sample_transactions.csv'),
                'sample_transactions.csv'
            ]
            
            sample_path = None
            for path in possible_paths:
                st.write(f"Checking path: {path}")
                if os.path.exists(path):
                    sample_path = path
                    st.write(f"Found sample data at: {path}")
                    break
            
            if sample_path:
                df = pd.read_csv(sample_path)
                st.session_state.data = df
                st.session_state.original_data = df.copy()
                st.success(f"Sample data loaded! Shape: {df.shape}")
                st.dataframe(df.head(10))
            else:
                st.warning("Sample data file not found, creating new sample...")
                
                # Create sample data if it doesn't exist
                np.random.seed(42)
                
                # Create more realistic sample data
                n_samples = 1000
                start_date = datetime(2023, 1, 1)
                
                sample_data = pd.DataFrame({
                    'transaction_id': [f'TXN{i:06d}' for i in range(1, n_samples + 1)],
                    'timestamp': [start_date + timedelta(days=np.random.randint(0, 365)) for _ in range(n_samples)],
                    'amount': np.random.lognormal(8, 1.5, n_samples),  # More realistic amounts
                    'sender_id': [f'CUST{np.random.randint(1, 201):03d}' for _ in range(n_samples)],
                    'receiver_id': [f'CUST{np.random.randint(1, 201):03d}' for _ in range(n_samples)],
                    'currency': ['USD'] * n_samples,
                    'description': [f'Transaction type {np.random.choice(["PAYMENT", "TRANSFER", "PURCHASE", "WITHDRAWAL"])}' for _ in range(n_samples)],
                    'transaction_type': np.random.choice(['debit', 'credit'], n_samples),
                    'merchant_category': np.random.choice(['retail', 'services', 'utilities', 'finance'], n_samples)
                })
                
                # Ensure data directory exists
                data_dir = os.path.join('..', 'data')
                if not os.path.exists(data_dir):
                    os.makedirs(data_dir)
                
                sample_path = os.path.join(data_dir, 'sample_transactions.csv')
                sample_data.to_csv(sample_path, index=False)
                st.write(f"Created sample data at: {sample_path}")
                
                # Load the newly created sample data
                st.session_state.data = sample_data
                st.session_state.original_data = sample_data.copy()
                st.success(f"Sample data loaded! Shape: {sample_data.shape}")
                st.dataframe(sample_data.head(10))
                
        except Exception as e:
            st.error(f"Error loading sample data: {str(e)}")
            st.write("### Error Details")
            st.text(traceback.format_exc())
    
    # Additional troubleshooting section
    st.markdown("---")
    st.subheader("Troubleshooting File Upload Issues")
    
    with st.expander("Click here if you're having trouble uploading files"):
        st.write("""
        ### Common Issues and Solutions:
        
        1. **File not uploading at all**:
           - Try using a different browser (Chrome, Firefox, or Edge work best)
           - Clear your browser cache and cookies
           - Check if your file size is too large (try with a smaller file first)
           - Make sure the file is not open in another program
           - Try refreshing the page and uploading again
        
        2. **CSV file encoding issues**:
           - Try saving your CSV file with UTF-8 encoding
           - Use Excel to "Save As" and select "CSV UTF-8" format
           - Try converting your file to Excel format (.xlsx)
           - Check if your CSV has special characters that might cause encoding issues
        
        3. **Permission issues**:
           - Make sure the file is not read-only
           - Try moving the file to your desktop first
           - Check if your antivirus software is blocking the upload
           - Try running the application with administrator privileges
        
        4. **Browser compatibility**:
           - Update your browser to the latest version
           - Try incognito/private browsing mode
           - Disable browser extensions temporarily
           - Try a different browser
        
        5. **File format issues**:
           - Make sure your CSV file has proper headers
           - Check if your file has consistent formatting
           - Try opening the file in Excel and re-saving it
           - Remove any special characters from column names
        
        ### Alternative Methods:
        
        If the file uploader still doesn't work, you can:
        1. Use the "Load Sample Data" button above
        2. Place your CSV file in the `data` folder with the name `sample_transactions.csv`
        3. Try converting your file to Excel format (.xlsx)
        4. Use the text area below to paste your CSV data directly:
        """)
        
        # Add text area for direct CSV input
        st.write("### Direct CSV Input")
        csv_text = st.text_area("Paste your CSV data here:", height=200)
        
        if st.button("Load from Text"):
            try:
                if csv_text.strip():
                    # Use StringIO to read the CSV data
                    df = pd.read_csv(StringIO(csv_text))
                    st.session_state.data = df
                    st.session_state.original_data = df.copy()
                    st.success(f"Data loaded from text! Shape: {df.shape}")
                    st.dataframe(df.head(10))
                else:
                    st.warning("Please paste some CSV data")
            except Exception as e:
                st.error(f"Error loading data from text: {str(e)}")
                st.write("### Error Details")
                st.text(traceback.format_exc())
# Column Mapping Page
def render_column_mapping():
    """Render the column mapping page"""
    st.title("üîÑ Column Mapping")
    
    if st.session_state.data is None:
        st.warning("Please upload data first")
        return
    
    st.markdown("""
    Map your column headers to the expected format. The system will attempt to auto-map columns,
    but you can review and adjust as needed.
    """)
    
    # Initialize column mapper
    mapper = ColumnMapper()
    
    # Get expected columns
    expected_columns = mapper.get_expected_columns()
    
    # Get actual columns
    actual_columns = list(st.session_state.data.columns)
    
    # Auto-map columns
    if st.session_state.column_mapping is None:
        with st.spinner("Auto-mapping columns..."):
            st.session_state.column_mapping = mapper.auto_map_columns(actual_columns, expected_columns)
    
    # Display mapping
    st.write("### Column Mapping")
    
    # Create a DataFrame for the mapping
    mapping_data = []
    for actual_col in actual_columns:
        expected_col = st.session_state.column_mapping.get(actual_col, "Not mapped")
        mapping_data.append({
            "Your Column": actual_col,
            "Maps To": expected_col
        })
    
    mapping_df = pd.DataFrame(mapping_data)
    st.dataframe(mapping_df)
    
    # Allow manual adjustment
    st.write("### Adjust Mapping")
    adjusted_mapping = {}
    
    for actual_col in actual_columns:
        current_mapping = st.session_state.column_mapping.get(actual_col, "Not mapped")
        options = ["Not mapped"] + expected_columns
        selected = st.selectbox(
            f"Map '{actual_col}' to:",
            options=options,
            index=options.index(current_mapping) if current_mapping in options else 0,
            key=f"map_{actual_col}"
        )
        
        if selected != "Not mapped":
            adjusted_mapping[actual_col] = selected
    
    # Update button
    if st.button("Update Mapping"):
        st.session_state.column_mapping = adjusted_mapping
        st.success("Column mapping updated!")
    
    # Show mapped columns preview
    if st.button("Preview Mapped Data"):
        try:
            mapped_data = mapper.apply_mapping(st.session_state.data, st.session_state.column_mapping)
            st.write("### Mapped Data Preview")
            st.dataframe(mapped_data.head(10))
        except Exception as e:
            st.error(f"Error applying mapping: {str(e)}")
    
    # Continue to analysis
    if st.button("Continue to Analysis"):
        try:
            mapped_data = mapper.apply_mapping(st.session_state.data, st.session_state.column_mapping)
            st.session_state.processed_data = mapped_data
            st.success("Data processed successfully! You can now configure analysis settings.")
        except Exception as e:
            st.error(f"Error processing data: {str(e)}")
# Analysis Settings Page
def render_analysis_settings():
    """Render the analysis settings page"""
    st.title("‚öôÔ∏è Analysis Settings")
    
    if st.session_state.processed_data is None:
        st.warning("Please process data first")
        return
    
    st.markdown("""
    Configure the fraud detection analysis settings. You can select which algorithms to run,
    adjust parameters, and set thresholds.
    """)
    
    # Feature engineering settings
    st.subheader("Feature Engineering")
    
    feature_options = {
        "Statistical Features": True,
        "Graph Features": True,
        "NLP Features": True,
        "Time Series Features": True
    }
    
    selected_features = {}
    for feature, default in feature_options.items():
        selected_features[feature] = st.checkbox(feature, value=default)
    
    # Model settings
    st.subheader("Model Selection")
    
    model_options = {
        "Unsupervised Models": True,
        "Supervised Models": True,
        "Rule-Based Models": True
    }
    
    selected_models = {}
    for model, default in model_options.items():
        selected_models[model] = st.checkbox(model, value=default)
    
    # Advanced settings
    with st.expander("Advanced Settings"):
        st.write("### Unsupervised Model Parameters")
        
        contamination = st.slider(
            "Contamination (expected fraud rate)",
            min_value=0.001,
            max_value=0.5,
            value=0.01,
            step=0.001,
            help="Expected proportion of outliers in the data"
        )
        
        st.write("### Supervised Model Parameters")
        
        if 'fraud_flag' in st.session_state.processed_data.columns:
            test_size = st.slider(
                "Test Size",
                min_value=0.1,
                max_value=0.5,
                value=0.2,
                step=0.05,
                help="Proportion of data to use for testing"
            )
        else:
            st.info("No fraud_flag column found. Supervised learning will use synthetic labels.")
            test_size = 0.2
        
        st.write("### Rule Engine Parameters")
        
        rule_threshold = st.slider(
            "Rule Threshold",
            min_value=0.0,
            max_value=1.0,
            value=0.7,
            step=0.05,
            help="Threshold for rule-based detection"
        )
    
    # Risk scoring settings
    st.subheader("Risk Scoring")
    
    # Map the internal values to display names
    method_display_names = {
        "weighted_average": "Weighted Average",
        "maximum": "Maximum Score", 
        "custom": "Custom Weights"
    }
    
    scoring_method = st.selectbox(
        "Scoring Method",
        list(method_display_names.keys()),
        format_func=lambda x: method_display_names[x],
        help="Method to combine scores from different models"
    )
    
    if scoring_method == "custom":
        st.write("### Custom Weights")
        unsupervised_weight = st.slider("Unsupervised Weight", 0.0, 1.0, 0.4, 0.05)
        supervised_weight = st.slider("Supervised Weight", 0.0, 1.0, 0.4, 0.05)
        rule_weight = st.slider("Rule Weight", 0.0, 1.0, 0.2, 0.05)
        
        # Normalize weights
        total = unsupervised_weight + supervised_weight + rule_weight
        if total > 0:
            unsupervised_weight /= total
            supervised_weight /= total
            rule_weight /= total
        
        custom_weights = {
            "unsupervised": unsupervised_weight,
            "supervised": supervised_weight,
            "rule": rule_weight
        }
    else:
        custom_weights = None
    
    # Threshold settings
    st.subheader("Detection Thresholds")
    
    threshold_method = st.selectbox(
        "Threshold Method",
        ["Fixed", "Percentile", "Dynamic"],
        help="Method to determine fraud threshold"
    )
    
    if threshold_method == "Fixed":
        threshold = st.slider(
            "Risk Threshold",
            min_value=0.0,
            max_value=1.0,
            value=0.5,
            step=0.05,
            help="Transactions above this threshold will be flagged as potential fraud"
        )
    elif threshold_method == "Percentile":
        percentile = st.slider(
            "Percentile",
            min_value=90,
            max_value=100,
            value=95,
            step=1,
            help="Transactions above this percentile of risk scores will be flagged"
        )
        threshold = None
    else:  # Dynamic
        st.info("Dynamic thresholds will be calculated based on data distribution")
        threshold = None
    
    # Save settings
    if st.button("Save Settings"):
        settings = {
            "features": selected_features,
            "models": selected_models,
            "contamination": contamination,
            "test_size": test_size,
            "rule_threshold": rule_threshold,
            "scoring_method": scoring_method,
            "custom_weights": custom_weights,
            "threshold_method": threshold_method,
            "threshold": threshold,
            "percentile": percentile if threshold_method == "Percentile" else None
        }
        
        st.session_state.settings = settings
        st.success("Settings saved! You can now run the detection analysis.")
# Clean dataframe function
def clean_dataframe(df):
    """
    Clean a DataFrame by handling infinity, NaN, and extreme values
    
    Args:
        df (DataFrame): Input DataFrame
        
    Returns:
        DataFrame: Cleaned DataFrame
    """
    try:
        # Replace infinity with NaN
        df = df.replace([np.inf, -np.inf], np.nan)
        
        # Handle extreme values for numeric columns
        for col in df.select_dtypes(include=[np.number]).columns:
            # Calculate percentiles
            p1 = np.nanpercentile(df[col], 1)
            p99 = np.nanpercentile(df[col], 99)
            
            # Cap extreme values
            if not np.isnan(p1) and not np.isnan(p99):
                # Use a more conservative capping approach
                iqr = p99 - p1
                lower_bound = p1 - 1.5 * iqr
                upper_bound = p99 + 1.5 * iqr
                
                df[col] = np.where(df[col] < lower_bound, lower_bound, df[col])
                df[col] = np.where(df[col] > upper_bound, upper_bound, df[col])
            
            # Replace any remaining NaN with median
            median_val = df[col].median()
            if not np.isnan(median_val):
                df[col] = df[col].fillna(median_val)
        
        # Handle categorical columns
        for col in df.select_dtypes(include=['object', 'category']).columns:
            # Fill NaN with mode or 'Unknown'
            mode_val = df[col].mode()
            if len(mode_val) > 0:
                df[col] = df[col].fillna(mode_val[0])
            else:
                df[col] = df[col].fillna('Unknown')
        
        return df
    except Exception as e:
        logger.error(f"Error cleaning DataFrame: {str(e)}")
        return df
# Run Detection Page
def render_run_detection():
    """Render the run detection page"""
    st.title("üöÄ Run Fraud Detection")
    
    if st.session_state.processed_data is None:
        st.warning("Please process data first")
        return
    
    if 'settings' not in st.session_state:
        st.warning("Please configure analysis settings first")
        return
    
    st.markdown("""
    Run the fraud detection analysis using the configured settings. This may take some time
    depending on the size of your dataset and selected algorithms.
    """)
    
    # Show settings summary
    st.subheader("Analysis Settings Summary")
    
    settings = st.session_state.settings
    
    st.write("#### Feature Engineering")
    for feature, enabled in settings["features"].items():
        status = "‚úÖ Enabled" if enabled else "‚ùå Disabled"
        st.write(f"- {feature}: {status}")
    
    st.write("#### Models")
    for model, enabled in settings["models"].items():
        status = "‚úÖ Enabled" if enabled else "‚ùå Disabled"
        st.write(f"- {model}: {status}")
    
    st.write("#### Risk Scoring")
    method_display_names = {
        "weighted_average": "Weighted Average",
        "maximum": "Maximum Score", 
        "custom": "Custom Weights"
    }
    st.write(f"- Method: {method_display_names.get(settings['scoring_method'], settings['scoring_method'])}")
    if settings['custom_weights']:
        st.write(f"- Unsupervised Weight: {settings['custom_weights']['unsupervised']:.2f}")
        st.write(f"- Supervised Weight: {settings['custom_weights']['supervised']:.2f}")
        st.write(f"- Rule Weight: {settings['custom_weights']['rule']:.2f}")
    
    st.write("#### Threshold")
    st.write(f"- Method: {settings['threshold_method']}")
    if settings['threshold_method'] == 'Fixed':
        st.write(f"- Value: {settings['threshold']}")
    elif settings['threshold_method'] == 'Percentile':
        st.write(f"- Percentile: {settings['percentile']}%")
    
    # Run button
    if st.button("Run Detection Analysis"):
        # Create progress bar
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        try:
            # Step 1: Feature Engineering
            status_text.text("Step 1/5: Feature Engineering...")
            progress_bar.progress(10)
            
            features_df = st.session_state.processed_data.copy()
            
            # Clean data before feature engineering
            features_df = clean_dataframe(features_df)
            
            # Statistical features
            if settings["features"]["Statistical Features"]:
                stat_features = StatisticalFeatures()
                features_df = stat_features.extract_features(features_df)
                # Clean after feature extraction
                features_df = clean_dataframe(features_df)
                progress_bar.progress(20)
            
            # Graph features
            if settings["features"]["Graph Features"]:
                graph_features = GraphFeatures()
                features_df = graph_features.extract_features(features_df)
                # Clean after feature extraction
                features_df = clean_dataframe(features_df)
                progress_bar.progress(30)
            
            # NLP features
            if settings["features"]["NLP Features"]:
                nlp_features = NLPFeatures()
                features_df = nlp_features.extract_features(features_df)
                # Clean after feature extraction
                features_df = clean_dataframe(features_df)
                progress_bar.progress(40)
            
            # Time series features
            if settings["features"]["Time Series Features"]:
                ts_features = TimeSeriesFeatures()
                features_df = ts_features.extract_features(features_df)
                # Clean after feature extraction
                features_df = clean_dataframe(features_df)
                progress_bar.progress(50)
            
            # Step 2: Model Training/Prediction
            status_text.text("Step 2/5: Running Models...")
            model_results = {}
            
            # Unsupervised models
            if settings["models"]["Unsupervised Models"]:
                unsupervised = UnsupervisedModels(contamination=settings["contamination"])
                unsupervised_results = unsupervised.run_models(features_df)
                model_results["unsupervised"] = unsupervised_results
                progress_bar.progress(60)
            
            # Supervised models
            if settings["models"]["Supervised Models"]:
                supervised = SupervisedModels(test_size=settings["test_size"])
                supervised_results = supervised.run_models(features_df)
                model_results["supervised"] = supervised_results
                progress_bar.progress(70)
            
            # Rule-based models
            if settings["models"]["Rule-Based Models"]:
                rule_engine = RuleEngine(threshold=settings["rule_threshold"])
                rule_results = rule_engine.apply_rules(features_df)
                model_results["rule"] = rule_results
                progress_bar.progress(80)
            
            # Step 3: Risk Scoring
            status_text.text("Step 3/5: Calculating Risk Scores...")
            
            # Initialize risk scorer with properly formatted method
            scoring_method = settings["scoring_method"]
            risk_scorer = RiskScorer(
                method=scoring_method,
                custom_weights=settings["custom_weights"]
            )
            
            risk_scores = risk_scorer.calculate_scores(model_results, features_df)
            progress_bar.progress(90)
            
            # Step 4: Apply Thresholds
            status_text.text("Step 4/5: Applying Thresholds...")
            if settings['threshold_method'] == "Fixed":
                threshold = settings['threshold']
                risk_scores["is_fraud"] = risk_scores["risk_score"] > threshold
            elif settings['threshold_method'] == "Percentile":
                percentile = settings['percentile']
                threshold = np.percentile(risk_scores["risk_score"], percentile)
                risk_scores["is_fraud"] = risk_scores["risk_score"] > threshold
            else:  # Dynamic
                # Calculate dynamic threshold based on distribution
                mean_score = risk_scores["risk_score"].mean()
                std_score = risk_scores["risk_score"].std()
                threshold = mean_score + 2 * std_score
                risk_scores["is_fraud"] = risk_scores["risk_score"] > threshold
            
            progress_bar.progress(95)
            
            # Step 5: Generate Explanations
            status_text.text("Step 5/5: Generating Explanations...")
            explainability = Explainability()
            explanations = explainability.generate_explanations(
                features_df, 
                risk_scores, 
                model_results
            )
            
            progress_bar.progress(100)
            
            # Store results in session state
            st.session_state.features_df = features_df
            st.session_state.model_results = model_results
            st.session_state.risk_scores = risk_scores
            st.session_state.explanations = explanations
            st.session_state.threshold = threshold
            st.session_state.processing_complete = True
            
            status_text.text("Analysis complete!")
            st.success(f"Fraud detection analysis complete! Found {risk_scores['is_fraud'].sum()} potentially fraudulent transactions out of {len(risk_scores)} total.")
            
        except Exception as e:
            st.error(f"Error during analysis: {str(e)}")
            st.exception(e)
# Results Dashboard Page
def render_results_dashboard():
    """Render the results dashboard page"""
    st.title("üìä Results Dashboard")
    
    if not st.session_state.processing_complete:
        st.warning("Please run the detection analysis first")
        return
    
    risk_scores = st.session_state.risk_scores
    
    # Summary statistics
    st.subheader("Summary Statistics")
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("Total Transactions", len(risk_scores))
    
    with col2:
        fraud_count = risk_scores['is_fraud'].sum()
        st.metric("Fraudulent Transactions", fraud_count)
    
    with col3:
        fraud_percentage = (fraud_count / len(risk_scores)) * 100
        st.metric("Fraud Percentage", f"{fraud_percentage:.2f}%")
    
    with col4:
        threshold = st.session_state.threshold
        st.metric("Detection Threshold", f"{threshold:.4f}")
    
    # Risk score distribution
    st.subheader("Risk Score Distribution")
    
    fig = px.histogram(
        risk_scores,
        x="risk_score",
        color="is_fraud",
        nbins=50,
        title="Distribution of Risk Scores",
        color_discrete_map={False: "blue", True: "red"},
        opacity=0.7
    )
    
    fig.add_vline(
        x=threshold,
        line_dash="dash",
        line_color="green",
        annotation_text=f"Threshold: {threshold:.4f}"
    )
    
    st.plotly_chart(fig, use_container_width=True)
    
    # Top risky transactions
    st.subheader("Top Risky Transactions")
    
    top_risky = risk_scores[risk_scores['is_fraud']].nlargest(10, 'risk_score')
    
    if len(top_risky) > 0:
        # Create a display dataframe with risk scores and available details
        display_data = top_risky.copy()
        
        # Try to add transaction details from original data if available
        if hasattr(st.session_state, 'original_data') and st.session_state.original_data is not None:
            original_data = st.session_state.original_data
            
            # Get columns that exist in both dataframes
            common_cols = set(original_data.columns) & set(display_data.columns)
            
            # If there are common columns, merge them
            if common_cols:
                display_data = display_data.drop(columns=list(common_cols), errors='ignore')
                display_data = display_data.merge(
                    original_data[list(common_cols)],
                    left_index=True,
                    right_index=True,
                    how='left'
                )
        
        # If we still don't have transaction_id, try to get it from features_df
        if 'transaction_id' not in display_data.columns and hasattr(st.session_state, 'features_df'):
            features_df = st.session_state.features_df
            if 'transaction_id' in features_df.columns:
                display_data = display_data.merge(
                    features_df[['transaction_id']],
                    left_index=True,
                    right_index=True,
                    how='left'
                )
        
        # If we still don't have transaction_id, use the index as a fallback
        if 'transaction_id' not in display_data.columns:
            display_data['transaction_id'] = display_data.index
        
        # Ensure we have the required columns for display
        display_cols = []
        
        # Always include transaction_id and risk_score
        if 'transaction_id' in display_data.columns:
            display_cols.append('transaction_id')
        display_cols.append('risk_score')
        
        # Add other available columns if they exist
        for col in ['amount', 'timestamp', 'sender_id', 'receiver_id']:
            if col in display_data.columns:
                display_cols.append(col)
        
        # Display the dataframe
        st.dataframe(
            display_data[display_cols].sort_values('risk_score', ascending=False)
        )
    else:
        st.info("No fraudulent transactions detected")
    
    # Feature importance
    st.subheader("Feature Importance")
    
    if 'supervised' in st.session_state.model_results and 'feature_importance' in st.session_state.model_results['supervised']:
        feature_importance = st.session_state.model_results['supervised']['feature_importance']
        
        # Get top 20 features
        top_features = feature_importance.head(20)
        
        fig = px.bar(
            top_features,
            x='importance',
            y='feature',
            orientation='h',
            title="Top 20 Feature Importance",
            color='importance',
            color_continuous_scale='Viridis'
        )
        
        fig.update_layout(yaxis={'categoryorder': 'total ascending'})
        st.plotly_chart(fig, use_container_width=True)
    else:
        st.info("Feature importance not available. Run supervised models to see feature importance.")
    
    # Model performance
    st.subheader("Model Performance")
    
    if 'supervised' in st.session_state.model_results and 'performance' in st.session_state.model_results['supervised']:
        performance = st.session_state.model_results['supervised']['performance']
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.write("##### Classification Report")
            st.dataframe(performance['classification_report'])
        
        with col2:
            st.write("##### Confusion Matrix")
            cm = performance['confusion_matrix']
            
            fig = px.imshow(
                cm,
                text_auto=True,
                aspect="auto",
                labels=dict(x="Predicted", y="Actual", color="Count"),
                x=["Not Fraud", "Fraud"],
                y=["Not Fraud", "Fraud"],
                color_continuous_scale='Blues'
            )
            
            st.plotly_chart(fig, use_container_width=True)
    else:
        st.info("Model performance metrics not available. Run supervised models with labeled data to see performance metrics.")
    
    # Time series analysis
    if hasattr(st.session_state, 'original_data') and st.session_state.original_data is not None and 'timestamp' in st.session_state.original_data.columns:
        st.subheader("Fraud Over Time")
        
        # Create a copy for time series analysis
        ts_data = st.session_state.original_data.copy()
        ts_data['risk_score'] = risk_scores['risk_score']
        ts_data['is_fraud'] = risk_scores['is_fraud']
        
        # Convert timestamp to datetime if needed
        if not pd.api.types.is_datetime64_any_dtype(ts_data['timestamp']):
            ts_data['timestamp'] = pd.to_datetime(ts_data['timestamp'])
        
        # Extract date components
        ts_data['date'] = ts_data['timestamp'].dt.date
        ts_data['hour'] = ts_data['timestamp'].dt.hour
        
        # Fraud by date
        fraud_by_date = ts_data.groupby('date').agg({
            'is_fraud': ['sum', 'count'],
            'risk_score': 'mean'
        }).reset_index()
        
        fraud_by_date.columns = ['date', 'fraud_count', 'total_count', 'avg_risk_score']
        fraud_by_date['fraud_rate'] = fraud_by_date['fraud_count'] / fraud_by_date['total_count']
        
        fig = px.line(
            fraud_by_date,
            x='date',
            y=['fraud_count', 'fraud_rate'],
            title="Fraud Count and Rate Over Time",
            labels={'value': 'Count / Rate', 'date': 'Date'}
        )
        
        st.plotly_chart(fig, use_container_width=True)
        
        # Fraud by hour of day
        fraud_by_hour = ts_data.groupby('hour').agg({
            'is_fraud': ['sum', 'count'],
            'risk_score': 'mean'
        }).reset_index()
        
        fraud_by_hour.columns = ['hour', 'fraud_count', 'total_count', 'avg_risk_score']
        fraud_by_hour['fraud_rate'] = fraud_by_hour['fraud_count'] / fraud_by_hour['total_count']
        
        fig = px.bar(
            fraud_by_hour,
            x='hour',
            y='fraud_count',
            title="Fraud Count by Hour of Day",
            color='avg_risk_score',
            color_continuous_scale='Viridis'
        )
        
        st.plotly_chart(fig, use_container_width=True)
    elif hasattr(st.session_state, 'features_df') and st.session_state.features_df is not None and 'timestamp' in st.session_state.features_df.columns:
        st.subheader("Fraud Over Time")
        
        # Create a copy for time series analysis
        ts_data = st.session_state.features_df.copy()
        ts_data['risk_score'] = risk_scores['risk_score']
        ts_data['is_fraud'] = risk_scores['is_fraud']
        
        # Convert timestamp to datetime if needed
        if not pd.api.types.is_datetime64_any_dtype(ts_data['timestamp']):
            ts_data['timestamp'] = pd.to_datetime(ts_data['timestamp'])
        
        # Extract date components
        ts_data['date'] = ts_data['timestamp'].dt.date
        ts_data['hour'] = ts_data['timestamp'].dt.hour
        
        # Fraud by date
        fraud_by_date = ts_data.groupby('date').agg({
            'is_fraud': ['sum', 'count'],
            'risk_score': 'mean'
        }).reset_index()
        
        fraud_by_date.columns = ['date', 'fraud_count', 'total_count', 'avg_risk_score']
        fraud_by_date['fraud_rate'] = fraud_by_date['fraud_count'] / fraud_by_date['total_count']
        
        fig = px.line(
            fraud_by_date,
            x='date',
            y=['fraud_count', 'fraud_rate'],
            title="Fraud Count and Rate Over Time",
            labels={'value': 'Count / Rate', 'date': 'Date'}
        )
        
        st.plotly_chart(fig, use_container_width=True)
        
        # Fraud by hour of day
        fraud_by_hour = ts_data.groupby('hour').agg({
            'is_fraud': ['sum', 'count'],
            'risk_score': 'mean'
        }).reset_index()
        
        fraud_by_hour.columns = ['hour', 'fraud_count', 'total_count', 'avg_risk_score']
        fraud_by_hour['fraud_rate'] = fraud_by_hour['fraud_count'] / fraud_by_hour['total_count']
        
        fig = px.bar(
            fraud_by_hour,
            x='hour',
            y='fraud_count',
            title="Fraud Count by Hour of Day",
            color='avg_risk_score',
            color_continuous_scale='Viridis'
        )
        
        st.plotly_chart(fig, use_container_width=True)
    else:
        st.info("Timestamp data not available for time series analysis")
# Explainability Page
def render_explainability():
    """Render the explainability page"""
    st.title("üîç Explainability")
    
    if not st.session_state.processing_complete:
        st.warning("Please run the detection analysis first")
        return
    
    risk_scores = st.session_state.risk_scores
    explanations = st.session_state.explanations
    
    # Transaction selector
    st.subheader("Transaction Analysis")
    
    # Get fraudulent transactions
    fraud_transactions = risk_scores[risk_scores['is_fraud']]
    
    if len(fraud_transactions) == 0:
        st.info("No fraudulent transactions detected")
        return
    
    # Create a selector for transaction
    # Check if we have transaction_id in features_df
    if hasattr(st.session_state, 'features_df') and st.session_state.features_df is not None and 'transaction_id' in st.session_state.features_df.columns:
        # Merge with features to get transaction IDs
        fraud_with_ids = fraud_transactions.copy()
        fraud_with_ids = fraud_with_ids.merge(
            st.session_state.features_df[['transaction_id']],
            left_index=True,
            right_index=True,
            how='left'
        )
        
        # Create transaction options
        transaction_options = []
        for idx in fraud_with_ids.index:
            # Check if transaction_id exists and is not NaN
            if 'transaction_id' in fraud_with_ids.columns and pd.notna(fraud_with_ids.loc[idx, 'transaction_id']):
                transaction_id = fraud_with_ids.loc[idx, 'transaction_id']
                transaction_options.append(f"{transaction_id} (Index: {idx})")
            else:
                transaction_options.append(f"Index: {idx}")
        
        selected_transaction = st.selectbox("Select a transaction to analyze", transaction_options)
        
        # Parse the selected option to get the index
        if "Index:" in selected_transaction:
            selected_index = int(selected_transaction.split("Index: ")[1])
        else:
            # Extract transaction_id from the selected option
            transaction_id = selected_transaction.split(" (Index: ")[0]
            # Find the index for this transaction_id
            selected_index = fraud_with_ids[fraud_with_ids['transaction_id'] == transaction_id].index[0]
    else:
        # Use index as identifier
        transaction_options = [f"Index: {idx}" for idx in fraud_transactions.index]
        selected_transaction = st.selectbox("Select a transaction to analyze", transaction_options)
        selected_index = int(selected_transaction.split("Index: ")[1])
    
    # Display transaction details
    st.write("### Transaction Details")
    
    # Try to get transaction details from features_df
    if hasattr(st.session_state, 'features_df') and st.session_state.features_df is not None:
        try:
            transaction_data = st.session_state.features_df.loc[selected_index]
            
            # Display key fields
            key_fields = ['transaction_id', 'amount', 'timestamp', 'sender_id', 'receiver_id', 
                         'transaction_type', 'transaction_category', 'description']
            
            for field in key_fields:
                if field in transaction_data:
                    st.write(f"**{field.replace('_', ' ').title()}:** {transaction_data[field]}")
        except Exception as e:
            st.warning(f"Could not retrieve transaction details: {str(e)}")
    
    # Risk score
    risk_score = risk_scores.loc[selected_index, 'risk_score']
    st.metric("Risk Score", f"{risk_score:.4f}")
    
    # Explanation
    st.write("### Fraud Explanation")
    
    if selected_index in explanations:
        explanation = explanations[selected_index]
        
        # Show top contributing factors
        if 'top_factors' in explanation:
            st.write("#### Top Contributing Factors")
            
            factors_df = pd.DataFrame(explanation['top_factors'])
            factors_df.columns = ['Feature', 'Contribution']
            
            # Create a bar chart
            fig = px.bar(
                factors_df,
                x='Contribution',
                y='Feature',
                orientation='h',
                title="Feature Contributions to Risk Score",
                color='Contribution',
                color_continuous_scale='RdBu'
            )
            
            fig.update_layout(yaxis={'categoryorder': 'total ascending'})
            st.plotly_chart(fig, use_container_width=True)
        
        # Show rule violations
        if 'rule_violations' in explanation and len(explanation['rule_violations']) > 0:
            st.write("#### Rule Violations")
            
            for rule in explanation['rule_violations']:
                st.write(f"- {rule}")
        
        # Show model predictions
        if 'model_predictions' in explanation:
            st.write("#### Model Predictions")
            
            model_df = pd.DataFrame([
                {'Model': model, 'Score': score} 
                for model, score in explanation['model_predictions'].items()
            ])
            
            st.dataframe(model_df)
        
        # Show natural language explanation
        if 'text_explanation' in explanation:
            st.write("#### Explanation Summary")
            st.write(explanation['text_explanation'])
    else:
        st.info("No explanation available for this transaction")
    
    # What-if analysis
    st.write("### What-If Analysis")
    
    st.write("Adjust feature values to see how they affect the risk score:")
    
    # Get top features for this transaction
    if selected_index in explanations and 'top_factors' in explanations[selected_index]:
        top_features = [factor[0] for factor in explanations[selected_index]['top_factors'][:5]]
        
        # Create sliders for top features
        adjustments = {}
        for feature in top_features:
            if feature in st.session_state.features_df.columns:
                current_value = st.session_state.features_df.loc[selected_index, feature]
                
                # Determine appropriate range
                min_val = st.session_state.features_df[feature].min()
                max_val = st.session_state.features_df[feature].max()
                
                # Handle categorical features
                if st.session_state.features_df[feature].dtype == 'object':
                    options = st.session_state.features_df[feature].unique().tolist()
                    adjusted_value = st.selectbox(
                        f"Adjust {feature}",
                        options=options,
                        index=options.index(current_value) if current_value in options else 0
                    )
                else:
                    adjusted_value = st.slider(
                        f"Adjust {feature}",
                        min_value=float(min_val),
                        max_value=float(max_val),
                        value=float(current_value)
                    )
                
                adjustments[feature] = adjusted_value
        
        # Recalculate risk score button
        if st.button("Recalculate Risk Score"):
            # Create a copy of the original data
            modified_data = st.session_state.features_df.copy().loc[[selected_index]]
            
            # Apply adjustments
            for feature, value in adjustments.items():
                modified_data.loc[selected_index, feature] = value
            
            # Recalculate features
            modified_features = modified_data.copy()
            
            # Statistical features
            if st.session_state.settings["features"]["Statistical Features"]:
                stat_features = StatisticalFeatures()
                modified_features = stat_features.extract_features(modified_features)
            
            # Graph features
            if st.session_state.settings["features"]["Graph Features"]:
                graph_features = GraphFeatures()
                modified_features = graph_features.extract_features(modified_features)
            
            # NLP features
            if st.session_state.settings["features"]["NLP Features"]:
                nlp_features = NLPFeatures()
                modified_features = nlp_features.extract_features(modified_features)
            
            # Time series features
            if st.session_state.settings["features"]["Time Series Features"]:
                ts_features = TimeSeriesFeatures()
                modified_features = ts_features.extract_features(modified_features)
            
            # Get model predictions
            model_predictions = {}
            
            # Unsupervised models
            if 'unsupervised' in st.session_state.model_results:
                for model_name, model in st.session_state.model_results['unsupervised']['models'].items():
                    try:
                        # Get features used by this model
                        if 'feature_names' in st.session_state.model_results['unsupervised']:
                            feature_names = st.session_state.model_results['unsupervised']['feature_names']
                            model_features = modified_features[feature_names[model_name]]
                            prediction = model.decision_function(model_features)[0]
                            model_predictions[f"unsupervised_{model_name}"] = prediction
                    except Exception as e:
                        st.warning(f"Error predicting with {model_name}: {str(e)}")
            
            # Supervised models
            if 'supervised' in st.session_state.model_results:
                for model_name, model in st.session_state.model_results['supervised']['models'].items():
                    try:
                        # Get features used by this model
                        if 'feature_names' in st.session_state.model_results['supervised']:
                            feature_names = st.session_state.model_results['supervised']['feature_names']
                            model_features = modified_features[feature_names[model_name]]
                            prediction = model.predict_proba(model_features)[0, 1]
                            model_predictions[f"supervised_{model_name}"] = prediction
                    except Exception as e:
                        st.warning(f"Error predicting with {model_name}: {str(e)}")
            
            # Rule-based models
            if 'rule' in st.session_state.model_results:
                rule_score = 0.0
                violated_rules = []
                
                for rule_name, rule_func in st.session_state.model_results['rule']['rules'].items():
                    try:
                        if rule_func(modified_data.iloc[0]):
                            rule_score += 1.0 / len(st.session_state.model_results['rule']['rules'])
                            violated_rules.append(rule_name)
                    except Exception as e:
                        st.warning(f"Error applying rule {rule_name}: {str(e)}")
                
                model_predictions["rule"] = rule_score
            
            # Calculate new risk score
            risk_scorer = RiskScorer(
                method=st.session_state.settings["scoring_method"],
                custom_weights=st.session_state.settings["custom_weights"]
            )
            
            new_risk_score = risk_scorer.calculate_scores(
                {"unsupervised": model_predictions, "supervised": model_predictions, "rule": model_predictions},
                modified_features
            )
            
            new_risk_score = new_risk_score.loc[selected_index, 'risk_score']
            
            # Display comparison
            st.write("#### Risk Score Comparison")
            
            col1, col2 = st.columns(2)
            
            with col1:
                st.metric("Original Risk Score", f"{risk_score:.4f}")
            
            with col2:
                st.metric("New Risk Score", f"{new_risk_score:.4f}", 
                         delta=f"{new_risk_score - risk_score:.4f}")
            
            # Show if it would still be flagged as fraud
            threshold = st.session_state.threshold
            is_fraud_original = risk_score > threshold
            is_fraud_new = new_risk_score > threshold
            
            st.write(f"Original classification: {'Fraud' if is_fraud_original else 'Not Fraud'}")
            st.write(f"New classification: {'Fraud' if is_fraud_new else 'Not Fraud'}")
            
            if is_fraud_original != is_fraud_new:
                st.warning("Classification changed with the adjustments!")
            else:
                st.info("Classification remains the same")
# Reports Page
def render_reports():
    """Render the reports page"""
    st.title("üìÑ Reports")
    
    if not st.session_state.processing_complete:
        st.warning("Please run the detection analysis first")
        return
    
    risk_scores = st.session_state.risk_scores
    features_df = st.session_state.features_df
    
    st.markdown("""
    Generate comprehensive reports for auditors and stakeholders. Reports can be downloaded
    in PDF format and include detailed analysis of fraudulent transactions.
    """)
    
    # Report options
    st.subheader("Report Options")
    
    report_type = st.selectbox(
        "Select Report Type",
        ["Executive Summary", "Detailed Fraud Analysis", "Technical Report", "Custom Report"]
    )
    
    include_charts = st.checkbox("Include Charts and Visualizations", value=True)
    include_explanations = st.checkbox("Include Detailed Explanations", value=True)
    include_recommendations = st.checkbox("Include Recommendations", value=True)
    
    # Transaction selection
    st.subheader("Transaction Selection")
    
    fraud_transactions = risk_scores[risk_scores['is_fraud']]
    
    if len(fraud_transactions) > 0:
        max_transactions = min(len(fraud_transactions), 100)  # Limit to 100 for performance
        num_transactions = st.slider(
            "Number of Fraudulent Transactions to Include",
            min_value=1,
            max_value=max_transactions,
            value=min(10, max_transactions),
            step=1
        )
        
        # Sort by risk score and select top N
        top_fraud = fraud_transactions.nlargest(num_transactions, 'risk_score')
    else:
        st.info("No fraudulent transactions detected")
        return
    
    # Report generation options
    st.subheader("Report Generation")
    
    if st.button("Generate Report"):
        with st.spinner("Generating report..."):
            try:
                # Initialize PDF generator
                pdf_generator = PDFGenerator()
                
                # Generate report based on type
                if report_type == "Executive Summary":
                    report_path = pdf_generator.generate_executive_summary(
                        features_df,
                        risk_scores,
                        top_fraud,
                        include_charts=include_charts,
                        include_recommendations=include_recommendations
                    )
                elif report_type == "Detailed Fraud Analysis":
                    report_path = pdf_generator.generate_detailed_fraud_analysis(
                        features_df,
                        risk_scores,
                        top_fraud,
                        st.session_state.explanations,
                        include_charts=include_charts,
                        include_explanations=include_explanations,
                        include_recommendations=include_recommendations
                    )
                elif report_type == "Technical Report":
                    report_path = pdf_generator.generate_technical_report(
                        features_df,
                        risk_scores,
                        st.session_state.model_results,
                        include_charts=include_charts
                    )
                else:  # Custom Report
                    report_path = pdf_generator.generate_custom_report(
                        features_df,
                        risk_scores,
                        top_fraud,
                        st.session_state.explanations,
                        st.session_state.model_results,
                        include_charts=include_charts,
                        include_explanations=include_explanations,
                        include_recommendations=include_recommendations
                    )
                
                # Display success message
                st.success(f"Report generated successfully!")
                
                # Provide download button
                with open(report_path, "rb") as file:
                    st.download_button(
                        label="Download Report",
                        data=file,
                        file_name=os.path.basename(report_path),
                        mime="application/pdf"
                    )
                
                # Show report preview
                st.subheader("Report Preview")
                st.info("Report preview is not available in this interface. Please download the PDF to view the full report.")
                
            except Exception as e:
                st.error(f"Error generating report: {str(e)}")
                st.exception(e)
# Main function
def main():
    """Main function to run the Streamlit app"""
    page = render_sidebar()
    
    if page == "Data Upload":
        render_data_upload()
    elif page == "Column Mapping":
        render_column_mapping()
    elif page == "Analysis Settings":
        render_analysis_settings()
    elif page == "Run Detection":
        render_run_detection()
    elif page == "Results Dashboard":
        render_results_dashboard()
    elif page == "Explainability":
        render_explainability()
    elif page == "Reports":
        render_reports()
if __name__ == "__main__":
    main()

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\app\.ipynb_checkpoints\main_dashboard-checkpoint.py ===
"""
Main Dashboard for Financial Fraud Detection System
Streamlit-based user interface for the fraud detection platform
"""
import streamlit as st
import pandas as pd
import numpy as np
import os
import sys
import time
import json
import plotly.express as px
import plotly.graph_objects as go
from datetime import datetime, timedelta
import yaml
from io import BytesIO, StringIO
import logging
import tempfile
import chardet
import traceback
from typing import Optional
# Add src to path to import our modules
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'src'))
from fraud_detection_engine.ingestion.data_loader import DataLoader
from fraud_detection_engine.ingestion.column_mapper import ColumnMapper
from fraud_detection_engine.features.statistical_features import StatisticalFeatures
from fraud_detection_engine.features.graph_features import GraphFeatures
from fraud_detection_engine.features.nlp_features import NLPFeatures
from fraud_detection_engine.features.timeseries_features import TimeSeriesFeatures
from fraud_detection_engine.models.unsupervised import UnsupervisedModels
from fraud_detection_engine.models.supervised import SupervisedModels
from fraud_detection_engine.models.rule_based import RuleEngine
from fraud_detection_engine.analysis.risk_scorer import RiskScorer
from fraud_detection_engine.analysis.explainability import Explainability
from fraud_detection_engine.reporting.pdf_generator import PDFGenerator
from fraud_detection_engine.utils.api_utils import is_api_available
# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)
# Set page configuration
st.set_page_config(
    page_title="Financial Fraud Detection System",
    page_icon="üõ°Ô∏è",
    layout="wide",
    initial_sidebar_state="expanded"
)
# Load configuration
@st.cache_resource
def load_config():
    """Load configuration from YAML files"""
    config = {}
    try:
        # Get the directory where this script is located
        script_dir = os.path.dirname(os.path.abspath(__file__))
        
        # Construct paths to config files relative to script location
        model_params_path = os.path.join(script_dir, '..', 'config', 'model_params.yml')
        rule_engine_path = os.path.join(script_dir, '..', 'config', 'rule_engine_config.yml')
        api_keys_path = os.path.join(script_dir, '..', 'config', 'api_keys.yml')
        
        with open(model_params_path, 'r') as f:
            config['model_params'] = yaml.safe_load(f)
        with open(rule_engine_path, 'r') as f:
            config['rule_engine'] = yaml.safe_load(f)
        with open(api_keys_path, 'r') as f:
            config['api_keys'] = yaml.safe_load(f)
    except Exception as e:
        st.error(f"Error loading configuration: {str(e)}")
        config = {
            'model_params': {},
            'rule_engine': {},
            'api_keys': {}
        }
    return config
config = load_config()
# Initialize session state variables
if 'data' not in st.session_state:
    st.session_state.data = None
if 'processed_data' not in st.session_state:
    st.session_state.processed_data = None
if 'risk_scores' not in st.session_state:
    st.session_state.risk_scores = None
if 'column_mapping' not in st.session_state:
    st.session_state.column_mapping = None
if 'model_results' not in st.session_state:
    st.session_state.model_results = {}
if 'explanations' not in st.session_state:
    st.session_state.explanations = None
if 'processing_complete' not in st.session_state:
    st.session_state.processing_complete = False
if 'original_data' not in st.session_state:
    st.session_state.original_data = None
if 'features_df' not in st.session_state:
    st.session_state.features_df = None
# Sidebar
def render_sidebar():
    """Render the sidebar with navigation and controls"""
    st.sidebar.title("üõ°Ô∏è Fraud Detection System")
    st.sidebar.image("https://www.streamlit.io/images/brand/streamlit-logo-secondary-colormark-darktext.png", width=200)
    
    # Navigation
    st.sidebar.subheader("Navigation")
    page = st.sidebar.radio("Go to", [
        "Data Upload", 
        "Column Mapping", 
        "Analysis Settings", 
        "Run Detection", 
        "Results Dashboard", 
        "Explainability", 
        "Reports"
    ])
    
    # System status
    st.sidebar.subheader("System Status")
    if st.session_state.data is not None:
        st.sidebar.success(f"Data loaded: {len(st.session_state.data)} transactions")
    else:
        st.sidebar.warning("No data loaded")
    
    if st.session_state.processing_complete:
        st.sidebar.success("Analysis complete")
    else:
        st.sidebar.info("Analysis pending")
    
    # API Status
    st.sidebar.subheader("API Status")
    api_status = {
        'Gemini': is_api_available('gemini'),
        'OpenAI': is_api_available('openai'),
        'News API': is_api_available('news_api'),
        'Geolocation': is_api_available('geolocation'),
        'Sanctions': is_api_available('sanctions'),
        'Tax Compliance': is_api_available('tax_compliance'),
        'Bank Verification': is_api_available('bank_verification'),
        'Identity Verification': is_api_available('identity_verification')
    }
    
    for api, status in api_status.items():
        if status:
            st.sidebar.success(f"{api}: ‚úÖ Available")
        else:
            st.sidebar.warning(f"{api}: ‚ùå Not Available")
    
    # Configuration
    st.sidebar.subheader("Configuration")
    if st.sidebar.button("Reload Config"):
        config = load_config()
        st.sidebar.success("Configuration reloaded")
    
    # About section
    st.sidebar.subheader("About")
    st.sidebar.info("""
    This is a comprehensive financial fraud detection system that uses advanced machine learning, statistical analysis, and AI techniques to identify potentially fraudulent transactions.
    """)
    
    return page
# Helper function to detect file encoding
def detect_file_encoding(file_content):
    """Detect the encoding of file content"""
    try:
        result = chardet.detect(file_content)
        return result['encoding']
    except Exception as e:
        logger.error(f"Error detecting file encoding: {str(e)}")
        return None
# Helper function to read CSV with multiple encoding attempts
def read_csv_with_encodings(file_content, filename):
    """Read CSV file trying multiple encodings"""
    encodings = ['utf-8', 'latin-1', 'cp1252', 'iso-8859-1', 'utf-16']
    
    # First try with detected encoding
    detected_encoding = detect_file_encoding(file_content)
    if detected_encoding:
        try:
            st.write(f"Trying detected encoding: {detected_encoding}")
            return pd.read_csv(StringIO(file_content.decode(detected_encoding)))
        except Exception as e:
            st.warning(f"Failed with detected encoding {detected_encoding}: {str(e)}")
    
    # Try common encodings
    for encoding in encodings:
        try:
            st.write(f"Trying encoding: {encoding}")
            return pd.read_csv(StringIO(file_content.decode(encoding)))
        except UnicodeDecodeError:
            st.warning(f"Failed with {encoding} encoding: Unicode decode error")
        except Exception as e:
            st.warning(f"Failed with {encoding} encoding: {str(e)}")
    
    return None
# Helper function to read Excel with multiple engines
def read_excel_with_engines(file_content, filename):
    """Read Excel file trying multiple engines"""
    # Try default engine first
    try:
        st.write("Trying default Excel engine")
        return pd.read_excel(BytesIO(file_content))
    except Exception as e:
        st.warning(f"Failed with default engine: {str(e)}")
    
    # Try specific engines based on file extension
    if filename.endswith('.xlsx'):
        try:
            st.write("Trying openpyxl engine")
            return pd.read_excel(BytesIO(file_content), engine='openpyxl')
        except Exception as e:
            st.warning(f"Failed with openpyxl: {str(e)}")
    elif filename.endswith('.xls'):
        try:
            st.write("Trying xlrd engine")
            return pd.read_excel(BytesIO(file_content), engine='xlrd')
        except Exception as e:
            st.warning(f"Failed with xlrd: {str(e)}")
    
    return None
# Data Upload Page
def render_data_upload():
    """Render the data upload page"""
    st.title("üìÅ Data Upload")
    st.markdown("""
    Upload your transaction data for analysis. The system supports CSV and Excel files.
    """)
    
    # Debug information
    st.write("### Debug Information")
    st.write(f"Current working directory: {os.getcwd()}")
    st.write(f"Script directory: {os.path.dirname(os.path.abspath(__file__))}")
    
    # File upload with enhanced error handling
    uploaded_file = st.file_uploader(
        "Choose a CSV or Excel file",
        type=['csv', 'xlsx', 'xls'],
        help="Upload a file containing transaction data",
        key="file_uploader"
    )
    
    # Debug: Show if file was uploaded
    if uploaded_file is not None:
        st.write("### File Upload Details")
        st.write(f"File name: {uploaded_file.name}")
        st.write(f"File type: {uploaded_file.type}")
        st.write(f"File size: {uploaded_file.size} bytes")
    
    if uploaded_file is not None:
        try:
            # Read file content directly
            file_content = uploaded_file.read()
            
            # Read the file based on its type
            df = None
            
            if uploaded_file.name.endswith('.csv'):
                st.write("### Processing CSV file")
                df = read_csv_with_encodings(file_content, uploaded_file.name)
                
                if df is None:
                    st.error("Could not read CSV file with any encoding")
                    
                    # Show file content for debugging
                    st.write("### File Content (first 1000 bytes)")
                    try:
                        st.write(file_content[:1000])
                    except Exception as e:
                        st.error(f"Could not display file content: {str(e)}")
                    
                    # Try to create a simple CSV from the content
                    st.write("### Attempting to create CSV from content")
                    try:
                        # Try different encodings directly
                        for encoding in ['utf-8', 'latin-1', 'cp1252']:
                            try:
                                content_str = file_content.decode(encoding, errors='ignore')
                                df = pd.read_csv(StringIO(content_str))
                                st.success(f"Successfully read CSV with {encoding} encoding")
                                break
                            except:
                                continue
                    except Exception as e:
                        st.error(f"Failed to create CSV from content: {str(e)}")
                
            elif uploaded_file.name.endswith(('.xlsx', '.xls')):
                st.write("### Processing Excel file")
                df = read_excel_with_engines(file_content, uploaded_file.name)
                
                if df is None:
                    st.error("Could not read Excel file with any engine")
            else:
                st.error(f"Unsupported file type: {uploaded_file.name}")
            
            # Validate the dataframe
            if df is None:
                st.error("Failed to read the uploaded file")
                return
            
            if df.empty:
                st.error("The uploaded file appears to be empty")
                return
            
            # Display data info
            st.success(f"File uploaded successfully! Shape: {df.shape}")
            st.write("### Data Preview")
            st.dataframe(df.head(10))
            
            # Store in session state
            st.session_state.data = df
            st.session_state.original_data = df.copy()
            st.write("Data stored in session state")
            
            # Show column information
            st.write("### Column Information")
            col_info = pd.DataFrame({
                'Column': df.columns,
                'Data Type': df.dtypes,
                'Non-Null Count': df.count(),
                'Null Count': df.isnull().sum(),
                'Unique Values': df.nunique()
            })
            st.dataframe(col_info)
            
            # Basic statistics
            st.write("### Basic Statistics")
            numeric_cols = df.select_dtypes(include=[np.number]).columns
            if len(numeric_cols) > 0:
                st.dataframe(df[numeric_cols].describe())
            else:
                st.info("No numeric columns found for statistics")
                
            # Check file structure
            st.write("### File Structure Check")
            if len(df.columns) > 0:
                st.write(f"Found {len(df.columns)} columns")
                st.write("Columns:", list(df.columns))
            else:
                st.error("No columns found in the file")
                
        except Exception as e:
            st.error(f"Error processing file: {str(e)}")
            st.write("### Error Details")
            st.text(traceback.format_exc())
    
    # Sample data option
    st.markdown("---")
    st.subheader("Or Use Sample Data")
    
    if st.button("Load Sample Data"):
        try:
            # Look for sample data in multiple locations
            possible_paths = [
                os.path.join('..', 'data', 'sample_transactions.csv'),
                os.path.join('data', 'sample_transactions.csv'),
                'sample_transactions.csv'
            ]
            
            sample_path = None
            for path in possible_paths:
                st.write(f"Checking path: {path}")
                if os.path.exists(path):
                    sample_path = path
                    st.write(f"Found sample data at: {path}")
                    break
            
            if sample_path:
                df = pd.read_csv(sample_path)
                st.session_state.data = df
                st.session_state.original_data = df.copy()
                st.success(f"Sample data loaded! Shape: {df.shape}")
                st.dataframe(df.head(10))
            else:
                st.warning("Sample data file not found, creating new sample...")
                
                # Create sample data if it doesn't exist
                np.random.seed(42)
                
                # Create more realistic sample data
                n_samples = 1000
                start_date = datetime(2023, 1, 1)
                
                sample_data = pd.DataFrame({
                    'transaction_id': [f'TXN{i:06d}' for i in range(1, n_samples + 1)],
                    'timestamp': [start_date + timedelta(days=np.random.randint(0, 365)) for _ in range(n_samples)],
                    'amount': np.random.lognormal(8, 1.5, n_samples),  # More realistic amounts
                    'sender_id': [f'CUST{np.random.randint(1, 201):03d}' for _ in range(n_samples)],
                    'receiver_id': [f'CUST{np.random.randint(1, 201):03d}' for _ in range(n_samples)],
                    'currency': ['USD'] * n_samples,
                    'description': [f'Transaction type {np.random.choice(["PAYMENT", "TRANSFER", "PURCHASE", "WITHDRAWAL"])}' for _ in range(n_samples)],
                    'transaction_type': np.random.choice(['debit', 'credit'], n_samples),
                    'merchant_category': np.random.choice(['retail', 'services', 'utilities', 'finance'], n_samples)
                })
                
                # Ensure data directory exists
                data_dir = os.path.join('..', 'data')
                if not os.path.exists(data_dir):
                    os.makedirs(data_dir)
                
                sample_path = os.path.join(data_dir, 'sample_transactions.csv')
                sample_data.to_csv(sample_path, index=False)
                st.write(f"Created sample data at: {sample_path}")
                
                # Load the newly created sample data
                st.session_state.data = sample_data
                st.session_state.original_data = sample_data.copy()
                st.success(f"Sample data loaded! Shape: {sample_data.shape}")
                st.dataframe(sample_data.head(10))
                
        except Exception as e:
            st.error(f"Error loading sample data: {str(e)}")
            st.write("### Error Details")
            st.text(traceback.format_exc())
    
    # Additional troubleshooting section
    st.markdown("---")
    st.subheader("Troubleshooting File Upload Issues")
    
    with st.expander("Click here if you're having trouble uploading files"):
        st.write("""
        ### Common Issues and Solutions:
        
        1. **File not uploading at all**:
           - Try using a different browser (Chrome, Firefox, or Edge work best)
           - Clear your browser cache and cookies
           - Check if your file size is too large (try with a smaller file first)
           - Make sure the file is not open in another program
           - Try refreshing the page and uploading again
        
        2. **CSV file encoding issues**:
           - Try saving your CSV file with UTF-8 encoding
           - Use Excel to "Save As" and select "CSV UTF-8" format
           - Try converting your file to Excel format (.xlsx)
           - Check if your CSV has special characters that might cause encoding issues
        
        3. **Permission issues**:
           - Make sure the file is not read-only
           - Try moving the file to your desktop first
           - Check if your antivirus software is blocking the upload
           - Try running the application with administrator privileges
        
        4. **Browser compatibility**:
           - Update your browser to the latest version
           - Try incognito/private browsing mode
           - Disable browser extensions temporarily
           - Try a different browser
        
        5. **File format issues**:
           - Make sure your CSV file has proper headers
           - Check if your file has consistent formatting
           - Try opening the file in Excel and re-saving it
           - Remove any special characters from column names
        
        ### Alternative Methods:
        
        If the file uploader still doesn't work, you can:
        1. Use the "Load Sample Data" button above
        2. Place your CSV file in the `data` folder with the name `sample_transactions.csv`
        3. Try converting your file to Excel format (.xlsx)
        4. Use the text area below to paste your CSV data directly:
        """)
        
        # Add text area for direct CSV input
        st.write("### Direct CSV Input")
        csv_text = st.text_area("Paste your CSV data here:", height=200)
        
        if st.button("Load from Text"):
            try:
                if csv_text.strip():
                    # Use StringIO to read the CSV data
                    df = pd.read_csv(StringIO(csv_text))
                    st.session_state.data = df
                    st.session_state.original_data = df.copy()
                    st.success(f"Data loaded from text! Shape: {df.shape}")
                    st.dataframe(df.head(10))
                else:
                    st.warning("Please paste some CSV data")
            except Exception as e:
                st.error(f"Error loading data from text: {str(e)}")
                st.write("### Error Details")
                st.text(traceback.format_exc())
# Column Mapping Page
def render_column_mapping():
    """Render the column mapping page"""
    st.title("üîÑ Column Mapping")
    
    if st.session_state.data is None:
        st.warning("Please upload data first")
        return
    
    st.markdown("""
    Map your column headers to the expected format. The system will attempt to auto-map columns,
    but you can review and adjust as needed.
    """)
    
    # Initialize column mapper
    mapper = ColumnMapper()
    
    # Get expected columns
    expected_columns = mapper.get_expected_columns()
    
    # Get actual columns
    actual_columns = list(st.session_state.data.columns)
    
    # Auto-map columns
    if st.session_state.column_mapping is None:
        with st.spinner("Auto-mapping columns..."):
            st.session_state.column_mapping = mapper.auto_map_columns(actual_columns, expected_columns)
    
    # Display mapping
    st.write("### Column Mapping")
    
    # Create a DataFrame for the mapping
    mapping_data = []
    for actual_col in actual_columns:
        expected_col = st.session_state.column_mapping.get(actual_col, "Not mapped")
        mapping_data.append({
            "Your Column": actual_col,
            "Maps To": expected_col
        })
    
    mapping_df = pd.DataFrame(mapping_data)
    st.dataframe(mapping_df)
    
    # Allow manual adjustment
    st.write("### Adjust Mapping")
    adjusted_mapping = {}
    
    for actual_col in actual_columns:
        current_mapping = st.session_state.column_mapping.get(actual_col, "Not mapped")
        options = ["Not mapped"] + expected_columns
        selected = st.selectbox(
            f"Map '{actual_col}' to:",
            options=options,
            index=options.index(current_mapping) if current_mapping in options else 0,
            key=f"map_{actual_col}"
        )
        
        if selected != "Not mapped":
            adjusted_mapping[actual_col] = selected
    
    # Update button
    if st.button("Update Mapping"):
        st.session_state.column_mapping = adjusted_mapping
        st.success("Column mapping updated!")
    
    # Show mapped columns preview
    if st.button("Preview Mapped Data"):
        try:
            mapped_data = mapper.apply_mapping(st.session_state.data, st.session_state.column_mapping)
            st.write("### Mapped Data Preview")
            st.dataframe(mapped_data.head(10))
        except Exception as e:
            st.error(f"Error applying mapping: {str(e)}")
    
    # Continue to analysis
    if st.button("Continue to Analysis"):
        try:
            mapped_data = mapper.apply_mapping(st.session_state.data, st.session_state.column_mapping)
            st.session_state.processed_data = mapped_data
            st.success("Data processed successfully! You can now configure analysis settings.")
        except Exception as e:
            st.error(f"Error processing data: {str(e)}")
# Analysis Settings Page
def render_analysis_settings():
    """Render the analysis settings page"""
    st.title("‚öôÔ∏è Analysis Settings")
    
    if st.session_state.processed_data is None:
        st.warning("Please process data first")
        return
    
    st.markdown("""
    Configure the fraud detection analysis settings. You can select which algorithms to run,
    adjust parameters, and set thresholds.
    """)
    
    # Feature engineering settings
    st.subheader("Feature Engineering")
    
    feature_options = {
        "Statistical Features": True,
        "Graph Features": True,
        "NLP Features": True,
        "Time Series Features": True
    }
    
    selected_features = {}
    for feature, default in feature_options.items():
        selected_features[feature] = st.checkbox(feature, value=default)
    
    # Model settings
    st.subheader("Model Selection")
    
    model_options = {
        "Unsupervised Models": True,
        "Supervised Models": True,
        "Rule-Based Models": True
    }
    
    selected_models = {}
    for model, default in model_options.items():
        selected_models[model] = st.checkbox(model, value=default)
    
    # Advanced settings
    with st.expander("Advanced Settings"):
        st.write("### Unsupervised Model Parameters")
        
        contamination = st.slider(
            "Contamination (expected fraud rate)",
            min_value=0.001,
            max_value=0.5,
            value=0.01,
            step=0.001,
            help="Expected proportion of outliers in the data"
        )
        
        st.write("### Supervised Model Parameters")
        
        if 'fraud_flag' in st.session_state.processed_data.columns:
            test_size = st.slider(
                "Test Size",
                min_value=0.1,
                max_value=0.5,
                value=0.2,
                step=0.05,
                help="Proportion of data to use for testing"
            )
        else:
            st.info("No fraud_flag column found. Supervised learning will use synthetic labels.")
            test_size = 0.2
        
        st.write("### Rule Engine Parameters")
        
        rule_threshold = st.slider(
            "Rule Threshold",
            min_value=0.0,
            max_value=1.0,
            value=0.7,
            step=0.05,
            help="Threshold for rule-based detection"
        )
    
    # Risk scoring settings
    st.subheader("Risk Scoring")
    
    # Map the internal values to display names
    method_display_names = {
        "weighted_average": "Weighted Average",
        "maximum": "Maximum Score", 
        "custom": "Custom Weights"
    }
    
    scoring_method = st.selectbox(
        "Scoring Method",
        list(method_display_names.keys()),
        format_func=lambda x: method_display_names[x],
        help="Method to combine scores from different models"
    )
    
    if scoring_method == "custom":
        st.write("### Custom Weights")
        unsupervised_weight = st.slider("Unsupervised Weight", 0.0, 1.0, 0.4, 0.05)
        supervised_weight = st.slider("Supervised Weight", 0.0, 1.0, 0.4, 0.05)
        rule_weight = st.slider("Rule Weight", 0.0, 1.0, 0.2, 0.05)
        
        # Normalize weights
        total = unsupervised_weight + supervised_weight + rule_weight
        if total > 0:
            unsupervised_weight /= total
            supervised_weight /= total
            rule_weight /= total
        
        custom_weights = {
            "unsupervised": unsupervised_weight,
            "supervised": supervised_weight,
            "rule": rule_weight
        }
    else:
        custom_weights = None
    
    # Threshold settings
    st.subheader("Detection Thresholds")
    
    threshold_method = st.selectbox(
        "Threshold Method",
        ["Fixed", "Percentile", "Dynamic"],
        help="Method to determine fraud threshold"
    )
    
    if threshold_method == "Fixed":
        threshold = st.slider(
            "Risk Threshold",
            min_value=0.0,
            max_value=1.0,
            value=0.5,
            step=0.05,
            help="Transactions above this threshold will be flagged as potential fraud"
        )
    elif threshold_method == "Percentile":
        percentile = st.slider(
            "Percentile",
            min_value=90,
            max_value=100,
            value=95,
            step=1,
            help="Transactions above this percentile of risk scores will be flagged"
        )
        threshold = None
    else:  # Dynamic
        st.info("Dynamic thresholds will be calculated based on data distribution")
        threshold = None
    
    # Save settings
    if st.button("Save Settings"):
        settings = {
            "features": selected_features,
            "models": selected_models,
            "contamination": contamination,
            "test_size": test_size,
            "rule_threshold": rule_threshold,
            "scoring_method": scoring_method,
            "custom_weights": custom_weights,
            "threshold_method": threshold_method,
            "threshold": threshold,
            "percentile": percentile if threshold_method == "Percentile" else None
        }
        
        st.session_state.settings = settings
        st.success("Settings saved! You can now run the detection analysis.")
# Clean dataframe function
def clean_dataframe(df):
    """
    Clean a DataFrame by handling infinity, NaN, and extreme values
    
    Args:
        df (DataFrame): Input DataFrame
        
    Returns:
        DataFrame: Cleaned DataFrame
    """
    try:
        # Replace infinity with NaN
        df = df.replace([np.inf, -np.inf], np.nan)
        
        # Handle extreme values for numeric columns
        for col in df.select_dtypes(include=[np.number]).columns:
            # Calculate percentiles
            p1 = np.nanpercentile(df[col], 1)
            p99 = np.nanpercentile(df[col], 99)
            
            # Cap extreme values
            if not np.isnan(p1) and not np.isnan(p99):
                # Use a more conservative capping approach
                iqr = p99 - p1
                lower_bound = p1 - 1.5 * iqr
                upper_bound = p99 + 1.5 * iqr
                
                df[col] = np.where(df[col] < lower_bound, lower_bound, df[col])
                df[col] = np.where(df[col] > upper_bound, upper_bound, df[col])
            
            # Replace any remaining NaN with median
            median_val = df[col].median()
            if not np.isnan(median_val):
                df[col] = df[col].fillna(median_val)
        
        # Handle categorical columns
        for col in df.select_dtypes(include=['object', 'category']).columns:
            # Fill NaN with mode or 'Unknown'
            mode_val = df[col].mode()
            if len(mode_val) > 0:
                df[col] = df[col].fillna(mode_val[0])
            else:
                df[col] = df[col].fillna('Unknown')
        
        return df
    except Exception as e:
        logger.error(f"Error cleaning DataFrame: {str(e)}")
        return df
# Run Detection Page
def render_run_detection():
    """Render the run detection page"""
    st.title("üöÄ Run Fraud Detection")
    
    if st.session_state.processed_data is None:
        st.warning("Please process data first")
        return
    
    if 'settings' not in st.session_state:
        st.warning("Please configure analysis settings first")
        return
    
    st.markdown("""
    Run the fraud detection analysis using the configured settings. This may take some time
    depending on the size of your dataset and selected algorithms.
    """)
    
    # Show settings summary
    st.subheader("Analysis Settings Summary")
    
    settings = st.session_state.settings
    
    st.write("#### Feature Engineering")
    for feature, enabled in settings["features"].items():
        status = "‚úÖ Enabled" if enabled else "‚ùå Disabled"
        st.write(f"- {feature}: {status}")
    
    st.write("#### Models")
    for model, enabled in settings["models"].items():
        status = "‚úÖ Enabled" if enabled else "‚ùå Disabled"
        st.write(f"- {model}: {status}")
    
    st.write("#### Risk Scoring")
    method_display_names = {
        "weighted_average": "Weighted Average",
        "maximum": "Maximum Score", 
        "custom": "Custom Weights"
    }
    st.write(f"- Method: {method_display_names.get(settings['scoring_method'], settings['scoring_method'])}")
    if settings['custom_weights']:
        st.write(f"- Unsupervised Weight: {settings['custom_weights']['unsupervised']:.2f}")
        st.write(f"- Supervised Weight: {settings['custom_weights']['supervised']:.2f}")
        st.write(f"- Rule Weight: {settings['custom_weights']['rule']:.2f}")
    
    st.write("#### Threshold")
    st.write(f"- Method: {settings['threshold_method']}")
    if settings['threshold_method'] == 'Fixed':
        st.write(f"- Value: {settings['threshold']}")
    elif settings['threshold_method'] == 'Percentile':
        st.write(f"- Percentile: {settings['percentile']}%")
    
    # Run button
    if st.button("Run Detection Analysis"):
        # Create progress bar
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        try:
            # Step 1: Feature Engineering
            status_text.text("Step 1/5: Feature Engineering...")
            progress_bar.progress(10)
            
            features_df = st.session_state.processed_data.copy()
            
            # Clean data before feature engineering
            features_df = clean_dataframe(features_df)
            
            # Statistical features
            if settings["features"]["Statistical Features"]:
                stat_features = StatisticalFeatures()
                features_df = stat_features.extract_features(features_df)
                # Clean after feature extraction
                features_df = clean_dataframe(features_df)
                progress_bar.progress(20)
            
            # Graph features
            if settings["features"]["Graph Features"]:
                graph_features = GraphFeatures()
                features_df = graph_features.extract_features(features_df)
                # Clean after feature extraction
                features_df = clean_dataframe(features_df)
                progress_bar.progress(30)
            
            # NLP features
            if settings["features"]["NLP Features"]:
                nlp_features = NLPFeatures()
                features_df = nlp_features.extract_features(features_df)
                # Clean after feature extraction
                features_df = clean_dataframe(features_df)
                progress_bar.progress(40)
            
            # Time series features
            if settings["features"]["Time Series Features"]:
                ts_features = TimeSeriesFeatures()
                features_df = ts_features.extract_features(features_df)
                # Clean after feature extraction
                features_df = clean_dataframe(features_df)
                progress_bar.progress(50)
            
            # Step 2: Model Training/Prediction
            status_text.text("Step 2/5: Running Models...")
            model_results = {}
            
            # Unsupervised models
            if settings["models"]["Unsupervised Models"]:
                unsupervised = UnsupervisedModels(contamination=settings["contamination"])
                unsupervised_results = unsupervised.run_models(features_df)
                model_results["unsupervised"] = unsupervised_results
                progress_bar.progress(60)
            
            # Supervised models
            if settings["models"]["Supervised Models"]:
                supervised = SupervisedModels(test_size=settings["test_size"])
                supervised_results = supervised.run_models(features_df)
                model_results["supervised"] = supervised_results
                progress_bar.progress(70)
            
            # Rule-based models
            if settings["models"]["Rule-Based Models"]:
                rule_engine = RuleEngine(threshold=settings["rule_threshold"])
                rule_results = rule_engine.apply_rules(features_df)
                model_results["rule"] = rule_results
                progress_bar.progress(80)
            
            # Step 3: Risk Scoring
            status_text.text("Step 3/5: Calculating Risk Scores...")
            
            # Initialize risk scorer with properly formatted method
            scoring_method = settings["scoring_method"]
            risk_scorer = RiskScorer(
                method=scoring_method,
                custom_weights=settings["custom_weights"]
            )
            
            risk_scores = risk_scorer.calculate_scores(model_results, features_df)
            progress_bar.progress(90)
            
            # Step 4: Apply Thresholds
            status_text.text("Step 4/5: Applying Thresholds...")
            if settings['threshold_method'] == "Fixed":
                threshold = settings['threshold']
                risk_scores["is_fraud"] = risk_scores["risk_score"] > threshold
            elif settings['threshold_method'] == "Percentile":
                percentile = settings['percentile']
                threshold = np.percentile(risk_scores["risk_score"], percentile)
                risk_scores["is_fraud"] = risk_scores["risk_score"] > threshold
            else:  # Dynamic
                # Calculate dynamic threshold based on distribution
                mean_score = risk_scores["risk_score"].mean()
                std_score = risk_scores["risk_score"].std()
                threshold = mean_score + 2 * std_score
                risk_scores["is_fraud"] = risk_scores["risk_score"] > threshold
            
            progress_bar.progress(95)
            
            # Step 5: Generate Explanations
            status_text.text("Step 5/5: Generating Explanations...")
            explainability = Explainability()
            explanations = explainability.generate_explanations(
                features_df, 
                risk_scores, 
                model_results
            )
            
            progress_bar.progress(100)
            
            # Store results in session state
            st.session_state.features_df = features_df
            st.session_state.model_results = model_results
            st.session_state.risk_scores = risk_scores
            st.session_state.explanations = explanations
            st.session_state.threshold = threshold
            st.session_state.processing_complete = True
            
            status_text.text("Analysis complete!")
            st.success(f"Fraud detection analysis complete! Found {risk_scores['is_fraud'].sum()} potentially fraudulent transactions out of {len(risk_scores)} total.")
            
        except Exception as e:
            st.error(f"Error during analysis: {str(e)}")
            st.exception(e)
# Results Dashboard Page
def render_results_dashboard():
    """Render the results dashboard page"""
    st.title("üìä Results Dashboard")
    
    if not st.session_state.processing_complete:
        st.warning("Please run the detection analysis first")
        return
    
    risk_scores = st.session_state.risk_scores
    
    # Summary statistics
    st.subheader("Summary Statistics")
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("Total Transactions", len(risk_scores))
    
    with col2:
        fraud_count = risk_scores['is_fraud'].sum()
        st.metric("Fraudulent Transactions", fraud_count)
    
    with col3:
        fraud_percentage = (fraud_count / len(risk_scores)) * 100
        st.metric("Fraud Percentage", f"{fraud_percentage:.2f}%")
    
    with col4:
        threshold = st.session_state.threshold
        st.metric("Detection Threshold", f"{threshold:.4f}")
    
    # Risk score distribution
    st.subheader("Risk Score Distribution")
    
    fig = px.histogram(
        risk_scores,
        x="risk_score",
        color="is_fraud",
        nbins=50,
        title="Distribution of Risk Scores",
        color_discrete_map={False: "blue", True: "red"},
        opacity=0.7
    )
    
    fig.add_vline(
        x=threshold,
        line_dash="dash",
        line_color="green",
        annotation_text=f"Threshold: {threshold:.4f}"
    )
    
    st.plotly_chart(fig, use_container_width=True)
    
    # Top risky transactions
    st.subheader("Top Risky Transactions")
    
    top_risky = risk_scores[risk_scores['is_fraud']].nlargest(10, 'risk_score')
    
    if len(top_risky) > 0:
        # Create a display dataframe with risk scores and available details
        display_data = top_risky.copy()
        
        # Try to add transaction details from original data if available
        if hasattr(st.session_state, 'original_data') and st.session_state.original_data is not None:
            original_data = st.session_state.original_data
            
            # Get columns that exist in both dataframes
            common_cols = set(original_data.columns) & set(display_data.columns)
            
            # If there are common columns, merge them
            if common_cols:
                display_data = display_data.drop(columns=list(common_cols), errors='ignore')
                display_data = display_data.merge(
                    original_data[list(common_cols)],
                    left_index=True,
                    right_index=True,
                    how='left'
                )
        
        # If we still don't have transaction_id, try to get it from features_df
        if 'transaction_id' not in display_data.columns and hasattr(st.session_state, 'features_df'):
            features_df = st.session_state.features_df
            if 'transaction_id' in features_df.columns:
                display_data = display_data.merge(
                    features_df[['transaction_id']],
                    left_index=True,
                    right_index=True,
                    how='left'
                )
        
        # If we still don't have transaction_id, use the index as a fallback
        if 'transaction_id' not in display_data.columns:
            display_data['transaction_id'] = display_data.index
        
        # Ensure we have the required columns for display
        display_cols = []
        
        # Always include transaction_id and risk_score
        if 'transaction_id' in display_data.columns:
            display_cols.append('transaction_id')
        display_cols.append('risk_score')
        
        # Add other available columns if they exist
        for col in ['amount', 'timestamp', 'sender_id', 'receiver_id']:
            if col in display_data.columns:
                display_cols.append(col)
        
        # Display the dataframe
        st.dataframe(
            display_data[display_cols].sort_values('risk_score', ascending=False)
        )
    else:
        st.info("No fraudulent transactions detected")
    
    # Feature importance
    st.subheader("Feature Importance")
    
    if 'supervised' in st.session_state.model_results and 'feature_importance' in st.session_state.model_results['supervised']:
        feature_importance = st.session_state.model_results['supervised']['feature_importance']
        
        # Get top 20 features
        top_features = feature_importance.head(20)
        
        fig = px.bar(
            top_features,
            x='importance',
            y='feature',
            orientation='h',
            title="Top 20 Feature Importance",
            color='importance',
            color_continuous_scale='Viridis'
        )
        
        fig.update_layout(yaxis={'categoryorder': 'total ascending'})
        st.plotly_chart(fig, use_container_width=True)
    else:
        st.info("Feature importance not available. Run supervised models to see feature importance.")
    
    # Model performance
    st.subheader("Model Performance")
    
    if 'supervised' in st.session_state.model_results and 'performance' in st.session_state.model_results['supervised']:
        performance = st.session_state.model_results['supervised']['performance']
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.write("##### Classification Report")
            st.dataframe(performance['classification_report'])
        
        with col2:
            st.write("##### Confusion Matrix")
            cm = performance['confusion_matrix']
            
            fig = px.imshow(
                cm,
                text_auto=True,
                aspect="auto",
                labels=dict(x="Predicted", y="Actual", color="Count"),
                x=["Not Fraud", "Fraud"],
                y=["Not Fraud", "Fraud"],
                color_continuous_scale='Blues'
            )
            
            st.plotly_chart(fig, use_container_width=True)
    else:
        st.info("Model performance metrics not available. Run supervised models with labeled data to see performance metrics.")
    
    # Time series analysis
    if hasattr(st.session_state, 'original_data') and st.session_state.original_data is not None and 'timestamp' in st.session_state.original_data.columns:
        st.subheader("Fraud Over Time")
        
        # Create a copy for time series analysis
        ts_data = st.session_state.original_data.copy()
        ts_data['risk_score'] = risk_scores['risk_score']
        ts_data['is_fraud'] = risk_scores['is_fraud']
        
        # Convert timestamp to datetime if needed
        if not pd.api.types.is_datetime64_any_dtype(ts_data['timestamp']):
            ts_data['timestamp'] = pd.to_datetime(ts_data['timestamp'])
        
        # Extract date components
        ts_data['date'] = ts_data['timestamp'].dt.date
        ts_data['hour'] = ts_data['timestamp'].dt.hour
        
        # Fraud by date
        fraud_by_date = ts_data.groupby('date').agg({
            'is_fraud': ['sum', 'count'],
            'risk_score': 'mean'
        }).reset_index()
        
        fraud_by_date.columns = ['date', 'fraud_count', 'total_count', 'avg_risk_score']
        fraud_by_date['fraud_rate'] = fraud_by_date['fraud_count'] / fraud_by_date['total_count']
        
        fig = px.line(
            fraud_by_date,
            x='date',
            y=['fraud_count', 'fraud_rate'],
            title="Fraud Count and Rate Over Time",
            labels={'value': 'Count / Rate', 'date': 'Date'}
        )
        
        st.plotly_chart(fig, use_container_width=True)
        
        # Fraud by hour of day
        fraud_by_hour = ts_data.groupby('hour').agg({
            'is_fraud': ['sum', 'count'],
            'risk_score': 'mean'
        }).reset_index()
        
        fraud_by_hour.columns = ['hour', 'fraud_count', 'total_count', 'avg_risk_score']
        fraud_by_hour['fraud_rate'] = fraud_by_hour['fraud_count'] / fraud_by_hour['total_count']
        
        fig = px.bar(
            fraud_by_hour,
            x='hour',
            y='fraud_count',
            title="Fraud Count by Hour of Day",
            color='avg_risk_score',
            color_continuous_scale='Viridis'
        )
        
        st.plotly_chart(fig, use_container_width=True)
    elif hasattr(st.session_state, 'features_df') and st.session_state.features_df is not None and 'timestamp' in st.session_state.features_df.columns:
        st.subheader("Fraud Over Time")
        
        # Create a copy for time series analysis
        ts_data = st.session_state.features_df.copy()
        ts_data['risk_score'] = risk_scores['risk_score']
        ts_data['is_fraud'] = risk_scores['is_fraud']
        
        # Convert timestamp to datetime if needed
        if not pd.api.types.is_datetime64_any_dtype(ts_data['timestamp']):
            ts_data['timestamp'] = pd.to_datetime(ts_data['timestamp'])
        
        # Extract date components
        ts_data['date'] = ts_data['timestamp'].dt.date
        ts_data['hour'] = ts_data['timestamp'].dt.hour
        
        # Fraud by date
        fraud_by_date = ts_data.groupby('date').agg({
            'is_fraud': ['sum', 'count'],
            'risk_score': 'mean'
        }).reset_index()
        
        fraud_by_date.columns = ['date', 'fraud_count', 'total_count', 'avg_risk_score']
        fraud_by_date['fraud_rate'] = fraud_by_date['fraud_count'] / fraud_by_date['total_count']
        
        fig = px.line(
            fraud_by_date,
            x='date',
            y=['fraud_count', 'fraud_rate'],
            title="Fraud Count and Rate Over Time",
            labels={'value': 'Count / Rate', 'date': 'Date'}
        )
        
        st.plotly_chart(fig, use_container_width=True)
        
        # Fraud by hour of day
        fraud_by_hour = ts_data.groupby('hour').agg({
            'is_fraud': ['sum', 'count'],
            'risk_score': 'mean'
        }).reset_index()
        
        fraud_by_hour.columns = ['hour', 'fraud_count', 'total_count', 'avg_risk_score']
        fraud_by_hour['fraud_rate'] = fraud_by_hour['fraud_count'] / fraud_by_hour['total_count']
        
        fig = px.bar(
            fraud_by_hour,
            x='hour',
            y='fraud_count',
            title="Fraud Count by Hour of Day",
            color='avg_risk_score',
            color_continuous_scale='Viridis'
        )
        
        st.plotly_chart(fig, use_container_width=True)
    else:
        st.info("Timestamp data not available for time series analysis")
# Explainability Page
def render_explainability():
    """Render the explainability page"""
    st.title("üîç Explainability")
    
    if not st.session_state.processing_complete:
        st.warning("Please run the detection analysis first")
        return
    
    risk_scores = st.session_state.risk_scores
    explanations = st.session_state.explanations
    
    # Transaction selector
    st.subheader("Transaction Analysis")
    
    # Get fraudulent transactions
    fraud_transactions = risk_scores[risk_scores['is_fraud']]
    
    if len(fraud_transactions) == 0:
        st.info("No fraudulent transactions detected")
        return
    
    # Create a selector for transaction
    # Check if we have transaction_id in features_df
    if hasattr(st.session_state, 'features_df') and st.session_state.features_df is not None and 'transaction_id' in st.session_state.features_df.columns:
        # Merge with features to get transaction IDs
        fraud_with_ids = fraud_transactions.copy()
        fraud_with_ids = fraud_with_ids.merge(
            st.session_state.features_df[['transaction_id']],
            left_index=True,
            right_index=True,
            how='left'
        )
        
        # Create transaction options
        transaction_options = []
        for idx in fraud_with_ids.index:
            # Check if transaction_id exists and is not NaN
            if 'transaction_id' in fraud_with_ids.columns and pd.notna(fraud_with_ids.loc[idx, 'transaction_id']):
                transaction_id = fraud_with_ids.loc[idx, 'transaction_id']
                transaction_options.append(f"{transaction_id} (Index: {idx})")
            else:
                transaction_options.append(f"Index: {idx}")
        
        selected_transaction = st.selectbox("Select a transaction to analyze", transaction_options)
        
        # Parse the selected option to get the index
        if "Index:" in selected_transaction:
            selected_index = int(selected_transaction.split("Index: ")[1])
        else:
            # Extract transaction_id from the selected option
            transaction_id = selected_transaction.split(" (Index: ")[0]
            # Find the index for this transaction_id
            selected_index = fraud_with_ids[fraud_with_ids['transaction_id'] == transaction_id].index[0]
    else:
        # Use index as identifier
        transaction_options = [f"Index: {idx}" for idx in fraud_transactions.index]
        selected_transaction = st.selectbox("Select a transaction to analyze", transaction_options)
        selected_index = int(selected_transaction.split("Index: ")[1])
    
    # Display transaction details
    st.write("### Transaction Details")
    
    # Try to get transaction details from features_df
    if hasattr(st.session_state, 'features_df') and st.session_state.features_df is not None:
        try:
            transaction_data = st.session_state.features_df.loc[selected_index]
            
            # Display key fields
            key_fields = ['transaction_id', 'amount', 'timestamp', 'sender_id', 'receiver_id', 
                         'transaction_type', 'transaction_category', 'description']
            
            for field in key_fields:
                if field in transaction_data:
                    st.write(f"**{field.replace('_', ' ').title()}:** {transaction_data[field]}")
        except Exception as e:
            st.warning(f"Could not retrieve transaction details: {str(e)}")
    
    # Risk score
    risk_score = risk_scores.loc[selected_index, 'risk_score']
    st.metric("Risk Score", f"{risk_score:.4f}")
    
    # Explanation
    st.write("### Fraud Explanation")
    
    if selected_index in explanations:
        explanation = explanations[selected_index]
        
        # Show top contributing factors
        if 'top_factors' in explanation:
            st.write("#### Top Contributing Factors")
            
            factors_df = pd.DataFrame(explanation['top_factors'])
            factors_df.columns = ['Feature', 'Contribution']
            
            # Create a bar chart
            fig = px.bar(
                factors_df,
                x='Contribution',
                y='Feature',
                orientation='h',
                title="Feature Contributions to Risk Score",
                color='Contribution',
                color_continuous_scale='RdBu'
            )
            
            fig.update_layout(yaxis={'categoryorder': 'total ascending'})
            st.plotly_chart(fig, use_container_width=True)
        
        # Show rule violations
        if 'rule_violations' in explanation and len(explanation['rule_violations']) > 0:
            st.write("#### Rule Violations")
            
            for rule in explanation['rule_violations']:
                st.write(f"- {rule}")
        
        # Show model predictions
        if 'model_predictions' in explanation:
            st.write("#### Model Predictions")
            
            model_df = pd.DataFrame([
                {'Model': model, 'Score': score} 
                for model, score in explanation['model_predictions'].items()
            ])
            
            st.dataframe(model_df)
        
        # Show natural language explanation
        if 'text_explanation' in explanation:
            st.write("#### Explanation Summary")
            st.write(explanation['text_explanation'])
    else:
        st.info("No explanation available for this transaction")
    
    # What-if analysis
    st.write("### What-If Analysis")
    
    st.write("Adjust feature values to see how they affect the risk score:")
    
    # Get top features for this transaction
    if selected_index in explanations and 'top_factors' in explanations[selected_index]:
        top_features = [factor[0] for factor in explanations[selected_index]['top_factors'][:5]]
        
        # Create sliders for top features
        adjustments = {}
        for feature in top_features:
            if feature in st.session_state.features_df.columns:
                current_value = st.session_state.features_df.loc[selected_index, feature]
                
                # Determine appropriate range
                min_val = st.session_state.features_df[feature].min()
                max_val = st.session_state.features_df[feature].max()
                
                # Handle categorical features
                if st.session_state.features_df[feature].dtype == 'object':
                    options = st.session_state.features_df[feature].unique().tolist()
                    adjusted_value = st.selectbox(
                        f"Adjust {feature}",
                        options=options,
                        index=options.index(current_value) if current_value in options else 0
                    )
                else:
                    adjusted_value = st.slider(
                        f"Adjust {feature}",
                        min_value=float(min_val),
                        max_value=float(max_val),
                        value=float(current_value)
                    )
                
                adjustments[feature] = adjusted_value
        
        # Recalculate risk score button
        if st.button("Recalculate Risk Score"):
            # Create a copy of the original data
            modified_data = st.session_state.features_df.copy().loc[[selected_index]]
            
            # Apply adjustments
            for feature, value in adjustments.items():
                modified_data.loc[selected_index, feature] = value
            
            # Recalculate features
            modified_features = modified_data.copy()
            
            # Statistical features
            if st.session_state.settings["features"]["Statistical Features"]:
                stat_features = StatisticalFeatures()
                modified_features = stat_features.extract_features(modified_features)
            
            # Graph features
            if st.session_state.settings["features"]["Graph Features"]:
                graph_features = GraphFeatures()
                modified_features = graph_features.extract_features(modified_features)
            
            # NLP features
            if st.session_state.settings["features"]["NLP Features"]:
                nlp_features = NLPFeatures()
                modified_features = nlp_features.extract_features(modified_features)
            
            # Time series features
            if st.session_state.settings["features"]["Time Series Features"]:
                ts_features = TimeSeriesFeatures()
                modified_features = ts_features.extract_features(modified_features)
            
            # Get model predictions
            model_predictions = {}
            
            # Unsupervised models
            if 'unsupervised' in st.session_state.model_results:
                for model_name, model in st.session_state.model_results['unsupervised']['models'].items():
                    try:
                        # Get features used by this model
                        if 'feature_names' in st.session_state.model_results['unsupervised']:
                            feature_names = st.session_state.model_results['unsupervised']['feature_names']
                            model_features = modified_features[feature_names[model_name]]
                            prediction = model.decision_function(model_features)[0]
                            model_predictions[f"unsupervised_{model_name}"] = prediction
                    except Exception as e:
                        st.warning(f"Error predicting with {model_name}: {str(e)}")
            
            # Supervised models
            if 'supervised' in st.session_state.model_results:
                for model_name, model in st.session_state.model_results['supervised']['models'].items():
                    try:
                        # Get features used by this model
                        if 'feature_names' in st.session_state.model_results['supervised']:
                            feature_names = st.session_state.model_results['supervised']['feature_names']
                            model_features = modified_features[feature_names[model_name]]
                            prediction = model.predict_proba(model_features)[0, 1]
                            model_predictions[f"supervised_{model_name}"] = prediction
                    except Exception as e:
                        st.warning(f"Error predicting with {model_name}: {str(e)}")
            
            # Rule-based models
            if 'rule' in st.session_state.model_results:
                rule_score = 0.0
                violated_rules = []
                
                for rule_name, rule_func in st.session_state.model_results['rule']['rules'].items():
                    try:
                        if rule_func(modified_data.iloc[0]):
                            rule_score += 1.0 / len(st.session_state.model_results['rule']['rules'])
                            violated_rules.append(rule_name)
                    except Exception as e:
                        st.warning(f"Error applying rule {rule_name}: {str(e)}")
                
                model_predictions["rule"] = rule_score
            
            # Calculate new risk score
            risk_scorer = RiskScorer(
                method=st.session_state.settings["scoring_method"],
                custom_weights=st.session_state.settings["custom_weights"]
            )
            
            new_risk_score = risk_scorer.calculate_scores(
                {"unsupervised": model_predictions, "supervised": model_predictions, "rule": model_predictions},
                modified_features
            )
            
            new_risk_score = new_risk_score.loc[selected_index, 'risk_score']
            
            # Display comparison
            st.write("#### Risk Score Comparison")
            
            col1, col2 = st.columns(2)
            
            with col1:
                st.metric("Original Risk Score", f"{risk_score:.4f}")
            
            with col2:
                st.metric("New Risk Score", f"{new_risk_score:.4f}", 
                         delta=f"{new_risk_score - risk_score:.4f}")
            
            # Show if it would still be flagged as fraud
            threshold = st.session_state.threshold
            is_fraud_original = risk_score > threshold
            is_fraud_new = new_risk_score > threshold
            
            st.write(f"Original classification: {'Fraud' if is_fraud_original else 'Not Fraud'}")
            st.write(f"New classification: {'Fraud' if is_fraud_new else 'Not Fraud'}")
            
            if is_fraud_original != is_fraud_new:
                st.warning("Classification changed with the adjustments!")
            else:
                st.info("Classification remains the same")
# Reports Page
def render_reports():
    """Render the reports page"""
    st.title("üìÑ Reports")
    
    if not st.session_state.processing_complete:
        st.warning("Please run the detection analysis first")
        return
    
    risk_scores = st.session_state.risk_scores
    features_df = st.session_state.features_df
    
    st.markdown("""
    Generate comprehensive reports for auditors and stakeholders. Reports can be downloaded
    in PDF format and include detailed analysis of fraudulent transactions.
    """)
    
    # Report options
    st.subheader("Report Options")
    
    report_type = st.selectbox(
        "Select Report Type",
        ["Executive Summary", "Detailed Fraud Analysis", "Technical Report", "Custom Report"]
    )
    
    include_charts = st.checkbox("Include Charts and Visualizations", value=True)
    include_explanations = st.checkbox("Include Detailed Explanations", value=True)
    include_recommendations = st.checkbox("Include Recommendations", value=True)
    
    # Transaction selection
    st.subheader("Transaction Selection")
    
    fraud_transactions = risk_scores[risk_scores['is_fraud']]
    
    if len(fraud_transactions) > 0:
        max_transactions = min(len(fraud_transactions), 100)  # Limit to 100 for performance
        num_transactions = st.slider(
            "Number of Fraudulent Transactions to Include",
            min_value=1,
            max_value=max_transactions,
            value=min(10, max_transactions),
            step=1
        )
        
        # Sort by risk score and select top N
        top_fraud = fraud_transactions.nlargest(num_transactions, 'risk_score')
    else:
        st.info("No fraudulent transactions detected")
        return
    
    # Report generation options
    st.subheader("Report Generation")
    
    if st.button("Generate Report"):
        with st.spinner("Generating report..."):
            try:
                # Initialize PDF generator
                pdf_generator = PDFGenerator()
                
                # Generate report based on type
                if report_type == "Executive Summary":
                    report_path = pdf_generator.generate_executive_summary(
                        features_df,
                        risk_scores,
                        top_fraud,
                        include_charts=include_charts,
                        include_recommendations=include_recommendations
                    )
                elif report_type == "Detailed Fraud Analysis":
                    report_path = pdf_generator.generate_detailed_fraud_analysis(
                        features_df,
                        risk_scores,
                        top_fraud,
                        st.session_state.explanations,
                        include_charts=include_charts,
                        include_explanations=include_explanations,
                        include_recommendations=include_recommendations
                    )
                elif report_type == "Technical Report":
                    report_path = pdf_generator.generate_technical_report(
                        features_df,
                        risk_scores,
                        st.session_state.model_results,
                        include_charts=include_charts
                    )
                else:  # Custom Report
                    report_path = pdf_generator.generate_custom_report(
                        features_df,
                        risk_scores,
                        top_fraud,
                        st.session_state.explanations,
                        st.session_state.model_results,
                        include_charts=include_charts,
                        include_explanations=include_explanations,
                        include_recommendations=include_recommendations
                    )
                
                # Display success message
                st.success(f"Report generated successfully!")
                
                # Provide download button
                with open(report_path, "rb") as file:
                    st.download_button(
                        label="Download Report",
                        data=file,
                        file_name=os.path.basename(report_path),
                        mime="application/pdf"
                    )
                
                # Show report preview
                st.subheader("Report Preview")
                st.info("Report preview is not available in this interface. Please download the PDF to view the full report.")
                
            except Exception as e:
                st.error(f"Error generating report: {str(e)}")
                st.exception(e)
# Main function
def main():
    """Main function to run the Streamlit app"""
    page = render_sidebar()
    
    if page == "Data Upload":
        render_data_upload()
    elif page == "Column Mapping":
        render_column_mapping()
    elif page == "Analysis Settings":
        render_analysis_settings()
    elif page == "Run Detection":
        render_run_detection()
    elif page == "Results Dashboard":
        render_results_dashboard()
    elif page == "Explainability":
        render_explainability()
    elif page == "Reports":
        render_reports()
if __name__ == "__main__":
    main()

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\config\api_keys.yml ===
# API keys for external services

# Gemini API for AI integration
gemini:
  api_key: "AIzaSyB_lLIZ_ZIE__qxMS-XwrqA6G5X6qY05W4"
  model: "gemini-pro"
  temperature: 0.2
  max_tokens: 1000
  top_p: 0.8
  top_k: 40

# OpenAI API (alternative)
openai:
  api_key: "sk-ijklmnop5678efghijklmnop5678efghijklmnop"
  model: "gpt-4"
  temperature: 0.2
  max_tokens: 1000

# News API for fraud pattern updates
news_api:
  api_key: "e42e914093885afa580f2084f00d06e7"
  sources: ["reuters", "bloomberg", "financial-times"]
  keywords: ["fraud", "money laundering", "financial crime", "cybersecurity"]
  lookback_days: 7

# Geolocation API
geolocation:
  api_key: "9520e3680de446a2b4dab44072e937d4"
  endpoint: "https://api.geolocation.example.com/v1"

# Sanctions list APIs - NOT AVAILABLE
sanctions:
  api_key: "NOT_AVAILABLE"
  update_frequency: "daily"

# Tax compliance database - NOT AVAILABLE
tax_compliance:
  api_key: "NOT_AVAILABLE"
  endpoint: "https://api.taxcompliance.example.com/v1"

# Bank verification API - NOT AVAILABLE
bank_verification:
  api_key: "NOT_AVAILABLE"
  endpoint: "https://api.bankverification.example.com/v1"

# Identity verification API - NOT AVAILABLE
identity_verification:
  api_key: "NOT_AVAILABLE"
  endpoint: "https://api.identityverification.example.com/v1"

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\config\model_params.yml ===
# Model parameters for fraud detection system

# Unsupervised model parameters
unsupervised:
  isolation_forest:
    n_estimators: 100
    max_samples: 'auto'
    contamination: 0.01
    max_features: 1.0
    bootstrap: False
    n_jobs: -1
    random_state: 42
    
  local_outlier_factor:
    n_neighbors: 20
    algorithm: 'auto'
    leaf_size: 30
    metric: 'minkowski'
    p: 2
    metric_params: null
    contamination: 0.01
    n_jobs: -1
    
  autoencoder:
    encoding_dim: 10
    hidden_layers: [50, 25]
    activation: 'relu'
    optimizer: 'adam'
    loss: 'mean_squared_error'
    epochs: 100
    batch_size: 32
    validation_split: 0.1
    verbose: 0

# Supervised model parameters
supervised:
  random_forest:
    n_estimators: 100
    criterion: 'gini'
    max_depth: null
    min_samples_split: 2
    min_samples_leaf: 1
    min_weight_fraction_leaf: 0.0
    max_features: 'auto'
    max_leaf_nodes: null
    min_impurity_decrease: 0.0
    bootstrap: True
    oob_score: False
    n_jobs: -1
    random_state: 42
    class_weight: 'balanced'
    
  xgboost:
    n_estimators: 100
    max_depth: 6
    learning_rate: 0.1
    subsample: 1.0
    colsample_bytree: 1.0
    gamma: 0
    reg_alpha: 0
    reg_lambda: 1
    random_state: 42
    scale_pos_weight: 1
    
  logistic_regression:
    penalty: 'l2'
    C: 1.0
    fit_intercept: True
    intercept_scaling: 1
    class_weight: 'balanced'
    random_state: 42
    solver: 'lbfgs'
    max_iter: 1000
    multi_class: 'auto'
    
  svm:
    C: 1.0
    kernel: 'rbf'
    degree: 3
    gamma: 'scale'
    coef0: 0.0
    shrinking: True
    probability: True
    tol: 0.001
    cache_size: 200
    class_weight: 'balanced'
    random_state: 42

# Neural network parameters
neural_network:
  hidden_layers: [128, 64, 32]
  activation: 'relu'
  optimizer: 'adam'
  loss: 'binary_crossentropy'
  metrics: ['accuracy', 'precision', 'recall']
  epochs: 100
  batch_size: 32
  validation_split: 0.2
  verbose: 1
  dropout_rate: 0.2
  early_stopping_patience: 10

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\config\rule_engine_config.yml ===
# Rule engine configuration for fraud detection

# Rule categories
categories:
  amount_rules:
    enabled: true
    weight: 0.3
    
  frequency_rules:
    enabled: true
    weight: 0.2
    
  location_rules:
    enabled: true
    weight: 0.2
    
  time_rules:
    enabled: true
    weight: 0.15
    
  identity_rules:
    enabled: true
    weight: 0.15

# Amount-based rules
amount_rules:
  high_amount:
    enabled: true
    threshold: 10000
    description: "Transaction amount exceeds threshold"
    
  unusual_amount_for_sender:
    enabled: true
    std_multiplier: 3
    min_transactions: 5
    description: "Amount is unusual for the sender"
    
  unusual_amount_for_receiver:
    enabled: true
    std_multiplier: 3
    min_transactions: 5
    description: "Amount is unusual for the receiver"
    
  round_amount:
    enabled: true
    threshold: 1000
    description: "Transaction amount is suspiciously round"
    
  amount_multiple:
    enabled: true
    multiple: 1000
    description: "Transaction amount is a multiple of a suspicious value"

# Frequency-based rules
frequency_rules:
  high_frequency_sender:
    enabled: true
    time_window: "1H"
    max_transactions: 10
    description: "High transaction frequency from sender"
    
  high_frequency_receiver:
    enabled: true
    time_window: "1H"
    max_transactions: 10
    description: "High transaction frequency to receiver"
    
  rapid_succession:
    enabled: true
    time_window: "5M"
    min_transactions: 3
    description: "Multiple transactions in rapid succession"
    
  burst_pattern:
    enabled: true
    time_window: "1H"
    threshold_multiplier: 3
    description: "Burst of transactions compared to normal pattern"

# Location-based rules
location_rules:
  cross_border:
    enabled: true
    description: "Transaction crosses international borders"
    
  high_risk_country:
    enabled: true
    countries: ["North Korea", "Iran", "Syria", "Cuba"]
    description: "Transaction involves high-risk country"
    
  unusual_location_for_sender:
    enabled: true
    description: "Transaction from unusual location for sender"
    
  unusual_location_for_receiver:
    enabled: true
    description: "Transaction to unusual location for receiver"
    
  ip_mismatch:
    enabled: true
    description: "IP location does not match sender location"

# Time-based rules
time_rules:
  unusual_hour:
    enabled: true
    start_hour: 23
    end_hour: 5
    description: "Transaction during unusual hours"
    
  weekend:
    enabled: true
    description: "Transaction on weekend"
    
  holiday:
    enabled: true
    description: "Transaction on public holiday"
    
  rapid_time_pattern:
    enabled: true
    time_window: "1D"
    min_transactions: 5
    description: "Unusual pattern of transaction timing"

# Identity-based rules
identity_rules:
  new_sender:
    enabled: true
    time_window: "7D"
    description: "First transaction from new sender"
    
  new_receiver:
    enabled: true
    time_window: "7D"
    description: "First transaction to new receiver"
    
  high_risk_entity:
    enabled: true
    description: "Transaction involves high-risk entity"
    
  sanctioned_entity:
    enabled: true
    description: "Transaction involves sanctioned entity"
    
  peer_to_peer_pattern:
    enabled: true
    description: "Pattern suggesting peer-to-peer transfer scheme"

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\config\.ipynb_checkpoints\api_keys-checkpoint.yml ===
# API keys for external services

# Gemini API for AI integration
gemini:
  api_key: "AIzaSyB_lLIZ_ZIE__qxMS-XwrqA6G5X6qY05W4"
  model: "gemini-pro"
  temperature: 0.2
  max_tokens: 1000
  top_p: 0.8
  top_k: 40

# OpenAI API (alternative)
openai:
  api_key: "sk-ijklmnop5678efghijklmnop5678efghijklmnop"
  model: "gpt-4"
  temperature: 0.2
  max_tokens: 1000

# News API for fraud pattern updates
news_api:
  api_key: "e42e914093885afa580f2084f00d06e7"
  sources: ["reuters", "bloomberg", "financial-times"]
  keywords: ["fraud", "money laundering", "financial crime", "cybersecurity"]
  lookback_days: 7

# Geolocation API
geolocation:
  api_key: "9520e3680de446a2b4dab44072e937d4"
  endpoint: "https://api.geolocation.example.com/v1"

# Sanctions list APIs - NOT AVAILABLE
sanctions:
  api_key: "NOT_AVAILABLE"
  update_frequency: "daily"

# Tax compliance database - NOT AVAILABLE
tax_compliance:
  api_key: "NOT_AVAILABLE"
  endpoint: "https://api.taxcompliance.example.com/v1"

# Bank verification API - NOT AVAILABLE
bank_verification:
  api_key: "NOT_AVAILABLE"
  endpoint: "https://api.bankverification.example.com/v1"

# Identity verification API - NOT AVAILABLE
identity_verification:
  api_key: "NOT_AVAILABLE"
  endpoint: "https://api.identityverification.example.com/v1"

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\config\.ipynb_checkpoints\model_params-checkpoint.yml ===
# Model parameters for fraud detection system

# Unsupervised model parameters
unsupervised:
  isolation_forest:
    n_estimators: 100
    max_samples: 'auto'
    contamination: 0.01
    max_features: 1.0
    bootstrap: False
    n_jobs: -1
    random_state: 42
    
  local_outlier_factor:
    n_neighbors: 20
    algorithm: 'auto'
    leaf_size: 30
    metric: 'minkowski'
    p: 2
    metric_params: null
    contamination: 0.01
    n_jobs: -1
    
  autoencoder:
    encoding_dim: 10
    hidden_layers: [50, 25]
    activation: 'relu'
    optimizer: 'adam'
    loss: 'mean_squared_error'
    epochs: 100
    batch_size: 32
    validation_split: 0.1
    verbose: 0

# Supervised model parameters
supervised:
  random_forest:
    n_estimators: 100
    criterion: 'gini'
    max_depth: null
    min_samples_split: 2
    min_samples_leaf: 1
    min_weight_fraction_leaf: 0.0
    max_features: 'auto'
    max_leaf_nodes: null
    min_impurity_decrease: 0.0
    bootstrap: True
    oob_score: False
    n_jobs: -1
    random_state: 42
    class_weight: 'balanced'
    
  xgboost:
    n_estimators: 100
    max_depth: 6
    learning_rate: 0.1
    subsample: 1.0
    colsample_bytree: 1.0
    gamma: 0
    reg_alpha: 0
    reg_lambda: 1
    random_state: 42
    scale_pos_weight: 1
    
  logistic_regression:
    penalty: 'l2'
    C: 1.0
    fit_intercept: True
    intercept_scaling: 1
    class_weight: 'balanced'
    random_state: 42
    solver: 'lbfgs'
    max_iter: 1000
    multi_class: 'auto'
    
  svm:
    C: 1.0
    kernel: 'rbf'
    degree: 3
    gamma: 'scale'
    coef0: 0.0
    shrinking: True
    probability: True
    tol: 0.001
    cache_size: 200
    class_weight: 'balanced'
    random_state: 42

# Neural network parameters
neural_network:
  hidden_layers: [128, 64, 32]
  activation: 'relu'
  optimizer: 'adam'
  loss: 'binary_crossentropy'
  metrics: ['accuracy', 'precision', 'recall']
  epochs: 100
  batch_size: 32
  validation_split: 0.2
  verbose: 1
  dropout_rate: 0.2
  early_stopping_patience: 10

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\config\.ipynb_checkpoints\rule_engine_config-checkpoint.yml ===
# Rule engine configuration for fraud detection

# Rule categories
categories:
  amount_rules:
    enabled: true
    weight: 0.3
    
  frequency_rules:
    enabled: true
    weight: 0.2
    
  location_rules:
    enabled: true
    weight: 0.2
    
  time_rules:
    enabled: true
    weight: 0.15
    
  identity_rules:
    enabled: true
    weight: 0.15

# Amount-based rules
amount_rules:
  high_amount:
    enabled: true
    threshold: 10000
    description: "Transaction amount exceeds threshold"
    
  unusual_amount_for_sender:
    enabled: true
    std_multiplier: 3
    min_transactions: 5
    description: "Amount is unusual for the sender"
    
  unusual_amount_for_receiver:
    enabled: true
    std_multiplier: 3
    min_transactions: 5
    description: "Amount is unusual for the receiver"
    
  round_amount:
    enabled: true
    threshold: 1000
    description: "Transaction amount is suspiciously round"
    
  amount_multiple:
    enabled: true
    multiple: 1000
    description: "Transaction amount is a multiple of a suspicious value"

# Frequency-based rules
frequency_rules:
  high_frequency_sender:
    enabled: true
    time_window: "1H"
    max_transactions: 10
    description: "High transaction frequency from sender"
    
  high_frequency_receiver:
    enabled: true
    time_window: "1H"
    max_transactions: 10
    description: "High transaction frequency to receiver"
    
  rapid_succession:
    enabled: true
    time_window: "5M"
    min_transactions: 3
    description: "Multiple transactions in rapid succession"
    
  burst_pattern:
    enabled: true
    time_window: "1H"
    threshold_multiplier: 3
    description: "Burst of transactions compared to normal pattern"

# Location-based rules
location_rules:
  cross_border:
    enabled: true
    description: "Transaction crosses international borders"
    
  high_risk_country:
    enabled: true
    countries: ["North Korea", "Iran", "Syria", "Cuba"]
    description: "Transaction involves high-risk country"
    
  unusual_location_for_sender:
    enabled: true
    description: "Transaction from unusual location for sender"
    
  unusual_location_for_receiver:
    enabled: true
    description: "Transaction to unusual location for receiver"
    
  ip_mismatch:
    enabled: true
    description: "IP location does not match sender location"

# Time-based rules
time_rules:
  unusual_hour:
    enabled: true
    start_hour: 23
    end_hour: 5
    description: "Transaction during unusual hours"
    
  weekend:
    enabled: true
    description: "Transaction on weekend"
    
  holiday:
    enabled: true
    description: "Transaction on public holiday"
    
  rapid_time_pattern:
    enabled: true
    time_window: "1D"
    min_transactions: 5
    description: "Unusual pattern of transaction timing"

# Identity-based rules
identity_rules:
  new_sender:
    enabled: true
    time_window: "7D"
    description: "First transaction from new sender"
    
  new_receiver:
    enabled: true
    time_window: "7D"
    description: "First transaction to new receiver"
    
  high_risk_entity:
    enabled: true
    description: "Transaction involves high-risk entity"
    
  sanctioned_entity:
    enabled: true
    description: "Transaction involves sanctioned entity"
    
  peer_to_peer_pattern:
    enabled: true
    description: "Pattern suggesting peer-to-peer transfer scheme"

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\data\sample_transactions.csv ===
transaction_id,timestamp,amount,currency,sender_id,receiver_id,sender_account_type,receiver_account_type,sender_bank,receiver_bank,sender_location,receiver_location,transaction_type,transaction_category,merchant_id,merchant_category,ip_address,device_id,description,notes,authorization_status,chargeback_flag,fraud_flag
1,2023-01-01 09:00:00,150.50,USD,sender_001,receiver_001,personal,business,Chase,Bank of America,New York,USA,transfer,retail,merch_001,electronics,192.168.1.1,device_001,Payment for services,Regular payment,approved,False,0
2,2023-01-01 10:30:00,2500.00,USD,sender_002,receiver_002,business,personal,Wells Fargo,Citi,Los Angeles,USA,transfer,services,merch_002,professional,192.168.1.2,device_002,Business transfer,Urgent payment,approved,False,0
3,2023-01-01 11:45:00,10000.00,USD,sender_003,receiver_003,personal,personal,Bank of America,Chase,Chicago,USA,transfer,retail,merch_003,electronics,192.168.1.3,device_003,High value transfer,Suspicious,approved,False,0
4,2023-01-01 14:20:00,75.25,USD,sender_004,receiver_004,personal,personal,Citi,Wells Fargo,Houston,USA,payment,retail,merch_004,groceries,192.168.1.4,device_004,Grocery shopping,Regular,approved,False,0
5,2023-01-01 16:55:00,5000.00,USD,sender_005,receiver_005,business,business,JPMorgan Chase,Goldman Sachs,New York,USA,transfer,investment,merch_005,financial,192.168.1.5,device_005,Investment transfer,Regular,approved,False,0
6,2023-01-02 08:15:00,200.00,USD,sender_006,receiver_006,personal,personal,Bank of America,Citi,Miami,USA,payment,retail,merch_006,dining,192.168.1.6,device_006,Restaurant payment,Regular,approved,False,0
7,2023-01-02 09:45:00,15000.00,USD,sender_007,receiver_007,business,personal,Goldman Sachs,Wells Fargo,New York,USA,transfer,investment,merch_007,financial,192.168.1.7,device_007,Large investment,Suspicious,approved,False,1
8,2023-01-02 13:30:00,125.75,USD,sender_008,receiver_008,personal,personal,Chase,Bank of America,Boston,USA,payment,retail,merch_008,electronics,192.168.1.8,device_008,Online purchase,Regular,approved,False,0
9,2023-01-02 15:20:00,7500.00,USD,sender_009,receiver_009,business,business,Wells Fargo,JPMorgan Chase,Chicago,USA,transfer,business,merch_009,professional,192.168.1.9,device_009,Business expense,Regular,approved,False,0
10,2023-01-02 17:45:00,300.00,USD,sender_010,receiver_010,personal,personal,Citi,Goldman Sachs,Los Angeles,USA,payment,retail,merch_010,groceries,192.168.1.10,device_010,Grocery shopping,Regular,approved,False,0

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\data\.ipynb_checkpoints\sample_transactions-checkpoint.csv ===
transaction_id,timestamp,amount,currency,sender_id,receiver_id,sender_account_type,receiver_account_type,sender_bank,receiver_bank,sender_location,receiver_location,transaction_type,transaction_category,merchant_id,merchant_category,ip_address,device_id,description,notes,authorization_status,chargeback_flag,fraud_flag
1,2023-01-01 09:00:00,150.50,USD,sender_001,receiver_001,personal,business,Chase,Bank of America,New York,USA,transfer,retail,merch_001,electronics,192.168.1.1,device_001,Payment for services,Regular payment,approved,False,0
2,2023-01-01 10:30:00,2500.00,USD,sender_002,receiver_002,business,personal,Wells Fargo,Citi,Los Angeles,USA,transfer,services,merch_002,professional,192.168.1.2,device_002,Business transfer,Urgent payment,approved,False,0
3,2023-01-01 11:45:00,10000.00,USD,sender_003,receiver_003,personal,personal,Bank of America,Chase,Chicago,USA,transfer,retail,merch_003,electronics,192.168.1.3,device_003,High value transfer,Suspicious,approved,False,0
4,2023-01-01 14:20:00,75.25,USD,sender_004,receiver_004,personal,personal,Citi,Wells Fargo,Houston,USA,payment,retail,merch_004,groceries,192.168.1.4,device_004,Grocery shopping,Regular,approved,False,0
5,2023-01-01 16:55:00,5000.00,USD,sender_005,receiver_005,business,business,JPMorgan Chase,Goldman Sachs,New York,USA,transfer,investment,merch_005,financial,192.168.1.5,device_005,Investment transfer,Regular,approved,False,0
6,2023-01-02 08:15:00,200.00,USD,sender_006,receiver_006,personal,personal,Bank of America,Citi,Miami,USA,payment,retail,merch_006,dining,192.168.1.6,device_006,Restaurant payment,Regular,approved,False,0
7,2023-01-02 09:45:00,15000.00,USD,sender_007,receiver_007,business,personal,Goldman Sachs,Wells Fargo,New York,USA,transfer,investment,merch_007,financial,192.168.1.7,device_007,Large investment,Suspicious,approved,False,1
8,2023-01-02 13:30:00,125.75,USD,sender_008,receiver_008,personal,personal,Chase,Bank of America,Boston,USA,payment,retail,merch_008,electronics,192.168.1.8,device_008,Online purchase,Regular,approved,False,0
9,2023-01-02 15:20:00,7500.00,USD,sender_009,receiver_009,business,business,Wells Fargo,JPMorgan Chase,Chicago,USA,transfer,business,merch_009,professional,192.168.1.9,device_009,Business expense,Regular,approved,False,0
10,2023-01-02 17:45:00,300.00,USD,sender_010,receiver_010,personal,personal,Citi,Goldman Sachs,Los Angeles,USA,payment,retail,merch_010,groceries,192.168.1.10,device_010,Grocery shopping,Regular,approved,False,0

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\data\external\sanctions_list.csv ===

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\data\external\.ipynb_checkpoints\sanctions_list-checkpoint.csv ===

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\__init__.py ===

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\.ipynb_checkpoints\__init__-checkpoint.py ===

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\analysis\explainability.py ===
"""
Explainability Module
Implements explainable AI techniques for fraud detection
"""

import pandas as pd
import numpy as np
import shap
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.inspection import permutation_importance, partial_dependence
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier, export_text, export_graphviz
import graphviz
import warnings
import logging
from typing import Dict, List, Tuple, Union

from fraud_detection_engine.utils.api_utils import is_api_available

warnings.filterwarnings('ignore')
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class Explainability:
    """
    Class for generating explanations for fraud detection predictions
    Implements SHAP, LIME, and other explainability techniques
    """
    
    def __init__(self, config=None):
        """
        Initialize Explainability
        
        Args:
            config (dict, optional): Configuration parameters
        """
        self.config = config or {}
        self.explainers = {}
        self.explanations = {}
        self.feature_names = {}
        self.fitted = False
    
    def generate_explanations(self, df, risk_scores, model_results):
        """
        Generate explanations for all transactions
        
        Args:
            df (DataFrame): Input data
            risk_scores (DataFrame): Risk scores
            model_results (dict): Results from different models
            
        Returns:
            dict: Explanations for each transaction
        """
        try:
            # Get feature columns
            feature_cols = [col for col in df.columns if col not in 
                           ['transaction_id', 'sender_id', 'receiver_id', 'fraud_flag']]
            
            # Prepare data
            X = df[feature_cols].fillna(0)
            
            # Store feature names
            self.feature_names = feature_cols
            
            # Initialize explanations dictionary
            explanations = {}
            
            # Generate explanations for each transaction
            for idx, row in df.iterrows():
                try:
                    # Get risk score
                    risk_score = risk_scores.loc[idx, 'risk_score']
                    
                    # Generate explanation only for high-risk transactions
                    if risk_score > 0.5:  # Threshold for explanation
                        explanation = self._generate_transaction_explanation(
                            idx, row, X, risk_score, model_results
                        )
                        explanations[idx] = explanation
                except Exception as e:
                    logger.warning(f"Error generating explanation for transaction {idx}: {str(e)}")
            
            self.explanations = explanations
            self.fitted = True
            
            logger.info(f"Generated explanations for {len(explanations)} transactions")
            return explanations
            
        except Exception as e:
            logger.error(f"Error generating explanations: {str(e)}")
            raise
    
    def _generate_transaction_explanation(self, idx, row, X, risk_score, model_results):
        """
        Generate explanation for a single transaction
        
        Args:
            idx (int): Transaction index
            row (Series): Transaction data
            X (DataFrame): Feature data
            risk_score (float): Risk score
            model_results (dict): Model results
            
        Returns:
            dict: Explanation for the transaction
        """
        try:
            explanation = {
                'transaction_id': row.get('transaction_id', idx),
                'risk_score': risk_score,
                'top_factors': [],
                'rule_violations': [],
                'model_predictions': {},
                'feature_contributions': {},
                'text_explanation': ''
            }
            
            # Get feature contributions from different models
            feature_contributions = {}
            
            # Process unsupervised models
            if 'unsupervised' in model_results:
                unsupervised_contributions = self._get_unsupervised_contributions(
                    idx, X, model_results['unsupervised']
                )
                feature_contributions.update(unsupervised_contributions)
                
                # Add model predictions
                for model_name, model_data in model_results['unsupervised'].items():
                    if 'scores' in model_data and idx < len(model_data['scores']):
                        explanation['model_predictions'][f'unsupervised_{model_name}'] = model_data['scores'][idx]
            
            # Process supervised models
            if 'supervised' in model_results:
                supervised_contributions = self._get_supervised_contributions(
                    idx, X, model_results['supervised']
                )
                feature_contributions.update(supervised_contributions)
                
                # Add model predictions
                for model_name, model_data in model_results['supervised'].items():
                    if 'probabilities' in model_data and idx < len(model_data['probabilities']):
                        explanation['model_predictions'][f'supervised_{model_name}'] = model_data['probabilities'][idx]
            
            # Process rule-based models
            if 'rule' in model_results:
                rule_contributions = self._get_rule_contributions(
                    idx, row, model_results['rule']
                )
                feature_contributions.update(rule_contributions)
                
                # Add rule violations
                if 'violated_rule_names' in model_results['rule'] and idx < len(model_results['rule']['violated_rule_names']):
                    explanation['rule_violations'] = model_results['rule']['violated_rule_names'][idx]
            
            # Aggregate feature contributions
            explanation['feature_contributions'] = feature_contributions
            
            # Get top contributing factors
            top_factors = sorted(
                feature_contributions.items(),
                key=lambda x: abs(x[1]),
                reverse=True
            )[:10]  # Top 10 factors
            
            explanation['top_factors'] = top_factors
            
            # Generate text explanation
            explanation['text_explanation'] = self._generate_text_explanation(
                explanation, row
            )
            
            return explanation
            
        except Exception as e:
            logger.error(f"Error generating explanation for transaction {idx}: {str(e)}")
            return {
                'transaction_id': row.get('transaction_id', idx),
                'risk_score': risk_score,
                'error': str(e)
            }
    
    def _get_unsupervised_contributions(self, idx, X, unsupervised_results):
        """
        Get feature contributions from unsupervised models
        
        Args:
            idx (int): Transaction index
            X (DataFrame): Feature data
            unsupervised_results (dict): Unsupervised model results
            
        Returns:
            dict: Feature contributions
        """
        try:
            contributions = {}
            
            # Process each unsupervised model
            for model_name, model_data in unsupervised_results.items():
                if 'model' in model_data and 'feature_names' in model_data:
                    model = model_data['model']
                    feature_names = model_data['feature_names']
                    
                    # Get feature values for this transaction
                    x = X.loc[idx:idx+1, feature_names]
                    
                    # Calculate feature contributions based on model type
                    if model_name == 'isolation_forest':
                        # For Isolation Forest, use feature importance
                        if hasattr(model, 'feature_importances_'):
                            feature_importance = model.feature_importances_
                            for i, feature in enumerate(feature_names):
                                if i < len(feature_importance):
                                    contributions[f'{feature}_isolation_forest'] = feature_importance[i]
                    
                    elif model_name == 'local_outlier_factor':
                        # For LOF, use distance to neighbors
                        if hasattr(model, '_fit_X') and hasattr(model, '_distances'):
                            # Get distances to neighbors
                            distances = model._distances[idx]
                            # Calculate contribution based on feature differences
                            for i, feature in enumerate(feature_names):
                                if i < x.shape[1]:
                                    feature_value = x.iloc[0, i]
                                    # Calculate how much this feature contributes to the distance
                                    feature_diff = np.abs(feature_value - model._fit_X[:, i].mean())
                                    contributions[f'{feature}_lof'] = feature_diff
                    
                    elif model_name == 'autoencoder':
                        # For Autoencoder, use reconstruction error per feature
                        if hasattr(model, 'predict'):
                            reconstruction = model.predict(x)
                            error_per_feature = np.power(x.values - reconstruction, 2)
                            for i, feature in enumerate(feature_names):
                                if i < error_per_feature.shape[1]:
                                    contributions[f'{feature}_autoencoder'] = error_per_feature[0, i]
            
            return contributions
            
        except Exception as e:
            logger.error(f"Error getting unsupervised contributions: {str(e)}")
            return {}
    
    def _get_supervised_contributions(self, idx, X, supervised_results):
        """
        Get feature contributions from supervised models
        
        Args:
            idx (int): Transaction index
            X (DataFrame): Feature data
            supervised_results (dict): Supervised model results
            
        Returns:
            dict: Feature contributions
        """
        try:
            contributions = {}
            
            # Process each supervised model
            for model_name, model_data in supervised_results.items():
                if 'model' in model_data and 'feature_names' in model_data:
                    model = model_data['model']
                    feature_names = model_data['feature_names']
                    
                    # Get feature values for this transaction
                    x = X.loc[idx:idx+1, feature_names]
                    
                    # Calculate SHAP values if available
                    if 'shap_values' in model_data:
                        shap_values = model_data['shap_values']
                        if isinstance(shap_values, list):
                            # For multi-class SHAP values
                            if len(shap_values) > 1:
                                shap_vals = shap_values[1][idx]  # Use positive class
                            else:
                                shap_vals = shap_values[0][idx]
                        else:
                            shap_vals = shap_values[idx]
                        
                        for i, feature in enumerate(feature_names):
                            if i < len(shap_vals):
                                contributions[f'{feature}_{model_name}'] = shap_vals[i]
                    
                    # Use feature importance as fallback
                    elif 'feature_importance' in model_data:
                        importance_df = model_data['feature_importance']
                        for _, row in importance_df.iterrows():
                            feature = row['feature']
                            importance = row['importance']
                            contributions[f'{feature}_{model_name}'] = importance
            
            return contributions
            
        except Exception as e:
            logger.error(f"Error getting supervised contributions: {str(e)}")
            return {}
    
    def _get_rule_contributions(self, idx, row, rule_results):
        """
        Get feature contributions from rule-based model
        
        Args:
            idx (int): Transaction index
            row (Series): Transaction data
            rule_results (dict): Rule model results
            
        Returns:
            dict: Feature contributions
        """
        try:
            contributions = {}
            
            # Get violated rules for this transaction
            if 'violated_rule_names' in rule_results and idx < len(rule_results['violated_rule_names']):
                violated_rules = rule_results['violated_rule_names'][idx]
                
                # Get rule weights
                rule_weights = rule_results.get('rule_weights', {})
                
                # Map rules to features
                rule_to_features = {
                    'high_amount': ['amount'],
                    'unusual_amount_for_sender': ['amount', 'sender_id'],
                    'unusual_amount_for_receiver': ['amount', 'receiver_id'],
                    'round_amount': ['amount'],
                    'high_frequency_sender': ['sender_id', 'timestamp'],
                    'high_frequency_receiver': ['receiver_id', 'timestamp'],
                    'rapid_succession': ['timestamp'],
                    'cross_border': ['sender_location', 'receiver_location'],
                    'high_risk_country': ['sender_location', 'receiver_location'],
                    'unusual_location_for_sender': ['sender_location'],
                    'unusual_hour': ['timestamp'],
                    'weekend': ['timestamp'],
                    'new_sender': ['sender_id'],
                    'new_receiver': ['receiver_id'],
                    'sanctions_check': ['sender_id', 'receiver_id'],
                    'tax_compliance': ['sender_id', 'receiver_id'],
                    'bank_verification': ['sender_id', 'receiver_id'],
                    'identity_verification': ['sender_id', 'receiver_id']
                }
                
                # Calculate contributions based on violated rules
                for rule in violated_rules:
                    weight = rule_weights.get(rule, 1.0)
                    features = rule_to_features.get(rule, [])
                    
                    for feature in features:
                        if feature in row:
                            contributions[f'{feature}_rule'] = contributions.get(f'{feature}_rule', 0) + weight
            
            return contributions
            
        except Exception as e:
            logger.error(f"Error getting rule contributions: {str(e)}")
            return {}
    
    def _generate_text_explanation(self, explanation, row):
        """
        Generate natural language explanation
        
        Args:
            explanation (dict): Explanation data
            row (Series): Transaction data
            
        Returns:
            str: Text explanation
        """
        try:
            # Start with risk level
            risk_score = explanation['risk_score']
            
            if risk_score >= 0.9:
                risk_level = "critical"
            elif risk_score >= 0.7:
                risk_level = "high"
            elif risk_score >= 0.5:
                risk_level = "medium"
            else:
                risk_level = "low"
            
            text = f"This transaction has a {risk_level} risk score of {risk_score:.2f}. "
            
            # Add top contributing factors
            if explanation['top_factors']:
                top_factors = explanation['top_factors'][:3]  # Top 3 factors
                factor_names = [factor[0].split('_')[0] for factor in top_factors]
                
                if len(factor_names) == 1:
                    text += f"The primary factor is {factor_names[0]}. "
                elif len(factor_names) == 2:
                    text += f"The main factors are {factor_names[0]} and {factor_names[1]}. "
                else:
                    text += f"The main factors are {', '.join(factor_names[:-1])}, and {factor_names[-1]}. "
            
            # Add rule violations
            if explanation['rule_violations']:
                if len(explanation['rule_violations']) == 1:
                    text += f"It violates the rule: {explanation['rule_violations'][0]}. "
                else:
                    text += f"It violates {len(explanation['rule_violations'])} rules: {', '.join(explanation['rule_violations'][:3])}. "
            
            # Add transaction details
            if 'amount' in row:
                text += f"The transaction amount is {row['amount']}. "
            
            if 'sender_id' in row and 'receiver_id' in row:
                text += f"It involves sender {row['sender_id']} and receiver {row['receiver_id']}. "
            
            if 'timestamp' in row:
                text += f"The transaction occurred at {row['timestamp']}. "
            
            # Add AI-generated explanation if APIs are available
            ai_explanation = ""
            if is_api_available('gemini'):
                # Here you would call Gemini API to generate explanation
                # For now, add a placeholder
                ai_explanation = "AI analysis pending implementation. "
            elif is_api_available('openai'):
                # Here you would call OpenAI API to generate explanation
                # For now, add a placeholder
                ai_explanation = "AI analysis pending implementation. "
            
            if ai_explanation:
                text += f"AI analysis: {ai_explanation}"
            
            # Add recommendation
            if risk_score >= 0.7:
                text += "This transaction should be reviewed immediately."
            elif risk_score >= 0.5:
                text += "This transaction should be reviewed."
            else:
                text += "This transaction appears to be low risk."
            
            return text
            
        except Exception as e:
            logger.error(f"Error generating text explanation: {str(e)}")
            return "Unable to generate explanation."
    
    def get_explanation(self, transaction_id):
        """
        Get explanation for a specific transaction
        
        Args:
            transaction_id (str): Transaction ID
            
        Returns:
            dict: Explanation for the transaction
        """
        if not self.fitted:
            raise ValueError("Explanations not generated. Call generate_explanations first.")
        
        # Find explanation by transaction ID
        for idx, explanation in self.explanations.items():
            if explanation.get('transaction_id') == transaction_id:
                return explanation
        
        return None
    
    def plot_feature_importance(self, model_name=None, top_n=20):
        """
        Plot feature importance for a model
        
        Args:
            model_name (str, optional): Name of the model
            top_n (int): Number of top features to show
        """
        try:
            # Aggregate feature contributions across all explanations
            feature_contributions = {}
            
            for explanation in self.explanations.values():
                if 'feature_contributions' in explanation:
                    for feature, contribution in explanation['feature_contributions'].items():
                        if model_name is None or model_name in feature:
                            feature_contributions[feature] = feature_contributions.get(feature, 0) + abs(contribution)
            
            if not feature_contributions:
                logger.warning("No feature contributions found")
                return None
            
            # Create DataFrame
            importance_df = pd.DataFrame({
                'feature': list(feature_contributions.keys()),
                'importance': list(feature_contributions.values())
            }).sort_values('importance', ascending=False)
            
            # Get top features
            top_features = importance_df.head(top_n)
            
            # Create plot
            plt.figure(figsize=(10, 8))
            sns.barplot(x='importance', y='feature', data=top_features)
            plt.title(f'Top {top_n} Feature Importance')
            plt.tight_layout()
            
            return plt.gcf()
            
        except Exception as e:
            logger.error(f"Error plotting feature importance: {str(e)}")
            raise
    
    def plot_shap_summary(self, model_name, X):
        """
        Plot SHAP summary for a model
        
        Args:
            model_name (str): Name of the model
            X (DataFrame): Feature data
        """
        try:
            # This would require access to the model and SHAP values
            # For demonstration, we'll create a placeholder plot
            
            plt.figure(figsize=(10, 8))
            plt.title(f'SHAP Summary for {model_name}')
            plt.text(0.5, 0.5, 'SHAP summary plot would be displayed here', 
                    ha='center', va='center', fontsize=12)
            plt.tight_layout()
            
            return plt.gcf()
            
        except Exception as e:
            logger.error(f"Error plotting SHAP summary: {str(e)}")
            raise
    
    def plot_decision_path(self, model_name, X, idx):
        """
        Plot decision path for a tree-based model
        
        Args:
            model_name (str): Name of the model
            X (DataFrame): Feature data
            idx (int): Transaction index
        """
        try:
            # This would require access to the model
            # For demonstration, we'll create a placeholder plot
            
            plt.figure(figsize=(10, 8))
            plt.title(f'Decision Path for {model_name}')
            plt.text(0.5, 0.5, 'Decision path would be displayed here', 
                    ha='center', va='center', fontsize=12)
            plt.tight_layout()
            
            return plt.gcf()
            
        except Exception as e:
            logger.error(f"Error plotting decision path: {str(e)}")
            raise
    
    def get_counterfactual_explanation(self, idx, X, model_name, target_class=0):
        """
        Generate counterfactual explanation
        
        Args:
            idx (int): Transaction index
            X (DataFrame): Feature data
            model_name (str): Name of the model
            target_class (int): Target class (0 for non-fraud, 1 for fraud)
            
        Returns:
            dict: Counterfactual explanation
        """
        try:
            # This is a simplified implementation
            # In practice, you would use more sophisticated methods
            
            # Get original prediction
            original_row = X.loc[idx:idx+1]
            
            # Generate counterfactual by modifying features
            counterfactual = original_row.copy()
            
            # Modify top features to change prediction
            # This is a placeholder implementation
            for col in counterfactual.columns:
                if col in ['amount']:
                    # Reduce amount by 50%
                    counterfactual[col] = counterfactual[col] * 0.5
            
            return {
                'original': original_row.to_dict('records')[0],
                'counterfactual': counterfactual.to_dict('records')[0],
                'changes': {
                    'amount': 'Reduced by 50%'
                }
            }
            
        except Exception as e:
            logger.error(f"Error generating counterfactual explanation: {str(e)}")
            return {}

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\analysis\risk_scorer.py ===
"""
Risk Scorer Module
Implements risk scoring for fraud detection
"""
import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
from sklearn.calibration import CalibratedClassifierCV
import warnings
import logging
from typing import Dict, List, Tuple, Union
warnings.filterwarnings('ignore')
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class RiskScorer:
    """
    Class for calculating risk scores from multiple models
    Implements weighted combination of algorithm outputs
    """
    
    def __init__(self, method='weighted_average', custom_weights=None):
        """
        Initialize RiskScorer
        
        Args:
            method (str): Scoring method ('weighted_average', 'maximum', 'custom')
            custom_weights (dict, optional): Custom weights for models
        """
        # Convert method to lowercase and replace spaces with underscores
        self.method = method.lower().replace(" ", "_")
        self.custom_weights = custom_weights or {
            'unsupervised': 0.4,
            'supervised': 0.4,
            'rule': 0.2
        }
        self.scalers = {}
        self.calibrators = {}
        self.fitted = False
    
    def calculate_scores(self, model_results, df):
        """
        Calculate risk scores from model results
        
        Args:
            model_results (dict): Results from different models
            df (DataFrame): Original data
            
        Returns:
            DataFrame: Risk scores
        """
        try:
            # Initialize result DataFrame
            result_df = df.copy()
            
            # Extract scores from each model type
            unsupervised_scores = None
            supervised_scores = None
            rule_scores = None
            
            # Process unsupervised model results
            if 'unsupervised' in model_results:
                unsupervised_scores = self._process_unsupervised_results(model_results['unsupervised'])
            
            # Process supervised model results
            if 'supervised' in model_results:
                supervised_scores = self._process_supervised_results(model_results['supervised'])
            
            # Process rule-based results
            if 'rule' in model_results:
                rule_scores = self._process_rule_results(model_results['rule'])
            
            # Combine scores based on method
            if self.method == 'weighted_average':
                combined_scores = self._weighted_average_combination(
                    unsupervised_scores, supervised_scores, rule_scores
                )
            elif self.method == 'maximum':
                combined_scores = self._maximum_combination(
                    unsupervised_scores, supervised_scores, rule_scores
                )
            elif self.method == 'custom':
                combined_scores = self._custom_combination(
                    unsupervised_scores, supervised_scores, rule_scores
                )
            else:
                raise ValueError(f"Unknown scoring method: {self.method}")
            
            # Add combined scores to result
            result_df['risk_score'] = combined_scores
            
            # Add individual model scores
            if unsupervised_scores is not None:
                result_df['unsupervised_score'] = unsupervised_scores
            
            if supervised_scores is not None:
                result_df['supervised_score'] = supervised_scores
            
            if rule_scores is not None:
                result_df['rule_score'] = rule_scores
            
            # Calculate percentile rank
            result_df['risk_percentile'] = result_df['risk_score'].rank(pct=True)
            
            # Calculate risk level
            result_df['risk_level'] = pd.cut(
                result_df['risk_percentile'],
                bins=[0, 0.7, 0.9, 0.95, 1.0],
                labels=['Low', 'Medium', 'High', 'Critical']
            )
            
            self.fitted = True
            return result_df
            
        except Exception as e:
            logger.error(f"Error calculating risk scores: {str(e)}")
            raise
    
    def _process_unsupervised_results(self, unsupervised_results):
        """
        Process unsupervised model results
        
        Args:
            unsupervised_results (dict): Results from unsupervised models
            
        Returns:
            array: Combined unsupervised scores
        """
        try:
            # Collect scores from all unsupervised models
            all_scores = []
            model_names = []
            
            for model_name, model_data in unsupervised_results.items():
                if 'scores' in model_data:
                    scores = model_data['scores']
                    all_scores.append(scores)
                    model_names.append(model_name)
            
            if not all_scores:
                return None
            
            # Convert to numpy array
            scores_array = np.array(all_scores).T
            
            # Calculate average score
            avg_scores = np.mean(scores_array, axis=1)
            
            # Normalize to 0-1 range
            scaler = MinMaxScaler()
            normalized_scores = scaler.fit_transform(avg_scores.reshape(-1, 1)).flatten()
            
            # Store scaler
            self.scalers['unsupervised'] = scaler
            
            return normalized_scores
            
        except Exception as e:
            logger.error(f"Error processing unsupervised results: {str(e)}")
            raise
    
    def _process_supervised_results(self, supervised_results):
        """
        Process supervised model results
        
        Args:
            supervised_results (dict): Results from supervised models
            
        Returns:
            array: Combined supervised scores
        """
        try:
            # Collect probabilities from all supervised models
            all_probabilities = []
            model_names = []
            
            for model_name, model_data in supervised_results.items():
                if 'probabilities' in model_data:
                    probabilities = model_data['probabilities']
                    all_probabilities.append(probabilities)
                    model_names.append(model_name)
            
            if not all_probabilities:
                return None
            
            # Convert to numpy array
            probabilities_array = np.array(all_probabilities).T
            
            # Calculate average probability
            avg_probabilities = np.mean(probabilities_array, axis=1)
            
            # Apply calibration if available
            if 'supervised' in self.calibrators:
                calibrated_scores = self.calibrators['supervised'].predict_proba(avg_probabilities.reshape(-1, 1))[:, 1]
            else:
                calibrated_scores = avg_probabilities
            
            # Store calibrator
            self.calibrators['supervised'] = CalibratedClassifierCV(
                method='isotonic', cv='prefit'
            )
            
            return calibrated_scores
            
        except Exception as e:
            logger.error(f"Error processing supervised results: {str(e)}")
            raise
    
    def _process_rule_results(self, rule_results):
        """
        Process rule-based results
        
        Args:
            rule_results (dict): Results from rule engine
            
        Returns:
            array: Combined rule scores
        """
        try:
            if 'normalized_scores' in rule_results:
                rule_scores = rule_results['normalized_scores']
                # Ensure rule_scores is a numpy array
                if not isinstance(rule_scores, np.ndarray):
                    rule_scores = np.array(rule_scores)
            else:
                return None
            
            # No calibration for now to avoid the reshape issue
            calibrated_scores = rule_scores
            
            # Store calibrator
            self.calibrators['rule'] = CalibratedClassifierCV(
                method='isotonic', cv='prefit'
            )
            
            return calibrated_scores
            
        except Exception as e:
            logger.error(f"Error processing rule results: {str(e)}")
            raise
    
    def _weighted_average_combination(self, unsupervised_scores, supervised_scores, rule_scores):
        """
        Combine scores using weighted average
        
        Args:
            unsupervised_scores (array): Unsupervised model scores
            supervised_scores (array): Supervised model scores
            rule_scores (array): Rule-based scores
            
        Returns:
            array: Combined scores
        """
        try:
            # Get weights
            unsupervised_weight = self.custom_weights.get('unsupervised', 0.4)
            supervised_weight = self.custom_weights.get('supervised', 0.4)
            rule_weight = self.custom_weights.get('rule', 0.2)
            
            # Normalize weights based on available scores
            available_weights = []
            if unsupervised_scores is not None:
                available_weights.append(unsupervised_weight)
            if supervised_scores is not None:
                available_weights.append(supervised_weight)
            if rule_scores is not None:
                available_weights.append(rule_weight)
            
            total_weight = sum(available_weights)
            if total_weight > 0:
                if unsupervised_scores is not None:
                    unsupervised_weight /= total_weight
                if supervised_scores is not None:
                    supervised_weight /= total_weight
                if rule_scores is not None:
                    rule_weight /= total_weight
            
            # Determine the length of the combined scores array
            if unsupervised_scores is not None:
                length = len(unsupervised_scores)
            elif supervised_scores is not None:
                length = len(supervised_scores)
            elif rule_scores is not None:
                length = len(rule_scores)
            else:
                # If no scores available, return empty array
                return np.array([])
            
            # Initialize combined scores
            combined_scores = np.zeros(length)
            
            # Add weighted scores
            if unsupervised_scores is not None:
                # Ensure unsupervised_scores is a numpy array
                if not isinstance(unsupervised_scores, np.ndarray):
                    unsupervised_scores = np.array(unsupervised_scores)
                combined_scores += unsupervised_weight * unsupervised_scores
            
            if supervised_scores is not None:
                # Ensure supervised_scores is a numpy array
                if not isinstance(supervised_scores, np.ndarray):
                    supervised_scores = np.array(supervised_scores)
                combined_scores += supervised_weight * supervised_scores
            
            if rule_scores is not None:
                # Ensure rule_scores is a numpy array
                if not isinstance(rule_scores, np.ndarray):
                    rule_scores = np.array(rule_scores)
                combined_scores += rule_weight * rule_scores
            
            return combined_scores
            
        except Exception as e:
            logger.error(f"Error in weighted average combination: {str(e)}")
            raise
    
    def _maximum_combination(self, unsupervised_scores, supervised_scores, rule_scores):
        """
        Combine scores using maximum
        
        Args:
            unsupervised_scores (array): Unsupervised model scores
            supervised_scores (array): Supervised model scores
            rule_scores (array): Rule-based scores
            
        Returns:
            array: Combined scores
        """
        try:
            # Collect all available scores
            all_scores = []
            
            if unsupervised_scores is not None:
                all_scores.append(unsupervised_scores)
            
            if supervised_scores is not None:
                all_scores.append(supervised_scores)
            
            if rule_scores is not None:
                all_scores.append(rule_scores)
            
            if not all_scores:
                return np.array([])
            
            # Take maximum score for each transaction
            combined_scores = np.max(all_scores, axis=0)
            
            return combined_scores
            
        except Exception as e:
            logger.error(f"Error in maximum combination: {str(e)}")
            raise
    
    def _custom_combination(self, unsupervised_scores, supervised_scores, rule_scores):
        """
        Combine scores using custom method
        
        Args:
            unsupervised_scores (array): Unsupervised model scores
            supervised_scores (array): Supervised model scores
            rule_scores (array): Rule-based scores
            
        Returns:
            array: Combined scores
        """
        try:
            # Get custom weights
            unsupervised_weight = self.custom_weights.get('unsupervised', 0.4)
            supervised_weight = self.custom_weights.get('supervised', 0.4)
            rule_weight = self.custom_weights.get('rule', 0.2)
            
            # Apply non-linear transformation to supervised scores
            if supervised_scores is not None:
                # Emphasize high scores
                supervised_transformed = np.power(supervised_scores, 1.5)
            else:
                supervised_transformed = None
            
            # Apply non-linear transformation to rule scores
            if rule_scores is not None:
                # Emphasize high scores even more
                rule_transformed = np.power(rule_scores, 2.0)
            else:
                rule_transformed = None
            
            # Combine using weighted average with transformed scores
            return self._weighted_average_combination(
                unsupervised_scores, supervised_transformed, rule_transformed
            )
            
        except Exception as e:
            logger.error(f"Error in custom combination: {str(e)}")
            raise
    
    def update_weights(self, new_weights):
        """
        Update the weights for combining scores
        
        Args:
            new_weights (dict): New weights for models
        """
        self.custom_weights.update(new_weights)
        logger.info(f"Updated weights: {self.custom_weights}")
    
    def get_weights(self):
        """
        Get current weights
        
        Returns:
            dict: Current weights
        """
        return self.custom_weights.copy()
    
    def calibrate_scores(self, method='isotonic'):
        """
        Calibrate scores to better reflect true probabilities
        
        Args:
            method (str): Calibration method ('isotonic', 'sigmoid')
        """
        try:
            # Update calibrators
            for model_type in ['unsupervised', 'supervised', 'rule']:
                if model_type in self.calibrators:
                    self.calibrators[model_type] = CalibratedClassifierCV(
                        method=method, cv='prefit'
                    )
            
            logger.info(f"Updated calibrators with method: {method}")
            
        except Exception as e:
            logger.error(f"Error calibrating scores: {str(e)}")
            raise

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\analysis\__init__.py ===

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\analysis\.ipynb_checkpoints\explainability-checkpoint.py ===
"""
Explainability Module
Implements explainable AI techniques for fraud detection
"""

import pandas as pd
import numpy as np
import shap
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.inspection import permutation_importance, partial_dependence
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier, export_text, export_graphviz
import graphviz
import warnings
import logging
from typing import Dict, List, Tuple, Union

from fraud_detection_engine.utils.api_utils import is_api_available

warnings.filterwarnings('ignore')
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class Explainability:
    """
    Class for generating explanations for fraud detection predictions
    Implements SHAP, LIME, and other explainability techniques
    """
    
    def __init__(self, config=None):
        """
        Initialize Explainability
        
        Args:
            config (dict, optional): Configuration parameters
        """
        self.config = config or {}
        self.explainers = {}
        self.explanations = {}
        self.feature_names = {}
        self.fitted = False
    
    def generate_explanations(self, df, risk_scores, model_results):
        """
        Generate explanations for all transactions
        
        Args:
            df (DataFrame): Input data
            risk_scores (DataFrame): Risk scores
            model_results (dict): Results from different models
            
        Returns:
            dict: Explanations for each transaction
        """
        try:
            # Get feature columns
            feature_cols = [col for col in df.columns if col not in 
                           ['transaction_id', 'sender_id', 'receiver_id', 'fraud_flag']]
            
            # Prepare data
            X = df[feature_cols].fillna(0)
            
            # Store feature names
            self.feature_names = feature_cols
            
            # Initialize explanations dictionary
            explanations = {}
            
            # Generate explanations for each transaction
            for idx, row in df.iterrows():
                try:
                    # Get risk score
                    risk_score = risk_scores.loc[idx, 'risk_score']
                    
                    # Generate explanation only for high-risk transactions
                    if risk_score > 0.5:  # Threshold for explanation
                        explanation = self._generate_transaction_explanation(
                            idx, row, X, risk_score, model_results
                        )
                        explanations[idx] = explanation
                except Exception as e:
                    logger.warning(f"Error generating explanation for transaction {idx}: {str(e)}")
            
            self.explanations = explanations
            self.fitted = True
            
            logger.info(f"Generated explanations for {len(explanations)} transactions")
            return explanations
            
        except Exception as e:
            logger.error(f"Error generating explanations: {str(e)}")
            raise
    
    def _generate_transaction_explanation(self, idx, row, X, risk_score, model_results):
        """
        Generate explanation for a single transaction
        
        Args:
            idx (int): Transaction index
            row (Series): Transaction data
            X (DataFrame): Feature data
            risk_score (float): Risk score
            model_results (dict): Model results
            
        Returns:
            dict: Explanation for the transaction
        """
        try:
            explanation = {
                'transaction_id': row.get('transaction_id', idx),
                'risk_score': risk_score,
                'top_factors': [],
                'rule_violations': [],
                'model_predictions': {},
                'feature_contributions': {},
                'text_explanation': ''
            }
            
            # Get feature contributions from different models
            feature_contributions = {}
            
            # Process unsupervised models
            if 'unsupervised' in model_results:
                unsupervised_contributions = self._get_unsupervised_contributions(
                    idx, X, model_results['unsupervised']
                )
                feature_contributions.update(unsupervised_contributions)
                
                # Add model predictions
                for model_name, model_data in model_results['unsupervised'].items():
                    if 'scores' in model_data and idx < len(model_data['scores']):
                        explanation['model_predictions'][f'unsupervised_{model_name}'] = model_data['scores'][idx]
            
            # Process supervised models
            if 'supervised' in model_results:
                supervised_contributions = self._get_supervised_contributions(
                    idx, X, model_results['supervised']
                )
                feature_contributions.update(supervised_contributions)
                
                # Add model predictions
                for model_name, model_data in model_results['supervised'].items():
                    if 'probabilities' in model_data and idx < len(model_data['probabilities']):
                        explanation['model_predictions'][f'supervised_{model_name}'] = model_data['probabilities'][idx]
            
            # Process rule-based models
            if 'rule' in model_results:
                rule_contributions = self._get_rule_contributions(
                    idx, row, model_results['rule']
                )
                feature_contributions.update(rule_contributions)
                
                # Add rule violations
                if 'violated_rule_names' in model_results['rule'] and idx < len(model_results['rule']['violated_rule_names']):
                    explanation['rule_violations'] = model_results['rule']['violated_rule_names'][idx]
            
            # Aggregate feature contributions
            explanation['feature_contributions'] = feature_contributions
            
            # Get top contributing factors
            top_factors = sorted(
                feature_contributions.items(),
                key=lambda x: abs(x[1]),
                reverse=True
            )[:10]  # Top 10 factors
            
            explanation['top_factors'] = top_factors
            
            # Generate text explanation
            explanation['text_explanation'] = self._generate_text_explanation(
                explanation, row
            )
            
            return explanation
            
        except Exception as e:
            logger.error(f"Error generating explanation for transaction {idx}: {str(e)}")
            return {
                'transaction_id': row.get('transaction_id', idx),
                'risk_score': risk_score,
                'error': str(e)
            }
    
    def _get_unsupervised_contributions(self, idx, X, unsupervised_results):
        """
        Get feature contributions from unsupervised models
        
        Args:
            idx (int): Transaction index
            X (DataFrame): Feature data
            unsupervised_results (dict): Unsupervised model results
            
        Returns:
            dict: Feature contributions
        """
        try:
            contributions = {}
            
            # Process each unsupervised model
            for model_name, model_data in unsupervised_results.items():
                if 'model' in model_data and 'feature_names' in model_data:
                    model = model_data['model']
                    feature_names = model_data['feature_names']
                    
                    # Get feature values for this transaction
                    x = X.loc[idx:idx+1, feature_names]
                    
                    # Calculate feature contributions based on model type
                    if model_name == 'isolation_forest':
                        # For Isolation Forest, use feature importance
                        if hasattr(model, 'feature_importances_'):
                            feature_importance = model.feature_importances_
                            for i, feature in enumerate(feature_names):
                                if i < len(feature_importance):
                                    contributions[f'{feature}_isolation_forest'] = feature_importance[i]
                    
                    elif model_name == 'local_outlier_factor':
                        # For LOF, use distance to neighbors
                        if hasattr(model, '_fit_X') and hasattr(model, '_distances'):
                            # Get distances to neighbors
                            distances = model._distances[idx]
                            # Calculate contribution based on feature differences
                            for i, feature in enumerate(feature_names):
                                if i < x.shape[1]:
                                    feature_value = x.iloc[0, i]
                                    # Calculate how much this feature contributes to the distance
                                    feature_diff = np.abs(feature_value - model._fit_X[:, i].mean())
                                    contributions[f'{feature}_lof'] = feature_diff
                    
                    elif model_name == 'autoencoder':
                        # For Autoencoder, use reconstruction error per feature
                        if hasattr(model, 'predict'):
                            reconstruction = model.predict(x)
                            error_per_feature = np.power(x.values - reconstruction, 2)
                            for i, feature in enumerate(feature_names):
                                if i < error_per_feature.shape[1]:
                                    contributions[f'{feature}_autoencoder'] = error_per_feature[0, i]
            
            return contributions
            
        except Exception as e:
            logger.error(f"Error getting unsupervised contributions: {str(e)}")
            return {}
    
    def _get_supervised_contributions(self, idx, X, supervised_results):
        """
        Get feature contributions from supervised models
        
        Args:
            idx (int): Transaction index
            X (DataFrame): Feature data
            supervised_results (dict): Supervised model results
            
        Returns:
            dict: Feature contributions
        """
        try:
            contributions = {}
            
            # Process each supervised model
            for model_name, model_data in supervised_results.items():
                if 'model' in model_data and 'feature_names' in model_data:
                    model = model_data['model']
                    feature_names = model_data['feature_names']
                    
                    # Get feature values for this transaction
                    x = X.loc[idx:idx+1, feature_names]
                    
                    # Calculate SHAP values if available
                    if 'shap_values' in model_data:
                        shap_values = model_data['shap_values']
                        if isinstance(shap_values, list):
                            # For multi-class SHAP values
                            if len(shap_values) > 1:
                                shap_vals = shap_values[1][idx]  # Use positive class
                            else:
                                shap_vals = shap_values[0][idx]
                        else:
                            shap_vals = shap_values[idx]
                        
                        for i, feature in enumerate(feature_names):
                            if i < len(shap_vals):
                                contributions[f'{feature}_{model_name}'] = shap_vals[i]
                    
                    # Use feature importance as fallback
                    elif 'feature_importance' in model_data:
                        importance_df = model_data['feature_importance']
                        for _, row in importance_df.iterrows():
                            feature = row['feature']
                            importance = row['importance']
                            contributions[f'{feature}_{model_name}'] = importance
            
            return contributions
            
        except Exception as e:
            logger.error(f"Error getting supervised contributions: {str(e)}")
            return {}
    
    def _get_rule_contributions(self, idx, row, rule_results):
        """
        Get feature contributions from rule-based model
        
        Args:
            idx (int): Transaction index
            row (Series): Transaction data
            rule_results (dict): Rule model results
            
        Returns:
            dict: Feature contributions
        """
        try:
            contributions = {}
            
            # Get violated rules for this transaction
            if 'violated_rule_names' in rule_results and idx < len(rule_results['violated_rule_names']):
                violated_rules = rule_results['violated_rule_names'][idx]
                
                # Get rule weights
                rule_weights = rule_results.get('rule_weights', {})
                
                # Map rules to features
                rule_to_features = {
                    'high_amount': ['amount'],
                    'unusual_amount_for_sender': ['amount', 'sender_id'],
                    'unusual_amount_for_receiver': ['amount', 'receiver_id'],
                    'round_amount': ['amount'],
                    'high_frequency_sender': ['sender_id', 'timestamp'],
                    'high_frequency_receiver': ['receiver_id', 'timestamp'],
                    'rapid_succession': ['timestamp'],
                    'cross_border': ['sender_location', 'receiver_location'],
                    'high_risk_country': ['sender_location', 'receiver_location'],
                    'unusual_location_for_sender': ['sender_location'],
                    'unusual_hour': ['timestamp'],
                    'weekend': ['timestamp'],
                    'new_sender': ['sender_id'],
                    'new_receiver': ['receiver_id'],
                    'sanctions_check': ['sender_id', 'receiver_id'],
                    'tax_compliance': ['sender_id', 'receiver_id'],
                    'bank_verification': ['sender_id', 'receiver_id'],
                    'identity_verification': ['sender_id', 'receiver_id']
                }
                
                # Calculate contributions based on violated rules
                for rule in violated_rules:
                    weight = rule_weights.get(rule, 1.0)
                    features = rule_to_features.get(rule, [])
                    
                    for feature in features:
                        if feature in row:
                            contributions[f'{feature}_rule'] = contributions.get(f'{feature}_rule', 0) + weight
            
            return contributions
            
        except Exception as e:
            logger.error(f"Error getting rule contributions: {str(e)}")
            return {}
    
    def _generate_text_explanation(self, explanation, row):
        """
        Generate natural language explanation
        
        Args:
            explanation (dict): Explanation data
            row (Series): Transaction data
            
        Returns:
            str: Text explanation
        """
        try:
            # Start with risk level
            risk_score = explanation['risk_score']
            
            if risk_score >= 0.9:
                risk_level = "critical"
            elif risk_score >= 0.7:
                risk_level = "high"
            elif risk_score >= 0.5:
                risk_level = "medium"
            else:
                risk_level = "low"
            
            text = f"This transaction has a {risk_level} risk score of {risk_score:.2f}. "
            
            # Add top contributing factors
            if explanation['top_factors']:
                top_factors = explanation['top_factors'][:3]  # Top 3 factors
                factor_names = [factor[0].split('_')[0] for factor in top_factors]
                
                if len(factor_names) == 1:
                    text += f"The primary factor is {factor_names[0]}. "
                elif len(factor_names) == 2:
                    text += f"The main factors are {factor_names[0]} and {factor_names[1]}. "
                else:
                    text += f"The main factors are {', '.join(factor_names[:-1])}, and {factor_names[-1]}. "
            
            # Add rule violations
            if explanation['rule_violations']:
                if len(explanation['rule_violations']) == 1:
                    text += f"It violates the rule: {explanation['rule_violations'][0]}. "
                else:
                    text += f"It violates {len(explanation['rule_violations'])} rules: {', '.join(explanation['rule_violations'][:3])}. "
            
            # Add transaction details
            if 'amount' in row:
                text += f"The transaction amount is {row['amount']}. "
            
            if 'sender_id' in row and 'receiver_id' in row:
                text += f"It involves sender {row['sender_id']} and receiver {row['receiver_id']}. "
            
            if 'timestamp' in row:
                text += f"The transaction occurred at {row['timestamp']}. "
            
            # Add AI-generated explanation if APIs are available
            ai_explanation = ""
            if is_api_available('gemini'):
                # Here you would call Gemini API to generate explanation
                # For now, add a placeholder
                ai_explanation = "AI analysis pending implementation. "
            elif is_api_available('openai'):
                # Here you would call OpenAI API to generate explanation
                # For now, add a placeholder
                ai_explanation = "AI analysis pending implementation. "
            
            if ai_explanation:
                text += f"AI analysis: {ai_explanation}"
            
            # Add recommendation
            if risk_score >= 0.7:
                text += "This transaction should be reviewed immediately."
            elif risk_score >= 0.5:
                text += "This transaction should be reviewed."
            else:
                text += "This transaction appears to be low risk."
            
            return text
            
        except Exception as e:
            logger.error(f"Error generating text explanation: {str(e)}")
            return "Unable to generate explanation."
    
    def get_explanation(self, transaction_id):
        """
        Get explanation for a specific transaction
        
        Args:
            transaction_id (str): Transaction ID
            
        Returns:
            dict: Explanation for the transaction
        """
        if not self.fitted:
            raise ValueError("Explanations not generated. Call generate_explanations first.")
        
        # Find explanation by transaction ID
        for idx, explanation in self.explanations.items():
            if explanation.get('transaction_id') == transaction_id:
                return explanation
        
        return None
    
    def plot_feature_importance(self, model_name=None, top_n=20):
        """
        Plot feature importance for a model
        
        Args:
            model_name (str, optional): Name of the model
            top_n (int): Number of top features to show
        """
        try:
            # Aggregate feature contributions across all explanations
            feature_contributions = {}
            
            for explanation in self.explanations.values():
                if 'feature_contributions' in explanation:
                    for feature, contribution in explanation['feature_contributions'].items():
                        if model_name is None or model_name in feature:
                            feature_contributions[feature] = feature_contributions.get(feature, 0) + abs(contribution)
            
            if not feature_contributions:
                logger.warning("No feature contributions found")
                return None
            
            # Create DataFrame
            importance_df = pd.DataFrame({
                'feature': list(feature_contributions.keys()),
                'importance': list(feature_contributions.values())
            }).sort_values('importance', ascending=False)
            
            # Get top features
            top_features = importance_df.head(top_n)
            
            # Create plot
            plt.figure(figsize=(10, 8))
            sns.barplot(x='importance', y='feature', data=top_features)
            plt.title(f'Top {top_n} Feature Importance')
            plt.tight_layout()
            
            return plt.gcf()
            
        except Exception as e:
            logger.error(f"Error plotting feature importance: {str(e)}")
            raise
    
    def plot_shap_summary(self, model_name, X):
        """
        Plot SHAP summary for a model
        
        Args:
            model_name (str): Name of the model
            X (DataFrame): Feature data
        """
        try:
            # This would require access to the model and SHAP values
            # For demonstration, we'll create a placeholder plot
            
            plt.figure(figsize=(10, 8))
            plt.title(f'SHAP Summary for {model_name}')
            plt.text(0.5, 0.5, 'SHAP summary plot would be displayed here', 
                    ha='center', va='center', fontsize=12)
            plt.tight_layout()
            
            return plt.gcf()
            
        except Exception as e:
            logger.error(f"Error plotting SHAP summary: {str(e)}")
            raise
    
    def plot_decision_path(self, model_name, X, idx):
        """
        Plot decision path for a tree-based model
        
        Args:
            model_name (str): Name of the model
            X (DataFrame): Feature data
            idx (int): Transaction index
        """
        try:
            # This would require access to the model
            # For demonstration, we'll create a placeholder plot
            
            plt.figure(figsize=(10, 8))
            plt.title(f'Decision Path for {model_name}')
            plt.text(0.5, 0.5, 'Decision path would be displayed here', 
                    ha='center', va='center', fontsize=12)
            plt.tight_layout()
            
            return plt.gcf()
            
        except Exception as e:
            logger.error(f"Error plotting decision path: {str(e)}")
            raise
    
    def get_counterfactual_explanation(self, idx, X, model_name, target_class=0):
        """
        Generate counterfactual explanation
        
        Args:
            idx (int): Transaction index
            X (DataFrame): Feature data
            model_name (str): Name of the model
            target_class (int): Target class (0 for non-fraud, 1 for fraud)
            
        Returns:
            dict: Counterfactual explanation
        """
        try:
            # This is a simplified implementation
            # In practice, you would use more sophisticated methods
            
            # Get original prediction
            original_row = X.loc[idx:idx+1]
            
            # Generate counterfactual by modifying features
            counterfactual = original_row.copy()
            
            # Modify top features to change prediction
            # This is a placeholder implementation
            for col in counterfactual.columns:
                if col in ['amount']:
                    # Reduce amount by 50%
                    counterfactual[col] = counterfactual[col] * 0.5
            
            return {
                'original': original_row.to_dict('records')[0],
                'counterfactual': counterfactual.to_dict('records')[0],
                'changes': {
                    'amount': 'Reduced by 50%'
                }
            }
            
        except Exception as e:
            logger.error(f"Error generating counterfactual explanation: {str(e)}")
            return {}

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\analysis\.ipynb_checkpoints\risk_scorer-checkpoint.py ===
"""
Risk Scorer Module
Implements risk scoring for fraud detection
"""
import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
from sklearn.calibration import CalibratedClassifierCV
import warnings
import logging
from typing import Dict, List, Tuple, Union
warnings.filterwarnings('ignore')
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class RiskScorer:
    """
    Class for calculating risk scores from multiple models
    Implements weighted combination of algorithm outputs
    """
    
    def __init__(self, method='weighted_average', custom_weights=None):
        """
        Initialize RiskScorer
        
        Args:
            method (str): Scoring method ('weighted_average', 'maximum', 'custom')
            custom_weights (dict, optional): Custom weights for models
        """
        # Convert method to lowercase and replace spaces with underscores
        self.method = method.lower().replace(" ", "_")
        self.custom_weights = custom_weights or {
            'unsupervised': 0.4,
            'supervised': 0.4,
            'rule': 0.2
        }
        self.scalers = {}
        self.calibrators = {}
        self.fitted = False
    
    def calculate_scores(self, model_results, df):
        """
        Calculate risk scores from model results
        
        Args:
            model_results (dict): Results from different models
            df (DataFrame): Original data
            
        Returns:
            DataFrame: Risk scores
        """
        try:
            # Initialize result DataFrame
            result_df = df.copy()
            
            # Extract scores from each model type
            unsupervised_scores = None
            supervised_scores = None
            rule_scores = None
            
            # Process unsupervised model results
            if 'unsupervised' in model_results:
                unsupervised_scores = self._process_unsupervised_results(model_results['unsupervised'])
            
            # Process supervised model results
            if 'supervised' in model_results:
                supervised_scores = self._process_supervised_results(model_results['supervised'])
            
            # Process rule-based results
            if 'rule' in model_results:
                rule_scores = self._process_rule_results(model_results['rule'])
            
            # Combine scores based on method
            if self.method == 'weighted_average':
                combined_scores = self._weighted_average_combination(
                    unsupervised_scores, supervised_scores, rule_scores
                )
            elif self.method == 'maximum':
                combined_scores = self._maximum_combination(
                    unsupervised_scores, supervised_scores, rule_scores
                )
            elif self.method == 'custom':
                combined_scores = self._custom_combination(
                    unsupervised_scores, supervised_scores, rule_scores
                )
            else:
                raise ValueError(f"Unknown scoring method: {self.method}")
            
            # Add combined scores to result
            result_df['risk_score'] = combined_scores
            
            # Add individual model scores
            if unsupervised_scores is not None:
                result_df['unsupervised_score'] = unsupervised_scores
            
            if supervised_scores is not None:
                result_df['supervised_score'] = supervised_scores
            
            if rule_scores is not None:
                result_df['rule_score'] = rule_scores
            
            # Calculate percentile rank
            result_df['risk_percentile'] = result_df['risk_score'].rank(pct=True)
            
            # Calculate risk level
            result_df['risk_level'] = pd.cut(
                result_df['risk_percentile'],
                bins=[0, 0.7, 0.9, 0.95, 1.0],
                labels=['Low', 'Medium', 'High', 'Critical']
            )
            
            self.fitted = True
            return result_df
            
        except Exception as e:
            logger.error(f"Error calculating risk scores: {str(e)}")
            raise
    
    def _process_unsupervised_results(self, unsupervised_results):
        """
        Process unsupervised model results
        
        Args:
            unsupervised_results (dict): Results from unsupervised models
            
        Returns:
            array: Combined unsupervised scores
        """
        try:
            # Collect scores from all unsupervised models
            all_scores = []
            model_names = []
            
            for model_name, model_data in unsupervised_results.items():
                if 'scores' in model_data:
                    scores = model_data['scores']
                    all_scores.append(scores)
                    model_names.append(model_name)
            
            if not all_scores:
                return None
            
            # Convert to numpy array
            scores_array = np.array(all_scores).T
            
            # Calculate average score
            avg_scores = np.mean(scores_array, axis=1)
            
            # Normalize to 0-1 range
            scaler = MinMaxScaler()
            normalized_scores = scaler.fit_transform(avg_scores.reshape(-1, 1)).flatten()
            
            # Store scaler
            self.scalers['unsupervised'] = scaler
            
            return normalized_scores
            
        except Exception as e:
            logger.error(f"Error processing unsupervised results: {str(e)}")
            raise
    
    def _process_supervised_results(self, supervised_results):
        """
        Process supervised model results
        
        Args:
            supervised_results (dict): Results from supervised models
            
        Returns:
            array: Combined supervised scores
        """
        try:
            # Collect probabilities from all supervised models
            all_probabilities = []
            model_names = []
            
            for model_name, model_data in supervised_results.items():
                if 'probabilities' in model_data:
                    probabilities = model_data['probabilities']
                    all_probabilities.append(probabilities)
                    model_names.append(model_name)
            
            if not all_probabilities:
                return None
            
            # Convert to numpy array
            probabilities_array = np.array(all_probabilities).T
            
            # Calculate average probability
            avg_probabilities = np.mean(probabilities_array, axis=1)
            
            # Apply calibration if available
            if 'supervised' in self.calibrators:
                calibrated_scores = self.calibrators['supervised'].predict_proba(avg_probabilities.reshape(-1, 1))[:, 1]
            else:
                calibrated_scores = avg_probabilities
            
            # Store calibrator
            self.calibrators['supervised'] = CalibratedClassifierCV(
                method='isotonic', cv='prefit'
            )
            
            return calibrated_scores
            
        except Exception as e:
            logger.error(f"Error processing supervised results: {str(e)}")
            raise
    
    def _process_rule_results(self, rule_results):
        """
        Process rule-based results
        
        Args:
            rule_results (dict): Results from rule engine
            
        Returns:
            array: Combined rule scores
        """
        try:
            if 'normalized_scores' in rule_results:
                rule_scores = rule_results['normalized_scores']
                # Ensure rule_scores is a numpy array
                if not isinstance(rule_scores, np.ndarray):
                    rule_scores = np.array(rule_scores)
            else:
                return None
            
            # No calibration for now to avoid the reshape issue
            calibrated_scores = rule_scores
            
            # Store calibrator
            self.calibrators['rule'] = CalibratedClassifierCV(
                method='isotonic', cv='prefit'
            )
            
            return calibrated_scores
            
        except Exception as e:
            logger.error(f"Error processing rule results: {str(e)}")
            raise
    
    def _weighted_average_combination(self, unsupervised_scores, supervised_scores, rule_scores):
        """
        Combine scores using weighted average
        
        Args:
            unsupervised_scores (array): Unsupervised model scores
            supervised_scores (array): Supervised model scores
            rule_scores (array): Rule-based scores
            
        Returns:
            array: Combined scores
        """
        try:
            # Get weights
            unsupervised_weight = self.custom_weights.get('unsupervised', 0.4)
            supervised_weight = self.custom_weights.get('supervised', 0.4)
            rule_weight = self.custom_weights.get('rule', 0.2)
            
            # Normalize weights based on available scores
            available_weights = []
            if unsupervised_scores is not None:
                available_weights.append(unsupervised_weight)
            if supervised_scores is not None:
                available_weights.append(supervised_weight)
            if rule_scores is not None:
                available_weights.append(rule_weight)
            
            total_weight = sum(available_weights)
            if total_weight > 0:
                if unsupervised_scores is not None:
                    unsupervised_weight /= total_weight
                if supervised_scores is not None:
                    supervised_weight /= total_weight
                if rule_scores is not None:
                    rule_weight /= total_weight
            
            # Determine the length of the combined scores array
            if unsupervised_scores is not None:
                length = len(unsupervised_scores)
            elif supervised_scores is not None:
                length = len(supervised_scores)
            elif rule_scores is not None:
                length = len(rule_scores)
            else:
                # If no scores available, return empty array
                return np.array([])
            
            # Initialize combined scores
            combined_scores = np.zeros(length)
            
            # Add weighted scores
            if unsupervised_scores is not None:
                # Ensure unsupervised_scores is a numpy array
                if not isinstance(unsupervised_scores, np.ndarray):
                    unsupervised_scores = np.array(unsupervised_scores)
                combined_scores += unsupervised_weight * unsupervised_scores
            
            if supervised_scores is not None:
                # Ensure supervised_scores is a numpy array
                if not isinstance(supervised_scores, np.ndarray):
                    supervised_scores = np.array(supervised_scores)
                combined_scores += supervised_weight * supervised_scores
            
            if rule_scores is not None:
                # Ensure rule_scores is a numpy array
                if not isinstance(rule_scores, np.ndarray):
                    rule_scores = np.array(rule_scores)
                combined_scores += rule_weight * rule_scores
            
            return combined_scores
            
        except Exception as e:
            logger.error(f"Error in weighted average combination: {str(e)}")
            raise
    
    def _maximum_combination(self, unsupervised_scores, supervised_scores, rule_scores):
        """
        Combine scores using maximum
        
        Args:
            unsupervised_scores (array): Unsupervised model scores
            supervised_scores (array): Supervised model scores
            rule_scores (array): Rule-based scores
            
        Returns:
            array: Combined scores
        """
        try:
            # Collect all available scores
            all_scores = []
            
            if unsupervised_scores is not None:
                all_scores.append(unsupervised_scores)
            
            if supervised_scores is not None:
                all_scores.append(supervised_scores)
            
            if rule_scores is not None:
                all_scores.append(rule_scores)
            
            if not all_scores:
                return np.array([])
            
            # Take maximum score for each transaction
            combined_scores = np.max(all_scores, axis=0)
            
            return combined_scores
            
        except Exception as e:
            logger.error(f"Error in maximum combination: {str(e)}")
            raise
    
    def _custom_combination(self, unsupervised_scores, supervised_scores, rule_scores):
        """
        Combine scores using custom method
        
        Args:
            unsupervised_scores (array): Unsupervised model scores
            supervised_scores (array): Supervised model scores
            rule_scores (array): Rule-based scores
            
        Returns:
            array: Combined scores
        """
        try:
            # Get custom weights
            unsupervised_weight = self.custom_weights.get('unsupervised', 0.4)
            supervised_weight = self.custom_weights.get('supervised', 0.4)
            rule_weight = self.custom_weights.get('rule', 0.2)
            
            # Apply non-linear transformation to supervised scores
            if supervised_scores is not None:
                # Emphasize high scores
                supervised_transformed = np.power(supervised_scores, 1.5)
            else:
                supervised_transformed = None
            
            # Apply non-linear transformation to rule scores
            if rule_scores is not None:
                # Emphasize high scores even more
                rule_transformed = np.power(rule_scores, 2.0)
            else:
                rule_transformed = None
            
            # Combine using weighted average with transformed scores
            return self._weighted_average_combination(
                unsupervised_scores, supervised_transformed, rule_transformed
            )
            
        except Exception as e:
            logger.error(f"Error in custom combination: {str(e)}")
            raise
    
    def update_weights(self, new_weights):
        """
        Update the weights for combining scores
        
        Args:
            new_weights (dict): New weights for models
        """
        self.custom_weights.update(new_weights)
        logger.info(f"Updated weights: {self.custom_weights}")
    
    def get_weights(self):
        """
        Get current weights
        
        Returns:
            dict: Current weights
        """
        return self.custom_weights.copy()
    
    def calibrate_scores(self, method='isotonic'):
        """
        Calibrate scores to better reflect true probabilities
        
        Args:
            method (str): Calibration method ('isotonic', 'sigmoid')
        """
        try:
            # Update calibrators
            for model_type in ['unsupervised', 'supervised', 'rule']:
                if model_type in self.calibrators:
                    self.calibrators[model_type] = CalibratedClassifierCV(
                        method=method, cv='prefit'
                    )
            
            logger.info(f"Updated calibrators with method: {method}")
            
        except Exception as e:
            logger.error(f"Error calibrating scores: {str(e)}")
            raise

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\analysis\.ipynb_checkpoints\__init__-checkpoint.py ===

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\analysis\__pycache__\explainability.cpython-39.pyc ===
a
    èöhUb  „                   @   s–   d Z ddlZddlZddlZddlmZ ddl	Z
ddlmZmZ ddlmZ ddlmZmZmZ ddlZddlZddlZddlmZmZmZmZ ddlmZ e†d° ejej d	ç e†!e"°Z#G d
dÑ dÉZ$dS )zP
Explainability Module
Implements explainable AI techniques for fraud detection
È    N)⁄permutation_importance⁄partial_dependence)⁄LogisticRegression)⁄DecisionTreeClassifier⁄export_text⁄export_graphviz)⁄Dict⁄List⁄Tuple⁄Union)⁄is_api_available⁄ignore)⁄levelc                   @   sv   e Zd ZdZdddÑZddÑ ZddÑ Zd	d
Ñ ZddÑ ZddÑ Z	ddÑ Z
ddÑ ZdddÑZddÑ ZddÑ ZdddÑZdS ) ⁄Explainabilityzä
    Class for generating explanations for fraud detection predictions
    Implements SHAP, LIME, and other explainability techniques
    Nc                 C   s&   |pi | _ i | _i | _i | _d| _dS )zÄ
        Initialize Explainability
        
        Args:
            config (dict, optional): Configuration parameters
        FN)⁄config⁄
explainers⁄explanations⁄feature_names⁄fitted)⁄selfr   © r   ˙mC:\Users\Tranquil Abyss\fraud-detection-platform\app\..\src\fraud_detection_engine\analysis\explainability.py⁄__init__   s
    
zExplainability.__init__c                 C   s  z÷ddÑ |j D É}|| †d°}|| _i }|†° D ]|\}}z4|j|df }	|	dkrl| †||||	|°}
|
||< W q2 ty¨ } z&t†d|õ dt	|Éõ ù° W Y d}~q2d}~0 0 q2|| _
d	| _t†d
t|Éõ dù° |W S  têy } z"t†dt	|Éõ ù° Ç W Y d}~n
d}~0 0 dS )a=  
        Generate explanations for all transactions
        
        Args:
            df (DataFrame): Input data
            risk_scores (DataFrame): Risk scores
            model_results (dict): Results from different models
            
        Returns:
            dict: Explanations for each transaction
        c                 S   s   g | ]}|d vr|ëqS ))⁄transaction_id⁄	sender_id⁄receiver_id⁄
fraud_flagr   )⁄.0⁄colr   r   r   ⁄
<listcomp>:   s   
ˇz8Explainability.generate_explanations.<locals>.<listcomp>r   ⁄
risk_scoreÁ      ‡?˙-Error generating explanation for transaction ˙: NTzGenerated explanations for z transactionszError generating explanations: )⁄columns⁄fillnar   ⁄iterrows⁄loc⁄!_generate_transaction_explanation⁄	Exception⁄logger⁄warning⁄strr   r   ⁄info⁄len⁄error)r   ⁄df⁄risk_scores⁄model_results⁄feature_cols⁄Xr   ⁄idx⁄rowr    ⁄explanation⁄er   r   r   ⁄generate_explanations,   s,    
ˇ2z$Explainability.generate_explanationsc              
   C   s˙  êzö|† d|°|g g i i ddú}i }d|v rê| †|||d °}|†|° |d †° D ]:\}	}
d|
v rT|t|
d Ék rT|
d | |d d|	õ ù< qTd|v r¸| †|||d °}|†|° |d †° D ]:\}	}
d	|
v r¿|t|
d	 Ék r¿|
d	 | |d d
|	õ ù< q¿d|v êrZ| †|||d °}|†|° d|d v êrZ|t|d d Ék êrZ|d d | |d< ||d< t|†° ddÑ ddçddÖ }||d< | †||°|d< |W S  t	êyÙ } z>t
†d|õ dt|Éõ ù° |† d|°|t|ÉdúW  Y d}~S d}~0 0 dS )a}  
        Generate explanation for a single transaction
        
        Args:
            idx (int): Transaction index
            row (Series): Transaction data
            X (DataFrame): Feature data
            risk_score (float): Risk score
            model_results (dict): Model results
            
        Returns:
            dict: Explanation for the transaction
        r   ⁄ )r   r    ⁄top_factors⁄rule_violations⁄model_predictions⁄feature_contributions⁄text_explanation⁄unsupervised⁄scoresr=   ⁄unsupervised_⁄
supervised⁄probabilities⁄supervised_⁄rule⁄violated_rule_namesr<   r>   c                 S   s   t | d ÉS )NÈ   )⁄abs)⁄xr   r   r   ⁄<lambda>§   Û    zBExplainability._generate_transaction_explanation.<locals>.<lambda>T)⁄key⁄reverseNÈ
   r;   r?   r"   r#   )r   r    r/   )⁄get⁄_get_unsupervised_contributions⁄update⁄itemsr.   ⁄_get_supervised_contributions⁄_get_rule_contributions⁄sorted⁄_generate_text_explanationr)   r*   r/   r,   )r   r5   r6   r4   r    r2   r7   r>   Zunsupervised_contributions⁄
model_name⁄
model_dataZsupervised_contributionsZrule_contributionsr;   r8   r   r   r   r(   _   sh    
˘
ˇ

ˇ


ˇ
$˝¸ˇ
˝z0Explainability._generate_transaction_explanationc              
   C   s÷  êzêi }|† ° D ê]z\}}d|v rd|v r|d }|d }|j||d Ö|f }	|dkröt|dÉrò|j}
t|ÉD ]&\}}|t|
Ék rp|
| ||õ dù< qpq|dkêr$t|dÉêråt|d	Éêrå|j| }t|ÉD ]R\}}||	jd k rŒ|	jd
|f }t	†
||jddÖ|f †°  °}|||õ dù< qŒq|dkrt|dÉr|†|	°}t	†|	j| d°}t|ÉD ]0\}}||jd k êrZ|d
|f ||õ dù< êqZq|W S  têy– } z$t†dt|Éõ ù° i W  Y d}~S d}~0 0 dS )a6  
        Get feature contributions from unsupervised models
        
        Args:
            idx (int): Transaction index
            X (DataFrame): Feature data
            unsupervised_results (dict): Unsupervised model results
            
        Returns:
            dict: Feature contributions
        ⁄modelr   rH   ⁄isolation_forest⁄feature_importances_Z_isolation_forest⁄local_outlier_factor⁄_fit_X⁄
_distancesr   N⁄_lof⁄autoencoder⁄predictÈ   Z_autoencoderz*Error getting unsupervised contributions: )rS   r'   ⁄hasattrr\   ⁄	enumerater.   r_   ⁄shape⁄iloc⁄nprI   r^   ⁄meanrb   ⁄power⁄valuesr)   r*   r/   r,   )r   r5   r4   ⁄unsupervised_results⁄contributionsrX   rY   rZ   r   rJ   ⁄feature_importance⁄i⁄feature⁄	distances⁄feature_valueZfeature_diffZreconstructionZerror_per_featurer8   r   r   r   rQ   π   s@    


 

z.Explainability._get_unsupervised_contributionsc              
   C   s^  êzi }|† ° D ê]\}}d|v rd|v r|d }|d }|j||d Ö|f }	d|v r–|d }
t|
tÉrít|
ÉdkrÑ|
d | }qö|
d | }n|
| }t|ÉD ]*\}}|t|Ék r¢|| ||õ d|õ ù< q¢qd|v r|d }|†° D ]*\}}|d }|d	 }|||õ d|õ ù< qËq|W S  têyX } z$t†	d
t
|Éõ ù° i W  Y d}~S d}~0 0 dS )a0  
        Get feature contributions from supervised models
        
        Args:
            idx (int): Transaction index
            X (DataFrame): Feature data
            supervised_results (dict): Supervised model results
            
        Returns:
            dict: Feature contributions
        rZ   r   rH   ⁄shap_valuesr   ⁄_rn   rp   ⁄
importancez(Error getting supervised contributions: N)rS   r'   ⁄
isinstance⁄listr.   re   r&   r)   r*   r/   r,   )r   r5   r4   ⁄supervised_resultsrm   rX   rY   rZ   r   rJ   rs   Z	shap_valsro   rp   ⁄importance_dfrt   r6   ru   r8   r   r   r   rT   ˆ   s6    
z,Explainability._get_supervised_contributionsc                 C   s4  zi }d|v rÏ|t |d Ék rÏ|d | }|†di °}dgddgddgdgddgddgdgddgddgdgdgdgdgdgddgddgddgddgd	ú}|D ]N}|†|d
°}	|†|g °}
|
D ],}||v rº|†|õ dùd°|	 ||õ dù< qºqú|W S  têy. } z$t†dt|Éõ ù° i W  Y d}~S d}~0 0 dS )a&  
        Get feature contributions from rule-based model
        
        Args:
            idx (int): Transaction index
            row (Series): Transaction data
            rule_results (dict): Rule model results
            
        Returns:
            dict: Feature contributions
        rG   ⁄rule_weights⁄amountr   r   ⁄	timestamp⁄sender_location⁄receiver_location)⁄high_amount⁄unusual_amount_for_sender⁄unusual_amount_for_receiver⁄round_amount⁄high_frequency_sender⁄high_frequency_receiver⁄rapid_succession⁄cross_border⁄high_risk_country⁄unusual_location_for_sender⁄unusual_hour⁄weekend⁄
new_sender⁄new_receiver⁄sanctions_check⁄tax_compliance⁄bank_verification⁄identity_verificationg      ?⁄_ruler   z"Error getting rule contributions: N)r.   rP   r)   r*   r/   r,   )r   r5   r6   ⁄rule_resultsrm   ⁄violated_rulesrz   Zrule_to_featuresrF   ⁄weight⁄featuresrp   r8   r   r   r   rU   ,  sD    Ó$z&Explainability._get_rule_contributionsc           
   
   C   sj  êz&|d }|dkrd}n |dkr(d}n|dkr6d}nd}d	|õ d
|dõdù}|d rÍ|d ddÖ }ddÑ |D É}t |Édkrñ|d|d õ dù7 }nTt |Édkr¬|d|d õ d|d õ dù7 }n(|dd†|ddÖ °õ d|d õ dù7 }|d êrPt |d Édkêr |d|d d õ dù7 }n0|dt |d Éõ dd†|d ddÖ °õ dù7 }d|v êrn|d |d õ dù7 }d!|v êr†d"|v êr†|d#|d! õ d$|d" õ dù7 }d%|v êræ|d&|d% õ dù7 }d'}td(Éêr“d)}ntd*Éêr‡d)}|êrÙ|d+|õ ù7 }|dkêr|d,7 }n|dkêr|d-7 }n|d.7 }|W S  têyd }	 z"t†d/t|	Éõ ù° W Y d}	~	d0S d}	~	0 0 dS )1zÍ
        Generate natural language explanation
        
        Args:
            explanation (dict): Explanation data
            row (Series): Transaction data
            
        Returns:
            str: Text explanation
        r    gÕÃÃÃÃÃÏ?⁄criticalgffffffÊ?⁄highr!   ⁄medium⁄lowzThis transaction has a z risk score of z.2fz. r;   NÈ   c                 S   s   g | ]}|d  † d°d  ëqS )r   rt   )⁄split)r   ⁄factorr   r   r   r   Ñ  rL   z=Explainability._generate_text_explanation.<locals>.<listcomp>rH   zThe primary factor is r   rc   zThe main factors are z and z, Èˇˇˇˇz, and r<   zIt violates the rule: zIt violates z rules: r{   zThe transaction amount is r   r   zIt involves sender z and receiver r|   zThe transaction occurred at r:   ⁄geminiz$AI analysis pending implementation. ⁄openaizAI analysis: z0This transaction should be reviewed immediately.z$This transaction should be reviewed.z(This transaction appears to be low risk.z#Error generating text explanation: zUnable to generate explanation.)r.   ⁄joinr   r)   r*   r/   r,   )
r   r7   r6   r    ⁄
risk_level⁄textr;   Zfactor_namesZai_explanationr8   r   r   r   rW   g  sX     (
0







z)Explainability._generate_text_explanationc                 C   s<   | j stdÉÇ| j†° D ]\}}|†d°|kr|  S qdS )z‘
        Get explanation for a specific transaction
        
        Args:
            transaction_id (str): Transaction ID
            
        Returns:
            dict: Explanation for the transaction
        z=Explanations not generated. Call generate_explanations first.r   N)r   ⁄
ValueErrorr   rS   rP   )r   r   r5   r7   r   r   r   ⁄get_explanation∫  s    

zExplainability.get_explanationÈ   c           
   
   C   s$  z‚i }| j †° D ]J}d|v r|d †° D ]0\}}|du s@||v r(|†|d°t|É ||< q(q|spt†d° W dS t†t	|†
° Ét	|†° Édú°jdddç}|†|°}tjd	d
ç tjdd|dç t†d|õ dù° t†°  t†° W S  têy }	 z"t†dt|	Éõ ù° Ç W Y d}	~	n
d}	~	0 0 dS )zæ
        Plot feature importance for a model
        
        Args:
            model_name (str, optional): Name of the model
            top_n (int): Number of top features to show
        r>   Nr   zNo feature contributions found)rp   ru   ru   F)⁄	ascending©rO   È   ©⁄figsizerp   )rJ   ⁄y⁄datazTop z Feature Importancez#Error plotting feature importance: )r   rk   rS   rP   rI   r*   r+   ⁄pd⁄	DataFramerw   ⁄keys⁄sort_values⁄head⁄plt⁄figure⁄sns⁄barplot⁄title⁄tight_layout⁄gcfr)   r/   r,   )
r   rX   ⁄top_nr>   r7   rp   ⁄contributionry   ⁄top_featuresr8   r   r   r   ⁄plot_feature_importanceŒ  s2    


˛˝

z&Explainability.plot_feature_importancec              
   C   sÑ   zDt jddç t †d|õ ù° t jdddddddç t †°  t †° W S  ty~ } z"t†d	t	|Éõ ù° Ç W Y d
}~n
d
}~0 0 d
S )zû
        Plot SHAP summary for a model
        
        Args:
            model_name (str): Name of the model
            X (DataFrame): Feature data
        rß   r©   zSHAP Summary for r!   z)SHAP summary plot would be displayed here⁄centerÈ   ©⁄ha⁄va⁄fontsizezError plotting SHAP summary: N©
r≤   r≥   r∂   r¢   r∑   r∏   r)   r*   r/   r,   )r   rX   r4   r8   r   r   r   ⁄plot_shap_summary˘  s    
ˇ
z Explainability.plot_shap_summaryc              
   C   sÑ   zDt jddç t †d|õ ù° t jdddddddç t †°  t †° W S  ty~ } z"t†d	t	|Éõ ù° Ç W Y d
}~n
d
}~0 0 d
S )z”
        Plot decision path for a tree-based model
        
        Args:
            model_name (str): Name of the model
            X (DataFrame): Feature data
            idx (int): Transaction index
        rß   r©   zDecision Path for r!   z%Decision path would be displayed hererΩ   ræ   rø   zError plotting decision path: Nr√   )r   rX   r4   r5   r8   r   r   r   ⁄plot_decision_path  s    	
ˇ
z!Explainability.plot_decision_pathr   c           	   
   C   s¶   zd|j ||d Ö }|†° }|jD ]}|dv r"|| d ||< q"|†d°d |†d°d ddidúW S  ty† } z$t†d	t|Éõ ù° i W  Y d
}~S d
}~0 0 d
S )ad  
        Generate counterfactual explanation
        
        Args:
            idx (int): Transaction index
            X (DataFrame): Feature data
            model_name (str): Name of the model
            target_class (int): Target class (0 for non-fraud, 1 for fraud)
            
        Returns:
            dict: Counterfactual explanation
        rH   )r{   r!   ⁄recordsr   r{   zReduced by 50%)⁄original⁄counterfactual⁄changesz-Error generating counterfactual explanation: N)r'   ⁄copyr$   ⁄to_dictr)   r*   r/   r,   )	r   r5   r4   rX   ⁄target_classZoriginal_rowr»   r   r8   r   r   r   ⁄get_counterfactual_explanation*  s    
ˇ˝z-Explainability.get_counterfactual_explanation)N)Nr•   )r   )⁄__name__⁄
__module__⁄__qualname__⁄__doc__r   r9   r(   rQ   rT   rU   rW   r§   rº   rƒ   r≈   rÕ   r   r   r   r   r      s   
3Z=6;S
+r   )%r—   ⁄pandasr≠   ⁄numpyrh   ⁄shap⁄matplotlib.pyplot⁄pyplotr≤   ⁄seabornr¥   ⁄sklearn.inspectionr   r   ⁄sklearn.linear_modelr   Zsklearn.treer   r   r   ⁄graphviz⁄warnings⁄logging⁄typingr   r	   r
   r   ⁄&fraud_detection_engine.utils.api_utilsr   ⁄filterwarnings⁄basicConfig⁄INFO⁄	getLoggerrŒ   r*   r   r   r   r   r   ⁄<module>   s"   



=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\analysis\__pycache__\risk_scorer.cpython-39.pyc ===
a
    èöhπ?  „                   @   sà   d Z ddlZddlZddlmZ ddlmZ ddl	Z	ddl
Z
ddlmZmZmZmZ e	†d° e
je
jdç e
†e°ZG dd	Ñ d	ÉZdS )
z@
Risk Scorer Module
Implements risk scoring for fraud detection
È    N)⁄MinMaxScaler)⁄CalibratedClassifierCV)⁄Dict⁄List⁄Tuple⁄Union⁄ignore)⁄levelc                   @   sl   e Zd ZdZdddÑZddÑ Zdd	Ñ Zd
dÑ ZddÑ ZddÑ Z	ddÑ Z
ddÑ ZddÑ ZddÑ ZdddÑZdS )⁄
RiskScorerzy
    Class for calculating risk scores from multiple models
    Implements weighted combination of algorithm outputs
    ⁄weighted_averageNc                 C   s:   |† ° †dd°| _|p ddddú| _i | _i | _d| _dS )zÿ
        Initialize RiskScorer
        
        Args:
            method (str): Scoring method ('weighted_average', 'maximum', 'custom')
            custom_weights (dict, optional): Custom weights for models
        ˙ ⁄_ÁöôôôôôŸ?Áöôôôôô…?©⁄unsupervised⁄
supervised⁄ruleFN)⁄lower⁄replace⁄method⁄custom_weights⁄scalers⁄calibrators⁄fitted)⁄selfr   r   © r   ˙jC:\Users\Tranquil Abyss\fraud-detection-platform\app\..\src\fraud_detection_engine\analysis\risk_scorer.py⁄__init__   s    	˝zRiskScorer.__init__c           	   
   C   sp  êz,|† ° }d}d}d}d|v r.| †|d °}d|v rD| †|d °}d|v rZ| †|d °}| jdkrt| †|||°}nD| jdkré| †|||°}n*| jdkr®| †|||°}ntd| jõ ùÉÇ||d	< |dur–||d
< |dur‡||d< |dur||d< |d	 j	ddç|d< t
j|d g d¢g d¢dç|d< d| _|W S  têyj } z"t†dt|Éõ ù° Ç W Y d}~n
d}~0 0 dS )z¸
        Calculate risk scores from model results
        
        Args:
            model_results (dict): Results from different models
            df (DataFrame): Original data
            
        Returns:
            DataFrame: Risk scores
        Nr   r   r   r   ⁄maximum⁄customzUnknown scoring method: ⁄
risk_scoreZunsupervised_scoreZsupervised_score⁄
rule_scoreT)⁄pctZrisk_percentile)r   gffffffÊ?gÕÃÃÃÃÃÏ?gffffffÓ?g      ?)⁄Low⁄Medium⁄HighZCritical)⁄bins⁄labelsZ
risk_levelzError calculating risk scores: )⁄copy⁄_process_unsupervised_results⁄_process_supervised_results⁄_process_rule_resultsr   ⁄_weighted_average_combination⁄_maximum_combination⁄_custom_combination⁄
ValueError⁄rank⁄pd⁄cutr   ⁄	Exception⁄logger⁄error⁄str)	r   ⁄model_results⁄df⁄	result_df⁄unsupervised_scores⁄supervised_scores⁄rule_scores⁄combined_scores⁄er   r   r   ⁄calculate_scores)   sT    
ˇ
ˇ
ˇ˝
zRiskScorer.calculate_scoresc              
   C   sŒ   zég }g }|† ° D ],\}}d|v r|d }|†|° |†|° q|sJW dS t†|°j}tj|ddç}tÉ }	|	†|†dd°°†	° }
|	| j
d< |
W S  ty» } z"t†dt|Éõ ù° Ç W Y d}~n
d}~0 0 dS )z„
        Process unsupervised model results
        
        Args:
            unsupervised_results (dict): Results from unsupervised models
            
        Returns:
            array: Combined unsupervised scores
        ⁄scoresNÈ   ©⁄axisÈˇˇˇˇr   z'Error processing unsupervised results: )⁄items⁄append⁄np⁄array⁄T⁄meanr   ⁄fit_transform⁄reshape⁄flattenr   r4   r5   r6   r7   )r   ⁄unsupervised_results⁄
all_scores⁄model_names⁄
model_name⁄
model_datarA   Zscores_arrayZ
avg_scores⁄scaler⁄normalized_scoresr?   r   r   r   r*   w   s&    


z(RiskScorer._process_unsupervised_resultsc              
   C   sÓ   zÆg }g }|† ° D ],\}}d|v r|d }|†|° |†|° q|sJW dS t†|°j}tj|ddç}d| jv rî| jd †|†dd°°ddÖdf }	n|}	t	ddd	ç| jd< |	W S  t
yË }
 z"t†d
t|
Éõ ù° Ç W Y d}
~
n
d}
~
0 0 dS )z€
        Process supervised model results
        
        Args:
            supervised_results (dict): Results from supervised models
            
        Returns:
            array: Combined supervised scores
        ⁄probabilitiesNrB   rC   r   rE   ⁄isotonic⁄prefit©r   ⁄cvz%Error processing supervised results: )rF   rG   rH   rI   rJ   rK   r   ⁄predict_probarM   r   r4   r5   r6   r7   )r   ⁄supervised_results⁄all_probabilitiesrQ   rR   rS   rV   Zprobabilities_arrayZavg_probabilities⁄calibrated_scoresr?   r   r   r   r+   ¢   s,    


&ˇz&RiskScorer._process_supervised_resultsc              
   C   sä   zJd|v r*|d }t |tjÉs0t†|°}nW dS |}tdddç| jd< |W S  tyÑ } z"t†dt	|Éõ ù° Ç W Y d}~n
d}~0 0 dS )z√
        Process rule-based results
        
        Args:
            rule_results (dict): Results from rule engine
            
        Returns:
            array: Combined rule scores
        rU   NrW   rX   rY   r   zError processing rule results: )
⁄
isinstancerH   ⁄ndarrayrI   r   r   r4   r5   r6   r7   )r   ⁄rule_resultsr=   r^   r?   r   r   r   r,   —   s    
ˇz RiskScorer._process_rule_resultsc              
   C   sƒ  êzÄ| j †dd°}| j †dd°}| j †dd°}g }|durD|†|° |durV|†|° |durh|†|° t|É}|dkr®|durà|| }|durò|| }|dur®|| }|dur∫t|É}	n0|durÃt|É}	n|durﬁt|É}	nt†g °W S t†|	°}
|duêr"t|tj	Éêst†|°}|
|| 7 }
|duêrPt|tj	ÉêsDt†|°}|
|| 7 }
|duêr~t|tj	Éêsrt†|°}|
|| 7 }
|
W S  t
êyæ } z"t†dt|Éõ ù° Ç W Y d}~n
d}~0 0 dS )	aD  
        Combine scores using weighted average
        
        Args:
            unsupervised_scores (array): Unsupervised model scores
            supervised_scores (array): Supervised model scores
            rule_scores (array): Rule-based scores
            
        Returns:
            array: Combined scores
        r   r   r   r   r   Nr   z'Error in weighted average combination: )r   ⁄getrG   ⁄sum⁄lenrH   rI   ⁄zerosr_   r`   r4   r5   r6   r7   )r   r;   r<   r=   ⁄unsupervised_weight⁄supervised_weight⁄rule_weightZavailable_weights⁄total_weight⁄lengthr>   r?   r   r   r   r-   Ú   sV    












z(RiskScorer._weighted_average_combinationc              
   C   sû   z^g }|dur|† |° |dur*|† |° |dur<|† |° |sLt†g °W S tj|ddç}|W S  tyò } z"t†dt|Éõ ù° Ç W Y d}~n
d}~0 0 dS )a;  
        Combine scores using maximum
        
        Args:
            unsupervised_scores (array): Unsupervised model scores
            supervised_scores (array): Supervised model scores
            rule_scores (array): Rule-based scores
            
        Returns:
            array: Combined scores
        Nr   rC   zError in maximum combination: )rG   rH   rI   ⁄maxr4   r5   r6   r7   )r   r;   r<   r=   rP   r>   r?   r   r   r   r.   =  s    


zRiskScorer._maximum_combinationc           
   
   C   sÆ   zn| j †dd°}| j †dd°}| j †dd°}|durBt†|d°}nd}|dur\t†|d°}nd}| †|||°W S  ty® }	 z"t†d	t|	Éõ ù° Ç W Y d}	~	n
d}	~	0 0 dS )
aA  
        Combine scores using custom method
        
        Args:
            unsupervised_scores (array): Unsupervised model scores
            supervised_scores (array): Supervised model scores
            rule_scores (array): Rule-based scores
            
        Returns:
            array: Combined scores
        r   r   r   r   r   Ng      ¯?g       @zError in custom combination: )	r   rb   rH   ⁄powerr-   r4   r5   r6   r7   )
r   r;   r<   r=   rf   rg   rh   Zsupervised_transformedZrule_transformedr?   r   r   r   r/   b  s     ˇzRiskScorer._custom_combinationc                 C   s"   | j †|° t†d| j õ ù° dS )zá
        Update the weights for combining scores
        
        Args:
            new_weights (dict): New weights for models
        zUpdated weights: N)r   ⁄updater5   ⁄info)r   ⁄new_weightsr   r   r   ⁄update_weightsã  s    zRiskScorer.update_weightsc                 C   s
   | j †° S )za
        Get current weights
        
        Returns:
            dict: Current weights
        )r   r)   )r   r   r   r   ⁄get_weightsï  s    zRiskScorer.get_weightsrW   c              
   C   sz   z:dD ] }|| j v rt|ddç| j |< qt†d|õ ù° W n: tyt } z"t†dt|Éõ ù° Ç W Y d}~n
d}~0 0 dS )z£
        Calibrate scores to better reflect true probabilities
        
        Args:
            method (str): Calibration method ('isotonic', 'sigmoid')
        r   rX   rY   z!Updated calibrators with method: zError calibrating scores: N)r   r   r5   rn   r4   r6   r7   )r   r   ⁄
model_typer?   r   r   r   ⁄calibrate_scoresû  s    
ˇzRiskScorer.calibrate_scores)r   N)rW   )⁄__name__⁄
__module__⁄__qualname__⁄__doc__r   r@   r*   r+   r,   r-   r.   r/   rp   rq   rs   r   r   r   r   r
      s   
N+/!K%)
	r
   )rw   ⁄pandasr2   ⁄numpyrH   ⁄sklearn.preprocessingr   Zsklearn.calibrationr   ⁄warnings⁄logging⁄typingr   r   r   r   ⁄filterwarnings⁄basicConfig⁄INFO⁄	getLoggerrt   r5   r
   r   r   r   r   ⁄<module>   s   



=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\analysis\__pycache__\__init__.cpython-39.pyc ===
a
    [îëh    „                   @   s   d S )N© r   r   r   ˙gC:\Users\Tranquil Abyss\fraud-detection-platform\app\..\src\fraud_detection_engine\analysis\__init__.py⁄<module>   Û    

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\features\graph_features.py ===
"""
Graph Features Module
Implements graph-based feature extraction techniques for fraud detection
"""

import pandas as pd
import numpy as np
import networkx as nx
from collections import defaultdict, Counter
import community as community_louvain
from sklearn.preprocessing import MinMaxScaler
import warnings
import logging
from typing import Dict, List, Tuple, Union

warnings.filterwarnings('ignore')
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class GraphFeatures:
    """
    Class for extracting graph-based features from transaction data
    Implements techniques like centrality measures, clustering coefficients, etc.
    """
    
    def __init__(self, config=None):
        """
        Initialize GraphFeatures
        
        Args:
            config (dict, optional): Configuration parameters
        """
        self.config = config or {}
        self.graph = None
        self.sender_graph = None
        self.receiver_graph = None
        self.bipartite_graph = None
        self.feature_names = []
        self.scaler = MinMaxScaler()
        self.fitted = False
        
    def extract_features(self, df):
        """
        Extract all graph features from the dataframe
        
        Args:
            df (DataFrame): Input transaction data
            
        Returns:
            DataFrame: DataFrame with extracted features
        """
        try:
            # Create a copy to avoid modifying the original
            result_df = df.copy()
            
            # Build graphs
            self._build_graphs(df)
            
            # Extract different types of graph features
            result_df = self._extract_centrality_features(result_df)
            result_df = self._extract_clustering_features(result_df)
            result_df = self._extract_community_features(result_df)
            result_df = self._extract_path_features(result_df)
            result_df = self._extract_subgraph_features(result_df)
            result_df = self._extract_temporal_graph_features(result_df)
            
            # Store feature names
            self.feature_names = [col for col in result_df.columns if col not in df.columns]
            
            logger.info(f"Extracted {len(self.feature_names)} graph features")
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting graph features: {str(e)}")
            raise
    
    def _build_graphs(self, df):
        """
        Build various graphs from the transaction data
        
        Args:
            df (DataFrame): Input transaction data
        """
        try:
            # Build transaction graph (directed)
            self.graph = nx.DiGraph()
            
            # Add edges with attributes
            for _, row in df.iterrows():
                if 'sender_id' in row and 'receiver_id' in row:
                    sender = row['sender_id']
                    receiver = row['receiver_id']
                    
                    # Get edge attributes
                    attrs = {}
                    if 'amount' in row:
                        attrs['amount'] = row['amount']
                    if 'timestamp' in row:
                        attrs['timestamp'] = row['timestamp']
                    if 'transaction_id' in row:
                        attrs['transaction_id'] = row['transaction_id']
                    
                    # Add edge or update existing edge
                    if self.graph.has_edge(sender, receiver):
                        # Update existing edge
                        edge_data = self.graph[sender][receiver]
                        if 'amount' in attrs:
                            edge_data['total_amount'] = edge_data.get('total_amount', 0) + attrs['amount']
                        edge_data['transaction_count'] = edge_data.get('transaction_count', 0) + 1
                        edge_data['transactions'].append(attrs)
                    else:
                        # Add new edge
                        attrs['total_amount'] = attrs.get('amount', 0)
                        attrs['transaction_count'] = 1
                        attrs['transactions'] = [attrs]
                        self.graph.add_edge(sender, receiver, **attrs)
            
            # Build sender graph (undirected, senders connected if they have common receivers)
            self.sender_graph = nx.Graph()
            
            # Create a mapping from receivers to senders
            receiver_to_senders = defaultdict(set)
            for _, row in df.iterrows():
                if 'sender_id' in row and 'receiver_id' in row:
                    receiver_to_senders[row['receiver_id']].add(row['sender_id'])
            
            # Connect senders with common receivers
            for receiver, senders in receiver_to_senders.items():
                senders_list = list(senders)
                for i in range(len(senders_list)):
                    for j in range(i+1, len(senders_list)):
                        sender1 = senders_list[i]
                        sender2 = senders_list[j]
                        
                        if self.sender_graph.has_edge(sender1, sender2):
                            self.sender_graph[sender1][sender2]['common_receivers'] += 1
                        else:
                            self.sender_graph.add_edge(sender1, sender2, common_receivers=1)
            
            # Build receiver graph (undirected, receivers connected if they have common senders)
            self.receiver_graph = nx.Graph()
            
            # Create a mapping from senders to receivers
            sender_to_receivers = defaultdict(set)
            for _, row in df.iterrows():
                if 'sender_id' in row and 'receiver_id' in row:
                    sender_to_receivers[row['sender_id']].add(row['receiver_id'])
            
            # Connect receivers with common senders
            for sender, receivers in sender_to_receivers.items():
                receivers_list = list(receivers)
                for i in range(len(receivers_list)):
                    for j in range(i+1, len(receivers_list)):
                        receiver1 = receivers_list[i]
                        receiver2 = receivers_list[j]
                        
                        if self.receiver_graph.has_edge(receiver1, receiver2):
                            self.receiver_graph[receiver1][receiver2]['common_senders'] += 1
                        else:
                            self.receiver_graph.add_edge(receiver1, receiver2, common_senders=1)
            
            # Build bipartite graph (senders and receivers as two separate sets)
            self.bipartite_graph = nx.Graph()
            
            # Add nodes with bipartite attribute
            for _, row in df.iterrows():
                if 'sender_id' in row:
                    self.bipartite_graph.add_node(row['sender_id'], bipartite=0)
                if 'receiver_id' in row:
                    self.bipartite_graph.add_node(row['receiver_id'], bipartite=1)
            
            # Add edges
            for _, row in df.iterrows():
                if 'sender_id' in row and 'receiver_id' in row:
                    sender = row['sender_id']
                    receiver = row['receiver_id']
                    
                    # Get edge attributes
                    attrs = {}
                    if 'amount' in row:
                        attrs['amount'] = row['amount']
                    if 'timestamp' in row:
                        attrs['timestamp'] = row['timestamp']
                    if 'transaction_id' in row:
                        attrs['transaction_id'] = row['transaction_id']
                    
                    # Add edge
                    self.bipartite_graph.add_edge(sender, receiver, **attrs)
            
            logger.info("Graphs built successfully")
            
        except Exception as e:
            logger.error(f"Error building graphs: {str(e)}")
            raise
    
    def _extract_centrality_features(self, df):
        """
        Extract centrality-based features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with centrality features
        """
        try:
            result_df = df.copy()
            
            if self.graph is None:
                logger.warning("Graph not built. Skipping centrality features.")
                return result_df
            
            # Calculate degree centrality
            in_degree_centrality = nx.in_degree_centrality(self.graph)
            out_degree_centrality = nx.out_degree_centrality(self.graph)
            degree_centrality = nx.degree_centrality(self.graph.to_undirected())
            
            # Map to dataframe
            if 'sender_id' in df.columns:
                result_df['sender_in_degree_centrality'] = df['sender_id'].map(in_degree_centrality).fillna(0)
                result_df['sender_out_degree_centrality'] = df['sender_id'].map(out_degree_centrality).fillna(0)
                result_df['sender_degree_centrality'] = df['sender_id'].map(degree_centrality).fillna(0)
            
            if 'receiver_id' in df.columns:
                result_df['receiver_in_degree_centrality'] = df['receiver_id'].map(in_degree_centrality).fillna(0)
                result_df['receiver_out_degree_centrality'] = df['receiver_id'].map(out_degree_centrality).fillna(0)
                result_df['receiver_degree_centrality'] = df['receiver_id'].map(degree_centrality).fillna(0)
            
            # Calculate betweenness centrality (sample for large graphs)
            if len(self.graph.nodes()) <= 1000:
                betweenness_centrality = nx.betweenness_centrality(self.graph)
            else:
                # Sample nodes for betweenness calculation
                sample_nodes = list(self.graph.nodes())[:1000]
                betweenness_centrality = nx.betweenness_centrality(self.graph, k=len(sample_nodes))
            
            # Map to dataframe
            if 'sender_id' in df.columns:
                result_df['sender_betweenness_centrality'] = df['sender_id'].map(betweenness_centrality).fillna(0)
            
            if 'receiver_id' in df.columns:
                result_df['receiver_betweenness_centrality'] = df['receiver_id'].map(betweenness_centrality).fillna(0)
            
            # Calculate closeness centrality (sample for large graphs)
            if len(self.graph.nodes()) <= 1000:
                closeness_centrality = nx.closeness_centrality(self.graph)
            else:
                # Use approximate closeness for large graphs
                closeness_centrality = nx.closeness_centrality(self.graph, distance='weight')
            
            # Map to dataframe
            if 'sender_id' in df.columns:
                result_df['sender_closeness_centrality'] = df['sender_id'].map(closeness_centrality).fillna(0)
            
            if 'receiver_id' in df.columns:
                result_df['receiver_closeness_centrality'] = df['receiver_id'].map(closeness_centrality).fillna(0)
            
            # Calculate eigenvector centrality (sample for large graphs)
            if len(self.graph.nodes()) <= 1000:
                try:
                    eigenvector_centrality = nx.eigenvector_centrality(self.graph, max_iter=1000)
                except:
                    eigenvector_centrality = {}
            else:
                eigenvector_centrality = {}
            
            # Map to dataframe
            if 'sender_id' in df.columns:
                result_df['sender_eigenvector_centrality'] = df['sender_id'].map(eigenvector_centrality).fillna(0)
            
            if 'receiver_id' in df.columns:
                result_df['receiver_eigenvector_centrality'] = df['receiver_id'].map(eigenvector_centrality).fillna(0)
            
            # Calculate PageRank
            pagerank = nx.pagerank(self.graph)
            
            # Map to dataframe
            if 'sender_id' in df.columns:
                result_df['sender_pagerank'] = df['sender_id'].map(pagerank).fillna(0)
            
            if 'receiver_id' in df.columns:
                result_df['receiver_pagerank'] = df['receiver_id'].map(pagerank).fillna(0)
            
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting centrality features: {str(e)}")
            return df
    
    def _extract_clustering_features(self, df):
        """
        Extract clustering-based features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with clustering features
        """
        try:
            result_df = df.copy()
            
            if self.graph is None:
                logger.warning("Graph not built. Skipping clustering features.")
                return result_df
            
            # Calculate clustering coefficient for transaction graph
            clustering_coeff = nx.clustering(self.graph.to_undirected())
            
            # Map to dataframe
            if 'sender_id' in df.columns:
                result_df['sender_clustering_coefficient'] = df['sender_id'].map(clustering_coeff).fillna(0)
            
            if 'receiver_id' in df.columns:
                result_df['receiver_clustering_coefficient'] = df['receiver_id'].map(clustering_coeff).fillna(0)
            
            # Calculate clustering coefficient for sender graph
            if self.sender_graph is not None and len(self.sender_graph.nodes()) > 0:
                sender_clustering_coeff = nx.clustering(self.sender_graph)
                
                # Map to dataframe
                if 'sender_id' in df.columns:
                    result_df['sender_graph_clustering_coefficient'] = df['sender_id'].map(sender_clustering_coeff).fillna(0)
            
            # Calculate clustering coefficient for receiver graph
            if self.receiver_graph is not None and len(self.receiver_graph.nodes()) > 0:
                receiver_clustering_coeff = nx.clustering(self.receiver_graph)
                
                # Map to dataframe
                if 'receiver_id' in df.columns:
                    result_df['receiver_graph_clustering_coefficient'] = df['receiver_id'].map(receiver_clustering_coeff).fillna(0)
            
            # Calculate average neighbor degree
            avg_neighbor_degree = nx.average_neighbor_degree(self.graph.to_undirected())
            
            # Map to dataframe
            if 'sender_id' in df.columns:
                result_df['sender_avg_neighbor_degree'] = df['sender_id'].map(avg_neighbor_degree).fillna(0)
            
            if 'receiver_id' in df.columns:
                result_df['receiver_avg_neighbor_degree'] = df['receiver_id'].map(avg_neighbor_degree).fillna(0)
            
            # Calculate square clustering
            square_clustering = nx.square_clustering(self.graph.to_undirected())
            
            # Map to dataframe
            if 'sender_id' in df.columns:
                result_df['sender_square_clustering'] = df['sender_id'].map(square_clustering).fillna(0)
            
            if 'receiver_id' in df.columns:
                result_df['receiver_square_clustering'] = df['receiver_id'].map(square_clustering).fillna(0)
            
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting clustering features: {str(e)}")
            return df
    
    def _extract_community_features(self, df):
        """
        Extract community-based features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with community features
        """
        try:
            result_df = df.copy()
            
            if self.graph is None:
                logger.warning("Graph not built. Skipping community features.")
                return result_df
            
            # Detect communities using Louvain algorithm
            undirected_graph = self.graph.to_undirected()
            communities = community_louvain.best_partition(undirected_graph)
            
            # Map to dataframe
            if 'sender_id' in df.columns:
                result_df['sender_community'] = df['sender_id'].map(communities).fillna(-1)
            
            if 'receiver_id' in df.columns:
                result_df['receiver_community'] = df['receiver_id'].map(communities).fillna(-1)
            
            # Calculate community size for each node
            community_sizes = Counter(communities.values())
            node_community_sizes = {node: community_sizes[comm] for node, comm in communities.items()}
            
            # Map to dataframe
            if 'sender_id' in df.columns:
                result_df['sender_community_size'] = df['sender_id'].map(node_community_sizes).fillna(0)
            
            if 'receiver_id' in df.columns:
                result_df['receiver_community_size'] = df['receiver_id'].map(node_community_sizes).fillna(0)
            
            # Calculate community degree centrality
            community_degree_centrality = {}
            for comm, nodes in community_sizes.items():
                comm_nodes = [node for node, c in communities.items() if c == comm]
                subgraph = undirected_graph.subgraph(comm_nodes)
                if len(subgraph.nodes()) > 0:
                    comm_centrality = nx.degree_centrality(subgraph)
                    for node in comm_nodes:
                        community_degree_centrality[node] = comm_centrality.get(node, 0)
            
            # Map to dataframe
            if 'sender_id' in df.columns:
                result_df['sender_community_degree_centrality'] = df['sender_id'].map(community_degree_centrality).fillna(0)
            
            if 'receiver_id' in df.columns:
                result_df['receiver_community_degree_centrality'] = df['receiver_id'].map(community_degree_centrality).fillna(0)
            
            # Check if sender and receiver are in the same community
            if 'sender_id' in df.columns and 'receiver_id' in df.columns:
                same_community = []
                for _, row in df.iterrows():
                    sender = row['sender_id']
                    receiver = row['receiver_id']
                    
                    if sender in communities and receiver in communities:
                        same_community.append(int(communities[sender] == communities[receiver]))
                    else:
                        same_community.append(0)
                
                result_df['same_community'] = same_community
            
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting community features: {str(e)}")
            return df
    
    def _extract_path_features(self, df):
        """
        Extract path-based features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with path features
        """
        try:
            result_df = df.copy()
            
            if self.graph is None:
                logger.warning("Graph not built. Skipping path features.")
                return result_df
            
            # Calculate shortest path lengths (sample for large graphs)
            if len(self.graph.nodes()) <= 500:
                # For small graphs, calculate all pairs shortest paths
                shortest_paths = dict(nx.all_pairs_shortest_path_length(self.graph))
                
                # Map to dataframe
                if 'sender_id' in df.columns and 'receiver_id' in df.columns:
                    path_lengths = []
                    for _, row in df.iterrows():
                        sender = row['sender_id']
                        receiver = row['receiver_id']
                        
                        if sender in shortest_paths and receiver in shortest_paths[sender]:
                            path_lengths.append(shortest_paths[sender][receiver])
                        else:
                            path_lengths.append(float('inf'))
                    
                    result_df['shortest_path_length'] = path_lengths
            else:
                # For large graphs, sample or use approximation
                if 'sender_id' in df.columns and 'receiver_id' in df.columns:
                    # Use BFS for a limited depth
                    path_lengths = []
                    for _, row in df.iterrows():
                        sender = row['sender_id']
                        receiver = row['receiver_id']
                        
                        try:
                            path_length = nx.shortest_path_length(self.graph, sender, receiver)
                            path_lengths.append(path_length)
                        except:
                            path_lengths.append(float('inf'))
                    
                    result_df['shortest_path_length'] = path_lengths
            
            # Calculate number of common neighbors
            if 'sender_id' in df.columns and 'receiver_id' in df.columns:
                common_neighbors = []
                for _, row in df.iterrows():
                    sender = row['sender_id']
                    receiver = row['receiver_id']
                    
                    try:
                        sender_neighbors = set(self.graph.predecessors(sender)) | set(self.graph.successors(sender))
                        receiver_neighbors = set(self.graph.predecessors(receiver)) | set(self.graph.successors(receiver))
                        common_neighbors.append(len(sender_neighbors & receiver_neighbors))
                    except:
                        common_neighbors.append(0)
                
                result_df['common_neighbors_count'] = common_neighbors
            
            # Calculate Jaccard coefficient
            if 'sender_id' in df.columns and 'receiver_id' in df.columns:
                jaccard_coeffs = []
                for _, row in df.iterrows():
                    sender = row['sender_id']
                    receiver = row['receiver_id']
                    
                    try:
                        sender_neighbors = set(self.graph.predecessors(sender)) | set(self.graph.successors(sender))
                        receiver_neighbors = set(self.graph.predecessors(receiver)) | set(self.graph.successors(receiver))
                        
                        union = sender_neighbors | receiver_neighbors
                        intersection = sender_neighbors & receiver_neighbors
                        
                        if len(union) > 0:
                            jaccard = len(intersection) / len(union)
                        else:
                            jaccard = 0
                        
                        jaccard_coeffs.append(jaccard)
                    except:
                        jaccard_coeffs.append(0)
                
                result_df['jaccard_coefficient'] = jaccard_coeffs
            
            # Calculate Adamic-Adar index
            if 'sender_id' in df.columns and 'receiver_id' in df.columns:
                adamic_adar = []
                for _, row in df.iterrows():
                    sender = row['sender_id']
                    receiver = row['receiver_id']
                    
                    try:
                        sender_neighbors = set(self.graph.predecessors(sender)) | set(self.graph.successors(sender))
                        receiver_neighbors = set(self.graph.predecessors(receiver)) | set(self.graph.successors(receiver))
                        common = sender_neighbors & receiver_neighbors
                        
                        # Calculate sum of 1/log(degree) for common neighbors
                        aa_index = 0
                        for node in common:
                            degree = self.graph.degree(node)
                            if degree > 1:
                                aa_index += 1 / np.log(degree)
                        
                        adamic_adar.append(aa_index)
                    except:
                        adamic_adar.append(0)
                
                result_df['adamic_adar_index'] = adamic_adar
            
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting path features: {str(e)}")
            return df
    
    def _extract_subgraph_features(self, df):
        """
        Extract subgraph-based features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with subgraph features
        """
        try:
            result_df = df.copy()
            
            if self.graph is None:
                logger.warning("Graph not built. Skipping subgraph features.")
                return result_df
            
            # Calculate ego network features for senders
            if 'sender_id' in df.columns:
                sender_ego_sizes = []
                sender_ego_densities = []
                
                for sender in df['sender_id']:
                    try:
                        # Get ego network
                        ego_graph = nx.ego_graph(self.graph.to_undirected(), sender, radius=1)
                        
                        # Calculate size and density
                        ego_size = len(ego_graph.nodes())
                        ego_density = nx.density(ego_graph)
                        
                        sender_ego_sizes.append(ego_size)
                        sender_ego_densities.append(ego_density)
                    except:
                        sender_ego_sizes.append(0)
                        sender_ego_densities.append(0)
                
                result_df['sender_ego_network_size'] = sender_ego_sizes
                result_df['sender_ego_network_density'] = sender_ego_densities
            
            # Calculate ego network features for receivers
            if 'receiver_id' in df.columns:
                receiver_ego_sizes = []
                receiver_ego_densities = []
                
                for receiver in df['receiver_id']:
                    try:
                        # Get ego network
                        ego_graph = nx.ego_graph(self.graph.to_undirected(), receiver, radius=1)
                        
                        # Calculate size and density
                        ego_size = len(ego_graph.nodes())
                        ego_density = nx.density(ego_graph)
                        
                        receiver_ego_sizes.append(ego_size)
                        receiver_ego_densities.append(ego_density)
                    except:
                        receiver_ego_sizes.append(0)
                        receiver_ego_densities.append(0)
                
                result_df['receiver_ego_network_size'] = receiver_ego_sizes
                result_df['receiver_ego_network_density'] = receiver_ego_densities
            
            # Calculate bipartite projection features
            if self.bipartite_graph is not None:
                # Get sender and receiver sets
                senders = {n for n, d in self.bipartite_graph.nodes(data=True) if d['bipartite'] == 0}
                receivers = {n for n, d in self.bipartite_graph.nodes(data=True) if d['bipartite'] == 1}
                
                # Project to sender graph
                sender_projection = nx.bipartite.projected_graph(self.bipartite_graph, senders)
                
                # Calculate features for sender projection
                if 'sender_id' in df.columns:
                    sender_proj_degrees = []
                    for sender in df['sender_id']:
                        try:
                            degree = sender_projection.degree(sender)
                            sender_proj_degrees.append(degree)
                        except:
                            sender_proj_degrees.append(0)
                    
                    result_df['sender_projection_degree'] = sender_proj_degrees
                
                # Project to receiver graph
                receiver_projection = nx.bipartite.projected_graph(self.bipartite_graph, receivers)
                
                # Calculate features for receiver projection
                if 'receiver_id' in df.columns:
                    receiver_proj_degrees = []
                    for receiver in df['receiver_id']:
                        try:
                            degree = receiver_projection.degree(receiver)
                            receiver_proj_degrees.append(degree)
                        except:
                            receiver_proj_degrees.append(0)
                    
                    result_df['receiver_projection_degree'] = receiver_proj_degrees
            
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting subgraph features: {str(e)}")
            return df
    
    def _extract_temporal_graph_features(self, df):
        """
        Extract temporal graph features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with temporal graph features
        """
        try:
            result_df = df.copy()
            
            if self.graph is None or 'timestamp' not in df.columns:
                logger.warning("Graph not built or timestamp not available. Skipping temporal graph features.")
                return result_df
            
            # Convert timestamp to datetime if needed
            if not pd.api.types.is_datetime64_any_dtype(df['timestamp']):
                df['timestamp'] = pd.to_datetime(df['timestamp'])
            
            # Sort by timestamp
            df_sorted = df.sort_values('timestamp')
            
            # Calculate time window features
            time_windows = ['1H', '6H', '24H', '7D']
            
            for window in time_windows:
                # Calculate sender activity in time window
                if 'sender_id' in df.columns:
                    sender_activity = []
                    
                    for _, row in df_sorted.iterrows():
                        sender = row['sender_id']
                        timestamp = row['timestamp']
                        
                        # Get transactions in time window before current transaction
                        window_start = timestamp - pd.Timedelta(window)
                        window_end = timestamp
                        
                        window_transactions = df_sorted[
                            (df_sorted['timestamp'] >= window_start) &
                            (df_sorted['timestamp'] < window_end) &
                            (df_sorted['sender_id'] == sender)
                        ]
                        
                        # Calculate activity metrics
                        activity_count = len(window_transactions)
                        activity_amount = window_transactions['amount'].sum() if 'amount' in window_transactions.columns else 0
                        
                        sender_activity.append({
                            f'sender_activity_count_{window}': activity_count,
                            f'sender_activity_amount_{window}': activity_amount
                        })
                    
                    # Add to result dataframe
                    activity_df = pd.DataFrame(sender_activity)
                    for col in activity_df.columns:
                        result_df[col] = activity_df[col].values
                
                # Calculate receiver activity in time window
                if 'receiver_id' in df.columns:
                    receiver_activity = []
                    
                    for _, row in df_sorted.iterrows():
                        receiver = row['receiver_id']
                        timestamp = row['timestamp']
                        
                        # Get transactions in time window before current transaction
                        window_start = timestamp - pd.Timedelta(window)
                        window_end = timestamp
                        
                        window_transactions = df_sorted[
                            (df_sorted['timestamp'] >= window_start) &
                            (df_sorted['timestamp'] < window_end) &
                            (df_sorted['receiver_id'] == receiver)
                        ]
                        
                        # Calculate activity metrics
                        activity_count = len(window_transactions)
                        activity_amount = window_transactions['amount'].sum() if 'amount' in window_transactions.columns else 0
                        
                        receiver_activity.append({
                            f'receiver_activity_count_{window}': activity_count,
                            f'receiver_activity_amount_{window}': activity_amount
                        })
                    
                    # Add to result dataframe
                    activity_df = pd.DataFrame(receiver_activity)
                    for col in activity_df.columns:
                        result_df[col] = activity_df[col].values
            
            # Calculate time since last transaction for sender
            if 'sender_id' in df.columns:
                time_since_last = []
                
                for _, row in df_sorted.iterrows():
                    sender = row['sender_id']
                    timestamp = row['timestamp']
                    
                    # Get previous transaction from same sender
                    prev_transactions = df_sorted[
                        (df_sorted['timestamp'] < timestamp) &
                        (df_sorted['sender_id'] == sender)
                    ]
                    
                    if len(prev_transactions) > 0:
                        last_timestamp = prev_transactions['timestamp'].max()
                        time_diff = (timestamp - last_timestamp).total_seconds()
                        time_since_last.append(time_diff)
                    else:
                        time_since_last.append(float('inf'))
                
                result_df['sender_time_since_last_transaction'] = time_since_last
            
            # Calculate time since last transaction for receiver
            if 'receiver_id' in df.columns:
                time_since_last = []
                
                for _, row in df_sorted.iterrows():
                    receiver = row['receiver_id']
                    timestamp = row['timestamp']
                    
                    # Get previous transaction to same receiver
                    prev_transactions = df_sorted[
                        (df_sorted['timestamp'] < timestamp) &
                        (df_sorted['receiver_id'] == receiver)
                    ]
                    
                    if len(prev_transactions) > 0:
                        last_timestamp = prev_transactions['timestamp'].max()
                        time_diff = (timestamp - last_timestamp).total_seconds()
                        time_since_last.append(time_diff)
                    else:
                        time_since_last.append(float('inf'))
                
                result_df['receiver_time_since_last_transaction'] = time_since_last
            
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting temporal graph features: {str(e)}")
            return df
    
    def fit_transform(self, df):
        """
        Fit the feature extractor and transform the data
        
        Args:
            df (DataFrame): Input data
            
        Returns:
            DataFrame: Transformed data with features
        """
        # Extract features
        result_df = self.extract_features(df)
        
        # Get feature columns
        feature_cols = [col for col in result_df.columns if col not in df.columns]
        
        if len(feature_cols) > 0:
            # Fit scaler
            self.scaler.fit(result_df[feature_cols])
            
            self.fitted = True
        
        return result_df
    
    def transform(self, df):
        """
        Transform new data using fitted feature extractor
        
        Args:
            df (DataFrame): Input data
            
        Returns:
            DataFrame: Transformed data with features
        """
        if not self.fitted:
            raise ValueError("Feature extractor not fitted. Call fit_transform first.")
        
        # Extract features
        result_df = self.extract_features(df)
        
        # Get feature columns
        feature_cols = [col for col in result_df.columns if col not in df.columns]
        
        if len(feature_cols) > 0:
            # Transform features using fitted scaler
            result_df[feature_cols] = self.scaler.transform(result_df[feature_cols])
        
        return result_df

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\features\nlp_features.py ===
"""
NLP Features Module
Implements natural language processing features for fraud detection
"""

import pandas as pd
import numpy as np
import re
import string
from collections import Counter
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.stem import WordNetLemmatizer, PorterStemmer
from nltk.sentiment import SentimentIntensityAnalyzer
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation
from textblob import TextBlob
import gensim
from gensim.models import Word2Vec, Doc2Vec
from gensim.models.doc2vec import TaggedDocument
import warnings
import logging
from typing import Dict, List, Tuple, Union

from fraud_detection_engine.utils.api_utils import is_api_available

warnings.filterwarnings('ignore')
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Download required NLTK data
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')
    
try:
    nltk.data.find('corpora/stopwords')
except LookupError:
    nltk.download('stopwords')
    
try:
    nltk.data.find('corpora/wordnet')
except LookupError:
    nltk.download('wordnet')
    
try:
    nltk.data.find('sentiment/vader_lexicon')
except LookupError:
    nltk.download('vader_lexicon')

class NLPFeatures:
    """
    Class for extracting NLP features from transaction data
    Implements techniques like sentiment analysis, topic modeling, etc.
    """
    
    def __init__(self, config=None):
        """
        Initialize NLPFeatures
        
        Args:
            config (dict, optional): Configuration parameters
        """
        self.config = config or {}
        self.stop_words = set(stopwords.words('english'))
        self.lemmatizer = WordNetLemmatizer()
        self.stemmer = PorterStemmer()
        self.sia = SentimentIntensityAnalyzer()
        self.tfidf_vectorizer = None
        self.count_vectorizer = None
        self.lda_model = None
        self.word2vec_model = None
        self.doc2vec_model = None
        self.feature_names = []
        self.fitted = False
        
        # Fraud-related keywords
        self.fraud_keywords = [
            'urgent', 'immediately', 'asap', 'hurry', 'quick', 'fast',
            'secret', 'confidential', 'private', 'hidden', 'discreet',
            'suspicious', 'unusual', 'strange', 'odd', 'weird',
            'illegal', 'fraud', 'scam', 'fake', 'counterfeit',
            'money', 'cash', 'payment', 'transfer', 'wire',
            'overseas', 'foreign', 'international', 'abroad',
            'inheritance', 'lottery', 'prize', 'winner', 'claim',
            'verify', 'confirm', 'update', 'account', 'information',
            'click', 'link', 'website', 'login', 'password',
            'bank', 'check', 'routing', 'account', 'number'
        ]
        
        # Suspicious patterns
        self.suspicious_patterns = [
            r'\$\d+,\d+\.\d{2}',  # Money format
            r'\d{4}[\s-]?\d{4}[\s-]?\d{4}[\s-]?\d{4}',  # Credit card pattern
            r'\b\d{3}-\d{2}-\d{4}\b',  # SSN pattern
            r'\b[A-Z0-9._%+-]+@[A-Z0-9.-]+\.[A-Z]{2,}\b',  # Email pattern
            r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+',  # URL pattern
            r'\b\d{10,}\b',  # Long numbers (could be account numbers)
            r'[A-Z]{2,}',  # All caps words
            r'\d+\.\d+\.\d+\.\d+',  # IP address pattern
        ]
    
    def extract_features(self, df):
        """
        Extract all NLP features from the dataframe
        
        Args:
            df (DataFrame): Input transaction data
            
        Returns:
            DataFrame: DataFrame with extracted features
        """
        try:
            # Create a copy to avoid modifying the original
            result_df = df.copy()
            
            # Extract different types of NLP features
            result_df = self._extract_basic_text_features(result_df)
            result_df = self._extract_sentiment_features(result_df)
            result_df = self._extract_keyword_features(result_df)
            result_df = self._extract_pattern_features(result_df)
            result_df = self._extract_topic_features(result_df)
            result_df = self._extract_embedding_features(result_df)
            
            # Store feature names
            self.feature_names = [col for col in result_df.columns if col not in df.columns]
            
            logger.info(f"Extracted {len(self.feature_names)} NLP features")
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting NLP features: {str(e)}")
            raise
    
    def _extract_basic_text_features(self, df):
        """
        Extract basic text features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with basic text features
        """
        try:
            result_df = df.copy()
            
            # Text columns to process
            text_columns = ['description', 'notes']
            
            for col in text_columns:
                if col in df.columns:
                    # Character count
                    result_df[f'{col}_char_count'] = df[col].fillna('').astype(str).apply(len)
                    
                    # Word count
                    result_df[f'{col}_word_count'] = df[col].fillna('').astype(str).apply(
                        lambda x: len(word_tokenize(x)) if x else 0
                    )
                    
                    # Sentence count
                    result_df[f'{col}_sentence_count'] = df[col].fillna('').astype(str).apply(
                        lambda x: len(sent_tokenize(x)) if x else 0
                    )
                    
                    # Average word length
                    result_df[f'{col}_avg_word_length'] = df[col].fillna('').astype(str).apply(
                        lambda x: np.mean([len(word) for word in word_tokenize(x)]) if word_tokenize(x) else 0
                    )
                    
                    # Average sentence length
                    result_df[f'{col}_avg_sentence_length'] = df[col].fillna('').astype(str).apply(
                        lambda x: np.mean([len(word_tokenize(sent)) for sent in sent_tokenize(x)]) if sent_tokenize(x) else 0
                    )
                    
                    # Punctuation count
                    result_df[f'{col}_punctuation_count'] = df[col].fillna('').astype(str).apply(
                        lambda x: sum(1 for char in x if char in string.punctuation)
                    )
                    
                    # Uppercase word count
                    result_df[f'{col}_uppercase_count'] = df[col].fillna('').astype(str).apply(
                        lambda x: sum(1 for word in word_tokenize(x) if word.isupper() and len(word) > 1)
                    )
                    
                    # Digit count
                    result_df[f'{col}_digit_count'] = df[col].fillna('').astype(str).apply(
                        lambda x: sum(1 for char in x if char.isdigit())
                    )
                    
                    # Unique word count
                    result_df[f'{col}_unique_word_count'] = df[col].fillna('').astype(str).apply(
                        lambda x: len(set(word_tokenize(x.lower()))) if x else 0
                    )
                    
                    # Lexical diversity (unique words / total words)
                    result_df[f'{col}_lexical_diversity'] = df[col].fillna('').astype(str).apply(
                        lambda x: len(set(word_tokenize(x.lower()))) / len(word_tokenize(x)) if word_tokenize(x) else 0
                    )
            
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting basic text features: {str(e)}")
            return df
    
    def _extract_sentiment_features(self, df):
        """
        Extract sentiment analysis features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with sentiment features
        """
        try:
            result_df = df.copy()
            
            # Text columns to process
            text_columns = ['description', 'notes']
            
            for col in text_columns:
                if col in df.columns:
                    # VADER sentiment scores
                    sentiment_scores = df[col].fillna('').astype(str).apply(
                        lambda x: self.sia.polarity_scores(x) if x else {'neg': 0, 'neu': 0, 'pos': 0, 'compound': 0}
                    )
                    
                    # Extract individual scores
                    result_df[f'{col}_sentiment_neg'] = sentiment_scores.apply(lambda x: x['neg'])
                    result_df[f'{col}_sentiment_neu'] = sentiment_scores.apply(lambda x: x['neu'])
                    result_df[f'{col}_sentiment_pos'] = sentiment_scores.apply(lambda x: x['pos'])
                    result_df[f'{col}_sentiment_compound'] = sentiment_scores.apply(lambda x: x['compound'])
                    
                    # TextBlob sentiment
                    textblob_sentiment = df[col].fillna('').astype(str).apply(
                        lambda x: TextBlob(x).sentiment if x else TextBlob('').sentiment
                    )
                    
                    result_df[f'{col}_textblob_polarity'] = textblob_sentiment.apply(lambda x: x.polarity)
                    result_df[f'{col}_textblob_subjectivity'] = textblob_sentiment.apply(lambda x: x.subjectivity)
                    
                    # Emotion indicators
                    result_df[f'{col}_is_negative'] = (result_df[f'{col}_sentiment_compound'] < -0.05).astype(int)
                    result_df[f'{col}_is_positive'] = (result_df[f'{col}_sentiment_compound'] > 0.05).astype(int)
                    result_df[f'{col}_is_neutral'] = (
                        (result_df[f'{col}_sentiment_compound'] >= -0.05) & 
                        (result_df[f'{col}_sentiment_compound'] <= 0.05)
                    ).astype(int)
            
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting sentiment features: {str(e)}")
            return df
    
    def _extract_keyword_features(self, df):
        """
        Extract keyword-based features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with keyword features
        """
        try:
            result_df = df.copy()
            
            # Text columns to process
            text_columns = ['description', 'notes']
            
            for col in text_columns:
                if col in df.columns:
                    # Preprocess text
                    processed_text = df[col].fillna('').astype(str).apply(self._preprocess_text)
                    
                    # Count fraud keywords
                    fraud_keyword_counts = processed_text.apply(
                        lambda x: sum(1 for word in x if word in self.fraud_keywords)
                    )
                    result_df[f'{col}_fraud_keyword_count'] = fraud_keyword_counts
                    
                    # Flag if any fraud keywords present
                    result_df[f'{col}_has_fraud_keywords'] = (fraud_keyword_counts > 0).astype(int)
                    
                    # Count specific keyword categories
                    urgency_keywords = ['urgent', 'immediately', 'asap', 'hurry', 'quick', 'fast']
                    secrecy_keywords = ['secret', 'confidential', 'private', 'hidden', 'discreet']
                    money_keywords = ['money', 'cash', 'payment', 'transfer', 'wire']
                    
                    result_df[f'{col}_urgency_keyword_count'] = processed_text.apply(
                        lambda x: sum(1 for word in x if word in urgency_keywords)
                    )
                    
                    result_df[f'{col}_secrecy_keyword_count'] = processed_text.apply(
                        lambda x: sum(1 for word in x if word in secrecy_keywords)
                    )
                    
                    result_df[f'{col}_money_keyword_count'] = processed_text.apply(
                        lambda x: sum(1 for word in x if word in money_keywords)
                    )
                    
                    # Calculate keyword density
                    result_df[f'{col}_fraud_keyword_density'] = fraud_keyword_counts / (
                        df[col].fillna('').astype(str).apply(lambda x: len(word_tokenize(x)) if x else 1)
                    )
                    
                    # TF-IDF for fraud keywords
                    if not self.fitted:
                        # Fit TF-IDF vectorizer
                        self.tfidf_vectorizer = TfidfVectorizer(
                            vocabulary=self.fraud_keywords,
                            ngram_range=(1, 2),
                            max_features=100
                        )
                        
                        # Fit on all text
                        all_text = pd.concat([df[col].fillna('').astype(str) for col in text_columns if col in df.columns])
                        self.tfidf_vectorizer.fit(all_text)
                    
                    # Transform text
                    tfidf_features = self.tfidf_vectorizer.transform(df[col].fillna('').astype(str))
                    
                    # Add top TF-IDF features
                    feature_names = self.tfidf_vectorizer.get_feature_names_out()
                    for i, feature in enumerate(feature_names[:10]):  # Top 10 features
                        result_df[f'{col}_tfidf_{feature}'] = tfidf_features[:, i].toarray().flatten()
            
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting keyword features: {str(e)}")
            return df
    
    def _extract_pattern_features(self, df):
        """
        Extract pattern-based features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with pattern features
        """
        try:
            result_df = df.copy()
            
            # Text columns to process
            text_columns = ['description', 'notes']
            
            for col in text_columns:
                if col in df.columns:
                    # Count suspicious patterns
                    pattern_counts = []
                    for text in df[col].fillna('').astype(str):
                        count = 0
                        for pattern in self.suspicious_patterns:
                            matches = re.findall(pattern, text, re.IGNORECASE)
                            count += len(matches)
                        pattern_counts.append(count)
                    
                    result_df[f'{col}_suspicious_pattern_count'] = pattern_counts
                    
                    # Flag if any suspicious patterns present
                    result_df[f'{col}_has_suspicious_patterns'] = (np.array(pattern_counts) > 0).astype(int)
                    
                    # Specific pattern counts
                    money_pattern = r'\$\d+,\d+\.\d{2}'
                    credit_card_pattern = r'\d{4}[\s-]?\d{4}[\s-]?\d{4}[\s-]?\d{4}'
                    ssn_pattern = r'\b\d{3}-\d{2}-\d{4}\b'
                    email_pattern = r'\b[A-Z0-9._%+-]+@[A-Z0-9.-]+\.[A-Z]{2,}\b'
                    url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'
                    
                    result_df[f'{col}_money_pattern_count'] = df[col].fillna('').astype(str).apply(
                        lambda x: len(re.findall(money_pattern, x))
                    )
                    
                    result_df[f'{col}_credit_card_pattern_count'] = df[col].fillna('').astype(str).apply(
                        lambda x: len(re.findall(credit_card_pattern, x))
                    )
                    
                    result_df[f'{col}_ssn_pattern_count'] = df[col].fillna('').astype(str).apply(
                        lambda x: len(re.findall(ssn_pattern, x))
                    )
                    
                    result_df[f'{col}_email_pattern_count'] = df[col].fillna('').astype(str).apply(
                        lambda x: len(re.findall(email_pattern, x))
                    )
                    
                    result_df[f'{col}_url_pattern_count'] = df[col].fillna('').astype(str).apply(
                        lambda x: len(re.findall(url_pattern, x))
                    )
                    
                    # Check for excessive punctuation
                    result_df[f'{col}_excessive_punctuation'] = (
                        df[col].fillna('').astype(str).apply(
                            lambda x: 1 if sum(1 for char in x if char in string.punctuation) / len(x) > 0.3 else 0
                        )
                    )
                    
                    # Check for excessive capitalization
                    result_df[f'{col}_excessive_capitalization'] = (
                        df[col].fillna('').astype(str).apply(
                            lambda x: 1 if sum(1 for char in x if char.isupper()) / len(x) > 0.5 else 0
                        )
                    )
            
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting pattern features: {str(e)}")
            return df
    
    def _extract_topic_features(self, df):
        """
        Extract topic modeling features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with topic features
        """
        try:
            result_df = df.copy()
            
            # Text columns to process
            text_columns = ['description', 'notes']
            
            # Combine text from all columns
            all_text = []
            for col in text_columns:
                if col in df.columns:
                    all_text.extend(df[col].fillna('').astype(str).tolist())
            
            if not all_text:
                return result_df
            
            # Preprocess text for topic modeling
            processed_docs = [self._preprocess_text(doc) for doc in all_text]
            processed_docs = [' '.join(doc) for doc in processed_docs if doc]
            
            if not processed_docs:
                return result_df
            
            # Fit CountVectorizer and LDA model if not fitted
            if not self.fitted:
                # Fit CountVectorizer
                self.count_vectorizer = CountVectorizer(
                    max_features=1000,
                    stop_words='english',
                    ngram_range=(1, 2)
                )
                doc_term_matrix = self.count_vectorizer.fit_transform(processed_docs)
                
                # Fit LDA model
                self.lda_model = LatentDirichletAllocation(
                    n_components=5,  # 5 topics
                    random_state=42,
                    max_iter=10,
                    learning_method='online'
                )
                self.lda_model.fit(doc_term_matrix)
            
            # Transform each text column
            for col in text_columns:
                if col in df.columns:
                    # Preprocess text
                    processed_text = df[col].fillna('').astype(str).apply(self._preprocess_text)
                    processed_text = processed_text.apply(lambda x: ' '.join(x) if x else '')
                    
                    # Transform to document-term matrix
                    doc_term_matrix = self.count_vectorizer.transform(processed_text)
                    
                    # Get topic distributions
                    topic_distributions = self.lda_model.transform(doc_term_matrix)
                    
                    # Add topic features
                    for i in range(topic_distributions.shape[1]):
                        result_df[f'{col}_topic_{i}_prob'] = topic_distributions[:, i]
                    
                    # Get dominant topic
                    dominant_topics = np.argmax(topic_distributions, axis=1)
                    result_df[f'{col}_dominant_topic'] = dominant_topics
            
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting topic features: {str(e)}")
            return df
    
    def _extract_embedding_features(self, df):
        """
        Extract word embedding features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with embedding features
        """
        try:
            result_df = df.copy()
            
            # Text columns to process
            text_columns = ['description', 'notes']
            
            # Combine text from all columns
            all_text = []
            for col in text_columns:
                if col in df.columns:
                    all_text.extend(df[col].fillna('').astype(str).tolist())
            
            if not all_text:
                return result_df
            
            # Preprocess text for embeddings
            processed_docs = [self._preprocess_text(doc) for doc in all_text]
            processed_docs = [doc for doc in processed_docs if doc]
            
            if not processed_docs:
                return result_df
            
            # Check if Gemini API is available
            if is_api_available('gemini'):
                # Here you would implement Gemini API calls for embeddings
                # For now, we'll skip and use local embeddings
                logger.info("Gemini API available, but implementation pending. Using local embeddings.")
            
            # Check if OpenAI API is available
            if is_api_available('openai'):
                # Here you would implement OpenAI API calls for embeddings
                # For now, we'll skip and use local embeddings
                logger.info("OpenAI API available, but implementation pending. Using local embeddings.")
            
            # Fit Word2Vec model (local implementation)
            self.word2vec_model = Word2Vec(
                sentences=processed_docs,
                vector_size=100,  # 100-dimensional vectors
                window=5,
                min_count=1,
                workers=4,
                sg=1  # Skip-gram model
            )
            
            # Train Doc2Vec model (local implementation)
            tagged_docs = [TaggedDocument(doc, [i]) for i, doc in enumerate(processed_docs)]
            self.doc2vec_model = Doc2Vec(
                documents=tagged_docs,
                vector_size=100,  # 100-dimensional vectors
                window=5,
                min_count=1,
                workers=4,
                epochs=10
            )
            
            # Transform each text column
            for col in text_columns:
                if col in df.columns:
                    # Preprocess text
                    processed_text = df[col].fillna('').astype(str).apply(self._preprocess_text)
                    
                    # Calculate average Word2Vec vectors
                    word2vec_vectors = []
                    for doc in processed_text:
                        if doc:
                            # Get vectors for words in document
                            word_vectors = [self.word2vec_model.wv[word] for word in doc if word in self.word2vec_model.wv]
                            
                            if word_vectors:
                                # Average the vectors
                                avg_vector = np.mean(word_vectors, axis=0)
                                word2vec_vectors.append(avg_vector)
                            else:
                                # Use zero vector if no words found
                                word2vec_vectors.append(np.zeros(100))
                        else:
                            # Use zero vector for empty documents
                            word2vec_vectors.append(np.zeros(100))
                    
                    # Add Word2Vec features (first 10 dimensions)
                    word2vec_vectors = np.array(word2vec_vectors)
                    for i in range(min(10, word2vec_vectors.shape[1])):
                        result_df[f'{col}_word2vec_dim_{i}'] = word2vec_vectors[:, i]
                    
                    # Calculate Doc2Vec vectors
                    doc2vec_vectors = []
                    for doc in processed_text:
                        if doc:
                            # Infer vector for document
                            vector = self.doc2vec_model.infer_vector(doc)
                            doc2vec_vectors.append(vector)
                        else:
                            # Use zero vector for empty documents
                            doc2vec_vectors.append(np.zeros(100))
                    
                    # Add Doc2Vec features (first 10 dimensions)
                    doc2vec_vectors = np.array(doc2vec_vectors)
                    for i in range(min(10, doc2vec_vectors.shape[1])):
                        result_df[f'{col}_doc2vec_dim_{i}'] = doc2vec_vectors[:, i]
            
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting embedding features: {str(e)}")
            return df
    
    def _preprocess_text(self, text):
        """
        Preprocess text for NLP analysis
        
        Args:
            text (str): Input text
            
        Returns:
            list: List of processed tokens
        """
        try:
            # Convert to lowercase
            text = text.lower()
            
            # Remove punctuation
            text = text.translate(str.maketrans('', '', string.punctuation))
            
            # Remove digits
            text = re.sub(r'\d+', '', text)
            
            # Tokenize
            tokens = word_tokenize(text)
            
            # Remove stop words
            tokens = [word for word in tokens if word not in self.stop_words]
            
            # Lemmatize
            tokens = [self.lemmatizer.lemmatize(word) for word in tokens]
            
            # Remove short words
            tokens = [word for word in tokens if len(word) > 2]
            
            return tokens
            
        except Exception as e:
            logger.error(f"Error preprocessing text: {str(e)}")
            return []
    
    def fit_transform(self, df):
        """
        Fit the feature extractor and transform the data
        
        Args:
            df (DataFrame): Input data
            
        Returns:
            DataFrame: Transformed data with features
        """
        # Extract features
        result_df = self.extract_features(df)
        
        # Get feature columns
        feature_cols = [col for col in result_df.columns if col not in df.columns]
        
        if len(feature_cols) > 0:
            self.fitted = True
        
        return result_df
    
    def transform(self, df):
        """
        Transform new data using fitted feature extractor
        
        Args:
            df (DataFrame): Input data
            
        Returns:
            DataFrame: Transformed data with features
        """
        if not self.fitted:
            raise ValueError("Feature extractor not fitted. Call fit_transform first.")
        
        # Extract features
        result_df = self.extract_features(df)
        
        return result_df

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\features\statistical_features.py ===
"""
Statistical Features Module
Implements various statistical feature extraction techniques for fraud detection
"""

import pandas as pd
import numpy as np
import scipy.stats as stats
from scipy.spatial.distance import mahalanobis
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
import warnings
import logging
from typing import Dict, List, Tuple, Union

warnings.filterwarnings('ignore')
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class StatisticalFeatures:
    """
    Class for extracting statistical features from transaction data
    Implements techniques like Benford's Law, Z-score, MAD, etc.
    """
    
    def __init__(self, config=None):
        """
        Initialize StatisticalFeatures
        
        Args:
            config (dict, optional): Configuration parameters
        """
        self.config = config or {}
        self.feature_names = []
        self.scaler = StandardScaler()
        self.pca = None
        self.fitted = False
        
    def extract_features(self, df):
        """
        Extract all statistical features from the dataframe
        
        Args:
            df (DataFrame): Input transaction data
            
        Returns:
            DataFrame: DataFrame with extracted features
        """
        try:
            # Create a copy to avoid modifying the original
            result_df = df.copy()
            
            # Extract different types of statistical features
            result_df = self._extract_benford_features(result_df)
            result_df = self._extract_zscore_features(result_df)
            result_df = self._extract_mad_features(result_df)
            result_df = self._extract_percentile_features(result_df)
            result_df = self._extract_distribution_features(result_df)
            result_df = self._extract_mahalanobis_features(result_df)
            result_df = self._extract_grubbs_features(result_df)
            result_df = self._extract_entropy_features(result_df)
            result_df = self._extract_correlation_features(result_df)
            
            # Store feature names
            self.feature_names = [col for col in result_df.columns if col not in df.columns]
            
            logger.info(f"Extracted {len(self.feature_names)} statistical features")
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting statistical features: {str(e)}")
            raise
    
    def _extract_benford_features(self, df):
        """
        Extract Benford's Law features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with Benford's Law features
        """
        try:
            result_df = df.copy()
            
            # Apply Benford's Law to amount column if it exists
            if 'amount' in df.columns:
                # Get first digit of amounts
                amounts = df['amount'].abs()
                first_digits = amounts.astype(str).str[0].replace('n', '0').astype(int)
                first_digits = first_digits[first_digits >= 1]  # Exclude 0
                
                if len(first_digits) > 0:
                    # Calculate actual distribution
                    actual_dist = first_digits.value_counts(normalize=True).sort_index()
                    
                    # Expected Benford's distribution
                    benford_dist = pd.Series([np.log10(1 + 1/d) for d in range(1, 10)], index=range(1, 10))
                    
                    # Calculate Chi-square statistic
                    chi_square = 0
                    for digit in range(1, 10):
                        expected_count = benford_dist[digit] * len(first_digits)
                        actual_count = actual_dist.get(digit, 0)
                        if expected_count > 0:
                            chi_square += (actual_count - expected_count) ** 2 / expected_count
                    
                    # Add features
                    result_df['benford_chi_square'] = chi_square
                    result_df['benford_p_value'] = 1 - stats.chi2.cdf(chi_square, 8)  # 8 degrees of freedom
                    
                    # Calculate deviation for each digit
                    for digit in range(1, 6):  # Just first 5 digits to avoid too many features
                        actual_pct = actual_dist.get(digit, 0)
                        expected_pct = benford_dist[digit]
                        result_df[f'benford_deviation_{digit}'] = abs(actual_pct - expected_pct)
            
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting Benford's Law features: {str(e)}")
            return df
    
    def _extract_zscore_features(self, df):
        """
        Extract Z-score based features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with Z-score features
        """
        try:
            result_df = df.copy()
            
            # Apply to amount column if it exists
            if 'amount' in df.columns:
                amounts = df['amount']
                
                # Calculate Z-scores
                mean_amount = amounts.mean()
                std_amount = amounts.std()
                
                if std_amount > 0:
                    z_scores = (amounts - mean_amount) / std_amount
                    result_df['amount_zscore'] = z_scores
                    
                    # Flag extreme values
                    result_df['amount_zscore_outlier'] = (np.abs(z_scores) > 3).astype(int)
                else:
                    result_df['amount_zscore'] = 0
                    result_df['amount_zscore_outlier'] = 0
            
            # Apply Z-score to sender and receiver if ID columns exist
            if 'sender_id' in df.columns and 'amount' in df.columns:
                # Calculate average amount per sender
                sender_avg = df.groupby('sender_id')['amount'].mean()
                sender_std = df.groupby('sender_id')['amount'].std()
                
                # Calculate Z-scores for each transaction relative to sender's history
                sender_zscores = []
                for _, row in df.iterrows():
                    sender_id = row['sender_id']
                    amount = row['amount']
                    
                    if sender_id in sender_avg and sender_id in sender_std and sender_std[sender_id] > 0:
                        z_score = (amount - sender_avg[sender_id]) / sender_std[sender_id]
                    else:
                        z_score = 0
                    
                    sender_zscores.append(z_score)
                
                result_df['sender_amount_zscore'] = sender_zscores
                result_df['sender_amount_zscore_outlier'] = (np.abs(sender_zscores) > 3).astype(int)
            
            if 'receiver_id' in df.columns and 'amount' in df.columns:
                # Calculate average amount per receiver
                receiver_avg = df.groupby('receiver_id')['amount'].mean()
                receiver_std = df.groupby('receiver_id')['amount'].std()
                
                # Calculate Z-scores for each transaction relative to receiver's history
                receiver_zscores = []
                for _, row in df.iterrows():
                    receiver_id = row['receiver_id']
                    amount = row['amount']
                    
                    if receiver_id in receiver_avg and receiver_id in receiver_std and receiver_std[receiver_id] > 0:
                        z_score = (amount - receiver_avg[receiver_id]) / receiver_std[receiver_id]
                    else:
                        z_score = 0
                    
                    receiver_zscores.append(z_score)
                
                result_df['receiver_amount_zscore'] = receiver_zscores
                result_df['receiver_amount_zscore_outlier'] = (np.abs(receiver_zscores) > 3).astype(int)
            
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting Z-score features: {str(e)}")
            return df
    
    def _extract_mad_features(self, df):
        """
        Extract Median Absolute Deviation (MAD) features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with MAD features
        """
        try:
            result_df = df.copy()
            
            # Apply to amount column if it exists
            if 'amount' in df.columns:
                amounts = df['amount']
                
                # Calculate MAD
                median_amount = amounts.median()
                abs_dev = np.abs(amounts - median_amount)
                mad = abs_dev.median()
                
                if mad > 0:
                    # Calculate modified Z-scores using MAD
                    modified_z_scores = 0.6745 * abs_dev / mad
                    result_df['amount_mad_zscore'] = modified_z_scores
                    
                    # Flag extreme values
                    result_df['amount_mad_outlier'] = (modified_z_scores > 3.5).astype(int)
                else:
                    result_df['amount_mad_zscore'] = 0
                    result_df['amount_mad_outlier'] = 0
            
            # Apply MAD to sender and receiver if ID columns exist
            if 'sender_id' in df.columns and 'amount' in df.columns:
                # Calculate MAD per sender
                sender_mad = df.groupby('sender_id')['amount'].apply(lambda x: np.median(np.abs(x - x.median())))
                sender_median = df.groupby('sender_id')['amount'].median()
                
                # Calculate MAD-based Z-scores for each transaction
                sender_mad_zscores = []
                for _, row in df.iterrows():
                    sender_id = row['sender_id']
                    amount = row['amount']
                    
                    if sender_id in sender_mad and sender_id in sender_median and sender_mad[sender_id] > 0:
                        abs_dev = abs(amount - sender_median[sender_id])
                        mad_z_score = 0.6745 * abs_dev / sender_mad[sender_id]
                    else:
                        mad_z_score = 0
                    
                    sender_mad_zscores.append(mad_z_score)
                
                result_df['sender_amount_mad_zscore'] = sender_mad_zscores
                result_df['sender_amount_mad_outlier'] = (np.array(sender_mad_zscores) > 3.5).astype(int)
            
            if 'receiver_id' in df.columns and 'amount' in df.columns:
                # Calculate MAD per receiver
                receiver_mad = df.groupby('receiver_id')['amount'].apply(lambda x: np.median(np.abs(x - x.median())))
                receiver_median = df.groupby('receiver_id')['amount'].median()
                
                # Calculate MAD-based Z-scores for each transaction
                receiver_mad_zscores = []
                for _, row in df.iterrows():
                    receiver_id = row['receiver_id']
                    amount = row['amount']
                    
                    if receiver_id in receiver_mad and receiver_id in receiver_median and receiver_mad[receiver_id] > 0:
                        abs_dev = abs(amount - receiver_median[receiver_id])
                        mad_z_score = 0.6745 * abs_dev / receiver_mad[receiver_id]
                    else:
                        mad_z_score = 0
                    
                    receiver_mad_zscores.append(mad_z_score)
                
                result_df['receiver_amount_mad_zscore'] = receiver_mad_zscores
                result_df['receiver_amount_mad_outlier'] = (np.array(receiver_mad_zscores) > 3.5).astype(int)
            
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting MAD features: {str(e)}")
            return df
    
    def _extract_percentile_features(self, df):
        """
        Extract percentile-based features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with percentile features
        """
        try:
            result_df = df.copy()
            
            # Apply to amount column if it exists
            if 'amount' in df.columns:
                amounts = df['amount']
                
                # Calculate percentiles
                percentiles = [5, 10, 25, 50, 75, 90, 95, 99]
                percentile_values = np.percentile(amounts, percentiles)
                
                # Add features for each percentile
                for i, p in enumerate(percentiles):
                    result_df[f'amount_above_{p}th_percentile'] = (amounts > percentile_values[i]).astype(int)
                
                # Calculate percentile rank for each amount
                result_df['amount_percentile_rank'] = amounts.rank(pct=True)
            
            # Apply percentiles to sender and receiver if ID columns exist
            if 'sender_id' in df.columns and 'amount' in df.columns:
                # Calculate percentile rank within each sender's transactions
                sender_percentile_ranks = df.groupby('sender_id')['amount'].rank(pct=True)
                result_df['sender_amount_percentile_rank'] = sender_percentile_ranks
                
                # Flag if amount is in top 5% for sender
                result_df['sender_amount_top_5pct'] = (sender_percentile_ranks > 0.95).astype(int)
            
            if 'receiver_id' in df.columns and 'amount' in df.columns:
                # Calculate percentile rank within each receiver's transactions
                receiver_percentile_ranks = df.groupby('receiver_id')['amount'].rank(pct=True)
                result_df['receiver_amount_percentile_rank'] = receiver_percentile_ranks
                
                # Flag if amount is in top 5% for receiver
                result_df['receiver_amount_top_5pct'] = (receiver_percentile_ranks > 0.95).astype(int)
            
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting percentile features: {str(e)}")
            return df
    
    def _extract_distribution_features(self, df):
        """
        Extract distribution-based features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with distribution features
        """
        try:
            result_df = df.copy()
            
            # Apply to amount column if it exists
            if 'amount' in df.columns:
                amounts = df['amount']
                
                # Skewness and kurtosis
                result_df['amount_skewness'] = stats.skew(amounts)
                result_df['amount_kurtosis'] = stats.kurtosis(amounts)
                
                # Normality tests
                _, normality_p = stats.normaltest(amounts)
                result_df['amount_normality_p'] = normality_p
                
                # Shapiro-Wilk test (for smaller samples)
                if len(amounts) <= 5000:
                    _, shapiro_p = stats.shapiro(amounts)
                    result_df['amount_shapiro_p'] = shapiro_p
                
                # Kolmogorov-Smirnov test against normal distribution
                _, ks_p = stats.kstest(amounts, 'norm', args=(amounts.mean(), amounts.std()))
                result_df['amount_ks_p'] = ks_p
                
                # Anderson-Darling test
                ad_result = stats.anderson(amounts)
                result_df['amount_ad_statistic'] = ad_result.statistic
                
                # Jarque-Bera test
                jb_stat, jb_p = stats.jarque_bera(amounts)
                result_df['amount_jb_statistic'] = jb_stat
                result_df['amount_jb_p'] = jb_p
            
            # Apply distribution features to sender and receiver if ID columns exist
            if 'sender_id' in df.columns and 'amount' in df.columns:
                # Calculate distribution features per sender
                sender_stats = df.groupby('sender_id')['amount'].agg([
                    ('skewness', lambda x: stats.skew(x) if len(x) >= 3 else 0),
                    ('kurtosis', lambda x: stats.kurtosis(x) if len(x) >= 4 else 0),
                    ('variance', 'var'),
                    ('range', lambda x: x.max() - x.min() if len(x) > 0 else 0)
                ])
                
                # Map back to each transaction
                for stat in ['skewness', 'kurtosis', 'variance', 'range']:
                    result_df[f'sender_amount_{stat}'] = df['sender_id'].map(sender_stats[stat]).fillna(0)
            
            if 'receiver_id' in df.columns and 'amount' in df.columns:
                # Calculate distribution features per receiver
                receiver_stats = df.groupby('receiver_id')['amount'].agg([
                    ('skewness', lambda x: stats.skew(x) if len(x) >= 3 else 0),
                    ('kurtosis', lambda x: stats.kurtosis(x) if len(x) >= 4 else 0),
                    ('variance', 'var'),
                    ('range', lambda x: x.max() - x.min() if len(x) > 0 else 0)
                ])
                
                # Map back to each transaction
                for stat in ['skewness', 'kurtosis', 'variance', 'range']:
                    result_df[f'receiver_amount_{stat}'] = df['receiver_id'].map(receiver_stats[stat]).fillna(0)
            
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting distribution features: {str(e)}")
            return df
    
    def _extract_mahalanobis_features(self, df):
        """
        Extract Mahalanobis distance features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with Mahalanobis features
        """
        try:
            result_df = df.copy()
            
            # Get numeric columns
            numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
            
            if len(numeric_cols) < 2:
                logger.warning("Not enough numeric columns for Mahalanobis distance calculation")
                return result_df
            
            # Prepare data
            X = df[numeric_cols].fillna(0)
            
            # Calculate covariance matrix
            cov_matrix = np.cov(X, rowvar=False)
            
            # Check if covariance matrix is invertible
            if np.linalg.det(cov_matrix) == 0:
                logger.warning("Covariance matrix is singular, using pseudo-inverse")
                inv_cov_matrix = np.linalg.pinv(cov_matrix)
            else:
                inv_cov_matrix = np.linalg.inv(cov_matrix)
            
            # Calculate mean vector
            mean_vector = np.mean(X, axis=0)
            
            # Calculate Mahalanobis distances
            mahalanobis_distances = []
            for i in range(len(X)):
                mahalanobis_distances.append(
                    mahalanobis(X.iloc[i], mean_vector, inv_cov_matrix)
                )
            
            result_df['mahalanobis_distance'] = mahalanobis_distances
            
            # Flag outliers based on Mahalanobis distance
            # Using chi-square distribution with degrees of freedom = number of features
            threshold = stats.chi2.ppf(0.975, df=len(numeric_cols))
            result_df['mahalanobis_outlier'] = (np.array(mahalanobis_distances) > threshold).astype(int)
            
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting Mahalanobis features: {str(e)}")
            return df
    
    def _extract_grubbs_features(self, df):
        """
        Extract Grubbs' test features for outliers
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with Grubbs' test features
        """
        try:
            result_df = df.copy()
            
            # Apply to amount column if it exists
            if 'amount' in df.columns:
                amounts = df['amount'].values
                
                # Calculate Grubbs' test statistic
                mean_amount = np.mean(amounts)
                std_amount = np.std(amounts)
                
                if std_amount > 0:
                    # Calculate absolute deviations
                    abs_deviations = np.abs(amounts - mean_amount)
                    max_deviation = np.max(abs_deviations)
                    
                    # Grubbs' test statistic
                    grubbs_stat = max_deviation / std_amount
                    result_df['grubbs_statistic'] = grubbs_stat
                    
                    # Calculate critical value for two-sided test
                    n = len(amounts)
                    t_critical = stats.t.ppf(1 - 0.025 / (2 * n), n - 2)
                    critical_value = (n - 1) * t_critical / np.sqrt(n * (n - 2 + t_critical**2))
                    
                    # Flag outliers
                    result_df['grubbs_outlier'] = (grubbs_stat > critical_value).astype(int)
                else:
                    result_df['grubbs_statistic'] = 0
                    result_df['grubbs_outlier'] = 0
            
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting Grubbs' test features: {str(e)}")
            return df
    
    def _extract_entropy_features(self, df):
        """
        Extract entropy-based features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with entropy features
        """
        try:
            result_df = df.copy()
            
            # Calculate entropy for categorical columns
            categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()
            
            for col in categorical_cols:
                # Calculate probability distribution
                value_counts = df[col].value_counts(normalize=True)
                
                # Calculate Shannon entropy
                entropy = -sum(p * np.log2(p) for p in value_counts if p > 0)
                result_df[f'{col}_entropy'] = entropy
                
                # Calculate normalized entropy (0 to 1)
                max_entropy = np.log2(len(value_counts))
                if max_entropy > 0:
                    normalized_entropy = entropy / max_entropy
                else:
                    normalized_entropy = 0
                result_df[f'{col}_normalized_entropy'] = normalized_entropy
            
            # Calculate entropy for numeric columns (after binning)
            numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
            
            for col in numeric_cols:
                # Bin the data
                try:
                    binned = pd.cut(df[col], bins=10, duplicates='drop')
                    
                    # Calculate probability distribution
                    value_counts = binned.value_counts(normalize=True)
                    
                    # Calculate Shannon entropy
                    entropy = -sum(p * np.log2(p) for p in value_counts if p > 0)
                    result_df[f'{col}_binned_entropy'] = entropy
                    
                    # Calculate normalized entropy (0 to 1)
                    max_entropy = np.log2(len(value_counts))
                    if max_entropy > 0:
                        normalized_entropy = entropy / max_entropy
                    else:
                        normalized_entropy = 0
                    result_df[f'{col}_binned_normalized_entropy'] = normalized_entropy
                except:
                    pass
            
            # Calculate transaction entropy for sender and receiver
            if 'sender_id' in df.columns:
                # Calculate entropy of transaction amounts per sender
                sender_entropy = df.groupby('sender_id')['amount'].apply(
                    lambda x: self._calculate_series_entropy(x)
                )
                result_df['sender_amount_entropy'] = df['sender_id'].map(sender_entropy).fillna(0)
            
            if 'receiver_id' in df.columns:
                # Calculate entropy of transaction amounts per receiver
                receiver_entropy = df.groupby('receiver_id')['amount'].apply(
                    lambda x: self._calculate_series_entropy(x)
                )
                result_df['receiver_amount_entropy'] = df['receiver_id'].map(receiver_entropy).fillna(0)
            
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting entropy features: {str(e)}")
            return df
    
    def _calculate_series_entropy(self, series):
        """
        Calculate entropy of a pandas Series
        
        Args:
            series (Series): Input series
            
        Returns:
            float: Entropy value
        """
        try:
            # Bin the data
            binned = pd.cut(series, bins=min(10, len(series)), duplicates='drop')
            
            # Calculate probability distribution
            value_counts = binned.value_counts(normalize=True)
            
            # Calculate Shannon entropy
            entropy = -sum(p * np.log2(p) for p in value_counts if p > 0)
            return entropy
        except:
            return 0
    
    def _extract_correlation_features(self, df):
        """
        Extract correlation-based features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with correlation features
        """
        try:
            result_df = df.copy()
            
            # Get numeric columns
            numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
            
            if len(numeric_cols) < 2:
                logger.warning("Not enough numeric columns for correlation calculation")
                return result_df
            
            # Calculate correlation matrix
            corr_matrix = df[numeric_cols].corr().abs()
            
            # Find highest correlation for each variable
            max_corr = {}
            for col in numeric_cols:
                # Get correlations with other variables
                corrs = corr_matrix[col].drop(col)
                max_corr[col] = corrs.max()
            
            # Add features
            for col in numeric_cols:
                result_df[f'{col}_max_correlation'] = max_corr[col]
            
            # Calculate average correlation
            avg_corr = {}
            for col in numeric_cols:
                # Get correlations with other variables
                corrs = corr_matrix[col].drop(col)
                avg_corr[col] = corrs.mean()
            
            # Add features
            for col in numeric_cols:
                result_df[f'{col}_avg_correlation'] = avg_corr[col]
            
            # Calculate variance inflation factor (VIF) for multicollinearity
            for i, col in enumerate(numeric_cols):
                # Prepare data for VIF calculation
                X = df[numeric_cols].copy()
                y = X[col]
                X = X.drop(col, axis=1)
                
                # Fit linear regression
                try:
                    from sklearn.linear_model import LinearRegression
                    model = LinearRegression()
                    model.fit(X, y)
                    
                    # Calculate R-squared
                    r_squared = model.score(X, y)
                    
                    # Calculate VIF
                    if r_squared < 1:
                        vif = 1 / (1 - r_squared)
                    else:
                        vif = float('inf')
                    
                    result_df[f'{col}_vif'] = vif
                except:
                    result_df[f'{col}_vif'] = 1
            
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting correlation features: {str(e)}")
            return df
    
    def fit_transform(self, df):
        """
        Fit the feature extractor and transform the data
        
        Args:
            df (DataFrame): Input data
            
        Returns:
            DataFrame: Transformed data with features
        """
        # Extract features
        result_df = self.extract_features(df)
        
        # Get feature columns
        feature_cols = [col for col in result_df.columns if col not in df.columns]
        
        if len(feature_cols) > 0:
            # Fit scaler
            self.scaler.fit(result_df[feature_cols])
            
            # Apply PCA if we have enough features
            if len(feature_cols) >= 10:
                self.pca = PCA(n_components=min(10, len(feature_cols)))
                self.pca.fit(result_df[feature_cols])
                
                # Add PCA components
                pca_components = self.pca.transform(result_df[feature_cols])
                for i in range(pca_components.shape[1]):
                    result_df[f'stat_pca_{i+1}'] = pca_components[:, i]
            
            self.fitted = True
        
        return result_df
    
    def transform(self, df):
        """
        Transform new data using fitted feature extractor
        
        Args:
            df (DataFrame): Input data
            
        Returns:
            DataFrame: Transformed data with features
        """
        if not self.fitted:
            raise ValueError("Feature extractor not fitted. Call fit_transform first.")
        
        # Extract features
        result_df = self.extract_features(df)
        
        # Get feature columns
        feature_cols = [col for col in result_df.columns if col not in df.columns]
        
        if len(feature_cols) > 0:
            # Transform features using fitted scaler
            result_df[feature_cols] = self.scaler.transform(result_df[feature_cols])
            
            # Apply PCA if it was fitted
            if self.pca is not None:
                pca_components = self.pca.transform(result_df[feature_cols])
                for i in range(pca_components.shape[1]):
                    result_df[f'stat_pca_{i+1}'] = pca_components[:, i]
        
        return result_df

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\features\timeseries_features.py ===
"""
Time Series Features Module
Implements time series feature extraction techniques for fraud detection
"""

import pandas as pd
import numpy as np
from scipy import stats
from scipy.signal import find_peaks
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tsa.stattools import acf, pacf
import warnings
import logging
from typing import Dict, List, Tuple, Union

warnings.filterwarnings('ignore')
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class TimeSeriesFeatures:
    """
    Class for extracting time series features from transaction data
    Implements techniques like burstiness analysis, gap analysis, etc.
    """
    
    def __init__(self, config=None):
        """
        Initialize TimeSeriesFeatures
        
        Args:
            config (dict, optional): Configuration parameters
        """
        self.config = config or {}
        self.feature_names = []
        self.fitted = False
        
    def extract_features(self, df):
        """
        Extract all time series features from the dataframe
        
        Args:
            df (DataFrame): Input transaction data
            
        Returns:
            DataFrame: DataFrame with extracted features
        """
        try:
            # Create a copy to avoid modifying the original
            result_df = df.copy()
            
            # Ensure timestamp is in datetime format
            if 'timestamp' in df.columns and not pd.api.types.is_datetime64_any_dtype(df['timestamp']):
                result_df['timestamp'] = pd.to_datetime(df['timestamp'])
            
            # Extract different types of time series features
            result_df = self._extract_temporal_features(result_df)
            result_df = self._extract_frequency_features(result_df)
            result_df = self._extract_burstiness_features(result_df)
            result_df = self._extract_gap_features(result_df)
            result_df = self._extract_seasonal_features(result_df)
            result_df = self._extract_autocorrelation_features(result_df)
            
            # Store feature names
            self.feature_names = [col for col in result_df.columns if col not in df.columns]
            
            logger.info(f"Extracted {len(self.feature_names)} time series features")
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting time series features: {str(e)}")
            raise
    
    def _extract_temporal_features(self, df):
        """
        Extract temporal features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with temporal features
        """
        try:
            result_df = df.copy()
            
            if 'timestamp' not in df.columns:
                logger.warning("Timestamp column not found. Skipping temporal features.")
                return result_df
            
            # Extract time components
            result_df['hour'] = result_df['timestamp'].dt.hour
            result_df['day'] = result_df['timestamp'].dt.day
            result_df['month'] = result_df['timestamp'].dt.month
            result_df['year'] = result_df['timestamp'].dt.year
            result_df['dayofweek'] = result_df['timestamp'].dt.dayofweek  # Monday=0, Sunday=6
            result_df['dayofyear'] = result_df['timestamp'].dt.dayofyear
            result_df['weekofyear'] = result_df['timestamp'].dt.isocalendar().week
            result_df['quarter'] = result_df['timestamp'].dt.quarter
            
            # Time-based flags
            result_df['is_weekend'] = (result_df['dayofweek'] >= 5).astype(int)
            result_df['is_month_start'] = result_df['timestamp'].dt.is_month_start.astype(int)
            result_df['is_month_end'] = result_df['timestamp'].dt.is_month_end.astype(int)
            result_df['is_quarter_start'] = result_df['timestamp'].dt.is_quarter_start.astype(int)
            result_df['is_quarter_end'] = result_df['timestamp'].dt.is_quarter_end.astype(int)
            result_df['is_year_start'] = result_df['timestamp'].dt.is_year_start.astype(int)
            result_df['is_year_end'] = result_df['timestamp'].dt.is_year_end.astype(int)
            
            # Time of day flags
            result_df['is_night'] = ((result_df['hour'] >= 22) | (result_df['hour'] < 6)).astype(int)
            result_df['is_morning'] = ((result_df['hour'] >= 6) & (result_df['hour'] < 12)).astype(int)
            result_df['is_afternoon'] = ((result_df['hour'] >= 12) & (result_df['hour'] < 18)).astype(int)
            result_df['is_evening'] = ((result_df['hour'] >= 18) & (result_df['hour'] < 22)).astype(int)
            
            # Business hours flag (9 AM to 5 PM, Monday to Friday)
            result_df['is_business_hours'] = (
                (result_df['hour'] >= 9) & 
                (result_df['hour'] < 17) & 
                (result_df['dayofweek'] < 5)
            ).astype(int)
            
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting temporal features: {str(e)}")
            return df
    
    def _extract_frequency_features(self, df):
        """
        Extract frequency-based features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with frequency features
        """
        try:
            result_df = df.copy()
            
            if 'timestamp' not in df.columns:
                logger.warning("Timestamp column not found. Skipping frequency features.")
                return result_df
            
            # Sort by timestamp
            df_sorted = result_df.sort_values('timestamp')
            
            # Time windows for frequency analysis
            time_windows = ['1H', '6H', '24H', '7D', '30D']
            
            for window in time_windows:
                # Calculate transaction frequency in time windows before each transaction
                window_counts = []
                
                for _, row in df_sorted.iterrows():
                    timestamp = row['timestamp']
                    
                    # Get transactions in time window before current transaction
                    window_start = timestamp - pd.Timedelta(window)
                    window_end = timestamp
                    
                    window_transactions = df_sorted[
                        (df_sorted['timestamp'] >= window_start) &
                        (df_sorted['timestamp'] < window_end)
                    ]
                    
                    # Count transactions in window
                    count = len(window_transactions)
                    window_counts.append(count)
                
                result_df[f'transaction_frequency_{window}'] = window_counts
            
            # Calculate frequency by sender
            if 'sender_id' in df.columns:
                for window in ['1H', '6H', '24H']:
                    sender_window_counts = []
                    
                    for _, row in df_sorted.iterrows():
                        sender = row['sender_id']
                        timestamp = row['timestamp']
                        
                        # Get transactions from same sender in time window
                        window_start = timestamp - pd.Timedelta(window)
                        window_end = timestamp
                        
                        window_transactions = df_sorted[
                            (df_sorted['timestamp'] >= window_start) &
                            (df_sorted['timestamp'] < window_end) &
                            (df_sorted['sender_id'] == sender)
                        ]
                        
                        # Count transactions in window
                        count = len(window_transactions)
                        sender_window_counts.append(count)
                    
                    result_df[f'sender_frequency_{window}'] = sender_window_counts
            
            # Calculate frequency by receiver
            if 'receiver_id' in df.columns:
                for window in ['1H', '6H', '24H']:
                    receiver_window_counts = []
                    
                    for _, row in df_sorted.iterrows():
                        receiver = row['receiver_id']
                        timestamp = row['timestamp']
                        
                        # Get transactions to same receiver in time window
                        window_start = timestamp - pd.Timedelta(window)
                        window_end = timestamp
                        
                        window_transactions = df_sorted[
                            (df_sorted['timestamp'] >= window_start) &
                            (df_sorted['timestamp'] < window_end) &
                            (df_sorted['receiver_id'] == receiver)
                        ]
                        
                        # Count transactions in window
                        count = len(window_transactions)
                        receiver_window_counts.append(count)
                    
                    result_df[f'receiver_frequency_{window}'] = receiver_window_counts
            
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting frequency features: {str(e)}")
            return df
    
    def _extract_burstiness_features(self, df):
        """
        Extract burstiness analysis features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with burstiness features
        """
        try:
            result_df = df.copy()
            
            if 'timestamp' not in df.columns:
                logger.warning("Timestamp column not found. Skipping burstiness features.")
                return result_df
            
            # Sort by timestamp
            df_sorted = result_df.sort_values('timestamp')
            
            # Calculate inter-transaction times
            inter_times = df_sorted['timestamp'].diff().dt.total_seconds().fillna(0)
            
            # Calculate burstiness coefficient
            if len(inter_times) > 1 and inter_times.std() > 0:
                burstiness = (inter_times.std() - inter_times.mean()) / (inter_times.std() + inter_times.mean())
            else:
                burstiness = 0
            
            result_df['burstiness_coefficient'] = burstiness
            
            # Calculate local burstiness (in sliding windows)
            window_sizes = [10, 50, 100]  # Number of transactions
            
            for window_size in window_sizes:
                local_burstiness = []
                
                for i in range(len(df_sorted)):
                    # Get window around current transaction
                    start_idx = max(0, i - window_size // 2)
                    end_idx = min(len(df_sorted), i + window_size // 2 + 1)
                    
                    window_inter_times = inter_times.iloc[start_idx:end_idx]
                    
                    # Calculate burstiness for window
                    if len(window_inter_times) > 1 and window_inter_times.std() > 0:
                        local_b = (window_inter_times.std() - window_inter_times.mean()) / (
                            window_inter_times.std() + window_inter_times.mean()
                        )
                    else:
                        local_b = 0
                    
                    local_burstiness.append(local_b)
                
                result_df[f'local_burstiness_{window_size}'] = local_burstiness
            
            # Detect burst periods
            # A burst is defined as a period with inter-transaction times significantly lower than average
            avg_inter_time = inter_times.mean()
            std_inter_time = inter_times.std()
            
            # Threshold for burst detection (2 standard deviations below mean)
            burst_threshold = max(0, avg_inter_time - 2 * std_inter_time)
            
            # Flag transactions in bursts
            result_df['is_in_burst'] = (inter_times < burst_threshold).astype(int)
            
            # Calculate burst duration (consecutive transactions in burst)
            burst_durations = []
            current_duration = 0
            
            for is_burst in result_df['is_in_burst']:
                if is_burst:
                    current_duration += 1
                else:
                    burst_durations.append(current_duration)
                    current_duration = 0
            
            # Add the last duration if we're still in a burst
            burst_durations.append(current_duration)
            
            # Map burst durations back to transactions
            burst_duration_map = []
            idx = 0
            for i, is_burst in enumerate(result_df['is_in_burst']):
                if is_burst:
                    burst_duration_map.append(burst_durations[idx])
                else:
                    burst_duration_map.append(0)
                    if i > 0 and not result_df['is_in_burst'].iloc[i-1]:
                        idx += 1
            
            result_df['burst_duration'] = burst_duration_map
            
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting burstiness features: {str(e)}")
            return df
    
    def _extract_gap_features(self, df):
        """
        Extract gap analysis features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with gap features
        """
        try:
            result_df = df.copy()
            
            if 'timestamp' not in df.columns:
                logger.warning("Timestamp column not found. Skipping gap features.")
                return result_df
            
            # Sort by timestamp
            df_sorted = result_df.sort_values('timestamp')
            
            # Calculate inter-transaction times
            inter_times = df_sorted['timestamp'].diff().dt.total_seconds().fillna(0)
            
            # Time since last transaction
            result_df['time_since_last_transaction'] = inter_times
            
            # Time until next transaction
            time_until_next = df_sorted['timestamp'].diff(-1).dt.total_seconds().abs().fillna(0)
            result_df['time_until_next_transaction'] = time_until_next
            
            # Detect gaps (unusually long inter-transaction times)
            if len(inter_times) > 1 and inter_times.std() > 0:
                # Threshold for gap detection (2 standard deviations above mean)
                gap_threshold = inter_times.mean() + 2 * inter_times.std()
                
                # Flag transactions after gaps
                result_df['is_after_gap'] = (inter_times > gap_threshold).astype(int)
                
                # Calculate gap sizes
                result_df['gap_size'] = np.where(inter_times > gap_threshold, inter_times, 0)
            else:
                result_df['is_after_gap'] = 0
                result_df['gap_size'] = 0
            
            # Calculate gap statistics by sender
            if 'sender_id' in df.columns:
                sender_gaps = []
                sender_gap_stats = {}
                
                # Calculate average gap for each sender
                for sender in df_sorted['sender_id'].unique():
                    sender_transactions = df_sorted[df_sorted['sender_id'] == sender].sort_values('timestamp')
                    if len(sender_transactions) > 1:
                        sender_inter_times = sender_transactions['timestamp'].diff().dt.total_seconds().fillna(0)
                        sender_gap_stats[sender] = {
                            'mean': sender_inter_times.mean(),
                            'std': sender_inter_times.std()
                        }
                
                # Calculate gap features for each transaction
                for _, row in df_sorted.iterrows():
                    sender = row['sender_id']
                    
                    if sender in sender_gap_stats:
                        stats = sender_gap_stats[sender]
                        
                        # Time since last transaction for this sender
                        sender_transactions = df_sorted[
                            (df_sorted['sender_id'] == sender) &
                            (df_sorted['timestamp'] < row['timestamp'])
                        ]
                        
                        if len(sender_transactions) > 0:
                            last_sender_time = sender_transactions['timestamp'].max()
                            sender_inter_time = (row['timestamp'] - last_sender_time).total_seconds()
                        else:
                            sender_inter_time = float('inf')
                        
                        # Calculate Z-score for this gap
                        if stats['std'] > 0:
                            gap_z_score = (sender_inter_time - stats['mean']) / stats['std']
                        else:
                            gap_z_score = 0
                        
                        sender_gaps.append({
                            'sender_time_since_last': sender_inter_time,
                            'sender_gap_z_score': gap_z_score
                        })
                    else:
                        sender_gaps.append({
                            'sender_time_since_last': float('inf'),
                            'sender_gap_z_score': 0
                        })
                
                # Add to result dataframe
                gap_df = pd.DataFrame(sender_gaps)
                result_df['sender_time_since_last'] = gap_df['sender_time_since_last']
                result_df['sender_gap_z_score'] = gap_df['sender_gap_z_score']
            
            # Calculate gap statistics by receiver
            if 'receiver_id' in df.columns:
                receiver_gaps = []
                receiver_gap_stats = {}
                
                # Calculate average gap for each receiver
                for receiver in df_sorted['receiver_id'].unique():
                    receiver_transactions = df_sorted[df_sorted['receiver_id'] == receiver].sort_values('timestamp')
                    if len(receiver_transactions) > 1:
                        receiver_inter_times = receiver_transactions['timestamp'].diff().dt.total_seconds().fillna(0)
                        receiver_gap_stats[receiver] = {
                            'mean': receiver_inter_times.mean(),
                            'std': receiver_inter_times.std()
                        }
                
                # Calculate gap features for each transaction
                for _, row in df_sorted.iterrows():
                    receiver = row['receiver_id']
                    
                    if receiver in receiver_gap_stats:
                        stats = receiver_gap_stats[receiver]
                        
                        # Time since last transaction to this receiver
                        receiver_transactions = df_sorted[
                            (df_sorted['receiver_id'] == receiver) &
                            (df_sorted['timestamp'] < row['timestamp'])
                        ]
                        
                        if len(receiver_transactions) > 0:
                            last_receiver_time = receiver_transactions['timestamp'].max()
                            receiver_inter_time = (row['timestamp'] - last_receiver_time).total_seconds()
                        else:
                            receiver_inter_time = float('inf')
                        
                        # Calculate Z-score for this gap
                        if stats['std'] > 0:
                            gap_z_score = (receiver_inter_time - stats['mean']) / stats['std']
                        else:
                            gap_z_score = 0
                        
                        receiver_gaps.append({
                            'receiver_time_since_last': receiver_inter_time,
                            'receiver_gap_z_score': gap_z_score
                        })
                    else:
                        receiver_gaps.append({
                            'receiver_time_since_last': float('inf'),
                            'receiver_gap_z_score': 0
                        })
                
                # Add to result dataframe
                gap_df = pd.DataFrame(receiver_gaps)
                result_df['receiver_time_since_last'] = gap_df['receiver_time_since_last']
                result_df['receiver_gap_z_score'] = gap_df['receiver_gap_z_score']
            
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting gap features: {str(e)}")
            return df
    
    def _extract_seasonal_features(self, df):
        """
        Extract seasonal features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with seasonal features
        """
        try:
            result_df = df.copy()
            
            if 'timestamp' not in df.columns:
                logger.warning("Timestamp column not found. Skipping seasonal features.")
                return result_df
            
            # Sort by timestamp
            df_sorted = result_df.sort_values('timestamp')
            
            # Create a time series of transaction counts
            # Resample to different frequencies
            frequencies = ['1H', '6H', '1D', '1W']
            
            for freq in frequencies:
                # Count transactions per time period
                ts = df_sorted.set_index('timestamp').resample(freq).size()
                
                if len(ts) > 10:  # Need enough data points for decomposition
                    try:
                        # Perform seasonal decomposition
                        decomposition = seasonal_decompose(ts, model='additive', period=min(24, len(ts)//2))
                        
                        # Extract components
                        trend = decomposition.trend
                        seasonal = decomposition.seasonal
                        residual = decomposition.resid
                        
                        # Map back to original dataframe
                        trend_map = {}
                        seasonal_map = {}
                        residual_map = {}
                        
                        for timestamp in df_sorted['timestamp']:
                            # Find the closest time period
                            period_start = timestamp.floor(freq)
                            
                            if period_start in trend.index:
                                trend_map[timestamp] = trend[period_start]
                                seasonal_map[timestamp] = seasonal[period_start]
                                residual_map[timestamp] = residual[period_start]
                            else:
                                trend_map[timestamp] = 0
                                seasonal_map[timestamp] = 0
                                residual_map[timestamp] = 0
                        
                        # Add to result dataframe
                        result_df[f'trend_{freq}'] = df_sorted['timestamp'].map(trend_map).fillna(0)
                        result_df[f'seasonal_{freq}'] = df_sorted['timestamp'].map(seasonal_map).fillna(0)
                        result_df[f'residual_{freq}'] = df_sorted['timestamp'].map(residual_map).fillna(0)
                        
                        # Calculate anomaly score based on residual
                        residual_mean = residual.mean()
                        residual_std = residual.std()
                        
                        if residual_std > 0:
                            anomaly_scores = (df_sorted['timestamp'].map(residual_map) - residual_mean) / residual_std
                            result_df[f'seasonal_anomaly_{freq}'] = anomaly_scores.fillna(0)
                        else:
                            result_df[f'seasonal_anomaly_{freq}'] = 0
                            
                    except Exception as e:
                        logger.warning(f"Error in seasonal decomposition for {freq}: {str(e)}")
            
            # Calculate periodicity features
            # Check for daily, weekly, and monthly patterns
            periodicity_features = {}
            
            # Daily pattern (hour of day)
            if 'hour' in result_df.columns:
                hourly_counts = result_df.groupby('hour').size()
                # Calculate entropy to measure uniformity
                hourly_probs = hourly_counts / hourly_counts.sum()
                daily_entropy = -sum(p * np.log2(p) for p in hourly_probs if p > 0)
                max_entropy = np.log2(24)  # Maximum possible entropy for 24 hours
                daily_uniformity = daily_entropy / max_entropy if max_entropy > 0 else 0
                
                periodicity_features['daily_pattern_strength'] = 1 - daily_uniformity  # Higher means stronger pattern
                
                # Calculate peak hour
                peak_hour = hourly_counts.idxmax()
                periodicity_features['peak_hour'] = peak_hour
            
            # Weekly pattern (day of week)
            if 'dayofweek' in result_df.columns:
                weekly_counts = result_df.groupby('dayofweek').size()
                # Calculate entropy to measure uniformity
                weekly_probs = weekly_counts / weekly_counts.sum()
                weekly_entropy = -sum(p * np.log2(p) for p in weekly_probs if p > 0)
                max_entropy = np.log2(7)  # Maximum possible entropy for 7 days
                weekly_uniformity = weekly_entropy / max_entropy if max_entropy > 0 else 0
                
                periodicity_features['weekly_pattern_strength'] = 1 - weekly_uniformity  # Higher means stronger pattern
                
                # Calculate peak day
                peak_day = weekly_counts.idxmax()
                periodicity_features['peak_day'] = peak_day
            
            # Add periodicity features to all rows
            for feature, value in periodicity_features.items():
                result_df[feature] = value
            
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting seasonal features: {str(e)}")
            return df
    
    def _extract_autocorrelation_features(self, df):
        """
        Extract autocorrelation features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with autocorrelation features
        """
        try:
            result_df = df.copy()
            
            if 'timestamp' not in df.columns:
                logger.warning("Timestamp column not found. Skipping autocorrelation features.")
                return result_df
            
            # Sort by timestamp
            df_sorted = result_df.sort_values('timestamp')
            
            # Create time series of transaction counts
            # Resample to different frequencies
            frequencies = ['1H', '6H', '1D']
            
            for freq in frequencies:
                # Count transactions per time period
                ts = df_sorted.set_index('timestamp').resample(freq).size()
                
                if len(ts) > 10:  # Need enough data points for autocorrelation
                    try:
                        # Calculate autocorrelation function
                        nlags = min(10, len(ts) // 2)
                        autocorr = acf(ts, nlags=nlags, fft=True)
                        
                        # Add autocorrelation features
                        for i in range(1, min(6, len(autocorr))):  # First 5 lags
                            result_df[f'autocorr_{freq}_lag_{i}'] = autocorr[i]
                        
                        # Calculate partial autocorrelation
                        pacf_values = pacf(ts, nlags=nlags)
                        
                        # Add partial autocorrelation features
                        for i in range(1, min(6, len(pacf_values))):  # First 5 lags
                            result_df[f'pacf_{freq}_lag_{i}'] = pacf_values[i]
                        
                        # Detect periodicity in autocorrelation
                        # Look for peaks in autocorrelation
                        peaks, _ = find_peaks(autocorr[1:], height=0.2)  # Ignore lag 0
                        
                        if len(peaks) > 0:
                            # Find the most significant peak
                            peak_lag = peaks[0] + 1  # +1 because we ignored lag 0
                            result_df[f'periodicity_{freq}'] = peak_lag
                            result_df[f'periodicity_strength_{freq}'] = autocorr[peak_lag]
                        else:
                            result_df[f'periodicity_{freq}'] = 0
                            result_df[f'periodicity_strength_{freq}'] = 0
                            
                    except Exception as e:
                        logger.warning(f"Error in autocorrelation analysis for {freq}: {str(e)}")
            
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting autocorrelation features: {str(e)}")
            return df
    
    def fit_transform(self, df):
        """
        Fit the feature extractor and transform the data
        
        Args:
            df (DataFrame): Input data
            
        Returns:
            DataFrame: Transformed data with features
        """
        # Extract features
        result_df = self.extract_features(df)
        
        # Get feature columns
        feature_cols = [col for col in result_df.columns if col not in df.columns]
        
        if len(feature_cols) > 0:
            self.fitted = True
        
        return result_df
    
    def transform(self, df):
        """
        Transform new data using fitted feature extractor
        
        Args:
            df (DataFrame): Input data
            
        Returns:
            DataFrame: Transformed data with features
        """
        if not self.fitted:
            raise ValueError("Feature extractor not fitted. Call fit_transform first.")
        
        # Extract features
        result_df = self.extract_features(df)
        
        return result_df

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\features\__init__.py ===

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\features\.ipynb_checkpoints\graph_features-checkpoint.py ===
"""
Graph Features Module
Implements graph-based feature extraction techniques for fraud detection
"""

import pandas as pd
import numpy as np
import networkx as nx
from collections import defaultdict, Counter
import community as community_louvain
from sklearn.preprocessing import MinMaxScaler
import warnings
import logging
from typing import Dict, List, Tuple, Union

warnings.filterwarnings('ignore')
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class GraphFeatures:
    """
    Class for extracting graph-based features from transaction data
    Implements techniques like centrality measures, clustering coefficients, etc.
    """
    
    def __init__(self, config=None):
        """
        Initialize GraphFeatures
        
        Args:
            config (dict, optional): Configuration parameters
        """
        self.config = config or {}
        self.graph = None
        self.sender_graph = None
        self.receiver_graph = None
        self.bipartite_graph = None
        self.feature_names = []
        self.scaler = MinMaxScaler()
        self.fitted = False
        
    def extract_features(self, df):
        """
        Extract all graph features from the dataframe
        
        Args:
            df (DataFrame): Input transaction data
            
        Returns:
            DataFrame: DataFrame with extracted features
        """
        try:
            # Create a copy to avoid modifying the original
            result_df = df.copy()
            
            # Build graphs
            self._build_graphs(df)
            
            # Extract different types of graph features
            result_df = self._extract_centrality_features(result_df)
            result_df = self._extract_clustering_features(result_df)
            result_df = self._extract_community_features(result_df)
            result_df = self._extract_path_features(result_df)
            result_df = self._extract_subgraph_features(result_df)
            result_df = self._extract_temporal_graph_features(result_df)
            
            # Store feature names
            self.feature_names = [col for col in result_df.columns if col not in df.columns]
            
            logger.info(f"Extracted {len(self.feature_names)} graph features")
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting graph features: {str(e)}")
            raise
    
    def _build_graphs(self, df):
        """
        Build various graphs from the transaction data
        
        Args:
            df (DataFrame): Input transaction data
        """
        try:
            # Build transaction graph (directed)
            self.graph = nx.DiGraph()
            
            # Add edges with attributes
            for _, row in df.iterrows():
                if 'sender_id' in row and 'receiver_id' in row:
                    sender = row['sender_id']
                    receiver = row['receiver_id']
                    
                    # Get edge attributes
                    attrs = {}
                    if 'amount' in row:
                        attrs['amount'] = row['amount']
                    if 'timestamp' in row:
                        attrs['timestamp'] = row['timestamp']
                    if 'transaction_id' in row:
                        attrs['transaction_id'] = row['transaction_id']
                    
                    # Add edge or update existing edge
                    if self.graph.has_edge(sender, receiver):
                        # Update existing edge
                        edge_data = self.graph[sender][receiver]
                        if 'amount' in attrs:
                            edge_data['total_amount'] = edge_data.get('total_amount', 0) + attrs['amount']
                        edge_data['transaction_count'] = edge_data.get('transaction_count', 0) + 1
                        edge_data['transactions'].append(attrs)
                    else:
                        # Add new edge
                        attrs['total_amount'] = attrs.get('amount', 0)
                        attrs['transaction_count'] = 1
                        attrs['transactions'] = [attrs]
                        self.graph.add_edge(sender, receiver, **attrs)
            
            # Build sender graph (undirected, senders connected if they have common receivers)
            self.sender_graph = nx.Graph()
            
            # Create a mapping from receivers to senders
            receiver_to_senders = defaultdict(set)
            for _, row in df.iterrows():
                if 'sender_id' in row and 'receiver_id' in row:
                    receiver_to_senders[row['receiver_id']].add(row['sender_id'])
            
            # Connect senders with common receivers
            for receiver, senders in receiver_to_senders.items():
                senders_list = list(senders)
                for i in range(len(senders_list)):
                    for j in range(i+1, len(senders_list)):
                        sender1 = senders_list[i]
                        sender2 = senders_list[j]
                        
                        if self.sender_graph.has_edge(sender1, sender2):
                            self.sender_graph[sender1][sender2]['common_receivers'] += 1
                        else:
                            self.sender_graph.add_edge(sender1, sender2, common_receivers=1)
            
            # Build receiver graph (undirected, receivers connected if they have common senders)
            self.receiver_graph = nx.Graph()
            
            # Create a mapping from senders to receivers
            sender_to_receivers = defaultdict(set)
            for _, row in df.iterrows():
                if 'sender_id' in row and 'receiver_id' in row:
                    sender_to_receivers[row['sender_id']].add(row['receiver_id'])
            
            # Connect receivers with common senders
            for sender, receivers in sender_to_receivers.items():
                receivers_list = list(receivers)
                for i in range(len(receivers_list)):
                    for j in range(i+1, len(receivers_list)):
                        receiver1 = receivers_list[i]
                        receiver2 = receivers_list[j]
                        
                        if self.receiver_graph.has_edge(receiver1, receiver2):
                            self.receiver_graph[receiver1][receiver2]['common_senders'] += 1
                        else:
                            self.receiver_graph.add_edge(receiver1, receiver2, common_senders=1)
            
            # Build bipartite graph (senders and receivers as two separate sets)
            self.bipartite_graph = nx.Graph()
            
            # Add nodes with bipartite attribute
            for _, row in df.iterrows():
                if 'sender_id' in row:
                    self.bipartite_graph.add_node(row['sender_id'], bipartite=0)
                if 'receiver_id' in row:
                    self.bipartite_graph.add_node(row['receiver_id'], bipartite=1)
            
            # Add edges
            for _, row in df.iterrows():
                if 'sender_id' in row and 'receiver_id' in row:
                    sender = row['sender_id']
                    receiver = row['receiver_id']
                    
                    # Get edge attributes
                    attrs = {}
                    if 'amount' in row:
                        attrs['amount'] = row['amount']
                    if 'timestamp' in row:
                        attrs['timestamp'] = row['timestamp']
                    if 'transaction_id' in row:
                        attrs['transaction_id'] = row['transaction_id']
                    
                    # Add edge
                    self.bipartite_graph.add_edge(sender, receiver, **attrs)
            
            logger.info("Graphs built successfully")
            
        except Exception as e:
            logger.error(f"Error building graphs: {str(e)}")
            raise
    
    def _extract_centrality_features(self, df):
        """
        Extract centrality-based features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with centrality features
        """
        try:
            result_df = df.copy()
            
            if self.graph is None:
                logger.warning("Graph not built. Skipping centrality features.")
                return result_df
            
            # Calculate degree centrality
            in_degree_centrality = nx.in_degree_centrality(self.graph)
            out_degree_centrality = nx.out_degree_centrality(self.graph)
            degree_centrality = nx.degree_centrality(self.graph.to_undirected())
            
            # Map to dataframe
            if 'sender_id' in df.columns:
                result_df['sender_in_degree_centrality'] = df['sender_id'].map(in_degree_centrality).fillna(0)
                result_df['sender_out_degree_centrality'] = df['sender_id'].map(out_degree_centrality).fillna(0)
                result_df['sender_degree_centrality'] = df['sender_id'].map(degree_centrality).fillna(0)
            
            if 'receiver_id' in df.columns:
                result_df['receiver_in_degree_centrality'] = df['receiver_id'].map(in_degree_centrality).fillna(0)
                result_df['receiver_out_degree_centrality'] = df['receiver_id'].map(out_degree_centrality).fillna(0)
                result_df['receiver_degree_centrality'] = df['receiver_id'].map(degree_centrality).fillna(0)
            
            # Calculate betweenness centrality (sample for large graphs)
            if len(self.graph.nodes()) <= 1000:
                betweenness_centrality = nx.betweenness_centrality(self.graph)
            else:
                # Sample nodes for betweenness calculation
                sample_nodes = list(self.graph.nodes())[:1000]
                betweenness_centrality = nx.betweenness_centrality(self.graph, k=len(sample_nodes))
            
            # Map to dataframe
            if 'sender_id' in df.columns:
                result_df['sender_betweenness_centrality'] = df['sender_id'].map(betweenness_centrality).fillna(0)
            
            if 'receiver_id' in df.columns:
                result_df['receiver_betweenness_centrality'] = df['receiver_id'].map(betweenness_centrality).fillna(0)
            
            # Calculate closeness centrality (sample for large graphs)
            if len(self.graph.nodes()) <= 1000:
                closeness_centrality = nx.closeness_centrality(self.graph)
            else:
                # Use approximate closeness for large graphs
                closeness_centrality = nx.closeness_centrality(self.graph, distance='weight')
            
            # Map to dataframe
            if 'sender_id' in df.columns:
                result_df['sender_closeness_centrality'] = df['sender_id'].map(closeness_centrality).fillna(0)
            
            if 'receiver_id' in df.columns:
                result_df['receiver_closeness_centrality'] = df['receiver_id'].map(closeness_centrality).fillna(0)
            
            # Calculate eigenvector centrality (sample for large graphs)
            if len(self.graph.nodes()) <= 1000:
                try:
                    eigenvector_centrality = nx.eigenvector_centrality(self.graph, max_iter=1000)
                except:
                    eigenvector_centrality = {}
            else:
                eigenvector_centrality = {}
            
            # Map to dataframe
            if 'sender_id' in df.columns:
                result_df['sender_eigenvector_centrality'] = df['sender_id'].map(eigenvector_centrality).fillna(0)
            
            if 'receiver_id' in df.columns:
                result_df['receiver_eigenvector_centrality'] = df['receiver_id'].map(eigenvector_centrality).fillna(0)
            
            # Calculate PageRank
            pagerank = nx.pagerank(self.graph)
            
            # Map to dataframe
            if 'sender_id' in df.columns:
                result_df['sender_pagerank'] = df['sender_id'].map(pagerank).fillna(0)
            
            if 'receiver_id' in df.columns:
                result_df['receiver_pagerank'] = df['receiver_id'].map(pagerank).fillna(0)
            
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting centrality features: {str(e)}")
            return df
    
    def _extract_clustering_features(self, df):
        """
        Extract clustering-based features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with clustering features
        """
        try:
            result_df = df.copy()
            
            if self.graph is None:
                logger.warning("Graph not built. Skipping clustering features.")
                return result_df
            
            # Calculate clustering coefficient for transaction graph
            clustering_coeff = nx.clustering(self.graph.to_undirected())
            
            # Map to dataframe
            if 'sender_id' in df.columns:
                result_df['sender_clustering_coefficient'] = df['sender_id'].map(clustering_coeff).fillna(0)
            
            if 'receiver_id' in df.columns:
                result_df['receiver_clustering_coefficient'] = df['receiver_id'].map(clustering_coeff).fillna(0)
            
            # Calculate clustering coefficient for sender graph
            if self.sender_graph is not None and len(self.sender_graph.nodes()) > 0:
                sender_clustering_coeff = nx.clustering(self.sender_graph)
                
                # Map to dataframe
                if 'sender_id' in df.columns:
                    result_df['sender_graph_clustering_coefficient'] = df['sender_id'].map(sender_clustering_coeff).fillna(0)
            
            # Calculate clustering coefficient for receiver graph
            if self.receiver_graph is not None and len(self.receiver_graph.nodes()) > 0:
                receiver_clustering_coeff = nx.clustering(self.receiver_graph)
                
                # Map to dataframe
                if 'receiver_id' in df.columns:
                    result_df['receiver_graph_clustering_coefficient'] = df['receiver_id'].map(receiver_clustering_coeff).fillna(0)
            
            # Calculate average neighbor degree
            avg_neighbor_degree = nx.average_neighbor_degree(self.graph.to_undirected())
            
            # Map to dataframe
            if 'sender_id' in df.columns:
                result_df['sender_avg_neighbor_degree'] = df['sender_id'].map(avg_neighbor_degree).fillna(0)
            
            if 'receiver_id' in df.columns:
                result_df['receiver_avg_neighbor_degree'] = df['receiver_id'].map(avg_neighbor_degree).fillna(0)
            
            # Calculate square clustering
            square_clustering = nx.square_clustering(self.graph.to_undirected())
            
            # Map to dataframe
            if 'sender_id' in df.columns:
                result_df['sender_square_clustering'] = df['sender_id'].map(square_clustering).fillna(0)
            
            if 'receiver_id' in df.columns:
                result_df['receiver_square_clustering'] = df['receiver_id'].map(square_clustering).fillna(0)
            
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting clustering features: {str(e)}")
            return df
    
    def _extract_community_features(self, df):
        """
        Extract community-based features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with community features
        """
        try:
            result_df = df.copy()
            
            if self.graph is None:
                logger.warning("Graph not built. Skipping community features.")
                return result_df
            
            # Detect communities using Louvain algorithm
            undirected_graph = self.graph.to_undirected()
            communities = community_louvain.best_partition(undirected_graph)
            
            # Map to dataframe
            if 'sender_id' in df.columns:
                result_df['sender_community'] = df['sender_id'].map(communities).fillna(-1)
            
            if 'receiver_id' in df.columns:
                result_df['receiver_community'] = df['receiver_id'].map(communities).fillna(-1)
            
            # Calculate community size for each node
            community_sizes = Counter(communities.values())
            node_community_sizes = {node: community_sizes[comm] for node, comm in communities.items()}
            
            # Map to dataframe
            if 'sender_id' in df.columns:
                result_df['sender_community_size'] = df['sender_id'].map(node_community_sizes).fillna(0)
            
            if 'receiver_id' in df.columns:
                result_df['receiver_community_size'] = df['receiver_id'].map(node_community_sizes).fillna(0)
            
            # Calculate community degree centrality
            community_degree_centrality = {}
            for comm, nodes in community_sizes.items():
                comm_nodes = [node for node, c in communities.items() if c == comm]
                subgraph = undirected_graph.subgraph(comm_nodes)
                if len(subgraph.nodes()) > 0:
                    comm_centrality = nx.degree_centrality(subgraph)
                    for node in comm_nodes:
                        community_degree_centrality[node] = comm_centrality.get(node, 0)
            
            # Map to dataframe
            if 'sender_id' in df.columns:
                result_df['sender_community_degree_centrality'] = df['sender_id'].map(community_degree_centrality).fillna(0)
            
            if 'receiver_id' in df.columns:
                result_df['receiver_community_degree_centrality'] = df['receiver_id'].map(community_degree_centrality).fillna(0)
            
            # Check if sender and receiver are in the same community
            if 'sender_id' in df.columns and 'receiver_id' in df.columns:
                same_community = []
                for _, row in df.iterrows():
                    sender = row['sender_id']
                    receiver = row['receiver_id']
                    
                    if sender in communities and receiver in communities:
                        same_community.append(int(communities[sender] == communities[receiver]))
                    else:
                        same_community.append(0)
                
                result_df['same_community'] = same_community
            
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting community features: {str(e)}")
            return df
    
    def _extract_path_features(self, df):
        """
        Extract path-based features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with path features
        """
        try:
            result_df = df.copy()
            
            if self.graph is None:
                logger.warning("Graph not built. Skipping path features.")
                return result_df
            
            # Calculate shortest path lengths (sample for large graphs)
            if len(self.graph.nodes()) <= 500:
                # For small graphs, calculate all pairs shortest paths
                shortest_paths = dict(nx.all_pairs_shortest_path_length(self.graph))
                
                # Map to dataframe
                if 'sender_id' in df.columns and 'receiver_id' in df.columns:
                    path_lengths = []
                    for _, row in df.iterrows():
                        sender = row['sender_id']
                        receiver = row['receiver_id']
                        
                        if sender in shortest_paths and receiver in shortest_paths[sender]:
                            path_lengths.append(shortest_paths[sender][receiver])
                        else:
                            path_lengths.append(float('inf'))
                    
                    result_df['shortest_path_length'] = path_lengths
            else:
                # For large graphs, sample or use approximation
                if 'sender_id' in df.columns and 'receiver_id' in df.columns:
                    # Use BFS for a limited depth
                    path_lengths = []
                    for _, row in df.iterrows():
                        sender = row['sender_id']
                        receiver = row['receiver_id']
                        
                        try:
                            path_length = nx.shortest_path_length(self.graph, sender, receiver)
                            path_lengths.append(path_length)
                        except:
                            path_lengths.append(float('inf'))
                    
                    result_df['shortest_path_length'] = path_lengths
            
            # Calculate number of common neighbors
            if 'sender_id' in df.columns and 'receiver_id' in df.columns:
                common_neighbors = []
                for _, row in df.iterrows():
                    sender = row['sender_id']
                    receiver = row['receiver_id']
                    
                    try:
                        sender_neighbors = set(self.graph.predecessors(sender)) | set(self.graph.successors(sender))
                        receiver_neighbors = set(self.graph.predecessors(receiver)) | set(self.graph.successors(receiver))
                        common_neighbors.append(len(sender_neighbors & receiver_neighbors))
                    except:
                        common_neighbors.append(0)
                
                result_df['common_neighbors_count'] = common_neighbors
            
            # Calculate Jaccard coefficient
            if 'sender_id' in df.columns and 'receiver_id' in df.columns:
                jaccard_coeffs = []
                for _, row in df.iterrows():
                    sender = row['sender_id']
                    receiver = row['receiver_id']
                    
                    try:
                        sender_neighbors = set(self.graph.predecessors(sender)) | set(self.graph.successors(sender))
                        receiver_neighbors = set(self.graph.predecessors(receiver)) | set(self.graph.successors(receiver))
                        
                        union = sender_neighbors | receiver_neighbors
                        intersection = sender_neighbors & receiver_neighbors
                        
                        if len(union) > 0:
                            jaccard = len(intersection) / len(union)
                        else:
                            jaccard = 0
                        
                        jaccard_coeffs.append(jaccard)
                    except:
                        jaccard_coeffs.append(0)
                
                result_df['jaccard_coefficient'] = jaccard_coeffs
            
            # Calculate Adamic-Adar index
            if 'sender_id' in df.columns and 'receiver_id' in df.columns:
                adamic_adar = []
                for _, row in df.iterrows():
                    sender = row['sender_id']
                    receiver = row['receiver_id']
                    
                    try:
                        sender_neighbors = set(self.graph.predecessors(sender)) | set(self.graph.successors(sender))
                        receiver_neighbors = set(self.graph.predecessors(receiver)) | set(self.graph.successors(receiver))
                        common = sender_neighbors & receiver_neighbors
                        
                        # Calculate sum of 1/log(degree) for common neighbors
                        aa_index = 0
                        for node in common:
                            degree = self.graph.degree(node)
                            if degree > 1:
                                aa_index += 1 / np.log(degree)
                        
                        adamic_adar.append(aa_index)
                    except:
                        adamic_adar.append(0)
                
                result_df['adamic_adar_index'] = adamic_adar
            
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting path features: {str(e)}")
            return df
    
    def _extract_subgraph_features(self, df):
        """
        Extract subgraph-based features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with subgraph features
        """
        try:
            result_df = df.copy()
            
            if self.graph is None:
                logger.warning("Graph not built. Skipping subgraph features.")
                return result_df
            
            # Calculate ego network features for senders
            if 'sender_id' in df.columns:
                sender_ego_sizes = []
                sender_ego_densities = []
                
                for sender in df['sender_id']:
                    try:
                        # Get ego network
                        ego_graph = nx.ego_graph(self.graph.to_undirected(), sender, radius=1)
                        
                        # Calculate size and density
                        ego_size = len(ego_graph.nodes())
                        ego_density = nx.density(ego_graph)
                        
                        sender_ego_sizes.append(ego_size)
                        sender_ego_densities.append(ego_density)
                    except:
                        sender_ego_sizes.append(0)
                        sender_ego_densities.append(0)
                
                result_df['sender_ego_network_size'] = sender_ego_sizes
                result_df['sender_ego_network_density'] = sender_ego_densities
            
            # Calculate ego network features for receivers
            if 'receiver_id' in df.columns:
                receiver_ego_sizes = []
                receiver_ego_densities = []
                
                for receiver in df['receiver_id']:
                    try:
                        # Get ego network
                        ego_graph = nx.ego_graph(self.graph.to_undirected(), receiver, radius=1)
                        
                        # Calculate size and density
                        ego_size = len(ego_graph.nodes())
                        ego_density = nx.density(ego_graph)
                        
                        receiver_ego_sizes.append(ego_size)
                        receiver_ego_densities.append(ego_density)
                    except:
                        receiver_ego_sizes.append(0)
                        receiver_ego_densities.append(0)
                
                result_df['receiver_ego_network_size'] = receiver_ego_sizes
                result_df['receiver_ego_network_density'] = receiver_ego_densities
            
            # Calculate bipartite projection features
            if self.bipartite_graph is not None:
                # Get sender and receiver sets
                senders = {n for n, d in self.bipartite_graph.nodes(data=True) if d['bipartite'] == 0}
                receivers = {n for n, d in self.bipartite_graph.nodes(data=True) if d['bipartite'] == 1}
                
                # Project to sender graph
                sender_projection = nx.bipartite.projected_graph(self.bipartite_graph, senders)
                
                # Calculate features for sender projection
                if 'sender_id' in df.columns:
                    sender_proj_degrees = []
                    for sender in df['sender_id']:
                        try:
                            degree = sender_projection.degree(sender)
                            sender_proj_degrees.append(degree)
                        except:
                            sender_proj_degrees.append(0)
                    
                    result_df['sender_projection_degree'] = sender_proj_degrees
                
                # Project to receiver graph
                receiver_projection = nx.bipartite.projected_graph(self.bipartite_graph, receivers)
                
                # Calculate features for receiver projection
                if 'receiver_id' in df.columns:
                    receiver_proj_degrees = []
                    for receiver in df['receiver_id']:
                        try:
                            degree = receiver_projection.degree(receiver)
                            receiver_proj_degrees.append(degree)
                        except:
                            receiver_proj_degrees.append(0)
                    
                    result_df['receiver_projection_degree'] = receiver_proj_degrees
            
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting subgraph features: {str(e)}")
            return df
    
    def _extract_temporal_graph_features(self, df):
        """
        Extract temporal graph features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with temporal graph features
        """
        try:
            result_df = df.copy()
            
            if self.graph is None or 'timestamp' not in df.columns:
                logger.warning("Graph not built or timestamp not available. Skipping temporal graph features.")
                return result_df
            
            # Convert timestamp to datetime if needed
            if not pd.api.types.is_datetime64_any_dtype(df['timestamp']):
                df['timestamp'] = pd.to_datetime(df['timestamp'])
            
            # Sort by timestamp
            df_sorted = df.sort_values('timestamp')
            
            # Calculate time window features
            time_windows = ['1H', '6H', '24H', '7D']
            
            for window in time_windows:
                # Calculate sender activity in time window
                if 'sender_id' in df.columns:
                    sender_activity = []
                    
                    for _, row in df_sorted.iterrows():
                        sender = row['sender_id']
                        timestamp = row['timestamp']
                        
                        # Get transactions in time window before current transaction
                        window_start = timestamp - pd.Timedelta(window)
                        window_end = timestamp
                        
                        window_transactions = df_sorted[
                            (df_sorted['timestamp'] >= window_start) &
                            (df_sorted['timestamp'] < window_end) &
                            (df_sorted['sender_id'] == sender)
                        ]
                        
                        # Calculate activity metrics
                        activity_count = len(window_transactions)
                        activity_amount = window_transactions['amount'].sum() if 'amount' in window_transactions.columns else 0
                        
                        sender_activity.append({
                            f'sender_activity_count_{window}': activity_count,
                            f'sender_activity_amount_{window}': activity_amount
                        })
                    
                    # Add to result dataframe
                    activity_df = pd.DataFrame(sender_activity)
                    for col in activity_df.columns:
                        result_df[col] = activity_df[col].values
                
                # Calculate receiver activity in time window
                if 'receiver_id' in df.columns:
                    receiver_activity = []
                    
                    for _, row in df_sorted.iterrows():
                        receiver = row['receiver_id']
                        timestamp = row['timestamp']
                        
                        # Get transactions in time window before current transaction
                        window_start = timestamp - pd.Timedelta(window)
                        window_end = timestamp
                        
                        window_transactions = df_sorted[
                            (df_sorted['timestamp'] >= window_start) &
                            (df_sorted['timestamp'] < window_end) &
                            (df_sorted['receiver_id'] == receiver)
                        ]
                        
                        # Calculate activity metrics
                        activity_count = len(window_transactions)
                        activity_amount = window_transactions['amount'].sum() if 'amount' in window_transactions.columns else 0
                        
                        receiver_activity.append({
                            f'receiver_activity_count_{window}': activity_count,
                            f'receiver_activity_amount_{window}': activity_amount
                        })
                    
                    # Add to result dataframe
                    activity_df = pd.DataFrame(receiver_activity)
                    for col in activity_df.columns:
                        result_df[col] = activity_df[col].values
            
            # Calculate time since last transaction for sender
            if 'sender_id' in df.columns:
                time_since_last = []
                
                for _, row in df_sorted.iterrows():
                    sender = row['sender_id']
                    timestamp = row['timestamp']
                    
                    # Get previous transaction from same sender
                    prev_transactions = df_sorted[
                        (df_sorted['timestamp'] < timestamp) &
                        (df_sorted['sender_id'] == sender)
                    ]
                    
                    if len(prev_transactions) > 0:
                        last_timestamp = prev_transactions['timestamp'].max()
                        time_diff = (timestamp - last_timestamp).total_seconds()
                        time_since_last.append(time_diff)
                    else:
                        time_since_last.append(float('inf'))
                
                result_df['sender_time_since_last_transaction'] = time_since_last
            
            # Calculate time since last transaction for receiver
            if 'receiver_id' in df.columns:
                time_since_last = []
                
                for _, row in df_sorted.iterrows():
                    receiver = row['receiver_id']
                    timestamp = row['timestamp']
                    
                    # Get previous transaction to same receiver
                    prev_transactions = df_sorted[
                        (df_sorted['timestamp'] < timestamp) &
                        (df_sorted['receiver_id'] == receiver)
                    ]
                    
                    if len(prev_transactions) > 0:
                        last_timestamp = prev_transactions['timestamp'].max()
                        time_diff = (timestamp - last_timestamp).total_seconds()
                        time_since_last.append(time_diff)
                    else:
                        time_since_last.append(float('inf'))
                
                result_df['receiver_time_since_last_transaction'] = time_since_last
            
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting temporal graph features: {str(e)}")
            return df
    
    def fit_transform(self, df):
        """
        Fit the feature extractor and transform the data
        
        Args:
            df (DataFrame): Input data
            
        Returns:
            DataFrame: Transformed data with features
        """
        # Extract features
        result_df = self.extract_features(df)
        
        # Get feature columns
        feature_cols = [col for col in result_df.columns if col not in df.columns]
        
        if len(feature_cols) > 0:
            # Fit scaler
            self.scaler.fit(result_df[feature_cols])
            
            self.fitted = True
        
        return result_df
    
    def transform(self, df):
        """
        Transform new data using fitted feature extractor
        
        Args:
            df (DataFrame): Input data
            
        Returns:
            DataFrame: Transformed data with features
        """
        if not self.fitted:
            raise ValueError("Feature extractor not fitted. Call fit_transform first.")
        
        # Extract features
        result_df = self.extract_features(df)
        
        # Get feature columns
        feature_cols = [col for col in result_df.columns if col not in df.columns]
        
        if len(feature_cols) > 0:
            # Transform features using fitted scaler
            result_df[feature_cols] = self.scaler.transform(result_df[feature_cols])
        
        return result_df

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\features\.ipynb_checkpoints\nlp_features-checkpoint.py ===
"""
NLP Features Module
Implements natural language processing features for fraud detection
"""

import pandas as pd
import numpy as np
import re
import string
from collections import Counter
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.stem import WordNetLemmatizer, PorterStemmer
from nltk.sentiment import SentimentIntensityAnalyzer
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation
from textblob import TextBlob
import gensim
from gensim.models import Word2Vec, Doc2Vec
from gensim.models.doc2vec import TaggedDocument
import warnings
import logging
from typing import Dict, List, Tuple, Union

from fraud_detection_engine.utils.api_utils import is_api_available

warnings.filterwarnings('ignore')
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Download required NLTK data
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')
    
try:
    nltk.data.find('corpora/stopwords')
except LookupError:
    nltk.download('stopwords')
    
try:
    nltk.data.find('corpora/wordnet')
except LookupError:
    nltk.download('wordnet')
    
try:
    nltk.data.find('sentiment/vader_lexicon')
except LookupError:
    nltk.download('vader_lexicon')

class NLPFeatures:
    """
    Class for extracting NLP features from transaction data
    Implements techniques like sentiment analysis, topic modeling, etc.
    """
    
    def __init__(self, config=None):
        """
        Initialize NLPFeatures
        
        Args:
            config (dict, optional): Configuration parameters
        """
        self.config = config or {}
        self.stop_words = set(stopwords.words('english'))
        self.lemmatizer = WordNetLemmatizer()
        self.stemmer = PorterStemmer()
        self.sia = SentimentIntensityAnalyzer()
        self.tfidf_vectorizer = None
        self.count_vectorizer = None
        self.lda_model = None
        self.word2vec_model = None
        self.doc2vec_model = None
        self.feature_names = []
        self.fitted = False
        
        # Fraud-related keywords
        self.fraud_keywords = [
            'urgent', 'immediately', 'asap', 'hurry', 'quick', 'fast',
            'secret', 'confidential', 'private', 'hidden', 'discreet',
            'suspicious', 'unusual', 'strange', 'odd', 'weird',
            'illegal', 'fraud', 'scam', 'fake', 'counterfeit',
            'money', 'cash', 'payment', 'transfer', 'wire',
            'overseas', 'foreign', 'international', 'abroad',
            'inheritance', 'lottery', 'prize', 'winner', 'claim',
            'verify', 'confirm', 'update', 'account', 'information',
            'click', 'link', 'website', 'login', 'password',
            'bank', 'check', 'routing', 'account', 'number'
        ]
        
        # Suspicious patterns
        self.suspicious_patterns = [
            r'\$\d+,\d+\.\d{2}',  # Money format
            r'\d{4}[\s-]?\d{4}[\s-]?\d{4}[\s-]?\d{4}',  # Credit card pattern
            r'\b\d{3}-\d{2}-\d{4}\b',  # SSN pattern
            r'\b[A-Z0-9._%+-]+@[A-Z0-9.-]+\.[A-Z]{2,}\b',  # Email pattern
            r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+',  # URL pattern
            r'\b\d{10,}\b',  # Long numbers (could be account numbers)
            r'[A-Z]{2,}',  # All caps words
            r'\d+\.\d+\.\d+\.\d+',  # IP address pattern
        ]
    
    def extract_features(self, df):
        """
        Extract all NLP features from the dataframe
        
        Args:
            df (DataFrame): Input transaction data
            
        Returns:
            DataFrame: DataFrame with extracted features
        """
        try:
            # Create a copy to avoid modifying the original
            result_df = df.copy()
            
            # Extract different types of NLP features
            result_df = self._extract_basic_text_features(result_df)
            result_df = self._extract_sentiment_features(result_df)
            result_df = self._extract_keyword_features(result_df)
            result_df = self._extract_pattern_features(result_df)
            result_df = self._extract_topic_features(result_df)
            result_df = self._extract_embedding_features(result_df)
            
            # Store feature names
            self.feature_names = [col for col in result_df.columns if col not in df.columns]
            
            logger.info(f"Extracted {len(self.feature_names)} NLP features")
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting NLP features: {str(e)}")
            raise
    
    def _extract_basic_text_features(self, df):
        """
        Extract basic text features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with basic text features
        """
        try:
            result_df = df.copy()
            
            # Text columns to process
            text_columns = ['description', 'notes']
            
            for col in text_columns:
                if col in df.columns:
                    # Character count
                    result_df[f'{col}_char_count'] = df[col].fillna('').astype(str).apply(len)
                    
                    # Word count
                    result_df[f'{col}_word_count'] = df[col].fillna('').astype(str).apply(
                        lambda x: len(word_tokenize(x)) if x else 0
                    )
                    
                    # Sentence count
                    result_df[f'{col}_sentence_count'] = df[col].fillna('').astype(str).apply(
                        lambda x: len(sent_tokenize(x)) if x else 0
                    )
                    
                    # Average word length
                    result_df[f'{col}_avg_word_length'] = df[col].fillna('').astype(str).apply(
                        lambda x: np.mean([len(word) for word in word_tokenize(x)]) if word_tokenize(x) else 0
                    )
                    
                    # Average sentence length
                    result_df[f'{col}_avg_sentence_length'] = df[col].fillna('').astype(str).apply(
                        lambda x: np.mean([len(word_tokenize(sent)) for sent in sent_tokenize(x)]) if sent_tokenize(x) else 0
                    )
                    
                    # Punctuation count
                    result_df[f'{col}_punctuation_count'] = df[col].fillna('').astype(str).apply(
                        lambda x: sum(1 for char in x if char in string.punctuation)
                    )
                    
                    # Uppercase word count
                    result_df[f'{col}_uppercase_count'] = df[col].fillna('').astype(str).apply(
                        lambda x: sum(1 for word in word_tokenize(x) if word.isupper() and len(word) > 1)
                    )
                    
                    # Digit count
                    result_df[f'{col}_digit_count'] = df[col].fillna('').astype(str).apply(
                        lambda x: sum(1 for char in x if char.isdigit())
                    )
                    
                    # Unique word count
                    result_df[f'{col}_unique_word_count'] = df[col].fillna('').astype(str).apply(
                        lambda x: len(set(word_tokenize(x.lower()))) if x else 0
                    )
                    
                    # Lexical diversity (unique words / total words)
                    result_df[f'{col}_lexical_diversity'] = df[col].fillna('').astype(str).apply(
                        lambda x: len(set(word_tokenize(x.lower()))) / len(word_tokenize(x)) if word_tokenize(x) else 0
                    )
            
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting basic text features: {str(e)}")
            return df
    
    def _extract_sentiment_features(self, df):
        """
        Extract sentiment analysis features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with sentiment features
        """
        try:
            result_df = df.copy()
            
            # Text columns to process
            text_columns = ['description', 'notes']
            
            for col in text_columns:
                if col in df.columns:
                    # VADER sentiment scores
                    sentiment_scores = df[col].fillna('').astype(str).apply(
                        lambda x: self.sia.polarity_scores(x) if x else {'neg': 0, 'neu': 0, 'pos': 0, 'compound': 0}
                    )
                    
                    # Extract individual scores
                    result_df[f'{col}_sentiment_neg'] = sentiment_scores.apply(lambda x: x['neg'])
                    result_df[f'{col}_sentiment_neu'] = sentiment_scores.apply(lambda x: x['neu'])
                    result_df[f'{col}_sentiment_pos'] = sentiment_scores.apply(lambda x: x['pos'])
                    result_df[f'{col}_sentiment_compound'] = sentiment_scores.apply(lambda x: x['compound'])
                    
                    # TextBlob sentiment
                    textblob_sentiment = df[col].fillna('').astype(str).apply(
                        lambda x: TextBlob(x).sentiment if x else TextBlob('').sentiment
                    )
                    
                    result_df[f'{col}_textblob_polarity'] = textblob_sentiment.apply(lambda x: x.polarity)
                    result_df[f'{col}_textblob_subjectivity'] = textblob_sentiment.apply(lambda x: x.subjectivity)
                    
                    # Emotion indicators
                    result_df[f'{col}_is_negative'] = (result_df[f'{col}_sentiment_compound'] < -0.05).astype(int)
                    result_df[f'{col}_is_positive'] = (result_df[f'{col}_sentiment_compound'] > 0.05).astype(int)
                    result_df[f'{col}_is_neutral'] = (
                        (result_df[f'{col}_sentiment_compound'] >= -0.05) & 
                        (result_df[f'{col}_sentiment_compound'] <= 0.05)
                    ).astype(int)
            
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting sentiment features: {str(e)}")
            return df
    
    def _extract_keyword_features(self, df):
        """
        Extract keyword-based features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with keyword features
        """
        try:
            result_df = df.copy()
            
            # Text columns to process
            text_columns = ['description', 'notes']
            
            for col in text_columns:
                if col in df.columns:
                    # Preprocess text
                    processed_text = df[col].fillna('').astype(str).apply(self._preprocess_text)
                    
                    # Count fraud keywords
                    fraud_keyword_counts = processed_text.apply(
                        lambda x: sum(1 for word in x if word in self.fraud_keywords)
                    )
                    result_df[f'{col}_fraud_keyword_count'] = fraud_keyword_counts
                    
                    # Flag if any fraud keywords present
                    result_df[f'{col}_has_fraud_keywords'] = (fraud_keyword_counts > 0).astype(int)
                    
                    # Count specific keyword categories
                    urgency_keywords = ['urgent', 'immediately', 'asap', 'hurry', 'quick', 'fast']
                    secrecy_keywords = ['secret', 'confidential', 'private', 'hidden', 'discreet']
                    money_keywords = ['money', 'cash', 'payment', 'transfer', 'wire']
                    
                    result_df[f'{col}_urgency_keyword_count'] = processed_text.apply(
                        lambda x: sum(1 for word in x if word in urgency_keywords)
                    )
                    
                    result_df[f'{col}_secrecy_keyword_count'] = processed_text.apply(
                        lambda x: sum(1 for word in x if word in secrecy_keywords)
                    )
                    
                    result_df[f'{col}_money_keyword_count'] = processed_text.apply(
                        lambda x: sum(1 for word in x if word in money_keywords)
                    )
                    
                    # Calculate keyword density
                    result_df[f'{col}_fraud_keyword_density'] = fraud_keyword_counts / (
                        df[col].fillna('').astype(str).apply(lambda x: len(word_tokenize(x)) if x else 1)
                    )
                    
                    # TF-IDF for fraud keywords
                    if not self.fitted:
                        # Fit TF-IDF vectorizer
                        self.tfidf_vectorizer = TfidfVectorizer(
                            vocabulary=self.fraud_keywords,
                            ngram_range=(1, 2),
                            max_features=100
                        )
                        
                        # Fit on all text
                        all_text = pd.concat([df[col].fillna('').astype(str) for col in text_columns if col in df.columns])
                        self.tfidf_vectorizer.fit(all_text)
                    
                    # Transform text
                    tfidf_features = self.tfidf_vectorizer.transform(df[col].fillna('').astype(str))
                    
                    # Add top TF-IDF features
                    feature_names = self.tfidf_vectorizer.get_feature_names_out()
                    for i, feature in enumerate(feature_names[:10]):  # Top 10 features
                        result_df[f'{col}_tfidf_{feature}'] = tfidf_features[:, i].toarray().flatten()
            
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting keyword features: {str(e)}")
            return df
    
    def _extract_pattern_features(self, df):
        """
        Extract pattern-based features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with pattern features
        """
        try:
            result_df = df.copy()
            
            # Text columns to process
            text_columns = ['description', 'notes']
            
            for col in text_columns:
                if col in df.columns:
                    # Count suspicious patterns
                    pattern_counts = []
                    for text in df[col].fillna('').astype(str):
                        count = 0
                        for pattern in self.suspicious_patterns:
                            matches = re.findall(pattern, text, re.IGNORECASE)
                            count += len(matches)
                        pattern_counts.append(count)
                    
                    result_df[f'{col}_suspicious_pattern_count'] = pattern_counts
                    
                    # Flag if any suspicious patterns present
                    result_df[f'{col}_has_suspicious_patterns'] = (np.array(pattern_counts) > 0).astype(int)
                    
                    # Specific pattern counts
                    money_pattern = r'\$\d+,\d+\.\d{2}'
                    credit_card_pattern = r'\d{4}[\s-]?\d{4}[\s-]?\d{4}[\s-]?\d{4}'
                    ssn_pattern = r'\b\d{3}-\d{2}-\d{4}\b'
                    email_pattern = r'\b[A-Z0-9._%+-]+@[A-Z0-9.-]+\.[A-Z]{2,}\b'
                    url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'
                    
                    result_df[f'{col}_money_pattern_count'] = df[col].fillna('').astype(str).apply(
                        lambda x: len(re.findall(money_pattern, x))
                    )
                    
                    result_df[f'{col}_credit_card_pattern_count'] = df[col].fillna('').astype(str).apply(
                        lambda x: len(re.findall(credit_card_pattern, x))
                    )
                    
                    result_df[f'{col}_ssn_pattern_count'] = df[col].fillna('').astype(str).apply(
                        lambda x: len(re.findall(ssn_pattern, x))
                    )
                    
                    result_df[f'{col}_email_pattern_count'] = df[col].fillna('').astype(str).apply(
                        lambda x: len(re.findall(email_pattern, x))
                    )
                    
                    result_df[f'{col}_url_pattern_count'] = df[col].fillna('').astype(str).apply(
                        lambda x: len(re.findall(url_pattern, x))
                    )
                    
                    # Check for excessive punctuation
                    result_df[f'{col}_excessive_punctuation'] = (
                        df[col].fillna('').astype(str).apply(
                            lambda x: 1 if sum(1 for char in x if char in string.punctuation) / len(x) > 0.3 else 0
                        )
                    )
                    
                    # Check for excessive capitalization
                    result_df[f'{col}_excessive_capitalization'] = (
                        df[col].fillna('').astype(str).apply(
                            lambda x: 1 if sum(1 for char in x if char.isupper()) / len(x) > 0.5 else 0
                        )
                    )
            
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting pattern features: {str(e)}")
            return df
    
    def _extract_topic_features(self, df):
        """
        Extract topic modeling features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with topic features
        """
        try:
            result_df = df.copy()
            
            # Text columns to process
            text_columns = ['description', 'notes']
            
            # Combine text from all columns
            all_text = []
            for col in text_columns:
                if col in df.columns:
                    all_text.extend(df[col].fillna('').astype(str).tolist())
            
            if not all_text:
                return result_df
            
            # Preprocess text for topic modeling
            processed_docs = [self._preprocess_text(doc) for doc in all_text]
            processed_docs = [' '.join(doc) for doc in processed_docs if doc]
            
            if not processed_docs:
                return result_df
            
            # Fit CountVectorizer and LDA model if not fitted
            if not self.fitted:
                # Fit CountVectorizer
                self.count_vectorizer = CountVectorizer(
                    max_features=1000,
                    stop_words='english',
                    ngram_range=(1, 2)
                )
                doc_term_matrix = self.count_vectorizer.fit_transform(processed_docs)
                
                # Fit LDA model
                self.lda_model = LatentDirichletAllocation(
                    n_components=5,  # 5 topics
                    random_state=42,
                    max_iter=10,
                    learning_method='online'
                )
                self.lda_model.fit(doc_term_matrix)
            
            # Transform each text column
            for col in text_columns:
                if col in df.columns:
                    # Preprocess text
                    processed_text = df[col].fillna('').astype(str).apply(self._preprocess_text)
                    processed_text = processed_text.apply(lambda x: ' '.join(x) if x else '')
                    
                    # Transform to document-term matrix
                    doc_term_matrix = self.count_vectorizer.transform(processed_text)
                    
                    # Get topic distributions
                    topic_distributions = self.lda_model.transform(doc_term_matrix)
                    
                    # Add topic features
                    for i in range(topic_distributions.shape[1]):
                        result_df[f'{col}_topic_{i}_prob'] = topic_distributions[:, i]
                    
                    # Get dominant topic
                    dominant_topics = np.argmax(topic_distributions, axis=1)
                    result_df[f'{col}_dominant_topic'] = dominant_topics
            
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting topic features: {str(e)}")
            return df
    
    def _extract_embedding_features(self, df):
        """
        Extract word embedding features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with embedding features
        """
        try:
            result_df = df.copy()
            
            # Text columns to process
            text_columns = ['description', 'notes']
            
            # Combine text from all columns
            all_text = []
            for col in text_columns:
                if col in df.columns:
                    all_text.extend(df[col].fillna('').astype(str).tolist())
            
            if not all_text:
                return result_df
            
            # Preprocess text for embeddings
            processed_docs = [self._preprocess_text(doc) for doc in all_text]
            processed_docs = [doc for doc in processed_docs if doc]
            
            if not processed_docs:
                return result_df
            
            # Check if Gemini API is available
            if is_api_available('gemini'):
                # Here you would implement Gemini API calls for embeddings
                # For now, we'll skip and use local embeddings
                logger.info("Gemini API available, but implementation pending. Using local embeddings.")
            
            # Check if OpenAI API is available
            if is_api_available('openai'):
                # Here you would implement OpenAI API calls for embeddings
                # For now, we'll skip and use local embeddings
                logger.info("OpenAI API available, but implementation pending. Using local embeddings.")
            
            # Fit Word2Vec model (local implementation)
            self.word2vec_model = Word2Vec(
                sentences=processed_docs,
                vector_size=100,  # 100-dimensional vectors
                window=5,
                min_count=1,
                workers=4,
                sg=1  # Skip-gram model
            )
            
            # Train Doc2Vec model (local implementation)
            tagged_docs = [TaggedDocument(doc, [i]) for i, doc in enumerate(processed_docs)]
            self.doc2vec_model = Doc2Vec(
                documents=tagged_docs,
                vector_size=100,  # 100-dimensional vectors
                window=5,
                min_count=1,
                workers=4,
                epochs=10
            )
            
            # Transform each text column
            for col in text_columns:
                if col in df.columns:
                    # Preprocess text
                    processed_text = df[col].fillna('').astype(str).apply(self._preprocess_text)
                    
                    # Calculate average Word2Vec vectors
                    word2vec_vectors = []
                    for doc in processed_text:
                        if doc:
                            # Get vectors for words in document
                            word_vectors = [self.word2vec_model.wv[word] for word in doc if word in self.word2vec_model.wv]
                            
                            if word_vectors:
                                # Average the vectors
                                avg_vector = np.mean(word_vectors, axis=0)
                                word2vec_vectors.append(avg_vector)
                            else:
                                # Use zero vector if no words found
                                word2vec_vectors.append(np.zeros(100))
                        else:
                            # Use zero vector for empty documents
                            word2vec_vectors.append(np.zeros(100))
                    
                    # Add Word2Vec features (first 10 dimensions)
                    word2vec_vectors = np.array(word2vec_vectors)
                    for i in range(min(10, word2vec_vectors.shape[1])):
                        result_df[f'{col}_word2vec_dim_{i}'] = word2vec_vectors[:, i]
                    
                    # Calculate Doc2Vec vectors
                    doc2vec_vectors = []
                    for doc in processed_text:
                        if doc:
                            # Infer vector for document
                            vector = self.doc2vec_model.infer_vector(doc)
                            doc2vec_vectors.append(vector)
                        else:
                            # Use zero vector for empty documents
                            doc2vec_vectors.append(np.zeros(100))
                    
                    # Add Doc2Vec features (first 10 dimensions)
                    doc2vec_vectors = np.array(doc2vec_vectors)
                    for i in range(min(10, doc2vec_vectors.shape[1])):
                        result_df[f'{col}_doc2vec_dim_{i}'] = doc2vec_vectors[:, i]
            
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting embedding features: {str(e)}")
            return df
    
    def _preprocess_text(self, text):
        """
        Preprocess text for NLP analysis
        
        Args:
            text (str): Input text
            
        Returns:
            list: List of processed tokens
        """
        try:
            # Convert to lowercase
            text = text.lower()
            
            # Remove punctuation
            text = text.translate(str.maketrans('', '', string.punctuation))
            
            # Remove digits
            text = re.sub(r'\d+', '', text)
            
            # Tokenize
            tokens = word_tokenize(text)
            
            # Remove stop words
            tokens = [word for word in tokens if word not in self.stop_words]
            
            # Lemmatize
            tokens = [self.lemmatizer.lemmatize(word) for word in tokens]
            
            # Remove short words
            tokens = [word for word in tokens if len(word) > 2]
            
            return tokens
            
        except Exception as e:
            logger.error(f"Error preprocessing text: {str(e)}")
            return []
    
    def fit_transform(self, df):
        """
        Fit the feature extractor and transform the data
        
        Args:
            df (DataFrame): Input data
            
        Returns:
            DataFrame: Transformed data with features
        """
        # Extract features
        result_df = self.extract_features(df)
        
        # Get feature columns
        feature_cols = [col for col in result_df.columns if col not in df.columns]
        
        if len(feature_cols) > 0:
            self.fitted = True
        
        return result_df
    
    def transform(self, df):
        """
        Transform new data using fitted feature extractor
        
        Args:
            df (DataFrame): Input data
            
        Returns:
            DataFrame: Transformed data with features
        """
        if not self.fitted:
            raise ValueError("Feature extractor not fitted. Call fit_transform first.")
        
        # Extract features
        result_df = self.extract_features(df)
        
        return result_df

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\features\.ipynb_checkpoints\statistical_features-checkpoint.py ===
"""
Statistical Features Module
Implements various statistical feature extraction techniques for fraud detection
"""

import pandas as pd
import numpy as np
import scipy.stats as stats
from scipy.spatial.distance import mahalanobis
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
import warnings
import logging
from typing import Dict, List, Tuple, Union

warnings.filterwarnings('ignore')
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class StatisticalFeatures:
    """
    Class for extracting statistical features from transaction data
    Implements techniques like Benford's Law, Z-score, MAD, etc.
    """
    
    def __init__(self, config=None):
        """
        Initialize StatisticalFeatures
        
        Args:
            config (dict, optional): Configuration parameters
        """
        self.config = config or {}
        self.feature_names = []
        self.scaler = StandardScaler()
        self.pca = None
        self.fitted = False
        
    def extract_features(self, df):
        """
        Extract all statistical features from the dataframe
        
        Args:
            df (DataFrame): Input transaction data
            
        Returns:
            DataFrame: DataFrame with extracted features
        """
        try:
            # Create a copy to avoid modifying the original
            result_df = df.copy()
            
            # Extract different types of statistical features
            result_df = self._extract_benford_features(result_df)
            result_df = self._extract_zscore_features(result_df)
            result_df = self._extract_mad_features(result_df)
            result_df = self._extract_percentile_features(result_df)
            result_df = self._extract_distribution_features(result_df)
            result_df = self._extract_mahalanobis_features(result_df)
            result_df = self._extract_grubbs_features(result_df)
            result_df = self._extract_entropy_features(result_df)
            result_df = self._extract_correlation_features(result_df)
            
            # Store feature names
            self.feature_names = [col for col in result_df.columns if col not in df.columns]
            
            logger.info(f"Extracted {len(self.feature_names)} statistical features")
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting statistical features: {str(e)}")
            raise
    
    def _extract_benford_features(self, df):
        """
        Extract Benford's Law features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with Benford's Law features
        """
        try:
            result_df = df.copy()
            
            # Apply Benford's Law to amount column if it exists
            if 'amount' in df.columns:
                # Get first digit of amounts
                amounts = df['amount'].abs()
                first_digits = amounts.astype(str).str[0].replace('n', '0').astype(int)
                first_digits = first_digits[first_digits >= 1]  # Exclude 0
                
                if len(first_digits) > 0:
                    # Calculate actual distribution
                    actual_dist = first_digits.value_counts(normalize=True).sort_index()
                    
                    # Expected Benford's distribution
                    benford_dist = pd.Series([np.log10(1 + 1/d) for d in range(1, 10)], index=range(1, 10))
                    
                    # Calculate Chi-square statistic
                    chi_square = 0
                    for digit in range(1, 10):
                        expected_count = benford_dist[digit] * len(first_digits)
                        actual_count = actual_dist.get(digit, 0)
                        if expected_count > 0:
                            chi_square += (actual_count - expected_count) ** 2 / expected_count
                    
                    # Add features
                    result_df['benford_chi_square'] = chi_square
                    result_df['benford_p_value'] = 1 - stats.chi2.cdf(chi_square, 8)  # 8 degrees of freedom
                    
                    # Calculate deviation for each digit
                    for digit in range(1, 6):  # Just first 5 digits to avoid too many features
                        actual_pct = actual_dist.get(digit, 0)
                        expected_pct = benford_dist[digit]
                        result_df[f'benford_deviation_{digit}'] = abs(actual_pct - expected_pct)
            
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting Benford's Law features: {str(e)}")
            return df
    
    def _extract_zscore_features(self, df):
        """
        Extract Z-score based features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with Z-score features
        """
        try:
            result_df = df.copy()
            
            # Apply to amount column if it exists
            if 'amount' in df.columns:
                amounts = df['amount']
                
                # Calculate Z-scores
                mean_amount = amounts.mean()
                std_amount = amounts.std()
                
                if std_amount > 0:
                    z_scores = (amounts - mean_amount) / std_amount
                    result_df['amount_zscore'] = z_scores
                    
                    # Flag extreme values
                    result_df['amount_zscore_outlier'] = (np.abs(z_scores) > 3).astype(int)
                else:
                    result_df['amount_zscore'] = 0
                    result_df['amount_zscore_outlier'] = 0
            
            # Apply Z-score to sender and receiver if ID columns exist
            if 'sender_id' in df.columns and 'amount' in df.columns:
                # Calculate average amount per sender
                sender_avg = df.groupby('sender_id')['amount'].mean()
                sender_std = df.groupby('sender_id')['amount'].std()
                
                # Calculate Z-scores for each transaction relative to sender's history
                sender_zscores = []
                for _, row in df.iterrows():
                    sender_id = row['sender_id']
                    amount = row['amount']
                    
                    if sender_id in sender_avg and sender_id in sender_std and sender_std[sender_id] > 0:
                        z_score = (amount - sender_avg[sender_id]) / sender_std[sender_id]
                    else:
                        z_score = 0
                    
                    sender_zscores.append(z_score)
                
                result_df['sender_amount_zscore'] = sender_zscores
                result_df['sender_amount_zscore_outlier'] = (np.abs(sender_zscores) > 3).astype(int)
            
            if 'receiver_id' in df.columns and 'amount' in df.columns:
                # Calculate average amount per receiver
                receiver_avg = df.groupby('receiver_id')['amount'].mean()
                receiver_std = df.groupby('receiver_id')['amount'].std()
                
                # Calculate Z-scores for each transaction relative to receiver's history
                receiver_zscores = []
                for _, row in df.iterrows():
                    receiver_id = row['receiver_id']
                    amount = row['amount']
                    
                    if receiver_id in receiver_avg and receiver_id in receiver_std and receiver_std[receiver_id] > 0:
                        z_score = (amount - receiver_avg[receiver_id]) / receiver_std[receiver_id]
                    else:
                        z_score = 0
                    
                    receiver_zscores.append(z_score)
                
                result_df['receiver_amount_zscore'] = receiver_zscores
                result_df['receiver_amount_zscore_outlier'] = (np.abs(receiver_zscores) > 3).astype(int)
            
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting Z-score features: {str(e)}")
            return df
    
    def _extract_mad_features(self, df):
        """
        Extract Median Absolute Deviation (MAD) features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with MAD features
        """
        try:
            result_df = df.copy()
            
            # Apply to amount column if it exists
            if 'amount' in df.columns:
                amounts = df['amount']
                
                # Calculate MAD
                median_amount = amounts.median()
                abs_dev = np.abs(amounts - median_amount)
                mad = abs_dev.median()
                
                if mad > 0:
                    # Calculate modified Z-scores using MAD
                    modified_z_scores = 0.6745 * abs_dev / mad
                    result_df['amount_mad_zscore'] = modified_z_scores
                    
                    # Flag extreme values
                    result_df['amount_mad_outlier'] = (modified_z_scores > 3.5).astype(int)
                else:
                    result_df['amount_mad_zscore'] = 0
                    result_df['amount_mad_outlier'] = 0
            
            # Apply MAD to sender and receiver if ID columns exist
            if 'sender_id' in df.columns and 'amount' in df.columns:
                # Calculate MAD per sender
                sender_mad = df.groupby('sender_id')['amount'].apply(lambda x: np.median(np.abs(x - x.median())))
                sender_median = df.groupby('sender_id')['amount'].median()
                
                # Calculate MAD-based Z-scores for each transaction
                sender_mad_zscores = []
                for _, row in df.iterrows():
                    sender_id = row['sender_id']
                    amount = row['amount']
                    
                    if sender_id in sender_mad and sender_id in sender_median and sender_mad[sender_id] > 0:
                        abs_dev = abs(amount - sender_median[sender_id])
                        mad_z_score = 0.6745 * abs_dev / sender_mad[sender_id]
                    else:
                        mad_z_score = 0
                    
                    sender_mad_zscores.append(mad_z_score)
                
                result_df['sender_amount_mad_zscore'] = sender_mad_zscores
                result_df['sender_amount_mad_outlier'] = (np.array(sender_mad_zscores) > 3.5).astype(int)
            
            if 'receiver_id' in df.columns and 'amount' in df.columns:
                # Calculate MAD per receiver
                receiver_mad = df.groupby('receiver_id')['amount'].apply(lambda x: np.median(np.abs(x - x.median())))
                receiver_median = df.groupby('receiver_id')['amount'].median()
                
                # Calculate MAD-based Z-scores for each transaction
                receiver_mad_zscores = []
                for _, row in df.iterrows():
                    receiver_id = row['receiver_id']
                    amount = row['amount']
                    
                    if receiver_id in receiver_mad and receiver_id in receiver_median and receiver_mad[receiver_id] > 0:
                        abs_dev = abs(amount - receiver_median[receiver_id])
                        mad_z_score = 0.6745 * abs_dev / receiver_mad[receiver_id]
                    else:
                        mad_z_score = 0
                    
                    receiver_mad_zscores.append(mad_z_score)
                
                result_df['receiver_amount_mad_zscore'] = receiver_mad_zscores
                result_df['receiver_amount_mad_outlier'] = (np.array(receiver_mad_zscores) > 3.5).astype(int)
            
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting MAD features: {str(e)}")
            return df
    
    def _extract_percentile_features(self, df):
        """
        Extract percentile-based features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with percentile features
        """
        try:
            result_df = df.copy()
            
            # Apply to amount column if it exists
            if 'amount' in df.columns:
                amounts = df['amount']
                
                # Calculate percentiles
                percentiles = [5, 10, 25, 50, 75, 90, 95, 99]
                percentile_values = np.percentile(amounts, percentiles)
                
                # Add features for each percentile
                for i, p in enumerate(percentiles):
                    result_df[f'amount_above_{p}th_percentile'] = (amounts > percentile_values[i]).astype(int)
                
                # Calculate percentile rank for each amount
                result_df['amount_percentile_rank'] = amounts.rank(pct=True)
            
            # Apply percentiles to sender and receiver if ID columns exist
            if 'sender_id' in df.columns and 'amount' in df.columns:
                # Calculate percentile rank within each sender's transactions
                sender_percentile_ranks = df.groupby('sender_id')['amount'].rank(pct=True)
                result_df['sender_amount_percentile_rank'] = sender_percentile_ranks
                
                # Flag if amount is in top 5% for sender
                result_df['sender_amount_top_5pct'] = (sender_percentile_ranks > 0.95).astype(int)
            
            if 'receiver_id' in df.columns and 'amount' in df.columns:
                # Calculate percentile rank within each receiver's transactions
                receiver_percentile_ranks = df.groupby('receiver_id')['amount'].rank(pct=True)
                result_df['receiver_amount_percentile_rank'] = receiver_percentile_ranks
                
                # Flag if amount is in top 5% for receiver
                result_df['receiver_amount_top_5pct'] = (receiver_percentile_ranks > 0.95).astype(int)
            
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting percentile features: {str(e)}")
            return df
    
    def _extract_distribution_features(self, df):
        """
        Extract distribution-based features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with distribution features
        """
        try:
            result_df = df.copy()
            
            # Apply to amount column if it exists
            if 'amount' in df.columns:
                amounts = df['amount']
                
                # Skewness and kurtosis
                result_df['amount_skewness'] = stats.skew(amounts)
                result_df['amount_kurtosis'] = stats.kurtosis(amounts)
                
                # Normality tests
                _, normality_p = stats.normaltest(amounts)
                result_df['amount_normality_p'] = normality_p
                
                # Shapiro-Wilk test (for smaller samples)
                if len(amounts) <= 5000:
                    _, shapiro_p = stats.shapiro(amounts)
                    result_df['amount_shapiro_p'] = shapiro_p
                
                # Kolmogorov-Smirnov test against normal distribution
                _, ks_p = stats.kstest(amounts, 'norm', args=(amounts.mean(), amounts.std()))
                result_df['amount_ks_p'] = ks_p
                
                # Anderson-Darling test
                ad_result = stats.anderson(amounts)
                result_df['amount_ad_statistic'] = ad_result.statistic
                
                # Jarque-Bera test
                jb_stat, jb_p = stats.jarque_bera(amounts)
                result_df['amount_jb_statistic'] = jb_stat
                result_df['amount_jb_p'] = jb_p
            
            # Apply distribution features to sender and receiver if ID columns exist
            if 'sender_id' in df.columns and 'amount' in df.columns:
                # Calculate distribution features per sender
                sender_stats = df.groupby('sender_id')['amount'].agg([
                    ('skewness', lambda x: stats.skew(x) if len(x) >= 3 else 0),
                    ('kurtosis', lambda x: stats.kurtosis(x) if len(x) >= 4 else 0),
                    ('variance', 'var'),
                    ('range', lambda x: x.max() - x.min() if len(x) > 0 else 0)
                ])
                
                # Map back to each transaction
                for stat in ['skewness', 'kurtosis', 'variance', 'range']:
                    result_df[f'sender_amount_{stat}'] = df['sender_id'].map(sender_stats[stat]).fillna(0)
            
            if 'receiver_id' in df.columns and 'amount' in df.columns:
                # Calculate distribution features per receiver
                receiver_stats = df.groupby('receiver_id')['amount'].agg([
                    ('skewness', lambda x: stats.skew(x) if len(x) >= 3 else 0),
                    ('kurtosis', lambda x: stats.kurtosis(x) if len(x) >= 4 else 0),
                    ('variance', 'var'),
                    ('range', lambda x: x.max() - x.min() if len(x) > 0 else 0)
                ])
                
                # Map back to each transaction
                for stat in ['skewness', 'kurtosis', 'variance', 'range']:
                    result_df[f'receiver_amount_{stat}'] = df['receiver_id'].map(receiver_stats[stat]).fillna(0)
            
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting distribution features: {str(e)}")
            return df
    
    def _extract_mahalanobis_features(self, df):
        """
        Extract Mahalanobis distance features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with Mahalanobis features
        """
        try:
            result_df = df.copy()
            
            # Get numeric columns
            numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
            
            if len(numeric_cols) < 2:
                logger.warning("Not enough numeric columns for Mahalanobis distance calculation")
                return result_df
            
            # Prepare data
            X = df[numeric_cols].fillna(0)
            
            # Calculate covariance matrix
            cov_matrix = np.cov(X, rowvar=False)
            
            # Check if covariance matrix is invertible
            if np.linalg.det(cov_matrix) == 0:
                logger.warning("Covariance matrix is singular, using pseudo-inverse")
                inv_cov_matrix = np.linalg.pinv(cov_matrix)
            else:
                inv_cov_matrix = np.linalg.inv(cov_matrix)
            
            # Calculate mean vector
            mean_vector = np.mean(X, axis=0)
            
            # Calculate Mahalanobis distances
            mahalanobis_distances = []
            for i in range(len(X)):
                mahalanobis_distances.append(
                    mahalanobis(X.iloc[i], mean_vector, inv_cov_matrix)
                )
            
            result_df['mahalanobis_distance'] = mahalanobis_distances
            
            # Flag outliers based on Mahalanobis distance
            # Using chi-square distribution with degrees of freedom = number of features
            threshold = stats.chi2.ppf(0.975, df=len(numeric_cols))
            result_df['mahalanobis_outlier'] = (np.array(mahalanobis_distances) > threshold).astype(int)
            
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting Mahalanobis features: {str(e)}")
            return df
    
    def _extract_grubbs_features(self, df):
        """
        Extract Grubbs' test features for outliers
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with Grubbs' test features
        """
        try:
            result_df = df.copy()
            
            # Apply to amount column if it exists
            if 'amount' in df.columns:
                amounts = df['amount'].values
                
                # Calculate Grubbs' test statistic
                mean_amount = np.mean(amounts)
                std_amount = np.std(amounts)
                
                if std_amount > 0:
                    # Calculate absolute deviations
                    abs_deviations = np.abs(amounts - mean_amount)
                    max_deviation = np.max(abs_deviations)
                    
                    # Grubbs' test statistic
                    grubbs_stat = max_deviation / std_amount
                    result_df['grubbs_statistic'] = grubbs_stat
                    
                    # Calculate critical value for two-sided test
                    n = len(amounts)
                    t_critical = stats.t.ppf(1 - 0.025 / (2 * n), n - 2)
                    critical_value = (n - 1) * t_critical / np.sqrt(n * (n - 2 + t_critical**2))
                    
                    # Flag outliers
                    result_df['grubbs_outlier'] = (grubbs_stat > critical_value).astype(int)
                else:
                    result_df['grubbs_statistic'] = 0
                    result_df['grubbs_outlier'] = 0
            
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting Grubbs' test features: {str(e)}")
            return df
    
    def _extract_entropy_features(self, df):
        """
        Extract entropy-based features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with entropy features
        """
        try:
            result_df = df.copy()
            
            # Calculate entropy for categorical columns
            categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()
            
            for col in categorical_cols:
                # Calculate probability distribution
                value_counts = df[col].value_counts(normalize=True)
                
                # Calculate Shannon entropy
                entropy = -sum(p * np.log2(p) for p in value_counts if p > 0)
                result_df[f'{col}_entropy'] = entropy
                
                # Calculate normalized entropy (0 to 1)
                max_entropy = np.log2(len(value_counts))
                if max_entropy > 0:
                    normalized_entropy = entropy / max_entropy
                else:
                    normalized_entropy = 0
                result_df[f'{col}_normalized_entropy'] = normalized_entropy
            
            # Calculate entropy for numeric columns (after binning)
            numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
            
            for col in numeric_cols:
                # Bin the data
                try:
                    binned = pd.cut(df[col], bins=10, duplicates='drop')
                    
                    # Calculate probability distribution
                    value_counts = binned.value_counts(normalize=True)
                    
                    # Calculate Shannon entropy
                    entropy = -sum(p * np.log2(p) for p in value_counts if p > 0)
                    result_df[f'{col}_binned_entropy'] = entropy
                    
                    # Calculate normalized entropy (0 to 1)
                    max_entropy = np.log2(len(value_counts))
                    if max_entropy > 0:
                        normalized_entropy = entropy / max_entropy
                    else:
                        normalized_entropy = 0
                    result_df[f'{col}_binned_normalized_entropy'] = normalized_entropy
                except:
                    pass
            
            # Calculate transaction entropy for sender and receiver
            if 'sender_id' in df.columns:
                # Calculate entropy of transaction amounts per sender
                sender_entropy = df.groupby('sender_id')['amount'].apply(
                    lambda x: self._calculate_series_entropy(x)
                )
                result_df['sender_amount_entropy'] = df['sender_id'].map(sender_entropy).fillna(0)
            
            if 'receiver_id' in df.columns:
                # Calculate entropy of transaction amounts per receiver
                receiver_entropy = df.groupby('receiver_id')['amount'].apply(
                    lambda x: self._calculate_series_entropy(x)
                )
                result_df['receiver_amount_entropy'] = df['receiver_id'].map(receiver_entropy).fillna(0)
            
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting entropy features: {str(e)}")
            return df
    
    def _calculate_series_entropy(self, series):
        """
        Calculate entropy of a pandas Series
        
        Args:
            series (Series): Input series
            
        Returns:
            float: Entropy value
        """
        try:
            # Bin the data
            binned = pd.cut(series, bins=min(10, len(series)), duplicates='drop')
            
            # Calculate probability distribution
            value_counts = binned.value_counts(normalize=True)
            
            # Calculate Shannon entropy
            entropy = -sum(p * np.log2(p) for p in value_counts if p > 0)
            return entropy
        except:
            return 0
    
    def _extract_correlation_features(self, df):
        """
        Extract correlation-based features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with correlation features
        """
        try:
            result_df = df.copy()
            
            # Get numeric columns
            numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
            
            if len(numeric_cols) < 2:
                logger.warning("Not enough numeric columns for correlation calculation")
                return result_df
            
            # Calculate correlation matrix
            corr_matrix = df[numeric_cols].corr().abs()
            
            # Find highest correlation for each variable
            max_corr = {}
            for col in numeric_cols:
                # Get correlations with other variables
                corrs = corr_matrix[col].drop(col)
                max_corr[col] = corrs.max()
            
            # Add features
            for col in numeric_cols:
                result_df[f'{col}_max_correlation'] = max_corr[col]
            
            # Calculate average correlation
            avg_corr = {}
            for col in numeric_cols:
                # Get correlations with other variables
                corrs = corr_matrix[col].drop(col)
                avg_corr[col] = corrs.mean()
            
            # Add features
            for col in numeric_cols:
                result_df[f'{col}_avg_correlation'] = avg_corr[col]
            
            # Calculate variance inflation factor (VIF) for multicollinearity
            for i, col in enumerate(numeric_cols):
                # Prepare data for VIF calculation
                X = df[numeric_cols].copy()
                y = X[col]
                X = X.drop(col, axis=1)
                
                # Fit linear regression
                try:
                    from sklearn.linear_model import LinearRegression
                    model = LinearRegression()
                    model.fit(X, y)
                    
                    # Calculate R-squared
                    r_squared = model.score(X, y)
                    
                    # Calculate VIF
                    if r_squared < 1:
                        vif = 1 / (1 - r_squared)
                    else:
                        vif = float('inf')
                    
                    result_df[f'{col}_vif'] = vif
                except:
                    result_df[f'{col}_vif'] = 1
            
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting correlation features: {str(e)}")
            return df
    
    def fit_transform(self, df):
        """
        Fit the feature extractor and transform the data
        
        Args:
            df (DataFrame): Input data
            
        Returns:
            DataFrame: Transformed data with features
        """
        # Extract features
        result_df = self.extract_features(df)
        
        # Get feature columns
        feature_cols = [col for col in result_df.columns if col not in df.columns]
        
        if len(feature_cols) > 0:
            # Fit scaler
            self.scaler.fit(result_df[feature_cols])
            
            # Apply PCA if we have enough features
            if len(feature_cols) >= 10:
                self.pca = PCA(n_components=min(10, len(feature_cols)))
                self.pca.fit(result_df[feature_cols])
                
                # Add PCA components
                pca_components = self.pca.transform(result_df[feature_cols])
                for i in range(pca_components.shape[1]):
                    result_df[f'stat_pca_{i+1}'] = pca_components[:, i]
            
            self.fitted = True
        
        return result_df
    
    def transform(self, df):
        """
        Transform new data using fitted feature extractor
        
        Args:
            df (DataFrame): Input data
            
        Returns:
            DataFrame: Transformed data with features
        """
        if not self.fitted:
            raise ValueError("Feature extractor not fitted. Call fit_transform first.")
        
        # Extract features
        result_df = self.extract_features(df)
        
        # Get feature columns
        feature_cols = [col for col in result_df.columns if col not in df.columns]
        
        if len(feature_cols) > 0:
            # Transform features using fitted scaler
            result_df[feature_cols] = self.scaler.transform(result_df[feature_cols])
            
            # Apply PCA if it was fitted
            if self.pca is not None:
                pca_components = self.pca.transform(result_df[feature_cols])
                for i in range(pca_components.shape[1]):
                    result_df[f'stat_pca_{i+1}'] = pca_components[:, i]
        
        return result_df

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\features\.ipynb_checkpoints\timeseries_features-checkpoint.py ===
"""
Time Series Features Module
Implements time series feature extraction techniques for fraud detection
"""

import pandas as pd
import numpy as np
from scipy import stats
from scipy.signal import find_peaks
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tsa.stattools import acf, pacf
import warnings
import logging
from typing import Dict, List, Tuple, Union

warnings.filterwarnings('ignore')
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class TimeSeriesFeatures:
    """
    Class for extracting time series features from transaction data
    Implements techniques like burstiness analysis, gap analysis, etc.
    """
    
    def __init__(self, config=None):
        """
        Initialize TimeSeriesFeatures
        
        Args:
            config (dict, optional): Configuration parameters
        """
        self.config = config or {}
        self.feature_names = []
        self.fitted = False
        
    def extract_features(self, df):
        """
        Extract all time series features from the dataframe
        
        Args:
            df (DataFrame): Input transaction data
            
        Returns:
            DataFrame: DataFrame with extracted features
        """
        try:
            # Create a copy to avoid modifying the original
            result_df = df.copy()
            
            # Ensure timestamp is in datetime format
            if 'timestamp' in df.columns and not pd.api.types.is_datetime64_any_dtype(df['timestamp']):
                result_df['timestamp'] = pd.to_datetime(df['timestamp'])
            
            # Extract different types of time series features
            result_df = self._extract_temporal_features(result_df)
            result_df = self._extract_frequency_features(result_df)
            result_df = self._extract_burstiness_features(result_df)
            result_df = self._extract_gap_features(result_df)
            result_df = self._extract_seasonal_features(result_df)
            result_df = self._extract_autocorrelation_features(result_df)
            
            # Store feature names
            self.feature_names = [col for col in result_df.columns if col not in df.columns]
            
            logger.info(f"Extracted {len(self.feature_names)} time series features")
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting time series features: {str(e)}")
            raise
    
    def _extract_temporal_features(self, df):
        """
        Extract temporal features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with temporal features
        """
        try:
            result_df = df.copy()
            
            if 'timestamp' not in df.columns:
                logger.warning("Timestamp column not found. Skipping temporal features.")
                return result_df
            
            # Extract time components
            result_df['hour'] = result_df['timestamp'].dt.hour
            result_df['day'] = result_df['timestamp'].dt.day
            result_df['month'] = result_df['timestamp'].dt.month
            result_df['year'] = result_df['timestamp'].dt.year
            result_df['dayofweek'] = result_df['timestamp'].dt.dayofweek  # Monday=0, Sunday=6
            result_df['dayofyear'] = result_df['timestamp'].dt.dayofyear
            result_df['weekofyear'] = result_df['timestamp'].dt.isocalendar().week
            result_df['quarter'] = result_df['timestamp'].dt.quarter
            
            # Time-based flags
            result_df['is_weekend'] = (result_df['dayofweek'] >= 5).astype(int)
            result_df['is_month_start'] = result_df['timestamp'].dt.is_month_start.astype(int)
            result_df['is_month_end'] = result_df['timestamp'].dt.is_month_end.astype(int)
            result_df['is_quarter_start'] = result_df['timestamp'].dt.is_quarter_start.astype(int)
            result_df['is_quarter_end'] = result_df['timestamp'].dt.is_quarter_end.astype(int)
            result_df['is_year_start'] = result_df['timestamp'].dt.is_year_start.astype(int)
            result_df['is_year_end'] = result_df['timestamp'].dt.is_year_end.astype(int)
            
            # Time of day flags
            result_df['is_night'] = ((result_df['hour'] >= 22) | (result_df['hour'] < 6)).astype(int)
            result_df['is_morning'] = ((result_df['hour'] >= 6) & (result_df['hour'] < 12)).astype(int)
            result_df['is_afternoon'] = ((result_df['hour'] >= 12) & (result_df['hour'] < 18)).astype(int)
            result_df['is_evening'] = ((result_df['hour'] >= 18) & (result_df['hour'] < 22)).astype(int)
            
            # Business hours flag (9 AM to 5 PM, Monday to Friday)
            result_df['is_business_hours'] = (
                (result_df['hour'] >= 9) & 
                (result_df['hour'] < 17) & 
                (result_df['dayofweek'] < 5)
            ).astype(int)
            
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting temporal features: {str(e)}")
            return df
    
    def _extract_frequency_features(self, df):
        """
        Extract frequency-based features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with frequency features
        """
        try:
            result_df = df.copy()
            
            if 'timestamp' not in df.columns:
                logger.warning("Timestamp column not found. Skipping frequency features.")
                return result_df
            
            # Sort by timestamp
            df_sorted = result_df.sort_values('timestamp')
            
            # Time windows for frequency analysis
            time_windows = ['1H', '6H', '24H', '7D', '30D']
            
            for window in time_windows:
                # Calculate transaction frequency in time windows before each transaction
                window_counts = []
                
                for _, row in df_sorted.iterrows():
                    timestamp = row['timestamp']
                    
                    # Get transactions in time window before current transaction
                    window_start = timestamp - pd.Timedelta(window)
                    window_end = timestamp
                    
                    window_transactions = df_sorted[
                        (df_sorted['timestamp'] >= window_start) &
                        (df_sorted['timestamp'] < window_end)
                    ]
                    
                    # Count transactions in window
                    count = len(window_transactions)
                    window_counts.append(count)
                
                result_df[f'transaction_frequency_{window}'] = window_counts
            
            # Calculate frequency by sender
            if 'sender_id' in df.columns:
                for window in ['1H', '6H', '24H']:
                    sender_window_counts = []
                    
                    for _, row in df_sorted.iterrows():
                        sender = row['sender_id']
                        timestamp = row['timestamp']
                        
                        # Get transactions from same sender in time window
                        window_start = timestamp - pd.Timedelta(window)
                        window_end = timestamp
                        
                        window_transactions = df_sorted[
                            (df_sorted['timestamp'] >= window_start) &
                            (df_sorted['timestamp'] < window_end) &
                            (df_sorted['sender_id'] == sender)
                        ]
                        
                        # Count transactions in window
                        count = len(window_transactions)
                        sender_window_counts.append(count)
                    
                    result_df[f'sender_frequency_{window}'] = sender_window_counts
            
            # Calculate frequency by receiver
            if 'receiver_id' in df.columns:
                for window in ['1H', '6H', '24H']:
                    receiver_window_counts = []
                    
                    for _, row in df_sorted.iterrows():
                        receiver = row['receiver_id']
                        timestamp = row['timestamp']
                        
                        # Get transactions to same receiver in time window
                        window_start = timestamp - pd.Timedelta(window)
                        window_end = timestamp
                        
                        window_transactions = df_sorted[
                            (df_sorted['timestamp'] >= window_start) &
                            (df_sorted['timestamp'] < window_end) &
                            (df_sorted['receiver_id'] == receiver)
                        ]
                        
                        # Count transactions in window
                        count = len(window_transactions)
                        receiver_window_counts.append(count)
                    
                    result_df[f'receiver_frequency_{window}'] = receiver_window_counts
            
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting frequency features: {str(e)}")
            return df
    
    def _extract_burstiness_features(self, df):
        """
        Extract burstiness analysis features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with burstiness features
        """
        try:
            result_df = df.copy()
            
            if 'timestamp' not in df.columns:
                logger.warning("Timestamp column not found. Skipping burstiness features.")
                return result_df
            
            # Sort by timestamp
            df_sorted = result_df.sort_values('timestamp')
            
            # Calculate inter-transaction times
            inter_times = df_sorted['timestamp'].diff().dt.total_seconds().fillna(0)
            
            # Calculate burstiness coefficient
            if len(inter_times) > 1 and inter_times.std() > 0:
                burstiness = (inter_times.std() - inter_times.mean()) / (inter_times.std() + inter_times.mean())
            else:
                burstiness = 0
            
            result_df['burstiness_coefficient'] = burstiness
            
            # Calculate local burstiness (in sliding windows)
            window_sizes = [10, 50, 100]  # Number of transactions
            
            for window_size in window_sizes:
                local_burstiness = []
                
                for i in range(len(df_sorted)):
                    # Get window around current transaction
                    start_idx = max(0, i - window_size // 2)
                    end_idx = min(len(df_sorted), i + window_size // 2 + 1)
                    
                    window_inter_times = inter_times.iloc[start_idx:end_idx]
                    
                    # Calculate burstiness for window
                    if len(window_inter_times) > 1 and window_inter_times.std() > 0:
                        local_b = (window_inter_times.std() - window_inter_times.mean()) / (
                            window_inter_times.std() + window_inter_times.mean()
                        )
                    else:
                        local_b = 0
                    
                    local_burstiness.append(local_b)
                
                result_df[f'local_burstiness_{window_size}'] = local_burstiness
            
            # Detect burst periods
            # A burst is defined as a period with inter-transaction times significantly lower than average
            avg_inter_time = inter_times.mean()
            std_inter_time = inter_times.std()
            
            # Threshold for burst detection (2 standard deviations below mean)
            burst_threshold = max(0, avg_inter_time - 2 * std_inter_time)
            
            # Flag transactions in bursts
            result_df['is_in_burst'] = (inter_times < burst_threshold).astype(int)
            
            # Calculate burst duration (consecutive transactions in burst)
            burst_durations = []
            current_duration = 0
            
            for is_burst in result_df['is_in_burst']:
                if is_burst:
                    current_duration += 1
                else:
                    burst_durations.append(current_duration)
                    current_duration = 0
            
            # Add the last duration if we're still in a burst
            burst_durations.append(current_duration)
            
            # Map burst durations back to transactions
            burst_duration_map = []
            idx = 0
            for i, is_burst in enumerate(result_df['is_in_burst']):
                if is_burst:
                    burst_duration_map.append(burst_durations[idx])
                else:
                    burst_duration_map.append(0)
                    if i > 0 and not result_df['is_in_burst'].iloc[i-1]:
                        idx += 1
            
            result_df['burst_duration'] = burst_duration_map
            
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting burstiness features: {str(e)}")
            return df
    
    def _extract_gap_features(self, df):
        """
        Extract gap analysis features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with gap features
        """
        try:
            result_df = df.copy()
            
            if 'timestamp' not in df.columns:
                logger.warning("Timestamp column not found. Skipping gap features.")
                return result_df
            
            # Sort by timestamp
            df_sorted = result_df.sort_values('timestamp')
            
            # Calculate inter-transaction times
            inter_times = df_sorted['timestamp'].diff().dt.total_seconds().fillna(0)
            
            # Time since last transaction
            result_df['time_since_last_transaction'] = inter_times
            
            # Time until next transaction
            time_until_next = df_sorted['timestamp'].diff(-1).dt.total_seconds().abs().fillna(0)
            result_df['time_until_next_transaction'] = time_until_next
            
            # Detect gaps (unusually long inter-transaction times)
            if len(inter_times) > 1 and inter_times.std() > 0:
                # Threshold for gap detection (2 standard deviations above mean)
                gap_threshold = inter_times.mean() + 2 * inter_times.std()
                
                # Flag transactions after gaps
                result_df['is_after_gap'] = (inter_times > gap_threshold).astype(int)
                
                # Calculate gap sizes
                result_df['gap_size'] = np.where(inter_times > gap_threshold, inter_times, 0)
            else:
                result_df['is_after_gap'] = 0
                result_df['gap_size'] = 0
            
            # Calculate gap statistics by sender
            if 'sender_id' in df.columns:
                sender_gaps = []
                sender_gap_stats = {}
                
                # Calculate average gap for each sender
                for sender in df_sorted['sender_id'].unique():
                    sender_transactions = df_sorted[df_sorted['sender_id'] == sender].sort_values('timestamp')
                    if len(sender_transactions) > 1:
                        sender_inter_times = sender_transactions['timestamp'].diff().dt.total_seconds().fillna(0)
                        sender_gap_stats[sender] = {
                            'mean': sender_inter_times.mean(),
                            'std': sender_inter_times.std()
                        }
                
                # Calculate gap features for each transaction
                for _, row in df_sorted.iterrows():
                    sender = row['sender_id']
                    
                    if sender in sender_gap_stats:
                        stats = sender_gap_stats[sender]
                        
                        # Time since last transaction for this sender
                        sender_transactions = df_sorted[
                            (df_sorted['sender_id'] == sender) &
                            (df_sorted['timestamp'] < row['timestamp'])
                        ]
                        
                        if len(sender_transactions) > 0:
                            last_sender_time = sender_transactions['timestamp'].max()
                            sender_inter_time = (row['timestamp'] - last_sender_time).total_seconds()
                        else:
                            sender_inter_time = float('inf')
                        
                        # Calculate Z-score for this gap
                        if stats['std'] > 0:
                            gap_z_score = (sender_inter_time - stats['mean']) / stats['std']
                        else:
                            gap_z_score = 0
                        
                        sender_gaps.append({
                            'sender_time_since_last': sender_inter_time,
                            'sender_gap_z_score': gap_z_score
                        })
                    else:
                        sender_gaps.append({
                            'sender_time_since_last': float('inf'),
                            'sender_gap_z_score': 0
                        })
                
                # Add to result dataframe
                gap_df = pd.DataFrame(sender_gaps)
                result_df['sender_time_since_last'] = gap_df['sender_time_since_last']
                result_df['sender_gap_z_score'] = gap_df['sender_gap_z_score']
            
            # Calculate gap statistics by receiver
            if 'receiver_id' in df.columns:
                receiver_gaps = []
                receiver_gap_stats = {}
                
                # Calculate average gap for each receiver
                for receiver in df_sorted['receiver_id'].unique():
                    receiver_transactions = df_sorted[df_sorted['receiver_id'] == receiver].sort_values('timestamp')
                    if len(receiver_transactions) > 1:
                        receiver_inter_times = receiver_transactions['timestamp'].diff().dt.total_seconds().fillna(0)
                        receiver_gap_stats[receiver] = {
                            'mean': receiver_inter_times.mean(),
                            'std': receiver_inter_times.std()
                        }
                
                # Calculate gap features for each transaction
                for _, row in df_sorted.iterrows():
                    receiver = row['receiver_id']
                    
                    if receiver in receiver_gap_stats:
                        stats = receiver_gap_stats[receiver]
                        
                        # Time since last transaction to this receiver
                        receiver_transactions = df_sorted[
                            (df_sorted['receiver_id'] == receiver) &
                            (df_sorted['timestamp'] < row['timestamp'])
                        ]
                        
                        if len(receiver_transactions) > 0:
                            last_receiver_time = receiver_transactions['timestamp'].max()
                            receiver_inter_time = (row['timestamp'] - last_receiver_time).total_seconds()
                        else:
                            receiver_inter_time = float('inf')
                        
                        # Calculate Z-score for this gap
                        if stats['std'] > 0:
                            gap_z_score = (receiver_inter_time - stats['mean']) / stats['std']
                        else:
                            gap_z_score = 0
                        
                        receiver_gaps.append({
                            'receiver_time_since_last': receiver_inter_time,
                            'receiver_gap_z_score': gap_z_score
                        })
                    else:
                        receiver_gaps.append({
                            'receiver_time_since_last': float('inf'),
                            'receiver_gap_z_score': 0
                        })
                
                # Add to result dataframe
                gap_df = pd.DataFrame(receiver_gaps)
                result_df['receiver_time_since_last'] = gap_df['receiver_time_since_last']
                result_df['receiver_gap_z_score'] = gap_df['receiver_gap_z_score']
            
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting gap features: {str(e)}")
            return df
    
    def _extract_seasonal_features(self, df):
        """
        Extract seasonal features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with seasonal features
        """
        try:
            result_df = df.copy()
            
            if 'timestamp' not in df.columns:
                logger.warning("Timestamp column not found. Skipping seasonal features.")
                return result_df
            
            # Sort by timestamp
            df_sorted = result_df.sort_values('timestamp')
            
            # Create a time series of transaction counts
            # Resample to different frequencies
            frequencies = ['1H', '6H', '1D', '1W']
            
            for freq in frequencies:
                # Count transactions per time period
                ts = df_sorted.set_index('timestamp').resample(freq).size()
                
                if len(ts) > 10:  # Need enough data points for decomposition
                    try:
                        # Perform seasonal decomposition
                        decomposition = seasonal_decompose(ts, model='additive', period=min(24, len(ts)//2))
                        
                        # Extract components
                        trend = decomposition.trend
                        seasonal = decomposition.seasonal
                        residual = decomposition.resid
                        
                        # Map back to original dataframe
                        trend_map = {}
                        seasonal_map = {}
                        residual_map = {}
                        
                        for timestamp in df_sorted['timestamp']:
                            # Find the closest time period
                            period_start = timestamp.floor(freq)
                            
                            if period_start in trend.index:
                                trend_map[timestamp] = trend[period_start]
                                seasonal_map[timestamp] = seasonal[period_start]
                                residual_map[timestamp] = residual[period_start]
                            else:
                                trend_map[timestamp] = 0
                                seasonal_map[timestamp] = 0
                                residual_map[timestamp] = 0
                        
                        # Add to result dataframe
                        result_df[f'trend_{freq}'] = df_sorted['timestamp'].map(trend_map).fillna(0)
                        result_df[f'seasonal_{freq}'] = df_sorted['timestamp'].map(seasonal_map).fillna(0)
                        result_df[f'residual_{freq}'] = df_sorted['timestamp'].map(residual_map).fillna(0)
                        
                        # Calculate anomaly score based on residual
                        residual_mean = residual.mean()
                        residual_std = residual.std()
                        
                        if residual_std > 0:
                            anomaly_scores = (df_sorted['timestamp'].map(residual_map) - residual_mean) / residual_std
                            result_df[f'seasonal_anomaly_{freq}'] = anomaly_scores.fillna(0)
                        else:
                            result_df[f'seasonal_anomaly_{freq}'] = 0
                            
                    except Exception as e:
                        logger.warning(f"Error in seasonal decomposition for {freq}: {str(e)}")
            
            # Calculate periodicity features
            # Check for daily, weekly, and monthly patterns
            periodicity_features = {}
            
            # Daily pattern (hour of day)
            if 'hour' in result_df.columns:
                hourly_counts = result_df.groupby('hour').size()
                # Calculate entropy to measure uniformity
                hourly_probs = hourly_counts / hourly_counts.sum()
                daily_entropy = -sum(p * np.log2(p) for p in hourly_probs if p > 0)
                max_entropy = np.log2(24)  # Maximum possible entropy for 24 hours
                daily_uniformity = daily_entropy / max_entropy if max_entropy > 0 else 0
                
                periodicity_features['daily_pattern_strength'] = 1 - daily_uniformity  # Higher means stronger pattern
                
                # Calculate peak hour
                peak_hour = hourly_counts.idxmax()
                periodicity_features['peak_hour'] = peak_hour
            
            # Weekly pattern (day of week)
            if 'dayofweek' in result_df.columns:
                weekly_counts = result_df.groupby('dayofweek').size()
                # Calculate entropy to measure uniformity
                weekly_probs = weekly_counts / weekly_counts.sum()
                weekly_entropy = -sum(p * np.log2(p) for p in weekly_probs if p > 0)
                max_entropy = np.log2(7)  # Maximum possible entropy for 7 days
                weekly_uniformity = weekly_entropy / max_entropy if max_entropy > 0 else 0
                
                periodicity_features['weekly_pattern_strength'] = 1 - weekly_uniformity  # Higher means stronger pattern
                
                # Calculate peak day
                peak_day = weekly_counts.idxmax()
                periodicity_features['peak_day'] = peak_day
            
            # Add periodicity features to all rows
            for feature, value in periodicity_features.items():
                result_df[feature] = value
            
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting seasonal features: {str(e)}")
            return df
    
    def _extract_autocorrelation_features(self, df):
        """
        Extract autocorrelation features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with autocorrelation features
        """
        try:
            result_df = df.copy()
            
            if 'timestamp' not in df.columns:
                logger.warning("Timestamp column not found. Skipping autocorrelation features.")
                return result_df
            
            # Sort by timestamp
            df_sorted = result_df.sort_values('timestamp')
            
            # Create time series of transaction counts
            # Resample to different frequencies
            frequencies = ['1H', '6H', '1D']
            
            for freq in frequencies:
                # Count transactions per time period
                ts = df_sorted.set_index('timestamp').resample(freq).size()
                
                if len(ts) > 10:  # Need enough data points for autocorrelation
                    try:
                        # Calculate autocorrelation function
                        nlags = min(10, len(ts) // 2)
                        autocorr = acf(ts, nlags=nlags, fft=True)
                        
                        # Add autocorrelation features
                        for i in range(1, min(6, len(autocorr))):  # First 5 lags
                            result_df[f'autocorr_{freq}_lag_{i}'] = autocorr[i]
                        
                        # Calculate partial autocorrelation
                        pacf_values = pacf(ts, nlags=nlags)
                        
                        # Add partial autocorrelation features
                        for i in range(1, min(6, len(pacf_values))):  # First 5 lags
                            result_df[f'pacf_{freq}_lag_{i}'] = pacf_values[i]
                        
                        # Detect periodicity in autocorrelation
                        # Look for peaks in autocorrelation
                        peaks, _ = find_peaks(autocorr[1:], height=0.2)  # Ignore lag 0
                        
                        if len(peaks) > 0:
                            # Find the most significant peak
                            peak_lag = peaks[0] + 1  # +1 because we ignored lag 0
                            result_df[f'periodicity_{freq}'] = peak_lag
                            result_df[f'periodicity_strength_{freq}'] = autocorr[peak_lag]
                        else:
                            result_df[f'periodicity_{freq}'] = 0
                            result_df[f'periodicity_strength_{freq}'] = 0
                            
                    except Exception as e:
                        logger.warning(f"Error in autocorrelation analysis for {freq}: {str(e)}")
            
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting autocorrelation features: {str(e)}")
            return df
    
    def fit_transform(self, df):
        """
        Fit the feature extractor and transform the data
        
        Args:
            df (DataFrame): Input data
            
        Returns:
            DataFrame: Transformed data with features
        """
        # Extract features
        result_df = self.extract_features(df)
        
        # Get feature columns
        feature_cols = [col for col in result_df.columns if col not in df.columns]
        
        if len(feature_cols) > 0:
            self.fitted = True
        
        return result_df
    
    def transform(self, df):
        """
        Transform new data using fitted feature extractor
        
        Args:
            df (DataFrame): Input data
            
        Returns:
            DataFrame: Transformed data with features
        """
        if not self.fitted:
            raise ValueError("Feature extractor not fitted. Call fit_transform first.")
        
        # Extract features
        result_df = self.extract_features(df)
        
        return result_df

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\features\.ipynb_checkpoints\__init__-checkpoint.py ===

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\features\__pycache__\graph_features.cpython-313.pyc ===
Û
    dÄöhπñ  „                   Û¸   ï S r SSKrSSKrSSKrSSKJrJ	r	  SSK
rSSKJr  SSKrSSKrSSKJrJrJrJr  \R*                  " S5        \R,                  " \R.                  S9  \R0                  " \5      r " S S	5      rg)
z`
Graph Features Module
Implements graph-based feature extraction techniques for fraud detection
È    N)⁄defaultdict⁄Counter)⁄MinMaxScaler)⁄Dict⁄List⁄Tuple⁄Union⁄ignore)⁄levelc                   Û^   ï \ rS rSrSrSS jrS rS rS rS r	S	 r
S
 rS rS rS rS rSrg)⁄GraphFeaturesÈ   zè
Class for extracting graph-based features from transaction data
Implements techniques like centrality measures, clustering coefficients, etc.
Nc                 Ûñ   ï U=(       d    0 U l         SU l        SU l        SU l        SU l        / U l        [        5       U l        SU l        g)zW
Initialize GraphFeatures

Args:
    config (dict, optional): Configuration parameters
NF)	⁄config⁄graph⁄sender_graph⁄receiver_graph⁄bipartite_graph⁄feature_namesr   ⁄scaler⁄fitted)⁄selfr   s     ⁄mC:\Users\Tranquil Abyss\fraud-detection-platform\app\..\src\fraud_detection_engine\features\graph_features.py⁄__init__⁄GraphFeatures.__init__   sF   Ä  ól†àåÿàå
ÿ à‘ÿ"à‘ÿ#à‘ÿà‘‹"ìnàåÿàçÛ    c                 ÛF  ï  UR                  5       nU R                  U5        U R                  U5      nU R                  U5      nU R	                  U5      nU R                  U5      nU R                  U5      nU R                  U5      nUR                   Vs/ s H  o3UR                  ;  d  M  UPM     snU l	        [        R                  S[        U R                  5       S35        U$ s  snf ! [         a'  n[        R                  S[        U5       35        e SnAff = f)z†
Extract all graph features from the dataframe

Args:
    df (DataFrame): Input transaction data
    
Returns:
    DataFrame: DataFrame with extracted features
z
Extracted z graph featuresz!Error extracting graph features: N)⁄copy⁄_build_graphs⁄_extract_centrality_features⁄_extract_clustering_features⁄_extract_community_features⁄_extract_path_features⁄_extract_subgraph_features⁄ _extract_temporal_graph_features⁄columnsr   ⁄logger⁄info⁄len⁄	Exception⁄error⁄str)r   ⁄df⁄	result_df⁄col⁄es        r   ⁄extract_features⁄GraphFeatures.extract_features*   s  Ä 	‡üôõ	àI ◊—òr‘" ◊9—9∏)”DàIÿ◊9—9∏)”DàIÿ◊8—8∏”CàIÿ◊3—3∞I”>àIÿ◊7—7∏	”BàIÿ◊=—=∏i”HàI 2;◊1B“1B”!\“1B®#–QS◊Q[—Q[—F[ß#—1B—!\àD‘‰èKâKò*§S®◊);—);”%<–$=∏_–M‘Nÿ–˘Ú "]¯Ù
 Û 	‹èLâL–<ºS¿ªV∏H–E‘Fÿ˚	˙s0   ÇBC/ ¬C*¬.C*¬45C/ √*C/ √/
D √9"DƒD c           	      Û™	  ï  [         R                  " 5       U l        UR                  5        GH
  u  p#SU;   d  M  SU;   d  M  US   nUS   n0 nSU;   a  US   US'   SU;   a  US   US'   SU;   a  US   US'   U R                  R	                  XE5      (       aa  U R                  U   U   nSU;   a  UR                  SS5      US   -   US'   UR                  SS5      S	-   US'   US
   R                  U5        MÕ  UR                  SS5      US'   S	US'   U/US
'   U R                  R                  " XE40 UD6  GM     [         R                  " 5       U l	        [        [        5      nUR                  5        H.  u  p#SU;   d  M  SU;   d  M  XÉS      R                  US   5        M0     UR                  5        H´  u  pY[        U	5      n
[        [!        U
5      5       HÉ  n[        US	-   [!        U
5      5       Hd  nX´   nX¨   nU R                  R	                  Xﬁ5      (       a  U R                  U   U   S==   S	-  ss'   MJ  U R                  R                  XﬁS	S9  Mf     MÖ     M≠     [         R                  " 5       U l        [        [        5      nUR                  5        H.  u  p#SU;   d  M  SU;   d  M  XÛS      R                  US   5        M0     UR                  5        H∞  u  nn[        U5      n[        [!        U5      5       Há  n[        US	-   [!        U5      5       Hh  nUU   nUU   nU R"                  R	                  UU5      (       a  U R"                  U   U   S==   S	-  ss'   MM  U R"                  R                  UUS	S9  Mj     Mâ     M≤     [         R                  " 5       U l        UR                  5        HM  u  p#SU;   a  U R$                  R'                  US   SS9  SU;   d  M0  U R$                  R'                  US   S	S9  MO     UR                  5        Hh  u  p#SU;   d  M  SU;   d  M  US   nUS   n0 nSU;   a  US   US'   SU;   a  US   US'   SU;   a  US   US'   U R$                  R                  " XE40 UD6  Mj     [(        R+                  S5        g! [,         a'  n[(        R/                  S[1        U5       35        e SnAff = f)zb
Build various graphs from the transaction data

Args:
    df (DataFrame): Input transaction data
⁄	sender_id⁄receiver_id⁄amount⁄	timestamp⁄transaction_id⁄total_amountr   ⁄transaction_countÈ   ⁄transactions⁄common_receivers)r=   ⁄common_senders)r>   )⁄	bipartitezGraphs built successfullyzError building graphs: N)⁄nx⁄DiGraphr   ⁄iterrows⁄has_edge⁄get⁄append⁄add_edge⁄Graphr   r   ⁄set⁄add⁄items⁄list⁄ranger)   r   r   ⁄add_noder'   r(   r*   r+   r,   )r   r-   ⁄_⁄row⁄sender⁄receiver⁄attrs⁄	edge_data⁄receiver_to_senders⁄senders⁄senders_list⁄i⁄j⁄sender1⁄sender2⁄sender_to_receivers⁄	receivers⁄receivers_list⁄	receiver1⁄	receiver2r0   s                        r   r   ⁄GraphFeatures._build_graphsM   s…  Ä n	‰üöõàDåJ ü+ô+ü-ëêÿ†#’%®-∏3’*>ÿ †—-êFÿ"†=—1êH êEÿ†3ìÿ*-®h©-òòhôÿ"†c”)ÿ-0∞—-=òòk—*ÿ'®3”.ÿ25–6F—2Gò–.—/ ózëz◊*—*®6◊<—<‡$(ßJ°J®v—$6∞x—$@ò	ÿ#†u”,ÿ8Aøπ¿n–VW”8X–[`–ai—[j—8jòI†n—5ÿ9Bøπ–GZ–\]”9^–ab—9bò	–"5—6ÿ!†.—1◊8—8∏÷? 16∑	±	∏(¿A”0Fòòn—-ÿ56ò–1—2ÿ16∞òòn—-ÿü
ô
◊+“+®F—F¿’FÒ7 (Ù< !#ß¢£
àD‘Ù #.¨c”"2–ÿü+ô+û-ëêÿ†#’%®-∏3’*>ÿ'®M—(:—;◊?—?¿¿K—@P÷QÒ (
 &9◊%>—%>÷%@—!ê‹#†Gõ}ê‹ús†<”0÷1êA‹"†1†Q°3¨®L”(9÷:òÿ".°/òÿ".°/ò‡◊,—,◊5—5∞g◊G—Gÿ ◊-—-®g—6∞w—?–@R”S–WX—X’S‡ ◊-—-◊6—6∞w–Z[–6”\Û ;Û 2Ò &AÙ #%ß(¢(£*àD‘Ù #.¨c”"2–ÿü+ô+û-ëêÿ†#’%®-∏3’*>ÿ'®K—(8—9◊=—=∏c¿-—>P÷QÒ (
 &9◊%>—%>÷%@—!êò	‹!%†i£ê‹ús†>”2÷3êA‹"†1†Q°3¨®N”(;÷<òÿ$2∞1—$5ò	ÿ$2∞1—$5ò	‡◊.—.◊7—7∏	¿9◊M—Mÿ ◊/—/∞	—:∏9—E–FV”W–[\—\’W‡ ◊/—/◊8—8∏¿I–^_–8”`Û =Û 4Ò &AÙ $&ß8¢8£:àD‘  ü+ô+û-ëêÿ†#”%ÿ◊(—(◊1—1∞#∞k—2B»a–1—Pÿ †C’'ÿ◊(—(◊1—1∞#∞m—2D–PQ–1”RÒ	 ( ü+ô+û-ëêÿ†#’%®-∏3’*>ÿ †—-êFÿ"†=—1êH êEÿ†3ìÿ*-®h©-òòhôÿ"†c”)ÿ-0∞—-=òòk—*ÿ'®3”.ÿ25–6F—2Gò–.—/ ◊(—(◊1“1∞&—L¿e‘LÒ (Ù" èKâK–3’4¯‰Û 	‹èLâL–2¥3∞q≥6∞(–;‘<ÿ˚	˙sP   Ç4R! ∫R! ¡D;R! ∆R! ∆	DR!  +R!  3D7R! œ.:R! –,R! –4A,R! “!
S“+"S”Sc                 Û¯	  ï  UR                  5       nU R                  c  [        R                  S5        U$ [        R
                  " U R                  5      n[        R                  " U R                  5      n[        R                  " U R                  R                  5       5      nSUR                  ;   ar  US   R                  U5      R                  S5      US'   US   R                  U5      R                  S5      US'   US   R                  U5      R                  S5      US'   SUR                  ;   ar  US   R                  U5      R                  S5      US	'   US   R                  U5      R                  S5      US
'   US   R                  U5      R                  S5      US'   [        U R                  R                  5       5      S::  a!  [        R                  " U R                  5      nOE[        U R                  R                  5       5      SS n[        R                  " U R                  US9nSUR                  ;   a&  US   R                  U5      R                  S5      US'   SUR                  ;   a&  US   R                  U5      R                  S5      US'   [        U R                  R                  5       5      S::  a!  [        R                   " U R                  5      nO[        R                   " U R                  SS9nSUR                  ;   a&  US   R                  U5      R                  S5      US'   SUR                  ;   a&  US   R                  U5      R                  S5      US'   [        U R                  R                  5       5      S::  a!   [        R"                  " U R                  SS9n	O0 n	SUR                  ;   a&  US   R                  U	5      R                  S5      US'   SUR                  ;   a&  US   R                  U	5      R                  S5      US'   [        R$                  " U R                  5      n
SUR                  ;   a&  US   R                  U
5      R                  S5      US'   SUR                  ;   a&  US   R                  U
5      R                  S5      US'   U$ !   0 n	 GN= f! [&         a-  n[        R)                  S[+        U5       35        Us SnA$ SnAff = f)zé
Extract centrality-based features

Args:
    df (DataFrame): Input dataframe
    
Returns:
    DataFrame: DataFrame with centrality features
Nz.Graph not built. Skipping centrality features.r4   r   ⁄sender_in_degree_centrality⁄sender_out_degree_centrality⁄sender_degree_centralityr5   ⁄receiver_in_degree_centrality⁄receiver_out_degree_centrality⁄receiver_degree_centralityiË  )⁄k⁄sender_betweenness_centrality⁄receiver_betweenness_centrality⁄weight)⁄distance⁄sender_closeness_centrality⁄receiver_closeness_centrality)⁄max_iter⁄sender_eigenvector_centrality⁄receiver_eigenvector_centrality⁄sender_pagerank⁄receiver_pagerankz&Error extracting centrality features: )r   r   r'   ⁄warningr@   ⁄in_degree_centrality⁄out_degree_centrality⁄degree_centrality⁄to_undirectedr&   ⁄map⁄fillnar)   ⁄nodes⁄betweenness_centralityrK   ⁄closeness_centrality⁄eigenvector_centrality⁄pagerankr*   r+   r,   )r   r-   r.   ru   rv   rw   r|   ⁄sample_nodesr}   r~   r   r0   s               r   r    ⁄*GraphFeatures._extract_centrality_featuresƒ   sm  Ä R	ÿüôõ	àI‡èzâz—!‹óë–O‘Pÿ – Ù $&◊#:“#:∏4ø:π:”#F– ‹$&◊$<“$<∏TøZπZ”$H–!‹ "◊ 4“ 4∞T∑Z±Z◊5M—5M”5O” P– òbüjôj”(ÿ;=∏kπ?◊;N—;N–Oc”;d◊;k—;k–lm”;nê	–7—8ÿ<>∏{πO◊<O—<O–Pe”<f◊<m—<m–no”<pê	–8—9ÿ8:∏;π◊8K—8K–L]”8^◊8e—8e–fg”8hê	–4—5‡†ß
°
”*ÿ=?¿—=N◊=R—=R–Sg”=h◊=o—=o–pq”=rê	–9—:ÿ>@¿—>O◊>S—>S–Ti”>j◊>q—>q–rs”>tê	–:—;ÿ:<∏]—:K◊:O—:O–Pa”:b◊:i—:i–jk”:lê	–6—7Ù ê4ó:ë:◊#—#”%”&®$”.‹)+◊)B“)B¿4«:¡:”)N—&Ù  $†DßJ°J◊$4—$4”$6”7∏∏–>ê‹)+◊)B“)B¿4«:¡:–Q]—)^–& òbüjôj”(ÿ=?¿π_◊=P—=P–Qg”=h◊=o—=o–pq”=rê	–9—:‡†ß
°
”*ÿ?A¿-—?P◊?T—?T–Uk”?l◊?s—?s–tu”?vê	–;—<Ù ê4ó:ë:◊#—#”%”&®$”.‹')◊'>“'>∏tøzπz”'J—$Ù (*◊'>“'>∏tøzπz–T\—']–$ òbüjôj”(ÿ;=∏kπ?◊;N—;N–Oc”;d◊;k—;k–lm”;nê	–7—8‡†ß
°
”*ÿ=?¿—=N◊=R—=R–Sg”=h◊=o—=o–pq”=rê	–9—:Ù ê4ó:ë:◊#—#”%”&®$”.0‹-/◊-F“-F¿t«z¡z–\`—-a—* *,–& òbüjôj”(ÿ=?¿π_◊=P—=P–Qg”=h◊=o—=o–pq”=rê	–9—:‡†ß
°
”*ÿ?A¿-—?P◊?T—?T–Uk”?l◊?s—?s–tu”?vê	–;—<Ù ó{í{†4ß:°:”.àH òbüjôj”(ÿ/1∞+©◊/B—/B¿8”/L◊/S—/S–TU”/Vê	–+—,‡†ß
°
”*ÿ13∞M—1B◊1F—1F¿x”1P◊1W—1W–XY”1Zê	–-—.‡–¯-0ÿ-/”*˚Ù. Û 	‹èLâL–Aƒ#¿a√&¿–J‘KÿçI˚	˙s<   Ç3S ∂M%S ŒR8 Œ;C<S “8R?“<S ”
S9”"S4”.S9”4S9c                 Û®  ï  UR                  5       nU R                  c  [        R                  S5        U$ [        R
                  " U R                  R                  5       5      nSUR                  ;   a&  US   R                  U5      R                  S5      US'   SUR                  ;   a&  US   R                  U5      R                  S5      US'   U R                  b}  [        U R                  R                  5       5      S:î  aV  [        R
                  " U R                  5      nSUR                  ;   a&  US   R                  U5      R                  S5      US'   U R                  b}  [        U R                  R                  5       5      S:î  aV  [        R
                  " U R                  5      nSUR                  ;   a&  US   R                  U5      R                  S5      US	'   [        R                  " U R                  R                  5       5      nSUR                  ;   a&  US   R                  U5      R                  S5      US
'   SUR                  ;   a&  US   R                  U5      R                  S5      US'   [        R                  " U R                  R                  5       5      nSUR                  ;   a&  US   R                  U5      R                  S5      US'   SUR                  ;   a&  US   R                  U5      R                  S5      US'   U$ ! [          a-  n[        R#                  S[%        U5       35        Us SnA$ SnAff = f)zé
Extract clustering-based features

Args:
    df (DataFrame): Input dataframe
    
Returns:
    DataFrame: DataFrame with clustering features
Nz.Graph not built. Skipping clustering features.r4   r   ⁄sender_clustering_coefficientr5   ⁄receiver_clustering_coefficient⁄#sender_graph_clustering_coefficient⁄%receiver_graph_clustering_coefficient⁄sender_avg_neighbor_degree⁄receiver_avg_neighbor_degree⁄sender_square_clustering⁄receiver_square_clusteringz&Error extracting clustering features: )r   r   r'   rt   r@   ⁄
clusteringrx   r&   ry   rz   r   r)   r{   r   ⁄average_neighbor_degree⁄square_clusteringr*   r+   r,   )	r   r-   r.   ⁄clustering_coeff⁄sender_clustering_coeff⁄receiver_clustering_coeff⁄avg_neighbor_degreerç   r0   s	            r   r!   ⁄*GraphFeatures._extract_clustering_features"  sÙ  Ä 9	ÿüôõ	àI‡èzâz—!‹óë–O‘Pÿ – Ù  "ü}ö}®TØZ©Z◊-E—-E”-G”H– òbüjôj”(ÿ=?¿π_◊=P—=P–Qa”=b◊=i—=i–jk”=lê	–9—:‡†ß
°
”*ÿ?A¿-—?P◊?T—?T–Ue”?f◊?m—?m–no”?pê	–;—< ◊ — —,¥∞T◊5F—5F◊5L—5L”5N”1O–RS”1S‹*,Ø-™-∏◊8I—8I”*J–' †"ß*°*”,ÿGI»+¡◊GZ—GZ–[r”Gs◊Gz—Gz–{|”G}êI–C—D ◊"—"—.¥3∞t◊7J—7J◊7P—7P”7R”3S–VW”3W‹,.ØM™M∏$◊:M—:M”,N–) !†BßJ°J”.ÿIK»M—IZ◊I^—I^–_x”Iy˜  JAÒ  JA  BCÛ  JDêI–E—FÙ #%◊"<“"<∏TøZπZ◊=U—=U”=W”"X– òbüjôj”(ÿ:<∏[π/◊:M—:M–Na”:b◊:i—:i–jk”:lê	–6—7‡†ß
°
”*ÿ<>∏}—<M◊<Q—<Q–Re”<f◊<m—<m–no”<pê	–8—9Ù !#◊ 4“ 4∞T∑Z±Z◊5M—5M”5O” P– òbüjôj”(ÿ8:∏;π◊8K—8K–L]”8^◊8e—8e–fg”8hê	–4—5‡†ß
°
”*ÿ:<∏]—:K◊:O—:O–Pa”:b◊:i—:i–jk”:lê	–6—7‡–¯‰Û 	‹èLâL–Aƒ#¿a√&¿–J‘KÿçI˚	˙s#   Ç3L ∂K#L Ã
MÃ$"MÕMÕMc                 ÛŒ  ï  UR                  5       nU R                  c  [        R                  S5        U$ U R                  R	                  5       n[
        R                  " U5      nSUR                  ;   a&  US   R                  U5      R                  S5      US'   SUR                  ;   a&  US   R                  U5      R                  S5      US'   [        UR                  5       5      nUR                  5        VVs0 s H
  u  pgXeU   _M     nnnSUR                  ;   a&  US   R                  U5      R                  S5      US	'   SUR                  ;   a&  US   R                  U5      R                  S5      US
'   0 n	UR                  5        Hì  u  pzUR                  5        VVs/ s H  u  pkX∑:X  d  M  UPM     nnnUR                  U5      n[        UR                  5       5      S:î  d  M`  [         R"                  " U5      nU H  nUR%                  US5      Xñ'   M     Mï     SUR                  ;   a&  US   R                  U	5      R                  S5      US'   SUR                  ;   a&  US   R                  U	5      R                  S5      US'   SUR                  ;   a|  SUR                  ;   al  / nUR'                  5        HR  u  nnUS   nUS   nUU;   a+  UU;   a%  UR)                  [+        UU   UU   :H  5      5        MA  UR)                  S5        MT     XÚS'   U$ s  snnf s  snnf ! [,         a-  n[        R/                  S[1        U5       35        Us SnA$ SnAff = f)zå
Extract community-based features

Args:
    df (DataFrame): Input dataframe
    
Returns:
    DataFrame: DataFrame with community features
Nz-Graph not built. Skipping community features.r4   Èˇˇˇˇ⁄sender_communityr5   ⁄receiver_communityr   ⁄sender_community_size⁄receiver_community_size⁄"sender_community_degree_centrality⁄$receiver_community_degree_centrality⁄same_communityz%Error extracting community features: )r   r   r'   rt   rx   ⁄community_louvain⁄best_partitionr&   ry   rz   r   ⁄valuesrJ   ⁄subgraphr)   r{   r@   rw   rD   rB   rE   ⁄intr*   r+   r,   )r   r-   r.   ⁄undirected_graph⁄communities⁄community_sizes⁄node⁄comm⁄node_community_sizes⁄community_degree_centralityr{   ⁄c⁄
comm_nodesrü   ⁄comm_centralityrõ   rN   rO   rP   rQ   r0   s                        r   r"   ⁄)GraphFeatures._extract_community_featuresg  s=  Ä @	ÿüôõ	àI‡èzâz—!‹óë–N‘Oÿ –   $üzôz◊7—7”9–‹+◊:“:–;K”LàK òbüjôj”(ÿ02∞;±◊0C—0C¿K”0P◊0W—0W–XZ”0[ê	–,—-‡†ß
°
”*ÿ24∞]—2C◊2G—2G»”2T◊2[—2[–\^”2_ê	–.—/Ù &†k◊&8—&8”&:”;àOÿR]◊Rc—Rc‘Re‘#f“Re¡J¿D†D∏$—*?“$?—Re– —#f òbüjôj”(ÿ57∏±_◊5H—5H–I]”5^◊5e—5e–fg”5hê	–1—2‡†ß
°
”*ÿ79∏-—7H◊7L—7L–Ma”7b◊7i—7i–jk”7lê	–3—4 +-–'ÿ.◊4—4÷6ëêÿ2=◊2C—2C‘2E‘S“2E°w†t»…üd—2Eê
—Sÿ+◊4—4∞Z”@ê‹êxó~ë~”'”(®1’,‹&(◊&:“&:∏8”&DêO€ *òÿ<K◊<O—<O–PT–VW”<X–3”9Û !+Ò  7 òbüjôj”(ÿBD¿[¡/◊BU—BU–Vq”Br◊By—By–z{”B|ê	–>—?‡†ß
°
”*ÿDF¿}—DU◊DY—DY–Zu”Dv◊D}—D}–~Û  EAê	–@—A òbüjôj”(®]∏bøjπj”-Hÿ!#êÿ ükôkûmëFêAêsÿ †—-êFÿ"†=—1êH‡†”,∞∏[”1Hÿ&◊-—-¨c∞+∏f—2E»–U]—I^—2^”._÷`‡&◊-—-®a÷0Ò , /=–*—+‡–˘ÛQ $g˘Û T¯Ù< Û 	‹èLâL–@ƒ¿Q√¿–I‘JÿçI˚	˙sO   Ç3L- ∂CL- √>L!ƒBL- ∆(L'∆7L'∆=/L- «0D0L- Ã!L- Ã-
M$Ã7"MÕM$ÕM$c                 Û  ï  UR                  5       nU R                  c  [        R                  S5        U$ [	        U R                  R                  5       5      S::  a≤  [        [        R                  " U R                  5      5      nSUR                  ;   ax  SUR                  ;   ah  / nUR                  5        HN  u  pVUS   nUS   nXs;   a   XÉU   ;   a  UR                  X7   U   5        M4  UR                  [        S5      5        MP     XBS'   O|SUR                  ;   al  SUR                  ;   a\  / nUR                  5        HB  u  pVUS   nUS   n [        R                  " U R                  Xx5      n	UR                  U	5        MD     XBS'   SUR                  ;   aË  SUR                  ;   aÿ  / n
UR                  5        Hæ  u  pVUS   nUS   n [        U R                  R                  U5      5      [        U R                  R!                  U5      5      -  n[        U R                  R                  U5      5      [        U R                  R!                  U5      5      -  nU
R                  [	        Xº-  5      5        M¿     X¢S	'   SUR                  ;   Ga  SUR                  ;   a˛  / nUR                  5        H‰  u  pVUS   nUS   n [        U R                  R                  U5      5      [        U R                  R!                  U5      5      -  n[        U R                  R                  U5      5      [        U R                  R!                  U5      5      -  nXº-  nXº-  n[	        U5      S:î  a  [	        U5      [	        U5      -  nOSnUR                  U5        MÊ     X“S
'   SUR                  ;   Ga/  SUR                  ;   Ga  / nUR                  5        GH  u  pVUS   nUS   n [        U R                  R                  U5      5      [        U R                  R!                  U5      5      -  n[        U R                  R                  U5      5      [        U R                  R!                  U5      5      -  nXº-  nSnU HB  nU R                  R#                  U5      nUS:î  d  M&  US[$        R&                  " U5      -  -  nMD     UR                  U5        GM     UUS'   U$ !   UR                  [        S5      5         GM√  = f!   U
R                  S5         GMa  = f!   UR                  S5         GMÇ  = f!   UR                  S5         GM|  = f! [(         a-  n[        R+                  S[-        U5       35        Us SnA$ SnAff = f)zÇ
Extract path-based features

Args:
    df (DataFrame): Input dataframe
    
Returns:
    DataFrame: DataFrame with path features
Nz(Graph not built. Skipping path features.iÙ  r4   r5   ⁄inf⁄shortest_path_lengthr   ⁄common_neighbors_count⁄jaccard_coefficientr;   ⁄adamic_adar_indexz Error extracting path features: )r   r   r'   rt   r)   r{   ⁄dictr@   ⁄all_pairs_shortest_path_lengthr&   rB   rE   ⁄floatrÆ   rH   ⁄predecessors⁄
successors⁄degree⁄np⁄logr*   r+   r,   )r   r-   r.   ⁄shortest_paths⁄path_lengthsrN   rO   rP   rQ   ⁄path_length⁄common_neighbors⁄sender_neighbors⁄receiver_neighbors⁄jaccard_coeffs⁄union⁄intersection⁄jaccard⁄adamic_adar⁄common⁄aa_indexr§   r∑   r0   s                          r   r#   ⁄$GraphFeatures._extract_path_features≥  s  Ä p	ÿüôõ	àI‡èzâz—!‹óë–I‘Jÿ – Ù ê4ó:ë:◊#—#”%”&®#”-‰!%§b◊&G“&G»œ
…
”&S”!Tê †"ß*°*”,∞¿"«*¡*”1Lÿ#%êLÿ"$ß+°+¶-ôòÿ!$†[—!1òÿ#&†}—#5ò‡!”3∏–SY—DZ”8Zÿ(◊/—/∞—0F¿x—0P÷Q‡(◊/—/¥∞e≥÷=Ò #0 9E–4—5¯ †"ß*°*”,∞¿"«*¡*”1L‡#%êLÿ"$ß+°+¶-ôòÿ!$†[—!1òÿ#&†}—#5ò>‹*,◊*A“*A¿$«*¡*»f”*_òKÿ(◊/—/∞÷<Ò #0 9E–4—5 òbüjôj”(®]∏bøjπj”-Hÿ#%– ÿ ükôkûmëFêAÿ †—-êFÿ"†=—1êH3‹+.®tØz©z◊/F—/F¿v”/N”+O‘RU–VZ◊V`—V`◊Vk—Vk–lr”Vs”Rt—+t–(‹-0∞∑±◊1H—1H»”1R”-S‘VY–Z^◊Zd—Zd◊Zo—Zo–px”Zy”Vz—-z–*ÿ(◊/—/¥–4D—4Y”0Z÷[Ò , 7G–2—3 òbüjôj‘(®]∏bøjπj”-Hÿ!#êÿ ükôkûmëFêAÿ †—-êFÿ"†=—1êH1‹+.®tØz©z◊/F—/F¿v”/N”+O‘RU–VZ◊V`—V`◊Vk—Vk–lr”Vs”Rt—+t–(‹-0∞∑±◊1H—1H»”1R”-S‘VY–Z^◊Zd—Zd◊Zo—Zo–px”Zy”Vz—-z–*‡ 0— Eòÿ'7—'Lò‰òuõ:®õ>‹&)®,”&7º#∏eª*—&DôG‡&'òG‡&◊-—-®g÷6Ò! ,( 4B–/—0 òbüjôj‘(®]∏bøjπj‘-Hÿ êÿ ükôkümëFêAÿ †—-êFÿ"†=—1êH.‹+.®tØz©z◊/F—/F¿v”/N”+O‘RU–VZ◊V`—V`◊Vk—Vk–lr”Vs”Rt—+t–(‹-0∞∑±◊1H—1H»”1R”-S‘VY–Z^◊Zd—Zd◊Zo—Zo–px”Zy”Vz—-z–*ÿ!1—!Fò $%ò€$*òDÿ%)ßZ°Z◊%6—%6∞t”%<òFÿ%®ùzÿ (®A¥∑≤∞v≥—,>— >¢Ò %+
 $◊*—*®8◊4Ò! ,( 2=ê	–-—.‡–¯O>ÿ(◊/—/¥∞e≥◊=–=˚3ÿ(◊/—/∞◊2–2˚01ÿ&◊-—-®a◊0–0˚0.ÿ#◊*—*®1◊-–-˚Ù Û 	‹èLâL–;ºC¿ªF∏8–D‘EÿçI˚	˙sè   Ç3U ∂DU ≈2S$∆A	U «B.T…;A
U ÀCT!ŒAU œ'B<T;“'1T;”U ”$T‘ U ‘T‘U ‘!T8‘4U ‘;U’U ’
V’"V÷V÷Vc                 Ûj  ï  UR                  5       nU R                  c  [        R                  S5        U$ SUR                  ;   aò  / n/ nUS    HÉ  n [
        R                  " U R                  R                  5       USS9n[        UR                  5       5      n[
        R                  " U5      nUR                  U5        UR                  U5        MÖ     X2S'   XBS'   S	UR                  ;   aò  / n	/ n
US	    HÉ  n [
        R                  " U R                  R                  5       USS9n[        UR                  5       5      n[
        R                  " U5      nU	R                  U5        U
R                  U5        MÖ     XíS
'   X¢S'   U R                  GbP  U R                  R                  SS9 VVs1 s H  u  pÕUS   S:X  d  M  UiM     nnnU R                  R                  SS9 VVs1 s H  u  pÕUS   S:X  d  M  UiM     nnn[
        R                  R                  U R                  U5      nSUR                  ;   a6  / nUS    H&  n UR                  U5      nUR                  U5        M(     UUS'   [
        R                  R                  U R                  U5      nS	UR                  ;   a6  / nUS	    H&  n UR                  U5      nUR                  U5        M(     UUS'   U$ !   UR                  S5        UR                  S5         GMø  = f!   U	R                  S5        U
R                  S5         GMB  = fs  snnf s  snnf !   UR                  S5         GM  = f!   UR                  S5         M√  = f! [          a-  n[        R#                  S[%        U5       35        Us SnA$ SnAff = f)zä
Extract subgraph-based features

Args:
    df (DataFrame): Input dataframe
    
Returns:
    DataFrame: DataFrame with subgraph features
Nz,Graph not built. Skipping subgraph features.r4   r;   )⁄radiusr   ⁄sender_ego_network_size⁄sender_ego_network_densityr5   ⁄receiver_ego_network_size⁄receiver_ego_network_densityT)⁄datar?   ⁄sender_projection_degree⁄receiver_projection_degreez$Error extracting subgraph features: )r   r   r'   rt   r&   r@   ⁄	ego_graphrx   r)   r{   ⁄densityrE   r   r?   ⁄projected_graphr∑   r*   r+   r,   )r   r-   r.   ⁄sender_ego_sizes⁄sender_ego_densitiesrP   r—   ⁄ego_size⁄ego_density⁄receiver_ego_sizes⁄receiver_ego_densitiesrQ   ⁄n⁄drU   r\   ⁄sender_projection⁄sender_proj_degreesr∑   ⁄receiver_projection⁄receiver_proj_degreesr0   s                         r   r$   ⁄(GraphFeatures._extract_subgraph_features/  sé  Ä ]	ÿüôõ	àI‡èzâz—!‹óë–M‘Nÿ –  òbüjôj”(ÿ#%– ÿ')–$‡ †úoêF7‰$&ßL¢L∞∑±◊1I—1I”1K»V–\]—$^ò	Ù $'†yß°”'8”#9ò‹&(ßj¢j∞”&;ò‡(◊/—/∞‘9ÿ,◊3—3∞K÷@Ò . 8H–3—4ÿ:N–6—7 †ß
°
”*ÿ%'–"ÿ)+–&‡ "†=‘ 1êH9‰$&ßL¢L∞∑±◊1I—1I”1K»X–^_—$`ò	Ù $'†yß°”'8”#9ò‹&(ßj¢j∞”&;ò‡*◊1—1∞(‘;ÿ.◊5—5∞k÷BÒ !2 :L–5—6ÿ<R–8—9 ◊#—#“/‡)-◊)=—)=◊)C—)C»–)C—)N‘f“)N°†–RS–T_—R`–de—Reü1—)Nê—fÿ+/◊+?—+?◊+E—+E»4–+E—+P‘h“+P°4†1–TU–Va—Tb–fg—TgüQ—+Pê	—hÙ %'ßL°L◊$@—$@¿◊AU—AU–W^”$_–! †"ß*°*”,ÿ*,–'ÿ"$†[§/ò:ÿ%6◊%=—%=∏f”%EòFÿ/◊6—6∞v÷>Ò #2 =PêI–8—9Ù ')ßl°l◊&B—&B¿4◊CW—CW–Yb”&c–# !†BßJ°J”.ÿ,.–)ÿ$&†}‘$5ò<ÿ%8◊%?—%?¿”%IòFÿ1◊8—8∏÷@Ò %6 ?TêI–:—;‡–¯E7ÿ(◊/—/∞‘2ÿ,◊3—3∞A◊6–6˚*9ÿ*◊1—1∞!‘4ÿ.◊5—5∞a◊8–8¸Û g˘€h¯:ÿ/◊6—6∞q◊9–9˚<ÿ1◊8—8∏◊;˚Ù Û 	‹èLâL–?ƒ¿A√∏x–H‘IÿçI˚	˙s™   Ç3M; ∂M; ¡A?K&√(M; √;A?L≈:6M; ∆0L<«L<«	M; «(M«;M»AM; …	"M…+AM;  9"M"À
M; À&$LÃ
M; Ã$L9Ã5M; ÕMÕM; Õ"M8Õ5M; Õ;
N2Œ"N-Œ'N2Œ-N2c                 Ûp  ï  UR                  5       nU R                  b  SUR                  ;  a  [        R	                  S5        U$ [
        R                  R                  R                  US   5      (       d  [
        R                  " US   5      US'   UR                  S5      n/ SQnU GHÌ  nSUR                  ;   a„  / nUR                  5        Hí  u  pxUS   n	US   n
U
[
        R                  " U5      -
  nU
nUUS   U:¨  US   U:  -  US   U	:H  -     n[        U5      nSUR                  ;   a  US   R                  5       OSnUR                  SU 3US	U 3U05        Mî     [
        R                   " U5      nUR                   H  nUU   R"                  UU'   M     S
UR                  ;   d  GM
  / nUR                  5        Hí  u  pxUS
   nUS   n
U
[
        R                  " U5      -
  nU
nUUS   U:¨  US   U:  -  US
   U:H  -     n[        U5      nSUR                  ;   a  US   R                  5       OSnUR                  SU 3USU 3U05        Mî     [
        R                   " U5      nUR                   H  nUU   R"                  UU'   M     GM     SUR                  ;   a†  / nUR                  5        HÖ  u  pxUS   n	US   n
UUS   U
:  US   U	:H  -     n[        U5      S:î  a9  US   R%                  5       nU
U-
  R'                  5       nUR                  U5        Mk  UR                  [)        S5      5        Má     UUS'   S
UR                  ;   a†  / nUR                  5        HÖ  u  pxUS
   nUS   n
UUS   U
:  US
   U:H  -     n[        U5      S:î  a9  US   R%                  5       nU
U-
  R'                  5       nUR                  U5        Mk  UR                  [)        S5      5        Má     UUS'   U$ ! [*         a-  n[        R-                  S[/        U5       35        Us SnA$ SnAff = f)zê
Extract temporal graph features

Args:
    df (DataFrame): Input dataframe
    
Returns:
    DataFrame: DataFrame with temporal graph features
Nr7   zMGraph not built or timestamp not available. Skipping temporal graph features.)⁄1H⁄6H⁄24H⁄7Dr4   r6   r   ⁄sender_activity_count_⁄sender_activity_amount_r5   ⁄receiver_activity_count_⁄receiver_activity_amount_r≠   ⁄"sender_time_since_last_transaction⁄$receiver_time_since_last_transactionz*Error extracting temporal graph features: )r   r   r&   r'   rt   ⁄pd⁄api⁄types⁄is_datetime64_any_dtype⁄to_datetime⁄sort_valuesrB   ⁄	Timedeltar)   ⁄sumrE   ⁄	DataFramerû   ⁄max⁄total_secondsr¥   r*   r+   r,   )r   r-   r.   ⁄	df_sorted⁄time_windows⁄window⁄sender_activityrN   rO   rP   r7   ⁄window_start⁄
window_end⁄window_transactions⁄activity_count⁄activity_amount⁄activity_dfr/   ⁄receiver_activityrQ   ⁄time_since_last⁄prev_transactions⁄last_timestamp⁄	time_diffr0   s                            r   r%   ⁄.GraphFeatures._extract_temporal_graph_featuresò  s©  Ä D	ÿüôõ	àI‡èzâz—!†[∏ø
π
”%B‹óë–n‘oÿ – Ù ó6ë6ó<ë<◊7—7∏∏;π◊H—H‹"$ß.¢.∞∞K±”"Aêê;ë üô†{”3àIÚ 5àL‰&ê‡†"ß*°*”,ÿ&(êO‡"+◊"4—"4÷"6ôòÿ!$†[—!1òÿ$'®—$4ò	 (1¥2∑<≤<¿”3G—'Gòÿ%.ò
‡.7ÿ&†{—3∞|—Cÿ&†{—3∞j—@ÒB‡&†{—3∞v—=Ò?Ò/–+Ù *-–-@”)AòÿQY–]p◊]x—]x”Qx–*=∏h—*G◊*K—*K‘*M–~ò‡'◊.—.ÿ4∞V∞H–=∏~ÿ5∞f∞X–>¿0ˆ Ò% #7Ù0 #%ß,¢,®”"?êKÿ*◊2‘2òÿ)4∞S—)9◊)@—)@ò	†#õÒ  3 !†BßJ°J÷.ÿ(*–%‡"+◊"4—"4÷"6ôòÿ#&†}—#5òÿ$'®—$4ò	 (1¥2∑<≤<¿”3G—'Gòÿ%.ò
‡.7ÿ&†{—3∞|—Cÿ&†{—3∞j—@ÒB‡&†}—5∏—AÒCÒ/–+Ù *-–-@”)AòÿQY–]p◊]x—]x”Qx–*=∏h—*G◊*K—*K‘*M–~ò‡)◊0—0ÿ6∞v∞h–?¿ÿ7∏∞x–@¿/2ˆ Ò% #7Ù0 #%ß,¢,–/@”"AêKÿ*◊2‘2òÿ)4∞S—)9◊)@—)@ò	†#õÙ  3Ò} 'D òbüjôj”(ÿ"$ê‡'◊0—0÷2ëFêAÿ †—-êFÿ #†K— 0êI )2ÿ"†;—/∞)—;ÿ"†;—/∞6—9Ò;Ò)–%Ù
 –,”-∞”1ÿ):∏;—)G◊)K—)K”)Mòÿ%.∞—%?◊$N—$N”$Pò	ÿ'◊.—.®y÷9‡'◊.—.¨u∞U´|÷<Ò 3" CRê	–>—? †ß
°
”*ÿ"$ê‡'◊0—0÷2ëFêAÿ"†=—1êHÿ #†K— 0êI )2ÿ"†;—/∞)—;ÿ"†=—1∞X—=Ò?Ò)–%Ù
 –,”-∞”1ÿ):∏;—)G◊)K—)K”)Mòÿ%.∞—%?◊$N—$N”$Pò	ÿ'◊.—.®y÷9‡'◊.—.¨u∞U´|÷<Ò 3" ETê	–@—A‡–¯‰Û 	‹èLâL–Eƒc»!√f¿X–N‘OÿçI˚	˙s,   ÇAO> ¡E)O> ∆4I	O> œ>
P5–"P0–*P5–0P5c                 ÛÚ   ï U R                  U5      nUR                   Vs/ s H  o3UR                  ;  d  M  UPM     nn[        U5      S:î  a$  U R                  R	                  X$   5        SU l        U$ s  snf )zî
Fit the feature extractor and transform the data

Args:
    df (DataFrame): Input data
    
Returns:
    DataFrame: Transformed data with features
r   T)r1   r&   r)   r   ⁄fitr   ©r   r-   r.   r/   ⁄feature_colss        r   ⁄fit_transform⁄GraphFeatures.fit_transform(  sn   Ä  ◊)—)®"”-à	 (1◊'8“'8”R“'8†¿r«z¡z—<Qü—'8à–R‰à|”òq” ‡èKâKèOâOòI—3‘4‡àDåK‡–˘Ú Ss
   †A4∑A4c                 Û   ï U R                   (       d  [        S5      eU R                  U5      nUR                   Vs/ s H  o3UR                  ;  d  M  UPM     nn[	        U5      S:î  a  U R
                  R                  X$   5      X$'   U$ s  snf )zï
Transform new data using fitted feature extractor

Args:
    df (DataFrame): Input data
    
Returns:
    DataFrame: Transformed data with features
z7Feature extractor not fitted. Call fit_transform first.r   )r   ⁄
ValueErrorr1   r&   r)   r   ⁄	transformr	  s        r   r  ⁄GraphFeatures.transform@  sÉ   Ä  è{è{‹–V”W–W ◊)—)®"”-à	 (1◊'8“'8”R“'8†¿r«z¡z—<Qü—'8à–R‰à|”òq” ‡&*ßk°k◊&;—&;∏I—<S”&TàI—#‡–˘Ú Ss   ºB¡B)r   r   r   r   r   r   r   r   )N)⁄__name__⁄
__module__⁄__qualname__⁄__firstlineno__⁄__doc__r   r1   r   r    r!   r"   r#   r$   r%   r  r  ⁄__static_attributes__© r   r   r   r      sM   Ü ÒÙ
Ú !ÚFuÚn\Ú|CÚJJÚXzÚxgÚRNÚ`ı0r   r   )r  ⁄pandasrÏ   ⁄numpyr∏   ⁄networkxr@   ⁄collectionsr   r   ⁄	communityrú   ⁄sklearn.preprocessingr   ⁄warnings⁄logging⁄typingr   r   r   r	   ⁄filterwarnings⁄basicConfig⁄INFO⁄	getLoggerr  r'   r   r  r   r   ⁄<module>r%     sf   ÒÛ
 € € ﬂ ,€ %› .€ € ﬂ +” +‡ ◊ “ ò‘ !ÿ ◊ “ ò'ü,ô,“ 'ÿ	◊	“	ò8”	$Ä˜CÚ Cr   

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\features\__pycache__\graph_features.cpython-39.pyc ===
a
    [¨hæñ  „                   @   sú   d Z ddlZddlZddlZddlmZm	Z	 ddl
ZddlmZ ddlZddlZddlmZmZmZmZ e†d° ejejdç e†e°ZG dd	Ñ d	ÉZdS )
z`
Graph Features Module
Implements graph-based feature extraction techniques for fraud detection
È    N)⁄defaultdict⁄Counter)⁄MinMaxScaler)⁄Dict⁄List⁄Tuple⁄Union⁄ignore)⁄levelc                   @   sj   e Zd ZdZdddÑZddÑ ZddÑ Zd	d
Ñ ZddÑ ZddÑ Z	ddÑ Z
ddÑ ZddÑ ZddÑ ZddÑ ZdS )⁄GraphFeatureszõ
    Class for extracting graph-based features from transaction data
    Implements techniques like centrality measures, clustering coefficients, etc.
    Nc                 C   s:   |pi | _ d| _d| _d| _d| _g | _tÉ | _d| _dS )z
        Initialize GraphFeatures
        
        Args:
            config (dict, optional): Configuration parameters
        NF)	⁄config⁄graph⁄sender_graph⁄receiver_graph⁄bipartite_graph⁄feature_namesr   ⁄scaler⁄fitted)⁄selfr   © r   ˙mC:\Users\Tranquil Abyss\fraud-detection-platform\app\..\src\fraud_detection_engine\features\graph_features.py⁄__init__   s    
zGraphFeatures.__init__c              
      s¬   zÇà † ° }| †à ° | †|°}| †|°}| †|°}| †|°}| †|°}| †|°}á fddÑ|jD É| _	t
†dt| j	Éõ dù° |W S  tyº } z"t
†dt|Éõ ù° Ç W Y d}~n
d}~0 0 dS )z‡
        Extract all graph features from the dataframe
        
        Args:
            df (DataFrame): Input transaction data
            
        Returns:
            DataFrame: DataFrame with extracted features
        c                    s   g | ]}|à j vr|ëqS r   ©⁄columns©⁄.0⁄col©⁄dfr   r   ⁄
<listcomp>D   Û    z2GraphFeatures.extract_features.<locals>.<listcomp>z
Extracted z graph featuresz!Error extracting graph features: N)⁄copy⁄_build_graphs⁄_extract_centrality_features⁄_extract_clustering_features⁄_extract_community_features⁄_extract_path_features⁄_extract_subgraph_features⁄ _extract_temporal_graph_featuresr   r   ⁄logger⁄info⁄len⁄	Exception⁄error⁄str)r   r   ⁄	result_df⁄er   r   r   ⁄extract_features*   s    







zGraphFeatures.extract_featuresc              
   C   s&  êz‚t †° | _|†° D ê] \}}d|v rd|v r|d }|d }i }d|v rX|d |d< d|v rl|d |d< d|v rÄ|d |d< | j†||°r‡| j| | }d|v rº|†dd°|d  |d< |†dd°d	 |d< |d
 †|° q|†dd°|d< d	|d< |g|d
< | jj||fi |§é qt †° | _	t
tÉ}|†° D ]4\}}d|v êr4d|v êr4||d  †|d ° êq4|†° D ]é\}}	t|	É}
tt|
ÉÉD ]n}t|d	 t|
ÉÉD ]T}|
| }|
| }| j	†||°êr‰| j	| | d  d	7  < n| j	j||d	dç êq§êqéêqrt †° | _t
tÉ}|†° D ]4\}}d|v êrd|v êr||d  †|d ° êq|†° D ]é\}}t|É}tt|ÉÉD ]n}t|d	 t|ÉÉD ]T}|| }|| }| j†||°êrÃ| j| | d  d	7  < n| jj||d	dç êqåêqvêqZt †° | _|†° D ]F\}}d|v êr"| jj|d ddç d|v êr¸| jj|d d	dç êq¸|†° D ]ä\}}d|v êrLd|v êrL|d }|d }i }d|v êrí|d |d< d|v êr®|d |d< d|v êræ|d |d< | jj||fi |§é êqLt†d° W n< têy  } z"t†dt|Éõ ù° Ç W Y d}~n
d}~0 0 dS )zä
        Build various graphs from the transaction data
        
        Args:
            df (DataFrame): Input transaction data
        ⁄	sender_id⁄receiver_id⁄amount⁄	timestamp⁄transaction_idZtotal_amountr   Ztransaction_countÈ   Ztransactions⁄common_receivers)r8   ⁄common_senders)r9   )⁄	bipartitezGraphs built successfullyzError building graphs: N)⁄nxZDiGraphr   ⁄iterrowsZhas_edge⁄get⁄appendZadd_edge⁄Graphr   r   ⁄set⁄add⁄items⁄list⁄ranger+   r   r   Zadd_noder)   r*   r,   r-   r.   )r   r   ⁄_⁄row⁄sender⁄receiver⁄attrsZ	edge_dataZreceiver_to_senders⁄sendersZsenders_list⁄i⁄jZsender1Zsender2Zsender_to_receivers⁄	receiversZreceivers_listZ	receiver1Z	receiver2r0   r   r   r   r"   M   sí    









zGraphFeatures._build_graphsc              
   C   s$  êzﬁ|† ° }| jdu r&t†d° |W S t†| j°}t†| j°}t†| j†° °}d|j	v r†|d †
|°†d°|d< |d †
|°†d°|d< |d †
|°†d°|d< d|j	v rÚ|d †
|°†d°|d	< |d †
|°†d°|d
< |d †
|°†d°|d< t| j†° Édkêrt†| j°}n*t| j†° ÉddÖ }tj| jt|Édç}d|j	v êrb|d †
|°†d°|d< d|j	v êrÜ|d †
|°†d°|d< t| j†° Édkêr®t†| j°}ntj| jddç}d|j	v êr‹|d †
|°†d°|d< d|j	v êr |d †
|°†d°|d< t| j†° Édkêr<ztj| jddç}	W n   i }	Y n0 ni }	d|j	v êrd|d †
|	°†d°|d< d|j	v êrà|d †
|	°†d°|d< t†| j°}
d|j	v êr∏|d †
|
°†d°|d< d|j	v êr‹|d †
|
°†d°|d< |W S  têy } z$t†dt|Éõ ù° |W  Y d}~S d}~0 0 dS )zŒ
        Extract centrality-based features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with centrality features
        Nz.Graph not built. Skipping centrality features.r2   r   Zsender_in_degree_centralityZsender_out_degree_centralityZsender_degree_centralityr3   Zreceiver_in_degree_centralityZreceiver_out_degree_centralityZreceiver_degree_centralityiË  )⁄kZsender_betweenness_centralityZreceiver_betweenness_centrality⁄weight)⁄distanceZsender_closeness_centralityZreceiver_closeness_centrality)⁄max_iterZsender_eigenvector_centralityZreceiver_eigenvector_centralityZsender_pagerankZreceiver_pagerankz&Error extracting centrality features: )r!   r   r)   ⁄warningr;   ⁄in_degree_centrality⁄out_degree_centrality⁄degree_centrality⁄to_undirectedr   ⁄map⁄fillnar+   ⁄nodes⁄betweenness_centralityrC   ⁄closeness_centrality⁄eigenvector_centrality⁄pagerankr,   r-   r.   )r   r   r/   rS   rT   rU   rZ   Zsample_nodesr[   r\   r]   r0   r   r   r   r#   ƒ   sd    




z*GraphFeatures._extract_centrality_featuresc           	   
   C   s  êz∆|† ° }| jdu r&t†d° |W S t†| j†° °}d|jv rX|d †|°†	d°|d< d|jv rz|d †|°†	d°|d< | j
durƒt| j
†° Édkrƒt†| j
°}d|jv rƒ|d †|°†	d°|d< | jduêrt| j†° Édkêrt†| j°}d|jv êr|d †|°†	d°|d	< t†| j†° °}d|jv êrH|d †|°†	d°|d
< d|jv êrl|d †|°†	d°|d< t†| j†° °}d|jv êr†|d †|°†	d°|d< d|jv êrƒ|d †|°†	d°|d< |W S  têy } z$t†dt|Éõ ù° |W  Y d}~S d}~0 0 dS )zŒ
        Extract clustering-based features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with clustering features
        Nz.Graph not built. Skipping clustering features.r2   r   Zsender_clustering_coefficientr3   Zreceiver_clustering_coefficientZ#sender_graph_clustering_coefficientZ%receiver_graph_clustering_coefficientZsender_avg_neighbor_degreeZreceiver_avg_neighbor_degreeZsender_square_clusteringZreceiver_square_clusteringz&Error extracting clustering features: )r!   r   r)   rR   r;   Z
clusteringrV   r   rW   rX   r   r+   rY   r   Zaverage_neighbor_degree⁄square_clusteringr,   r-   r.   )	r   r   r/   Zclustering_coeffZsender_clustering_coeffZreceiver_clustering_coeffZavg_neighbor_degreer^   r0   r   r   r   r$   "  s@    





 z*GraphFeatures._extract_clustering_featuresc              
      sb  êz|† ° }| jdu r&t†d° |W S | j†° }t†|°}d|jv r\|d †|°†	d°|d< d|jv r~|d †|°†	d°|d< t
|†° Éâáfdd	Ñ|†° D É}d|jv r¬|d †|°†	d
°|d< d|jv r‰|d †|°†	d
°|d< i }à†° D ]^\â }á fddÑ|†° D É}|†|°}	t|	†° Éd
krt†|	°}
|D ]}|
†|d
°||< êq6qd|jv êrt|d †|°†	d
°|d< d|jv êrò|d †|°†	d
°|d< d|jv êrd|jv êrg }|†° D ]T\}}|d }|d }||v êr||v êr|†t|| || kÉ° n
|†d
° êqº||d< |W S  têy\ } z$t†dt|Éõ ù° |W  Y d}~S d}~0 0 dS )zÃ
        Extract community-based features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with community features
        Nz-Graph not built. Skipping community features.r2   ÈˇˇˇˇZsender_communityr3   Zreceiver_communityc                    s   i | ]\}}|à | ìqS r   r   )r   ⁄node⁄comm)⁄community_sizesr   r   ⁄
<dictcomp>Ö  r    z=GraphFeatures._extract_community_features.<locals>.<dictcomp>r   Zsender_community_sizeZreceiver_community_sizec                    s   g | ]\}}|à kr|ëqS r   r   )r   r`   ⁄c)ra   r   r   r   ë  r    z=GraphFeatures._extract_community_features.<locals>.<listcomp>Z"sender_community_degree_centralityZ$receiver_community_degree_centrality⁄same_communityz%Error extracting community features: )r!   r   r)   rR   rV   ⁄community_louvainZbest_partitionr   rW   rX   r   ⁄valuesrB   ⁄subgraphr+   rY   r;   rU   r=   r<   r>   ⁄intr,   r-   r.   )r   r   r/   Zundirected_graphZcommunitiesZnode_community_sizesZcommunity_degree_centralityrY   Z
comm_nodesrh   Zcomm_centralityr`   re   rE   rF   rG   rH   r0   r   )ra   rb   r   r%   g  sT    










z)GraphFeatures._extract_community_featuresc              
   C   s  êz |† ° }| jdu r&t†d° |W S t| j†° Édkr¬tt†| j°É}d|j	v r¿d|j	v r¿g }|†
° D ]N\}}|d }|d }||v r®||| v r®|†|| | ° qh|†tdÉ° qh||d< nÄd|j	v êrBd|j	v êrBg }|†
° D ]R\}}|d }|d }zt†| j||°}	|†|	° W qÊ   |†tdÉ° Y qÊ0 qÊ||d< d|j	v êr¯d|j	v êr¯g }
|†
° D ]à\}}|d }|d }zVt| j†|°Ét| j†|°ÉB }t| j†|°Ét| j†|°ÉB }|
†t||@ É° W n   |
†d° Y n0 êqf|
|d	< d|j	v êr⁄d|j	v êr⁄g }|†
° D ]¥\}}|d }|d }zÇt| j†|°Ét| j†|°ÉB }t| j†|°Ét| j†|°ÉB }||B }||@ }t|Édkêr¶t|Ét|É }nd}|†|° W n   |†d° Y n0 êq||d
< d|j	v êr»d|j	v êr»g }|†
° D ]¿\}}|d }|d }zét| j†|°Ét| j†|°ÉB }t| j†|°Ét| j†|°ÉB }||@ }d}|D ].}| j†|°}|dkêrh|dt†|° 7 }êqh|†|° W n   |†d° Y n0 êq˛||d< |W S  têy
 } z$t†dt|Éõ ù° |W  Y d}~S d}~0 0 dS )z¬
        Extract path-based features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with path features
        Nz(Graph not built. Skipping path features.iÙ  r2   r3   ⁄inf⁄shortest_path_lengthr   Zcommon_neighbors_countZjaccard_coefficientr7   Zadamic_adar_indexz Error extracting path features: )r!   r   r)   rR   r+   rY   ⁄dictr;   Zall_pairs_shortest_path_lengthr   r<   r>   ⁄floatrk   r@   ⁄predecessorsZ
successors⁄degree⁄np⁄logr,   r-   r.   )r   r   r/   Zshortest_pathsZpath_lengthsrE   rF   rG   rH   Zpath_lengthZcommon_neighborsZsender_neighborsZreceiver_neighborsZjaccard_coeffs⁄union⁄intersection⁄jaccardZadamic_adar⁄commonZaa_indexr`   ro   r0   r   r   r   r&   ≥  sú    



      
z$GraphFeatures._extract_path_featuresc              
   C   s®  êzb|† ° }| jdu r&t†d° |W S d|jv rºg }g }|d D ]j}zDtj| j†° |ddç}t|†	° É}t†
|°}|†|° |†|° W q@   |†d° |†d° Y q@0 q@||d< ||d< d	|jv êrTg }	g }
|d	 D ]j}zDtj| j†° |ddç}t|†	° É}t†
|°}|	†|° |
†|° W qÿ   |	†d° |
†d° Y qÿ0 qÿ|	|d
< |
|d< | jduêr`ddÑ | jj	ddçD É}ddÑ | jj	ddçD É}tj†| j|°}d|jv êr¯g }|d D ]6}z|†|°}|†|° W n   |†d° Y n0 êq∏||d< tj†| j|°}d	|jv êr`g }|d	 D ]6}z|†|°}|†|° W n   |†d° Y n0 êq ||d< |W S  têy¢ } z$t†dt|Éõ ù° |W  Y d}~S d}~0 0 dS )z 
        Extract subgraph-based features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with subgraph features
        Nz,Graph not built. Skipping subgraph features.r2   r7   )⁄radiusr   Zsender_ego_network_sizeZsender_ego_network_densityr3   Zreceiver_ego_network_sizeZreceiver_ego_network_densityc                 S   s    h | ]\}}|d  dkr|íqS )r:   r   r   ©r   ⁄n⁄dr   r   r   ⁄	<setcomp>q  r    z;GraphFeatures._extract_subgraph_features.<locals>.<setcomp>T)⁄datac                 S   s    h | ]\}}|d  dkr|íqS )r:   r7   r   rw   r   r   r   rz   r  r    Zsender_projection_degreeZreceiver_projection_degreez$Error extracting subgraph features: )r!   r   r)   rR   r   r;   ⁄	ego_graphrV   r+   rY   ⁄densityr>   r   r:   Zprojected_graphro   r,   r-   r.   )r   r   r/   Zsender_ego_sizesZsender_ego_densitiesrG   r|   Zego_sizeZego_densityZreceiver_ego_sizesZreceiver_ego_densitiesrH   rJ   rM   Zsender_projectionZsender_proj_degreesro   Zreceiver_projectionZreceiver_proj_degreesr0   r   r   r   r'   /  s|    











z(GraphFeatures._extract_subgraph_featuresc              
   C   sê  êzJ|† ° }| jdu s d|jvr0t†d° |W S tjj†|d °sTt†	|d °|d< |†
d°}g d¢}|D ê]™}d|jv êrBg }|†° D ]í\}}|d }	|d }
|
t†|° }|
}||d |k|d |k @ |d |	k@  }t|É}d|jv r¯|d †° nd}|†d|õ ù|d	|õ ù|i° qàt†|°}|jD ]}|| j||< êq,d
|jv rjg }|†° D ]ñ\}}|d
 }|d }
|
t†|° }|
}||d |k|d |k @ |d
 |k@  }t|É}d|jv êr |d †° nd}|†d|õ ù|d|õ ù|i° êqXt†|°}|jD ]}|| j||< êq qjd|jv êr∞g }|†° D ]v\}}|d }	|d }
||d |
k |d |	k@  }t|Édkêrñ|d †° }|
| †° }|†|° n|†tdÉ° êq0||d< d
|jv êrHg }|†° D ]v\}}|d
 }|d }
||d |
k |d
 |k@  }t|Édkêr.|d †° }|
| †° }|†|° n|†tdÉ° êq»||d< |W S  têyä } z$t†dt|Éõ ù° |W  Y d}~S d}~0 0 dS )z–
        Extract temporal graph features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with temporal graph features
        Nr5   zMGraph not built or timestamp not available. Skipping temporal graph features.)⁄1HZ6HZ24HZ7Dr2   r4   r   Zsender_activity_count_Zsender_activity_amount_r3   Zreceiver_activity_count_Zreceiver_activity_amount_rj   Z"sender_time_since_last_transactionZ$receiver_time_since_last_transactionz*Error extracting temporal graph features: )r!   r   r   r)   rR   ⁄pd⁄api⁄types⁄is_datetime64_any_dtype⁄to_datetime⁄sort_valuesr<   ⁄	Timedeltar+   ⁄sumr>   ⁄	DataFramerg   ⁄max⁄total_secondsrm   r,   r-   r.   )r   r   r/   ⁄	df_sortedZtime_windows⁄windowZsender_activityrE   rF   rG   r5   Zwindow_startZ
window_endZwindow_transactionsZactivity_countZactivity_amountZactivity_dfr   Zreceiver_activityrH   Ztime_since_lastZprev_transactionsZlast_timestampZ	time_diffr0   r   r   r   r(   ò  s∏    





ˇ
˛ˇ

˛




ˇ
˛ˇ

˛




ˇˇ

ˇˇz.GraphFeatures._extract_temporal_graph_featuresc                    sD   | † à °}á fddÑ|jD É}t|Édkr@| j†|| ° d| _|S )z‘
        Fit the feature extractor and transform the data
        
        Args:
            df (DataFrame): Input data
            
        Returns:
            DataFrame: Transformed data with features
        c                    s   g | ]}|à j vr|ëqS r   r   r   r   r   r   r   6  r    z/GraphFeatures.fit_transform.<locals>.<listcomp>r   T)r1   r   r+   r   ⁄fitr   ©r   r   r/   ⁄feature_colsr   r   r   ⁄fit_transform(  s    
zGraphFeatures.fit_transformc                    sP   | j stdÉÇ| †à °}á fddÑ|jD É}t|ÉdkrL| j†|| °||< |S )z’
        Transform new data using fitted feature extractor
        
        Args:
            df (DataFrame): Input data
            
        Returns:
            DataFrame: Transformed data with features
        z7Feature extractor not fitted. Call fit_transform first.c                    s   g | ]}|à j vr|ëqS r   r   r   r   r   r   r   Q  r    z+GraphFeatures.transform.<locals>.<listcomp>r   )r   ⁄
ValueErrorr1   r   r+   r   ⁄	transformrç   r   r   r   rë   @  s    

zGraphFeatures.transform)N)⁄__name__⁄
__module__⁄__qualname__⁄__doc__r   r1   r"   r#   r$   r%   r&   r'   r(   rè   rë   r   r   r   r   r      s   
#w^EL|i r   )rï   ⁄pandasr   ⁄numpyrp   Znetworkxr;   ⁄collectionsr   r   Z	communityrf   ⁄sklearn.preprocessingr   ⁄warnings⁄logging⁄typingr   r   r   r   ⁄filterwarnings⁄basicConfig⁄INFO⁄	getLoggerrí   r)   r   r   r   r   r   ⁄<module>   s   



=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\features\__pycache__\nlp_features.cpython-39.pyc ===
a
    èöhq  „                   @   s‰  d Z ddlZddlZddlZddlZddlmZ ddl	Z	ddl
mZ ddlmZmZ ddlmZmZ ddlmZ ddlmZmZ dd	lmZ dd
lmZ ddlZddlmZmZ ddlm Z  ddl!Z!ddl"Z"ddl#m$Z$m%Z%m&Z&m'Z' ddl(m)Z) e!†*d° e"j+e"j,dç e"†-e.°Z/ze	j0†1d° W n e2êy@   e	†3d° Y n0 ze	j0†1d° W n e2êyp   e	†3d° Y n0 ze	j0†1d° W n e2êy†   e	†3d° Y n0 ze	j0†1d° W n e2êy–   e	†3d° Y n0 G ddÑ dÉZ4dS )zY
NLP Features Module
Implements natural language processing features for fraud detection
È    N)⁄Counter)⁄	stopwords)⁄word_tokenize⁄sent_tokenize)⁄WordNetLemmatizer⁄PorterStemmer)⁄SentimentIntensityAnalyzer)⁄TfidfVectorizer⁄CountVectorizer)⁄LatentDirichletAllocation)⁄TextBlob)⁄Word2Vec⁄Doc2Vec©⁄TaggedDocument)⁄Dict⁄List⁄Tuple⁄Union)⁄is_api_available⁄ignore)⁄levelztokenizers/punktZpunktzcorpora/stopwordsr   zcorpora/wordnetZwordnetzsentiment/vader_lexiconZvader_lexiconc                   @   sj   e Zd ZdZdddÑZddÑ ZddÑ Zd	d
Ñ ZddÑ ZddÑ Z	ddÑ Z
ddÑ ZddÑ ZddÑ ZddÑ ZdS )⁄NLPFeatureszâ
    Class for extracting NLP features from transaction data
    Implements techniques like sentiment analysis, topic modeling, etc.
    Nc                 C   st   |pi | _ tt†d°É| _tÉ | _tÉ | _t	É | _
d| _d| _d| _d| _d| _g | _d| _g d¢| _g d¢| _dS )z}
        Initialize NLPFeatures
        
        Args:
            config (dict, optional): Configuration parameters
        ⁄englishNF)2⁄urgent⁄immediately⁄asap⁄hurry⁄quick⁄fast⁄secret⁄confidential⁄private⁄hidden⁄discreetZ
suspiciousZunusualZstrange⁄oddZweird⁄illegal⁄fraudZscamZfakeZcounterfeit⁄money⁄cash⁄payment⁄transfer⁄wireZoverseasZforeign⁄internationalZabroadZinheritanceZlotteryZprize⁄winnerZclaim⁄verify⁄confirm⁄update⁄accountZinformation⁄click⁄linkZwebsite⁄login⁄passwordZbank⁄check⁄routingr2   ⁄number)˙\$\d+,\d+\.\d{2}˙&\d{4}[\s-]?\d{4}[\s-]?\d{4}[\s-]?\d{4}˙\b\d{3}-\d{2}-\d{4}\b˙)\b[A-Z0-9._%+-]+@[A-Z0-9.-]+\.[A-Z]{2,}\b˙Phttp[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+z\b\d{10,}\bz	[A-Z]{2,}z\d+\.\d+\.\d+\.\d+)⁄config⁄setr   ⁄words⁄
stop_wordsr   ⁄
lemmatizerr   Zstemmerr   ⁄sia⁄tfidf_vectorizer⁄count_vectorizer⁄	lda_model⁄word2vec_model⁄doc2vec_model⁄feature_names⁄fitted⁄fraud_keywords⁄suspicious_patterns)⁄selfr?   © rO   ˙kC:\Users\Tranquil Abyss\fraud-detection-platform\app\..\src\fraud_detection_engine\features\nlp_features.py⁄__init__;   s    

zNLPFeatures.__init__c              
      s∏   zxà † ° }| †|°}| †|°}| †|°}| †|°}| †|°}| †|°}á fddÑ|jD É| _t	†
dt| jÉõ dù° |W S  ty≤ } z"t	†dt|Éõ ù° Ç W Y d}~n
d}~0 0 dS )zﬁ
        Extract all NLP features from the dataframe
        
        Args:
            df (DataFrame): Input transaction data
            
        Returns:
            DataFrame: DataFrame with extracted features
        c                    s   g | ]}|à j vr|ëqS rO   ©⁄columns©⁄.0⁄col©⁄dfrO   rP   ⁄
<listcomp>Ä   Û    z0NLPFeatures.extract_features.<locals>.<listcomp>z
Extracted z NLP featureszError extracting NLP features: N)⁄copy⁄_extract_basic_text_features⁄_extract_sentiment_features⁄_extract_keyword_features⁄_extract_pattern_features⁄_extract_topic_features⁄_extract_embedding_featuresrS   rJ   ⁄logger⁄info⁄len⁄	Exception⁄error⁄str)rN   rX   ⁄	result_df⁄erO   rW   rP   ⁄extract_featuresi   s    






zNLPFeatures.extract_featuresc              
   C   s˛  êz∏|† ° }ddg}|D ê]ö}||jv r|| †d°†t°†t°||õ dù< || †d°†t°†ddÑ °||õ dù< || †d°†t°†ddÑ °||õ d	ù< || †d°†t°†d
dÑ °||õ dù< || †d°†t°†ddÑ °||õ dù< || †d°†t°†ddÑ °||õ dù< || †d°†t°†ddÑ °||õ dù< || †d°†t°†ddÑ °||õ dù< || †d°†t°†ddÑ °||õ dù< || †d°†t°†ddÑ °||õ dù< q|W S  têy¯ } z$t†	dt|Éõ ù° |W  Y d}~S d}~0 0 dS )z»
        Extract basic text features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with basic text features
        ⁄description⁄notes⁄ Z_char_countc                 S   s   | rt t| ÉÉS dS ©Nr   ©rd   r   ©⁄xrO   rO   rP   ⁄<lambda>†   rZ   z:NLPFeatures._extract_basic_text_features.<locals>.<lambda>Z_word_countc                 S   s   | rt t| ÉÉS dS rn   )rd   r   rp   rO   rO   rP   rr   •   rZ   Z_sentence_countc                 S   s$   t | Ér t†ddÑ t | ÉD É°S dS )Nc                 S   s   g | ]}t |ÉëqS rO   ©rd   ©rU   ⁄wordrO   rO   rP   rY   ™   rZ   ˙NNLPFeatures._extract_basic_text_features.<locals>.<lambda>.<locals>.<listcomp>r   )r   ⁄np⁄meanrp   rO   rO   rP   rr   ™   rZ   Z_avg_word_lengthc                 S   s$   t | Ér t†ddÑ t | ÉD É°S dS )Nc                 S   s   g | ]}t t|ÉÉëqS rO   ro   )rU   ⁄sentrO   rO   rP   rY   Ø   rZ   rv   r   )r   rw   rx   rp   rO   rO   rP   rr   Ø   rZ   Z_avg_sentence_lengthc                 S   s   t ddÑ | D ÉÉS )Nc                 s   s   | ]}|t jv rd V  qdS ©È   N©⁄string⁄punctuation©rU   ⁄charrO   rO   rP   ⁄	<genexpr>¥   rZ   ˙MNLPFeatures._extract_basic_text_features.<locals>.<lambda>.<locals>.<genexpr>©⁄sumrp   rO   rO   rP   rr   ¥   rZ   Z_punctuation_countc                 S   s   t ddÑ t| ÉD ÉÉS )Nc                 s   s&   | ]}|† ° rt|Éd krd V  qdS rz   )⁄isupperrd   rt   rO   rO   rP   rÅ   π   rZ   rÇ   )rÑ   r   rp   rO   rO   rP   rr   π   rZ   Z_uppercase_countc                 S   s   t ddÑ | D ÉÉS )Nc                 s   s   | ]}|† ° rd V  qdS rz   )⁄isdigitr   rO   rO   rP   rÅ   æ   rZ   rÇ   rÉ   rp   rO   rO   rP   rr   æ   rZ   Z_digit_countc                 S   s   | rt tt| †° ÉÉÉS dS rn   )rd   r@   r   ⁄lowerrp   rO   rO   rP   rr   √   rZ   Z_unique_word_countc                 S   s,   t | Ér(ttt | †° ÉÉÉtt | ÉÉ S dS rn   )r   rd   r@   rá   rp   rO   rO   rP   rr   »   rZ   Z_lexical_diversityz&Error extracting basic text features: N)
r[   rS   ⁄fillna⁄astyperg   ⁄applyrd   re   rb   rf   )rN   rX   rh   ⁄text_columnsrV   ri   rO   rO   rP   r\   â   sJ    


$ˇˇˇˇˇˇˇˇˇz(NLPFeatures._extract_basic_text_featuresc              
      s∫  êzt|† ° }ddg}|D ê]V}||jv r|| †d°†t°†á fddÑ°}|†ddÑ °||õ dù< |†ddÑ °||õ d	ù< |†d
dÑ °||õ dù< |†ddÑ °||õ dù< || †d°†t°†ddÑ °}|†ddÑ °||õ dù< |†ddÑ °||õ dù< ||õ dù dk †t°||õ dù< ||õ dù dk†t°||õ dù< ||õ dù dk||õ dù dk@ †t°||õ dù< q|W S  têy¥ } z$t†	dt|Éõ ù° |W  Y d}~S d}~0 0 dS )zœ
        Extract sentiment analysis features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with sentiment features
        rk   rl   rm   c                    s   | rà j †| °S dddddúS )Nr   )⁄neg⁄neu⁄pos⁄compound)rD   Zpolarity_scoresrp   ©rN   rO   rP   rr   Â   rZ   z9NLPFeatures._extract_sentiment_features.<locals>.<lambda>c                 S   s   | d S )Nrå   rO   rp   rO   rO   rP   rr   È   rZ   Z_sentiment_negc                 S   s   | d S )Nrç   rO   rp   rO   rO   rP   rr   Í   rZ   Z_sentiment_neuc                 S   s   | d S )Nré   rO   rp   rO   rO   rP   rr   Î   rZ   Z_sentiment_posc                 S   s   | d S )Nrè   rO   rp   rO   rO   rP   rr   Ï   rZ   Z_sentiment_compoundc                 S   s   | rt | ÉjS t dÉjS )Nrm   )r   ⁄	sentimentrp   rO   rO   rP   rr      rZ   c                 S   s   | j S ©N)Zpolarityrp   rO   rO   rP   rr   Û   rZ   Z_textblob_polarityc                 S   s   | j S rí   )Zsubjectivityrp   rO   rO   rP   rr   Ù   rZ   Z_textblob_subjectivitygöôôôôô©øZ_is_negativegöôôôôô©?Z_is_positiveZ_is_neutralz%Error extracting sentiment features: N)
r[   rS   rà   râ   rg   rä   ⁄intre   rb   rf   )rN   rX   rh   rã   rV   Zsentiment_scoresZtextblob_sentimentri   rO   rê   rP   r]   —   s:    



ˇˇ""ˇˇ˝z'NLPFeatures._extract_sentiment_featuresc              
      s  êzæà † ° }ddg}|D ê]†}|à jv rà | †d°†t°†àj°}|†áfddÑ°}|||õ dù< |dk†t°||õ dù< g d	¢âg d
¢âg d¢â|†áfddÑ°||õ dù< |†áfddÑ°||õ dù< |†áfddÑ°||õ dù< |à | †d°†t°†ddÑ ° ||õ dù< àjêsRt	àj
dddçà_t†á fddÑ|D É°}àj†|° àj†à | †d°†t°°}àj†° }	t|	ddÖ ÉD ]0\}
}|ddÖ|
f †° †° ||õ d|õ ù< êqàq|W S  têy˛ } z$t†dt|Éõ ù° à W  Y d}~S d}~0 0 dS )z»
        Extract keyword-based features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with keyword features
        rk   rl   rm   c                    s   t á fddÑ| D ÉÉS )Nc                 3   s   | ]}|à j v rd V  qdS rz   )rL   rt   rê   rO   rP   rÅ     rZ   ˙JNLPFeatures._extract_keyword_features.<locals>.<lambda>.<locals>.<genexpr>rÉ   rp   rê   rO   rP   rr     rZ   z7NLPFeatures._extract_keyword_features.<locals>.<lambda>Z_fraud_keyword_countr   Z_has_fraud_keywords)r   r   r   r   r   r   )r    r!   r"   r#   r$   )r(   r)   r*   r+   r,   c                    s   t á fddÑ| D ÉÉS )Nc                 3   s   | ]}|à v rd V  qdS rz   rO   rt   ©⁄urgency_keywordsrO   rP   rÅ   (  rZ   rî   rÉ   rp   rï   rO   rP   rr   (  rZ   Z_urgency_keyword_countc                    s   t á fddÑ| D ÉÉS )Nc                 3   s   | ]}|à v rd V  qdS rz   rO   rt   ©⁄secrecy_keywordsrO   rP   rÅ   ,  rZ   rî   rÉ   rp   ró   rO   rP   rr   ,  rZ   Z_secrecy_keyword_countc                    s   t á fddÑ| D ÉÉS )Nc                 3   s   | ]}|à v rd V  qdS rz   rO   rt   ©⁄money_keywordsrO   rP   rÅ   0  rZ   rî   rÉ   rp   rô   rO   rP   rr   0  rZ   Z_money_keyword_countc                 S   s   | rt t| ÉÉS dS )Nr{   ro   rp   rO   rO   rP   rr   5  rZ   Z_fraud_keyword_density©r{   È   Èd   )Z
vocabulary⁄ngram_range⁄max_featuresc                    s*   g | ]"}|à j v rà | †d °†t°ëqS )rm   )rS   rà   râ   rg   rT   rW   rO   rP   rY   B  rZ   z9NLPFeatures._extract_keyword_features.<locals>.<listcomp>NÈ
   Z_tfidf_z#Error extracting keyword features: )r[   rS   rà   râ   rg   rä   ⁄_preprocess_textrì   rK   r	   rL   rE   ⁄pd⁄concat⁄fit⁄	transform⁄get_feature_names_out⁄	enumerate⁄toarray⁄flattenre   rb   rf   )rN   rX   rh   rã   rV   ⁄processed_textZfraud_keyword_counts⁄all_textZtfidf_featuresrJ   ⁄i⁄featureri   rO   )rX   rö   rò   rN   rñ   rP   r^     sT    



ˇ
ˇ
ˇ
ˇˇ˝
,z%NLPFeatures._extract_keyword_featuresc              
      s2  êzÏ|† ° }ddg}|D ê]Œ}||jv rg }|| †d°†t°D ]:}d}| jD ] }t†||tj°}	|t	|	É7 }qN|†
|° q@|||õ dù< t†|°dk†t°||õ dù< dâdâ d	âd
âdâ|| †d°†t°†áfddÑ°||õ dù< || †d°†t°†á fddÑ°||õ dù< || †d°†t°†áfddÑ°||õ dù< || †d°†t°†áfddÑ°||õ dù< || †d°†t°†áfddÑ°||õ dù< || †d°†t°†ddÑ °||õ dù< || †d°†t°†ddÑ °||õ dù< q|W S  têy, }
 z$t†dt|
Éõ ù° |W  Y d}
~
S d}
~
0 0 dS )z»
        Extract pattern-based features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with pattern features
        rk   rl   rm   r   Z_suspicious_pattern_countZ_has_suspicious_patternsr:   r;   r<   r=   r>   c                    s   t t†à | °ÉS rí   ©rd   ⁄re⁄findallrp   )⁄money_patternrO   rP   rr   {  rZ   z7NLPFeatures._extract_pattern_features.<locals>.<lambda>Z_money_pattern_countc                    s   t t†à | °ÉS rí   rÆ   rp   )⁄credit_card_patternrO   rP   rr     rZ   Z_credit_card_pattern_countc                    s   t t†à | °ÉS rí   rÆ   rp   )⁄ssn_patternrO   rP   rr   É  rZ   Z_ssn_pattern_countc                    s   t t†à | °ÉS rí   rÆ   rp   )⁄email_patternrO   rP   rr   á  rZ   Z_email_pattern_countc                    s   t t†à | °ÉS rí   rÆ   rp   )⁄url_patternrO   rP   rr   ã  rZ   Z_url_pattern_countc                 S   s&   t ddÑ | D ÉÉt| É dkr"dS dS )Nc                 s   s   | ]}|t jv rd V  qdS rz   r|   r   rO   rO   rP   rÅ   ë  rZ   ˙JNLPFeatures._extract_pattern_features.<locals>.<lambda>.<locals>.<genexpr>g333333”?r{   r   ©rÑ   rd   rp   rO   rO   rP   rr   ë  rZ   Z_excessive_punctuationc                 S   s&   t ddÑ | D ÉÉt| É dkr"dS dS )Nc                 s   s   | ]}|† ° rd V  qdS rz   )rÖ   r   rO   rO   rP   rÅ   ò  rZ   r∂   g      ‡?r{   r   r∑   rp   rO   rO   rP   rr   ò  rZ   Z_excessive_capitalizationz#Error extracting pattern features: N)r[   rS   rà   râ   rg   rM   rØ   r∞   ⁄
IGNORECASErd   ⁄appendrw   ⁄arrayrì   rä   re   rb   rf   )rN   rX   rh   rã   rV   Zpattern_counts⁄text⁄count⁄pattern⁄matchesri   rO   )r≤   r¥   r±   r≥   rµ   rP   r_   S  s\    




ˇ
ˇ
ˇ
ˇ
ˇˇˇˇˇz%NLPFeatures._extract_pattern_featuresc              
      sÆ  êzh|† ° }ddg}g }|D ],}||jv r|†|| †d°†t°†° ° q|sT|W S á fddÑ|D É}ddÑ |D É}|s~|W S à jsætddd	d
çà _	à j	†
|°}tdddddçà _à j†|° |D ]¢}||jv r¬|| †d°†t°†à j°}|†ddÑ °}à j	†|°}à j†|°}	t|	jd ÉD ]&}
|	ddÖ|
f ||õ d|
õ dù< êq tj|	ddç}|||õ dù< q¬|W S  têy® } z$t†dt|Éõ ù° |W  Y d}~S d}~0 0 dS )z«
        Extract topic modeling features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with topic features
        rk   rl   rm   c                    s   g | ]}à † |°ëqS rO   ©r°   ©rU   ⁄docrê   rO   rP   rY   º  rZ   z7NLPFeatures._extract_topic_features.<locals>.<listcomp>c                 S   s   g | ]}|rd † |°ëqS )˙ ©⁄joinr¿   rO   rO   rP   rY   Ω  rZ   iË  r   rõ   )rü   rB   rû   È   È*   r†   ⁄online)⁄n_components⁄random_state⁄max_iter⁄learning_methodc                 S   s   | rd† | °S dS )Nr¬   rm   r√   rp   rO   rO   rP   rr   ⁄  rZ   z5NLPFeatures._extract_topic_features.<locals>.<lambda>r{   NZ_topic_Z_prob©⁄axisZ_dominant_topicz!Error extracting topic features: )r[   rS   ⁄extendrà   râ   rg   ⁄tolistrK   r
   rF   ⁄fit_transformr   rG   r§   rä   r°   r•   ⁄range⁄shaperw   ⁄argmaxre   rb   rf   )rN   rX   rh   rã   r´   rV   ⁄processed_docsZdoc_term_matrixr™   Ztopic_distributionsr¨   Zdominant_topicsri   rO   rê   rP   r`   ¢  sR    

 ˝¸
$z#NLPFeatures._extract_topic_featuresc              
      sä  êzD|† ° }ddg}g }|D ],}||jv r|†|| †d°†t°†° ° q|sT|W S á fddÑ|D É}ddÑ |D É}|s~|W S tdÉrêt†	d° td	Ér¢t†	d
° t
|ddddddçà _ddÑ t|ÉD É}t|ddddddçà _|D ê]Z}||jv r‰|| †d°†t°†à j°}g }	|D ]`}
|
êrfá fddÑ|
D É}|êrTtj|ddç}|	†|° n|	†t†d°° n|	†t†d°° êqt†|	°}	ttd|	jd ÉÉD ]$}|	ddÖ|f ||õ d|õ ù< êqòg }|D ]4}
|
êrËà j†|
°}|†|° n|†t†d°° êq∆t†|°}ttd|jd ÉÉD ]$}|ddÖ|f ||õ d|õ ù< êqq‰|W S  têyÑ } z$t†dt|Éõ ù° |W  Y d}~S d}~0 0 dS )zÀ
        Extract word embedding features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with embedding features
        rk   rl   rm   c                    s   g | ]}à † |°ëqS rO   rø   r¿   rê   rO   rP   rY   
  rZ   z;NLPFeatures._extract_embedding_features.<locals>.<listcomp>c                 S   s   g | ]}|r|ëqS rO   rO   r¿   rO   rO   rP   rY     rZ   ⁄geminizIGemini API available, but implementation pending. Using local embeddings.⁄openaizIOpenAI API available, but implementation pending. Using local embeddings.rù   r≈   r{   È   )Z	sentences⁄vector_size⁄window⁄	min_count⁄workers⁄sgc                 S   s   g | ]\}}t ||gÉëqS rO   r   )rU   r¨   r¡   rO   rO   rP   rY   '  rZ   r†   )⁄	documentsrÿ   rŸ   r⁄   r€   Zepochsc                    s$   g | ]}|à j jv rà j j| ëqS rO   )rH   Zwvrt   rê   rO   rP   rY   <  rZ   r   rÃ   NZ_word2vec_dim_Z_doc2vec_dim_z%Error extracting embedding features: )r[   rS   rŒ   rà   râ   rg   rœ   r   rb   rc   r   rH   rß   r   rI   rä   r°   rw   rx   rπ   ⁄zerosr∫   r—   ⁄minr“   Zinfer_vectorre   rf   )rN   rX   rh   rã   r´   rV   r‘   Ztagged_docsr™   Zword2vec_vectorsr¡   Zword_vectorsZ
avg_vectorr¨   Zdoc2vec_vectors⁄vectorri   rO   rê   rP   ra     s|    

 

˙
˙



"
$z'NLPFeatures._extract_embedding_featuresc              
      sÆ   zl|† ° }|†t†ddtj°°}t†dd|°}t|É}á fddÑ|D É}á fddÑ|D É}ddÑ |D É}|W S  t	y® } z$t
†dt|Éõ ù° g W  Y d}~S d}~0 0 dS )	zµ
        Preprocess text for NLP analysis
        
        Args:
            text (str): Input text
            
        Returns:
            list: List of processed tokens
        rm   z\d+c                    s   g | ]}|à j vr|ëqS rO   )rB   rt   rê   rO   rP   rY   |  rZ   z0NLPFeatures._preprocess_text.<locals>.<listcomp>c                    s   g | ]}à j †|°ëqS rO   )rC   Z	lemmatizert   rê   rO   rP   rY     rZ   c                 S   s   g | ]}t |Éd kr|ëqS )rú   rs   rt   rO   rO   rP   rY   Ç  rZ   zError preprocessing text: N)rá   ⁄	translaterg   ⁄	maketransr}   r~   rØ   ⁄subr   re   rb   rf   )rN   rª   ⁄tokensri   rO   rê   rP   r°   d  s    
zNLPFeatures._preprocess_textc                    s4   | † à °}á fddÑ|jD É}t|Édkr0d| _|S )z‘
        Fit the feature extractor and transform the data
        
        Args:
            df (DataFrame): Input data
            
        Returns:
            DataFrame: Transformed data with features
        c                    s   g | ]}|à j vr|ëqS rO   rR   rT   rW   rO   rP   rY   ò  rZ   z-NLPFeatures.fit_transform.<locals>.<listcomp>r   T)rj   rS   rd   rK   )rN   rX   rh   ⁄feature_colsrO   rW   rP   r–   ä  s
    
zNLPFeatures.fit_transformc                 C   s   | j stdÉÇ| †|°}|S )z’
        Transform new data using fitted feature extractor
        
        Args:
            df (DataFrame): Input data
            
        Returns:
            DataFrame: Transformed data with features
        z7Feature extractor not fitted. Call fit_transform first.)rK   ⁄
ValueErrorrj   )rN   rX   rh   rO   rO   rP   r•   ü  s    

zNLPFeatures.transform)N)⁄__name__⁄
__module__⁄__qualname__⁄__doc__rQ   rj   r\   r]   r^   r_   r`   ra   r°   r–   r•   rO   rO   rO   rP   r   5   s   
. H3OONt&r   )5rÍ   ⁄pandasr¢   ⁄numpyrw   rØ   r}   ⁄collectionsr   ZnltkZnltk.corpusr   Znltk.tokenizer   r   Z	nltk.stemr   r   Znltk.sentimentr   Zsklearn.feature_extraction.textr	   r
   ⁄sklearn.decompositionr   Ztextblobr   ZgensimZgensim.modelsr   r   Zgensim.models.doc2vecr   ⁄warnings⁄logging⁄typingr   r   r   r   ⁄&fraud_detection_engine.utils.api_utilsr   ⁄filterwarnings⁄basicConfig⁄INFO⁄	getLoggerrÁ   rb   ⁄data⁄find⁄LookupError⁄downloadr   rO   rO   rO   rP   ⁄<module>   sP   



=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\features\__pycache__\statistical_features.cpython-313.pyc ===
Û
    dÄöh™|  „                   Û   ï S r SSKrSSKrSSKJr  SSKJr  SSK	J
r
  SSKJr  SSKrSSKrSSKJrJrJrJr  \R(                  " S5        \R*                  " \R,                  S9  \R.                  " \5      r " S	 S
5      rg)zn
Statistical Features Module
Implements various statistical feature extraction techniques for fraud detection
È    N)⁄mahalanobis)⁄StandardScaler)⁄PCA)⁄Dict⁄List⁄Tuple⁄Union⁄ignore)⁄levelc                   Ûp   ï \ rS rSrSrSS jrS rS rS rS r	S	 r
S
 rS rS rS rS rS rS rS rSrg)⁄StatisticalFeaturesÈ   z~
Class for extracting statistical features from transaction data
Implements techniques like Benford's Law, Z-score, MAD, etc.
Nc                 Ûl   ï U=(       d    0 U l         / U l        [        5       U l        SU l        SU l        g)z]
Initialize StatisticalFeatures

Args:
    config (dict, optional): Configuration parameters
NF)⁄config⁄feature_namesr   ⁄scaler⁄pca⁄fitted)⁄selfr   s     ⁄sC:\Users\Tranquil Abyss\fraud-detection-platform\app\..\src\fraud_detection_engine\features\statistical_features.py⁄__init__⁄StatisticalFeatures.__init__   s/   Ä  ól†àåÿà‘‹$”&àåÿàåÿàçÛ    c                 Ûä  ï  UR                  5       nU R                  U5      nU R                  U5      nU R                  U5      nU R	                  U5      nU R                  U5      nU R                  U5      nU R                  U5      nU R                  U5      nU R                  U5      nUR                   Vs/ s H  o3UR                  ;  d  M  UPM     snU l        [        R                  S[        U R                  5       S35        U$ s  snf ! [         a'  n[        R!                  S[#        U5       35        e SnAff = f)z¶
Extract all statistical features from the dataframe

Args:
    df (DataFrame): Input transaction data
    
Returns:
    DataFrame: DataFrame with extracted features
z
Extracted z statistical featuresz'Error extracting statistical features: N)⁄copy⁄_extract_benford_features⁄_extract_zscore_features⁄_extract_mad_features⁄_extract_percentile_features⁄_extract_distribution_features⁄_extract_mahalanobis_features⁄_extract_grubbs_features⁄_extract_entropy_features⁄_extract_correlation_features⁄columnsr   ⁄logger⁄info⁄len⁄	Exception⁄error⁄str)r   ⁄df⁄	result_df⁄col⁄es        r   ⁄extract_features⁄$StatisticalFeatures.extract_features'   s,  Ä 	‡üôõ	àI ◊6—6∞y”AàIÿ◊5—5∞i”@àIÿ◊2—2∞9”=àIÿ◊9—9∏)”DàIÿ◊;—;∏I”FàIÿ◊:—:∏9”EàIÿ◊5—5∞i”@àIÿ◊6—6∞y”AàIÿ◊:—:∏9”EàI 2;◊1B“1B”!\“1B®#–QS◊Q[—Q[—F[ß#—1B—!\àD‘‰èKâKò*§S®◊);—);”%<–$=–=R–S‘Tÿ–˘Ú "]¯Ù
 Û 	‹èLâL–Bƒ3¿q√6¿(–K‘Lÿ˚	˙s0   ÇB7D ¬9D√D√5D ƒD ƒ
Eƒ"D=ƒ=Ec           
      ÛÏ  ï  UR                  5       nSUR                  ;   Gaí  US   R                  5       nUR                  [        5      R                  S   R                  SS5      R                  [        5      nXDS:¨     n[        U5      S:î  Ga#  UR                  SS9R                  5       n[        R                  " [        SS5       Vs/ s H  n[        R                  " SSU-  -   5      PM!     sn[        SS5      S	9nSn[        SS5       H:  n	Xy   [        U5      -  n
UR                  U	S5      nU
S:î  d  M-  XãU
-
  S
-  U
-  -  nM<     XÇS'   S[         R"                  R%                  US5      -
  US'   [        SS5       H,  n	UR                  U	S5      nXy   n[        XÕ-
  5      USU	 3'   M.     U$ s  snf ! [&         a-  n[(        R+                  S[	        U5       35        Us SnA$ SnAff = f)zé
Extract Benford's Law features

Args:
    df (DataFrame): Input dataframe
    
Returns:
    DataFrame: DataFrame with Benford's Law features
⁄amountr   ⁄n⁄0È   T©⁄	normalizeÈ
   )⁄indexÈ   ⁄benford_chi_squareÈ   ⁄benford_p_valueÈ   ⁄benford_deviation_z)Error extracting Benford's Law features: N)r   r%   ⁄abs⁄astyper+   ⁄replace⁄intr(   ⁄value_counts⁄
sort_index⁄pd⁄Series⁄range⁄np⁄log10⁄get⁄stats⁄chi2⁄cdfr)   r&   r*   )r   r,   r-   ⁄amounts⁄first_digits⁄actual_dist⁄d⁄benford_dist⁄
chi_square⁄digit⁄expected_count⁄actual_count⁄
actual_pct⁄expected_pctr/   s                  r   r   ⁄-StatisticalFeatures._extract_benford_featuresJ   s„  Ä '	ÿüôõ	àI ò2ü:ô:‘%‡òXô,◊*—*”,êÿ&ü~ô~¨c”2◊6—6∞q—9◊A—A¿#¿s”K◊R—R‘SV”Wêÿ+∏A—,=—>ê‰ê|”$†q‘(‡".◊";—";¿d–";–"K◊"V—"V”"XêKÙ $&ß9¢9Ã»q–RTÃ”-V ¿A¨bØh™h∞q∏1∏Qπ3±w÷.?…—-V‘^c–de–gi”^j—#kêL "#êJ‹!&†q®"¶òÿ)5—)<ºs¿<”?P—)Pòÿ'2ß°∞u∏a”'@òÿ)®A’-ÿ&∏.—+H»Q—*N–Q_—*_—_öJÒ	 ". 7A–2—3ÿ34¥u∑z±z∑~±~¿j–RS”7T—3TêI–/—0Ù "'†q®!¶òÿ%0ß_°_∞U∏A”%>ò
ÿ'3—':ò‹BE¿j—F_”B`ò	–$6∞u∞g–">”?Ò "-
 –˘Ú) .W¯Ù, Û 	‹èLâL–DƒS»√V¿H–M‘NÿçI˚	˙s8   ÇCF< √&F7√3AF< ƒ>A8F< ∆7F< ∆<
G3«"G.«(G3«.G3c                 Û^  ï  UR                  5       nSUR                  ;   ap  US   nUR                  5       nUR                  5       nUS:î  a;  X4-
  U-  nXbS'   [        R
                  " U5      S:Ñ  R                  [        5      US'   O
SUS'   SUS'   SUR                  ;   aﬁ  SUR                  ;   aŒ  UR                  S5      S   R                  5       nUR                  S5      S   R                  5       n/ n	UR                  5        HA  u  p´US   nUS   nX«;   a  X»;   a  Xå   S:î  a  X◊U   -
  Xå   -  nOSnU	R                  U5        MC     XíS'   [        R
                  " U	5      S:Ñ  R                  [        5      US'   S	UR                  ;   a„  SUR                  ;   a”  UR                  S	5      S   R                  5       nUR                  S	5      S   R                  5       n/ nUR                  5        HE  u  p´US	   nUS   nUU;   a  UU;   a  UU   S:î  a  XﬂU   -
  UU   -  nOSnUR                  U5        MG     UUS
'   [        R
                  " U5      S:Ñ  R                  [        5      US'   U$ ! [         a-  n[        R                  S[        U5       35        Us SnA$ SnAff = f)zà
Extract Z-score based features

Args:
    df (DataFrame): Input dataframe
    
Returns:
    DataFrame: DataFrame with Z-score features
r3   r   ⁄amount_zscoreÈ   ⁄amount_zscore_outlier⁄	sender_id⁄sender_amount_zscore⁄sender_amount_zscore_outlier⁄receiver_id⁄receiver_amount_zscore⁄receiver_amount_zscore_outlierz#Error extracting Z-score features: N)r   r%   ⁄mean⁄stdrJ   rA   rB   rD   ⁄groupby⁄iterrows⁄appendr)   r&   r*   r+   )r   r,   r-   rP   ⁄mean_amount⁄
std_amount⁄z_scores⁄
sender_avg⁄
sender_std⁄sender_zscores⁄_⁄rowr`   r3   ⁄z_score⁄receiver_avg⁄receiver_std⁄receiver_zscoresrc   r/   s                       r   r   ⁄,StatisticalFeatures._extract_zscore_features}   s¥  Ä D	ÿüôõ	àI ò2ü:ô:”%ÿòXô,ê &ülôlõnêÿ$ü[ô[õ]ê
‡†ì>ÿ '— 5∏—CêHÿ19òo—.Ù ;=ø&∫&¿”:J»Q—:N◊9V—9V‘WZ”9[êI–5“6‡12êIòo—.ÿ9:êI–5—6 òbüjôj”(®X∏øπ”-C‡üZôZ®”4∞X—>◊C—C”Eê
ÿüZôZ®”4∞X—>◊B—B”Dê
 "$êÿ ükôkûmëFêAÿ #†K— 0êIÿ †ô]êF‡ ”.∞9”3J»z—Od–gh”Ohÿ#)∞y—,A—#A¿Z—EZ—"Zô‡"#ò‡"◊)—)®'÷2Ò , 5C–0—1‹=?øV∫V¿N”=S–VW—=W◊<_—<_‘`c”<dê	–8—9‡†ß
°
”*®x∏2ø:π:”/E‡!üzôz®-”8∏—B◊G—G”Iêÿ!üzôz®-”8∏—B◊F—F”Hê $&– ÿ ükôkûmëFêAÿ"%†m—"4êKÿ †ô]êF‡"†l”2∞{¿l”7R–Wc–do—Wp–st”Wtÿ#)∏—,E—#E»–Va—Ib—"bô‡"#ò‡$◊+—+®G÷4Ò , 7Gê	–2—3‹?Aøv∫v–FV”?W–Z[—?[◊>c—>c‘dg”>hê	–:—;‡–¯‰Û 	‹èLâL–>ºs¿1ªv∏h–G‘HÿçI˚	˙s   ÇI2I5 …5
J,…?"J' !J, 'J,c                 Û™  ï  UR                  5       nSUR                  ;   au  US   nUR                  5       n[        R                  " X4-
  5      nUR                  5       nUS:î  a(  SU-  U-  nXrS'   US:Ñ  R                  [        5      US'   O
SUS'   SUS'   SUR                  ;   aÓ  SUR                  ;   aﬁ  UR                  S5      S   R                  S 5      nUR                  S5      S   R                  5       n	/ n
UR                  5        HO  u  pºUS   nUS   nXÿ;   a(  XŸ;   a#  Xç   S:î  a  [	        XÈU   -
  5      nSU-  Xç   -  nOSnU
R                  U5        MQ     X¢S	'   [        R                  " U
5      S:Ñ  R                  [        5      US
'   SUR                  ;   aÙ  SUR                  ;   a‰  UR                  S5      S   R                  S 5      nUR                  S5      S   R                  5       n/ nUR                  5        HT  u  pºUS   nUS   nUU;   a,  UU;   a&  UU   S:î  a  [	        UUU   -
  5      nSU-  UU   -  nOSnUR                  U5        MV     UUS'   [        R                  " U5      S:Ñ  R                  [        5      US'   U$ ! [         a-  n[        R                  S[        U5       35        Us SnA$ SnAff = f)zñ
Extract Median Absolute Deviation (MAD) features

Args:
    df (DataFrame): Input dataframe
    
Returns:
    DataFrame: DataFrame with MAD features
r3   r   g/›$ÅïÂ?⁄amount_mad_zscoreg      @⁄amount_mad_outlierr`   c                 Ûv   ï [         R                  " [         R                  " X R                  5       -
  5      5      $ ©N©rJ   ⁄medianrA   ©⁄xs    r   ⁄<lambda>⁄;StatisticalFeatures._extract_mad_features.<locals>.<lambda>Ò   s&   Ä Ãrœy y‘Y[◊Y_“Y_–`a◊dl—dl”dn—`n”Yo‘Opr   ⁄sender_amount_mad_zscore⁄sender_amount_mad_outlierrc   c                 Ûv   ï [         R                  " [         R                  " X R                  5       -
  5      5      $ r|   r}   r   s    r   rÅ   rÇ     s)   Ä ‘SU◊S\“S\‘]_◊]c“]c–de◊hp—hp”hr—dr”]s‘Str   ⁄receiver_amount_mad_zscore⁄receiver_amount_mad_outlierzError extracting MAD features: N)r   r%   r~   rJ   rA   rB   rD   rh   ⁄applyri   rj   ⁄arrayr)   r&   r*   r+   )r   r,   r-   rP   ⁄median_amount⁄abs_dev⁄mad⁄modified_z_scores⁄
sender_mad⁄sender_median⁄sender_mad_zscoresrq   rr   r`   r3   ⁄mad_z_score⁄receiver_mad⁄receiver_median⁄receiver_mad_zscoresrc   r/   s                        r   r   ⁄)StatisticalFeatures._extract_mad_featuresÕ   sÏ  Ä H	ÿüôõ	àI ò2ü:ô:”%ÿòXô,ê !(ß°” 0ê‹ü&ö&†—!8”9êÿónën”&ê‡òì7‡(.∞—(8∏3—(>–%ÿ5F–1—2 8I»3—7N◊6V—6V‘WZ”6[êI–2“3‡56êI–1—2ÿ67êI–2—3 òbüjôj”(®X∏øπ”-C‡üZôZ®”4∞X—>◊D—D—Ep”qê
ÿ "ß
°
®;” 7∏— A◊ H— H” Jê &(–"ÿ ükôkûmëFêAÿ #†K— 0êIÿ †ô]êF‡ ”.∞9”3M–R\—Rg–jk”Rk‹"%†f∏Y—/G—&G”"Hòÿ&,®w—&6∏—9N—&Nô‡&'ò‡&◊-—-®k÷:Ò , 9K–4—5‹:<ø(∫(–CU”:V–Y\—:\◊9d—9d‘eh”9iê	–5—6‡†ß
°
”*®x∏2ø:π:”/E‡!üzôz®-”8∏—B◊H—H—It”uêÿ"$ß*°*®]”";∏H—"E◊"L—"L”"Nê (*–$ÿ ükôkûmëFêAÿ"%†m—"4êKÿ †ô]êF‡"†l”2∞{¿o”7U–Zf–gr—Zs–vw”Zw‹"%†f®∏{—/K—&K”"Lòÿ&,®w—&6∏¿k—9R—&Rô‡&'ò‡(◊/—/∞÷<Ò , ;Oê	–6—7‹<>øH∫H–EY”<Z–]`—<`◊;h—;h‘il”;mê	–7—8‡–¯‰Û 	‹èLâL–:º3∏qª6∏(–C‘DÿçI˚	˙s   ÇJJ  
K %"KÀKÀKc                 Û  ï  UR                  5       nSUR                  ;   af  US   n/ SQn[        R                  " X45      n[	        U5       H&  u  pgX5U   :Ñ  R                  [        5      USU S3'   M(     UR                  SS9US'   SUR                  ;   aP  SUR                  ;   a@  UR                  S5      S   R                  SS9nXÇS	'   US
:Ñ  R                  [        5      US'   SUR                  ;   aP  SUR                  ;   a@  UR                  S5      S   R                  SS9n	XíS'   U	S
:Ñ  R                  [        5      US'   U$ ! [         a-  n
[        R                  S[        U
5       35        Us Sn
A
$ Sn
A
ff = f)zé
Extract percentile-based features

Args:
    df (DataFrame): Input dataframe
    
Returns:
    DataFrame: DataFrame with percentile features
r3   )È   r9   È   È2   ÈK   ÈZ   È_   Èc   ⁄amount_above_⁄th_percentileT)⁄pct⁄amount_percentile_rankr`   ⁄sender_amount_percentile_rankgffffffÓ?⁄sender_amount_top_5pctrc   ⁄receiver_amount_percentile_rank⁄receiver_amount_top_5pctz&Error extracting percentile features: N)r   r%   rJ   ⁄
percentile⁄	enumeraterB   rD   ⁄rankrh   r)   r&   r*   r+   )r   r,   r-   rP   ⁄percentiles⁄percentile_values⁄i⁄p⁄sender_percentile_ranks⁄receiver_percentile_ranksr/   s              r   r   ⁄0StatisticalFeatures._extract_percentile_features!  sç  Ä '	ÿüôõ	àI ò2ü:ô:”%ÿòXô,êÚ >ê‹$&ßM¢M∞'”$G–!Ù &†k÷2ëDêAÿCJ–_`—Ma—Ca◊Bi—Bi‘jm”BnêI†®a®S∞–>”?Ò 3 7>∑l±l¿t∞l–6Lê	–2—3 òbüjôj”(®X∏øπ”-C‡*,Ø*©*∞[”*A¿(—*K◊*P—*P–UY–*P–*Z–'ÿ=T–9—: 8O–QU—7U◊6]—6]‘^a”6bê	–2—3‡†ß
°
”*®x∏2ø:π:”/E‡,.ØJ©J∞}”,E¿h—,O◊,T—,T–Y]–,T–,^–)ÿ?X–;—< :S–UY—9Y◊8a—8a‘be”8fê	–4—5‡–¯‰Û 	‹èLâL–Aƒ#¿a√&¿–J‘KÿçI˚	˙s   ÇEE
 ≈

F≈"E<≈6F≈<Fc                 Û∆  ï  UR                  5       nSUR                  ;   a˝  US   n[        R                  " U5      US'   [        R                  " U5      US'   [        R
                  " U5      u  pEXRS'   [        U5      S::  a  [        R                  " U5      u  pFXbS'   [        R                  " USUR                  5       UR                  5       4S9u  pGXrS	'   [        R                  " U5      nUR                  US
'   [        R                  " U5      u  pöXíS'   X¢S'   SUR                  ;   at  SUR                  ;   ad  UR                  S5      S   R                  SS 4SS 4SSS 4/5      nS H.  nUS   R!                  Xº   5      R#                  S5      USU 3'   M0     SUR                  ;   at  SUR                  ;   ad  UR                  S5      S   R                  SS 4SS 4SSS 4/5      nS H.  nUS   R!                  X‹   5      R#                  S5      USU 3'   M0     U$ ! [$         a-  n[&        R)                  S[+        U5       35        Us SnA$ SnAff = f)zí
Extract distribution-based features

Args:
    df (DataFrame): Input dataframe
    
Returns:
    DataFrame: DataFrame with distribution features
r3   ⁄amount_skewness⁄amount_kurtosis⁄amount_normality_pià  ⁄amount_shapiro_p⁄norm)⁄args⁄amount_ks_p⁄amount_ad_statistic⁄amount_jb_statistic⁄amount_jb_pr`   ⁄skewnessc                 ÛP   ï [        U 5      S:º  a  [        R                  " U 5      $ S$ ©Nr^   r   ©r(   rM   ⁄skewr   s    r   rÅ   ⁄DStatisticalFeatures._extract_distribution_features.<locals>.<lambda>É  Û   Ä ºC¿ªF¿aªK¨5Ø:™:∞a´=–+N»Q–+Nr   ⁄kurtosisc                 ÛP   ï [        U 5      S:º  a  [        R                  " U 5      $ S$ ©NÈ   r   ©r(   rM   r¬   r   s    r   rÅ   r¿   Ñ  Û    Ä ƒ¿A√»!√¨5Ø>™>∏!”+<–+R–QR–+Rr   )⁄variance⁄varrI   c                 Ûf   ï [        U 5      S:î  a!  U R                  5       U R                  5       -
  $ S$ ©Nr   ©r(   ⁄max⁄minr   s    r   rÅ   r¿   Ü  Û'   Ä ºS¿ªV¿aªZ®Ø©´∞!∑%±%≥'—(9–(N»Q–(Nr   )rª   r¬   r»   rI   r   ⁄sender_amount_rc   c                 ÛP   ï [        U 5      S:º  a  [        R                  " U 5      $ S$ rΩ   ræ   r   s    r   rÅ   r¿   ê  r¡   r   c                 ÛP   ï [        U 5      S:º  a  [        R                  " U 5      $ S$ rƒ   r∆   r   s    r   rÅ   r¿   ë  r«   r   c                 Ûf   ï [        U 5      S:î  a!  U R                  5       U R                  5       -
  $ S$ rÀ   rÃ   r   s    r   rÅ   r¿   ì  rœ   r   ⁄receiver_amount_z(Error extracting distribution features: N)r   r%   rM   rø   r¬   ⁄
normaltestr(   ⁄shapiro⁄kstestrf   rg   ⁄anderson⁄	statistic⁄jarque_berarh   ⁄agg⁄map⁄fillnar)   r&   r*   r+   )r   r,   r-   rP   rq   ⁄normality_p⁄	shapiro_p⁄ks_p⁄	ad_result⁄jb_stat⁄jb_p⁄sender_stats⁄stat⁄receiver_statsr/   s                  r   r    ⁄2StatisticalFeatures._extract_distribution_featuresT  su  Ä @	ÿüôõ	àI ò2ü:ô:”%ÿòXô,êÙ 05Øz™z∏'”/Bê	–+—,‹/4Ø~™~∏g”/Fê	–+—,Ù "'◊!1“!1∞'”!:ëêÿ2=–.—/Ù êwì<†4”'‹#(ß=¢=∞”#9ëLêAÿ4=–0—1Ù  ü,ö,†w∞∏gølπlªn»gœk…kÀm–=\—]ëêÿ+/ò-—(Ù "üNöN®7”3ê	ÿ3<◊3F—3Fê	–/—0Ù !&◊ 1“ 1∞'” :ëêÿ3:–/—0ÿ+/ò-—( òbüjôj”(®X∏øπ”-C‡!üzôz®+”6∞x—@◊D—Dÿ—!N–Oÿ—!R–Sÿ'ÿ—N–O	FÛ  êÛ JêDÿ9;∏Kπ◊9L—9L»\—M_”9`◊9g—9g–hi”9jêI†®t®f–5”6Ò J †ß
°
”*®x∏2ø:π:”/E‡!#ß°®M”!:∏8—!D◊!H—!Hÿ—!N–Oÿ—!R–Sÿ'ÿ—N–O	JÛ "êÛ JêDÿ;=∏m—;L◊;P—;P–Q_—Qe”;f◊;m—;m–no”;pêI– 0∞∞–7”8Ò J –¯‰Û 	‹èLâL–CƒC»√F¿8–L‘MÿçI˚	˙s   ÇH&H) »)
I »3"I…I …I c                 Û˙  ï  UR                  5       nUR                  [        R                  /S9R                  R                  5       n[        U5      S:  a  [        R                  S5        U$ X   R                  S5      n[        R                  " USS9n[        R                  R                  U5      S:X  a5  [        R                  S5        [        R                  R                  U5      nO[        R                  R                  U5      n[        R                  " USS9n/ n[!        [        U5      5       H+  n	UR#                  [%        UR&                  U	   Xv5      5        M-     XÇS	'   [(        R*                  R-                  S
[        U5      S9n
[        R.                  " U5      U
:Ñ  R1                  [2        5      US'   U$ ! [4         a-  n[        R7                  S[9        U5       35        Us SnA$ SnAff = f)zì
Extract Mahalanobis distance features

Args:
    df (DataFrame): Input dataframe
    
Returns:
    DataFrame: DataFrame with Mahalanobis features
©⁄includer;   z?Not enough numeric columns for Mahalanobis distance calculationr   F)⁄rowvarz3Covariance matrix is singular, using pseudo-inverse©⁄axis⁄mahalanobis_distanceg333333Ô?)r,   ⁄mahalanobis_outlierz'Error extracting Mahalanobis features: N)r   ⁄select_dtypesrJ   ⁄numberr%   ⁄tolistr(   r&   ⁄warningr›   ⁄cov⁄linalg⁄det⁄pinv⁄invrf   rI   rj   r   ⁄ilocrM   rN   ⁄ppfrâ   rB   rD   r)   r*   r+   )r   r,   r-   ⁄numeric_cols⁄X⁄
cov_matrix⁄inv_cov_matrix⁄mean_vector⁄mahalanobis_distancesr´   ⁄	thresholdr/   s               r   r!   ⁄1StatisticalFeatures._extract_mahalanobis_features†  sö  Ä ,	ÿüôõ	àI ◊+—+¥R∑Y±Y∞K–+–@◊H—H◊O—O”QàL‰ê<” †1”$‹óë–`‘aÿ –  — ◊'—'®”*àAÙ üö†®%—0àJÙ èyâyè}â}òZ”(®A”-‹óë–T‘U‹!#ß°ß°∞
”!;ë‰!#ß°ß°®z”!:êÙ ü'ö'†!®!—,àK %'–!‹ú3òqõ6ñ]êÿ%◊,—,‹†ß°†q°	®;”GˆÒ #
 1F–,—-Ù ü
ô
üô†u¥∞\”1Bò–CàI‹02∑≤–9N”0O–R[—0[◊/c—/c‘dg”/hàI–+—,‡–¯‰Û 	‹èLâL–Bƒ3¿q√6¿(–K‘LÿçI˚	˙s%   ÇA+G ¡.EG «
G:«"G5«/G:«5G:c                 Û∏  ï  UR                  5       nSUR                  ;   a˛  US   R                  n[        R                  " U5      n[        R
                  " U5      nUS:î  a≥  [        R                  " X4-
  5      n[        R                  " U5      nXu-  nXÇS'   [        U5      n	[        R                  R                  SSSU	-  -  -
  U	S-
  5      n
U	S-
  U
-  [        R                  " XôS-
  U
S-  -   -  5      -  nXã:Ñ  R                  [        5      US'   U$ SUS'   SUS'   U$ ! [         a-  n[         R#                  S[%        U5       35        Us S	nA$ S	nAff = f)
zô
Extract Grubbs' test features for outliers

Args:
    df (DataFrame): Input dataframe
    
Returns:
    DataFrame: DataFrame with Grubbs' test features
r3   r   ⁄grubbs_statisticr6   göôôôôôô?r;   ⁄grubbs_outlierz(Error extracting Grubbs' test features: N)r   r%   ⁄valuesrJ   rf   rg   rA   rÕ   r(   rM   ⁄tr˙   ⁄sqrtrB   rD   r)   r&   r*   r+   )r   r,   r-   rP   rk   rl   ⁄abs_deviations⁄max_deviation⁄grubbs_statr4   ⁄
t_critical⁄critical_valuer/   s                r   r"   ⁄,StatisticalFeatures._extract_grubbs_featuresÿ  sS  Ä #	ÿüôõ	àI ò2ü:ô:”%ÿòXô,◊-—-êÙ !ügög†g”.ê‹üVöV†Gõ_ê
‡†ì>‰%'ßV¢V®G—,A”%BêN‹$&ßF¢F®>”$:êM #0—"<êKÿ4?–0—1Ù òGõêA‹!&ß°ß°®Q∞∏!∏aπ%±—-@¿!¿a¡%”!HêJÿ&'®!°e®z—%9ºBøG∫G¿A»Q…–Q[–]^—Q^—I^—D_”<`—%`êN 4?—3O◊2W—2W‘X[”2\êI–.—/
 – 56êI–0—1ÿ23êI–.—/‡–¯‰Û 	‹èLâL–CƒC»√F¿8–L‘MÿçI˚	˙s$   ÇDD" ƒD" ƒ"
Eƒ,"E≈E≈Ec                 Û¨  ^ ï  UR                  5       nUR                  SS/S9R                  R                  5       nU Ha  nX   R	                  SS9n[        S U 5       5      * nXbU S3'   [        R                  " [        U5      5      nUS:î  a  Xg-  nOSnXÇU S	3'   Mc     UR                  [        R                  /S9R                  R                  5       n	U	 Hx  n [        R                  " X   S
SS9n
U
R	                  SS9n[        S U 5       5      * nXbU S3'   [        R                  " [        U5      5      nUS:î  a  Xg-  nOSnXÇU S3'   Mz     SUR                  ;   aM  UR                  S5      S   R                  U 4S j5      nUS   R                  U5      R                  S5      US'   SUR                  ;   aM  UR                  S5      S   R                  U 4S j5      nUS   R                  U5      R                  S5      US'   U$ !    GM>  = f! [          a-  n["        R%                  S['        U5       35        Us SnA$ SnAff = f)zà
Extract entropy-based features

Args:
    df (DataFrame): Input dataframe
    
Returns:
    DataFrame: DataFrame with entropy features
⁄object⁄categoryrÈ   Tr7   c              3   Ûb   #   ï U  H%  oS :î  d  M
  U[         R                  " U5      -  v ï  M'     g7f©r   N©rJ   ⁄log2©⁄.0r¨   s     r   ⁄	<genexpr>⁄@StatisticalFeatures._extract_entropy_features.<locals>.<genexpr>  s"   È Ä –M≤|∞!»1¡uõ~òq§2ß7¢7®1£:û~≤|˘Û   Ç	/è /⁄_entropyr   ⁄_normalized_entropyr9   ⁄drop©⁄bins⁄
duplicatesc              3   Ûb   #   ï U  H%  oS :î  d  M
  U[         R                  " U5      -  v ï  M'     g7fr  r  r  s     r   r  r  3  s"   È Ä –"Q∫<∞a»q…5£>†1§rßw¢w®q£z¶>∫<˘r  ⁄_binned_entropy⁄_binned_normalized_entropyr`   r3   c                 Û&   >ï TR                  U 5      $ r|   ©⁄_calculate_series_entropy©rÄ   r   s    Är   rÅ   ⁄?StatisticalFeatures._extract_entropy_features.<locals>.<lambda>D  Û   ¯Ä òd◊<—<∏Q‘?r   ⁄sender_amount_entropyrc   c                 Û&   >ï TR                  U 5      $ r|   r%  r'  s    Är   rÅ   r(  K  r)  r   ⁄receiver_amount_entropyz#Error extracting entropy features: N)r   r   r%   rÚ   rE   ⁄sumrJ   r  r(   rÒ   rG   ⁄cutrh   rà   r‹   r›   r)   r&   r*   r+   )r   r,   r-   ⁄categorical_colsr.   rE   ⁄entropy⁄max_entropy⁄normalized_entropyr˚   ⁄binned⁄sender_entropy⁄receiver_entropyr/   s   `             r   r#   ⁄-StatisticalFeatures._extract_entropy_features  sj  ¯Ä B	ÿüôõ	àI  "◊/—/∏¿:–8N–/–O◊W—W◊^—^”`–„'ê‡!ôw◊3—3∏d–3–CêÙ —M±|”M”M–Mêÿ.5òSòE†–*—+Ù !ügög§c®,”&7”8êÿ†ì?ÿ)0—)>—&‡)*–&ÿ9KòSòE–!4–5”6Ò (" ◊+—+¥R∑Y±Y∞K–+–@◊H—H◊O—O”QàL„#ê‹üVöV†B°G∞"¿—HêF $*◊#6—#6¿–#6–#FêLÙ  #—"Qπ<”"Q”Q–QêGÿ9@†††_–5—6Ù #%ß'¢'¨#®l”*;”"<êKÿ"†Qìÿ-4—-B—*‡-.–*ÿDV††–%?–@”AÒ% $. òbüjôj”(‡!#ß°®K”!8∏—!B◊!H—!H‹?Û"ê 68∏±_◊5H—5H»”5X◊5_—5_–`a”5bê	–1—2‡†ß
°
”*‡#%ß:°:®m”#<∏X—#F◊#L—#L‹?Û$–  8:∏-—7H◊7L—7L–M]”7^◊7e—7e–fg”7hê	–3—4‡–¯%€˚Ù& Û 	‹èLâL–>ºs¿1ªv∏h–G‘HÿçI˚	˙s8   ÉCH √A4H≈B?H »H»H »
I»&"I…I…Ic           	      Û¨   ï  [         R                  " U[        S[        U5      5      SS9nUR	                  SS9n[        S U 5       5      * nU$ !    g= f)zv
Calculate entropy of a pandas Series

Args:
    series (Series): Input series
    
Returns:
    float: Entropy value
r9   r  r  Tr7   c              3   Ûb   #   ï U  H%  oS :î  d  M
  U[         R                  " U5      -  v ï  M'     g7fr  r  r  s     r   r  ⁄@StatisticalFeatures._calculate_series_entropy.<locals>.<genexpr>g  s"   È Ä –I≤<®a¿q¡5õ>ò1úrüwöw†qõzû>≤<˘r  r   )rG   r.  rŒ   r(   rE   r-  )r   ⁄seriesr3  rE   r0  s        r   r&  ⁄-StatisticalFeatures._calculate_series_entropyU  s]   Ä 	‰óVíVòF¨®R¥∞V≥”)=»&—QàF "◊.—.∏–.–>àLÙ —I±<”I”I–IàGÿàN¯	Ÿ˙s   ÇAA ¡Ac                 Û–  ï  UR                  5       nUR                  [        R                  /S9R                  R                  5       n[        U5      S:  a  [        R                  S5        U$ X   R                  5       R                  5       n0 nU H(  nXF   R                  U5      nUR                  5       XV'   M*     U H  nXV   X& S3'   M     0 nU H(  nXF   R                  U5      nUR                  5       XÜ'   M*     U H  nXÜ   X& S3'   M     [        U5       H|  u  pñX   R                  5       n
X¶   nU
R                  USS9n
 SS	KJn  U" 5       nUR#                  X´5        UR%                  X´5      nUS:  a	  SSU-
  -  nO['        S
5      nXÚU S3'   M~     U$ !   SX& S3'    Mé  = f! [(         a-  n[        R+                  S[-        U5       35        Us SnA$ SnAff = f)zê
Extract correlation-based features

Args:
    df (DataFrame): Input dataframe
    
Returns:
    DataFrame: DataFrame with correlation features
rÈ   r;   z6Not enough numeric columns for correlation calculation⁄_max_correlation⁄_avg_correlationr6   rÏ   r   )⁄LinearRegression⁄inf⁄_vifz'Error extracting correlation features: N)r   r   rJ   rÒ   r%   rÚ   r(   r&   rÛ   ⁄corrrA   r  rÕ   rf   rß   ⁄sklearn.linear_modelr?  ⁄fit⁄score⁄floatr)   r*   r+   )r   r,   r-   r˚   ⁄corr_matrix⁄max_corrr.   ⁄corrs⁄avg_corrr´   r¸   ⁄yr?  ⁄model⁄	r_squared⁄vifr/   s                    r   r$   ⁄1StatisticalFeatures._extract_correlation_featuresl  s¯  Ä A	ÿüôõ	àI ◊+—+¥R∑Y±Y∞K–+–@◊H—H◊O—O”QàL‰ê<” †1”$‹óë–W‘Xÿ –  —*◊/—/”1◊5—5”7àK àH€#ê‡#—(◊-—-®c”2êÿ %ß	°	£êìÒ $Û $êÿ6>±mê	òE–!1–2”3Ò $ àH€#ê‡#—(◊-—-®c”2êÿ %ß
°
£êìÒ $Û $êÿ6>±mê	òE–!1–2”3Ò $Ù $†L÷1ëê‡—$◊)—)”+êÿëFêÿóFëFò3†QêF–'ê0›EŸ,”.êEÿóIëIòaîO !&ß°®A” 1êI !†1ì}ÿ†1†y°=—1ô‰#†Eõlò‡.1†††Tòl”+Ò+ 22 –¯0ÿ./êI††Tòl‘+˚Ù Û 	‹èLâL–Bƒ3¿q√6¿(–K‘LÿçI˚	˙s>   ÇA+F. ¡.CF. ≈	AF∆F. ∆	F+∆(F. ∆.
G%∆8"G «G%« G%c                 Û(  ï U R                  U5      nUR                   Vs/ s H  o3UR                  ;  d  M  UPM     nn[        U5      S:î  aø  U R                  R	                  X$   5        [        U5      S:º  aå  [        [        S[        U5      5      S9U l        U R                  R	                  X$   5        U R                  R                  X$   5      n[        UR                  S   5       H  nUSS2U4   USUS-    3'   M     SU l        U$ s  snf )zî
Fit the feature extractor and transform the data

Args:
    df (DataFrame): Input data
    
Returns:
    DataFrame: Transformed data with features
r   r9   )⁄n_componentsr6   N⁄	stat_pca_T)r0   r%   r(   r   rD  r   rŒ   r   ⁄	transformrI   ⁄shaper   ©r   r,   r-   r.   ⁄feature_cols⁄pca_componentsr´   s          r   ⁄fit_transform⁄!StatisticalFeatures.fit_transformπ  s˚   Ä  ◊)—)®"”-à	 (1◊'8“'8”R“'8†¿r«z¡z—<Qü—'8à–R‰à|”òq” ‡èKâKèOâOòI—3‘4Ù ê<” †B”&‹¨C∞¥C∏”4E”,F—GêîÿóëóëòY—4‘5 "&ß°◊!3—!3∞I—4K”!Lê‹ò~◊3—3∞A—6÷7êAÿ3A¬!¿Q¿$—3GêI†	®!®A©#®–/”0Ò 8 àDåK‡–˘Ú% Ss
   †D∑Dc                 Û÷  ï U R                   (       d  [        S5      eU R                  U5      nUR                   Vs/ s H  o3UR                  ;  d  M  UPM     nn[	        U5      S:î  az  U R
                  R                  X$   5      X$'   U R                  bN  U R                  R                  X$   5      n[        UR                  S   5       H  nUSS2U4   USUS-    3'   M     U$ s  snf )zï
Transform new data using fitted feature extractor

Args:
    df (DataFrame): Input data
    
Returns:
    DataFrame: Transformed data with features
z7Feature extractor not fitted. Call fit_transform first.r   Nr6   rR  )
r   ⁄
ValueErrorr0   r%   r(   r   rS  r   rI   rT  rU  s          r   rS  ⁄StatisticalFeatures.transform€  s‡   Ä  è{è{‹–V”W–W ◊)—)®"”-à	 (1◊'8“'8”R“'8†¿r«z¡z—<Qü—'8à–R‰à|”òq” ‡&*ßk°k◊&;—&;∏I—<S”&TàI—# èxâx—#ÿ!%ß°◊!3—!3∞I—4K”!Lê‹ò~◊3—3∞A—6÷7êAÿ3A¬!¿Q¿$—3GêI†	®!®A©#®–/”0Ò 8 –˘Ú Ss   ºC&¡C&)r   r   r   r   r   r|   )⁄__name__⁄
__module__⁄__qualname__⁄__firstlineno__⁄__doc__r   r0   r   r   r   r   r    r!   r"   r#   r&  r$   rX  rS  ⁄__static_attributes__© r   r   r   r      s]   Ü ÒÙ
Ú!ÚF1ÚfNÚ`RÚh1ÚfJÚX6Úp-Ú^LÚ\Ú.KÚZ ıDr   r   )ra  ⁄pandasrG   ⁄numpyrJ   ⁄scipy.statsrM   ⁄scipy.spatial.distancer   ⁄sklearn.preprocessingr   ⁄sklearn.decompositionr   ⁄warnings⁄logging⁄typingr   r   r   r	   ⁄filterwarnings⁄basicConfig⁄INFO⁄	getLoggerr]  r&   r   rc  r   r   ⁄<module>rq     sf   ÒÛ
 € › › .› 0› %€ € ﬂ +” +‡ ◊ “ ò‘ !ÿ ◊ “ ò'ü,ô,“ 'ÿ	◊	“	ò8”	$Ä˜dÚ dr   

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\features\__pycache__\statistical_features.cpython-39.pyc ===
a
    èöh™|  „                   @   s†   d Z ddlZddlZddlmZ ddlmZ ddl	m
Z
 ddlmZ ddlZddlZddlmZmZmZmZ e†d° ejejdç e†e°ZG d	d
Ñ d
ÉZdS )zn
Statistical Features Module
Implements various statistical feature extraction techniques for fraud detection
È    N)⁄mahalanobis)⁄StandardScaler)⁄PCA)⁄Dict⁄List⁄Tuple⁄Union⁄ignore)⁄levelc                   @   sÇ   e Zd ZdZdddÑZddÑ ZddÑ Zd	d
Ñ ZddÑ ZddÑ Z	ddÑ Z
ddÑ ZddÑ ZddÑ ZddÑ ZddÑ ZddÑ ZddÑ ZdS ) ⁄StatisticalFeatureszä
    Class for extracting statistical features from transaction data
    Implements techniques like Benford's Law, Z-score, MAD, etc.
    Nc                 C   s(   |pi | _ g | _tÉ | _d| _d| _dS )zÖ
        Initialize StatisticalFeatures
        
        Args:
            config (dict, optional): Configuration parameters
        NF)⁄config⁄feature_namesr   ⁄scaler⁄pca⁄fitted)⁄selfr   © r   ˙sC:\Users\Tranquil Abyss\fraud-detection-platform\app\..\src\fraud_detection_engine\features\statistical_features.py⁄__init__   s
    
zStatisticalFeatures.__init__c              
      s÷   zñà † ° }| †|°}| †|°}| †|°}| †|°}| †|°}| †|°}| †|°}| †|°}| †	|°}á fddÑ|j
D É| _t†dt| jÉõ dù° |W S  ty– } z"t†dt|Éõ ù° Ç W Y d}~n
d}~0 0 dS )zÊ
        Extract all statistical features from the dataframe
        
        Args:
            df (DataFrame): Input transaction data
            
        Returns:
            DataFrame: DataFrame with extracted features
        c                    s   g | ]}|à j vr|ëqS r   ©⁄columns©⁄.0⁄col©⁄dfr   r   ⁄
<listcomp>A   Û    z8StatisticalFeatures.extract_features.<locals>.<listcomp>z
Extracted z statistical featuresz'Error extracting statistical features: N)⁄copy⁄_extract_benford_features⁄_extract_zscore_features⁄_extract_mad_features⁄_extract_percentile_features⁄_extract_distribution_features⁄_extract_mahalanobis_features⁄_extract_grubbs_features⁄_extract_entropy_features⁄_extract_correlation_featuresr   r   ⁄logger⁄info⁄len⁄	Exception⁄error⁄str)r   r   ⁄	result_df⁄er   r   r   ⁄extract_features'   s"    









z$StatisticalFeatures.extract_featuresc              
   C   s~  êz8|† ° }d|jv êr6|d †° }|†t°jd †dd°†t°}||dk }t|Édkêr6|jddç†	° }t
jdd	Ñ tdd
ÉD Étdd
Édç}d}tdd
ÉD ]<}|| t|É }	|†|d°}
|	dkrû||
|	 d |	 7 }qû||d< dtj†|d° |d< tddÉD ]0}|†|d°}|| }t|| É|d|õ ù< êq|W S  têyx } z$t†dt|Éõ ù° |W  Y d}~S d}~0 0 dS )zŒ
        Extract Benford's Law features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with Benford's Law features
        ⁄amountr   ⁄n⁄0È   T©⁄	normalizec                 S   s   g | ]}t †d d |  °ëqS )r4   )⁄np⁄log10)r   ⁄dr   r   r   r   c   r   zAStatisticalFeatures._extract_benford_features.<locals>.<listcomp>È
   )⁄indexÈ   Zbenford_chi_squareÈ   Zbenford_p_valueÈ   Zbenford_deviation_z)Error extracting Benford's Law features: N)r   r   ⁄abs⁄astyper-   ⁄replace⁄intr*   ⁄value_counts⁄
sort_index⁄pd⁄Series⁄range⁄get⁄stats⁄chi2⁄cdfr+   r(   r,   )r   r   r.   ⁄amountsZfirst_digitsZactual_distZbenford_distZ
chi_square⁄digitZexpected_count⁄actual_countZ
actual_pctZexpected_pctr/   r   r   r   r   J   s2    
$z-StatisticalFeatures._extract_benford_featuresc              
   C   sN  êz|† ° }d|jv rt|d }|†° }|†° }|dkrd|| | }||d< t†|°dk†t°|d< nd|d< d|d< d|jv êr<d|jv êr<|†d°d †° }|†d°d †° }g }	|†	° D ]^\}
}|d }|d }||v êr||v êr|| dkêr|||  ||  }nd}|	†
|° qº|	|d< t†|	°dk†t°|d< d	|jv êrd|jv êr|†d	°d †° }|†d	°d †° }g }|†	° D ]`\}
}|d	 }|d }||v êr‘||v êr‘|| dkêr‘|||  ||  }nd}|†
|° êqÑ||d
< t†|°dk†t°|d< |W S  têyH } z$t†dt|Éõ ù° |W  Y d}~S d}~0 0 dS )z»
        Extract Z-score based features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with Z-score features
        r1   r   Zamount_zscoreÈ   Zamount_zscore_outlier⁄	sender_idZsender_amount_zscoreZsender_amount_zscore_outlier⁄receiver_idZreceiver_amount_zscoreZreceiver_amount_zscore_outlierz#Error extracting Z-score features: N)r   r   ⁄mean⁄stdr7   r?   r@   rB   ⁄groupby⁄iterrows⁄appendr+   r(   r,   r-   )r   r   r.   rL   ⁄mean_amount⁄
std_amountZz_scoresZ
sender_avgZ
sender_stdZsender_zscores⁄_⁄rowrP   r1   Zz_scoreZreceiver_avgZreceiver_stdZreceiver_zscoresrQ   r/   r   r   r   r    }   sT    

""z,StatisticalFeatures._extract_zscore_featuresc              
   C   sz  êz4|† ° }d|jv r||d }|†° }t†|| °}|†° }|dkrld| | }||d< |dk†t°|d< nd|d< d|d< d|jv êrVd|jv êrV|†d°d †dd	Ñ °}|†d°d †° }	g }
|†	° D ]j\}}|d }|d }||v êr&||	v êr&|| dkêr&t||	|  É}d| ||  }nd}|
†
|° q |
|d
< t†|
°dk†t°|d< d|jv êr2d|jv êr2|†d°d †dd	Ñ °}|†d°d †° }g }|†	° D ]l\}}|d }|d }||v êr ||v êr || dkêr t|||  É}d| ||  }nd}|†
|° êq§||d< t†|°dk†t°|d< |W S  têyt } z$t†dt|Éõ ù° |W  Y d}~S d}~0 0 dS )z÷
        Extract Median Absolute Deviation (MAD) features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with MAD features
        r1   r   g/›$ÅïÂ?Zamount_mad_zscoreg      @Zamount_mad_outlierrP   c                 S   s   t †t †| | †°  °°S ©N©r7   ⁄medianr?   ©⁄xr   r   r   ⁄<lambda>Ò   r   z;StatisticalFeatures._extract_mad_features.<locals>.<lambda>Zsender_amount_mad_zscoreZsender_amount_mad_outlierrQ   c                 S   s   t †t †| | †°  °°S r[   r\   r^   r   r   r   r`     r   Zreceiver_amount_mad_zscoreZreceiver_amount_mad_outlierzError extracting MAD features: N)r   r   r]   r7   r?   r@   rB   rT   ⁄applyrU   rV   ⁄arrayr+   r(   r,   r-   )r   r   r.   rL   Zmedian_amountZabs_devZmadZmodified_z_scoresZ
sender_madZsender_medianZsender_mad_zscoresrY   rZ   rP   r1   Zmad_z_scoreZreceiver_madZreceiver_medianZreceiver_mad_zscoresrQ   r/   r   r   r   r!   Õ   sZ    

""z)StatisticalFeatures._extract_mad_featuresc              
   C   sB  ê z¸|† ° }d|jv rr|d }g d¢}t†||°}t|ÉD ]&\}}||| k†t°|d|õ dù< q:|jddç|d< d|jv r∂d|jv r∂|†d°d jddç}||d	< |d
k†t°|d< d|jv r˙d|jv r˙|†d°d jddç}	|	|d< |	d
k†t°|d< |W S  t	êy< }
 z$t
†dt|
Éõ ù° |W  Y d}
~
S d}
~
0 0 dS )zŒ
        Extract percentile-based features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with percentile features
        r1   )È   r:   È   È2   ÈK   ÈZ   È_   Èc   Zamount_above_Zth_percentileT)⁄pctZamount_percentile_rankrP   Zsender_amount_percentile_rankgffffffÓ?Zsender_amount_top_5pctrQ   Zreceiver_amount_percentile_rankZreceiver_amount_top_5pctz&Error extracting percentile features: N)r   r   r7   ⁄
percentile⁄	enumerater@   rB   ⁄rankrT   r+   r(   r,   r-   )r   r   r.   rL   ⁄percentilesZpercentile_values⁄i⁄pZsender_percentile_ranksZreceiver_percentile_ranksr/   r   r   r   r"   !  s*    

 z0StatisticalFeatures._extract_percentile_featuresc              
   C   s  êz¬|† ° }d|jv rÃ|d }t†|°|d< t†|°|d< t†|°\}}||d< t|Édkrrt†|°\}}||d< tj|d|†	° |†
° fdç\}}||d	< t†|°}|j|d
< t†|°\}	}
|	|d< |
|d< d|jv êrFd|jv êrF|†d°d †dddÑ fdddÑ fddddÑ fg°}dD ](}|d †|| °†d°|d|õ ù< êqd|jv êr¿d|jv êr¿|†d°d †dddÑ fdddÑ fddddÑ fg°}dD ](}|d †|| °†d°|d|õ ù< êqñ|W S  têy } z$t†dt|Éõ ù° |W  Y d}~S d}~0 0 dS ) z“
        Extract distribution-based features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with distribution features
        r1   Zamount_skewnessZamount_kurtosisZamount_normality_pià  Zamount_shapiro_p⁄norm)⁄argsZamount_ks_pZamount_ad_statisticZamount_jb_statisticZamount_jb_prP   ⁄skewnessc                 S   s   t | Édkrt†| °S dS ©NrO   r   ©r*   rI   ⁄skewr^   r   r   r   r`   É  r   zDStatisticalFeatures._extract_distribution_features.<locals>.<lambda>⁄kurtosisc                 S   s   t | Édkrt†| °S dS ©NÈ   r   ©r*   rI   rw   r^   r   r   r   r`   Ñ  r   )⁄variance⁄varrG   c                 S   s    t | Édkr| †° | †°  S dS ©Nr   ©r*   ⁄max⁄minr^   r   r   r   r`   Ü  r   )rs   rw   r{   rG   r   Zsender_amount_rQ   c                 S   s   t | Édkrt†| °S dS rt   ru   r^   r   r   r   r`   ê  r   c                 S   s   t | Édkrt†| °S dS rx   rz   r^   r   r   r   r`   ë  r   c                 S   s    t | Édkr| †° | †°  S dS r}   r~   r^   r   r   r   r`   ì  r   Zreceiver_amount_z(Error extracting distribution features: N)r   r   rI   rv   rw   Z
normaltestr*   ZshapiroZkstestrR   rS   ZandersonZ	statisticZjarque_berarT   ⁄agg⁄map⁄fillnar+   r(   r,   r-   )r   r   r.   rL   rY   Znormality_pZ	shapiro_pZks_pZ	ad_resultZjb_statZjb_pZsender_stats⁄statZreceiver_statsr/   r   r   r   r#   T  sP    

 




¸&


¸&z2StatisticalFeatures._extract_distribution_featuresc              
   C   sF  êz |† ° }|jtjgdçj†° }t|Édk r>t†d° |W S || †	d°}tj
|ddç}tj†|°dkrÇt†d° tj†|°}ntj†|°}tj|ddç}g }tt|ÉÉD ]}	|†t|j|	 ||É° q¨||d	< tjjd
t|Édç}
t†|°|
k†t°|d< |W S  têy@ } z$t†dt|Éõ ù° |W  Y d}~S d}~0 0 dS )z”
        Extract Mahalanobis distance features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with Mahalanobis features
        ©⁄includer<   z?Not enough numeric columns for Mahalanobis distance calculationr   F)⁄rowvarz3Covariance matrix is singular, using pseudo-inverse©⁄axisZmahalanobis_distanceg333333Ô?r   Zmahalanobis_outlierz'Error extracting Mahalanobis features: N)r   ⁄select_dtypesr7   ⁄numberr   ⁄tolistr*   r(   ⁄warningrÉ   ⁄cov⁄linalg⁄det⁄pinv⁄invrR   rG   rV   r   ⁄ilocrI   rJ   ⁄ppfrb   r@   rB   r+   r,   r-   )r   r   r.   ⁄numeric_cols⁄XZ
cov_matrixZinv_cov_matrixZmean_vectorZmahalanobis_distancesro   ⁄	thresholdr/   r   r   r   r$   †  s2    


ˇz1StatisticalFeatures._extract_mahalanobis_featuresc              
   C   s  z÷|† ° }d|jv r“|d j}t†|°}t†|°}|dkr¬t†|| °}t†|°}|| }||d< t|É}	t	j
†ddd|	   |	d °}
|	d |
 t†|	|	d |
d   ° }||k†t°|d< nd|d< d|d< |W S  têy } z$t†dt|Éõ ù° |W  Y d	}~S d	}~0 0 d	S )
zŸ
        Extract Grubbs' test features for outliers
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with Grubbs' test features
        r1   r   Zgrubbs_statisticr4   göôôôôôô?r<   Zgrubbs_outlierz(Error extracting Grubbs' test features: N)r   r   ⁄valuesr7   rR   rS   r?   r   r*   rI   ⁄trî   ⁄sqrtr@   rB   r+   r(   r,   r-   )r   r   r.   rL   rW   rX   Zabs_deviationsZmax_deviationZgrubbs_statr2   Z
t_critical⁄critical_valuer/   r   r   r   r%   ÿ  s*    





&z,StatisticalFeatures._extract_grubbs_featuresc              
      s   êz∫|† ° }|jddgdçj†° }|D ]h}|| jddç}tddÑ |D ÉÉ }|||õ dù< t†t|É°}|d	kr||| }nd	}|||õ d
ù< q&|jtj	gdçj†° }	|	D ]å}zzt
j|| dddç}
|
jddç}tddÑ |D ÉÉ }|||õ dù< t†t|É°}|d	kêr|| }nd	}|||õ dù< W q™   Y q™0 q™d|jv êrx|†d°d †á fddÑ°}|d †|°†d	°|d< d|jv êr∏|†d°d †á fddÑ°}|d †|°†d	°|d< |W S  têy˙ } z$t†dt|Éõ ù° |W  Y d}~S d}~0 0 dS )z»
        Extract entropy-based features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with entropy features
        ⁄object⁄categoryrÖ   Tr5   c                 s   s$   | ]}|d kr|t †|° V  qdS ©r   N©r7   ⁄log2©r   rp   r   r   r   ⁄	<genexpr>  r   z@StatisticalFeatures._extract_entropy_features.<locals>.<genexpr>Z_entropyr   Z_normalized_entropyr:   ⁄drop©⁄bins⁄
duplicatesc                 s   s$   | ]}|d kr|t †|° V  qdS rû   rü   r°   r   r   r   r¢   3  r   Z_binned_entropyZ_binned_normalized_entropyrP   r1   c                    s
   à † | °S r[   ©⁄_calculate_series_entropyr^   ©r   r   r   r`   D  r   z?StatisticalFeatures._extract_entropy_features.<locals>.<lambda>Zsender_amount_entropyrQ   c                    s
   à † | °S r[   rß   r^   r©   r   r   r`   K  r   Zreceiver_amount_entropyz#Error extracting entropy features: N)r   rä   r   rå   rC   ⁄sumr7   r†   r*   rã   rE   ⁄cutrT   ra   rÇ   rÉ   r+   r(   r,   r-   )r   r   r.   Zcategorical_colsr   rC   ⁄entropyZmax_entropyZnormalized_entropyrï   ⁄binnedZsender_entropyZreceiver_entropyr/   r   r©   r   r&     sP    




ˇ
ˇz-StatisticalFeatures._extract_entropy_featuresc                 C   sT   z@t j|tdt|ÉÉddç}|jddç}tddÑ |D ÉÉ }|W S    Y dS 0 d	S )
z∂
        Calculate entropy of a pandas Series
        
        Args:
            series (Series): Input series
            
        Returns:
            float: Entropy value
        r:   r£   r§   Tr5   c                 s   s$   | ]}|d kr|t †|° V  qdS rû   rü   r°   r   r   r   r¢   g  r   z@StatisticalFeatures._calculate_series_entropy.<locals>.<genexpr>r   N)rE   r´   rÄ   r*   rC   r™   )r   ⁄seriesr≠   rC   r¨   r   r   r   r®   U  s    
z-StatisticalFeatures._calculate_series_entropyc              
   C   s   êzÑ|† ° }|jtjgdçj†° }t|Édk r>t†d° |W S || †	° †
° }i }|D ]}|| †|°}|†° ||< qV|D ]}|| ||õ dù< qzi }|D ]}|| †|°}|†° ||< qö|D ]}|| ||õ dù< qæt|ÉD ]¢\}	}|| † ° }
|
| }|
j|ddç}
z\dd	lm} |É }|†|
|° |†|
|°}|dk êrLdd|  }ntd
É}|||õ dù< W qﬁ   d||õ dù< Y qﬁ0 qﬁ|W S  têyƒ } z$t†dt|Éõ ù° |W  Y d}~S d}~0 0 dS )z–
        Extract correlation-based features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with correlation features
        rÖ   r<   z6Not enough numeric columns for correlation calculationZ_max_correlationZ_avg_correlationr4   rà   r   )⁄LinearRegression⁄infZ_vifz'Error extracting correlation features: N)r   rä   r7   rã   r   rå   r*   r(   rç   ⁄corrr?   r£   r   rR   rl   Zsklearn.linear_modelrØ   ⁄fit⁄score⁄floatr+   r,   r-   )r   r   r.   rï   Zcorr_matrixZmax_corrr   ZcorrsZavg_corrro   rñ   ⁄yrØ   ⁄modelZ	r_squaredZvifr/   r   r   r   r'   l  sL    


z1StatisticalFeatures._extract_correlation_featuresc                    s∏   | † à °}á fddÑ|jD É}t|Édkr¥| j†|| ° t|ÉdkrÆttdt|ÉÉdç| _| j†|| ° | j†|| °}t	|j
d ÉD ]"}|ddÖ|f |d|d õ ù< qäd	| _|S )
z‘
        Fit the feature extractor and transform the data
        
        Args:
            df (DataFrame): Input data
            
        Returns:
            DataFrame: Transformed data with features
        c                    s   g | ]}|à j vr|ëqS r   r   r   r   r   r   r   «  r   z5StatisticalFeatures.fit_transform.<locals>.<listcomp>r   r:   )⁄n_componentsr4   N⁄	stat_pca_T)r0   r   r*   r   r≤   r   rÄ   r   ⁄	transformrG   ⁄shaper   ©r   r   r.   Zfeature_colsZpca_componentsro   r   r   r   ⁄fit_transformπ  s    
 z!StatisticalFeatures.fit_transformc                    sú   | j stdÉÇ| †à °}á fddÑ|jD É}t|Édkrò| j†|| °||< | jdurò| j†|| °}t|j	d ÉD ]"}|ddÖ|f |d|d õ ù< qt|S )z’
        Transform new data using fitted feature extractor
        
        Args:
            df (DataFrame): Input data
            
        Returns:
            DataFrame: Transformed data with features
        z7Feature extractor not fitted. Call fit_transform first.c                    s   g | ]}|à j vr|ëqS r   r   r   r   r   r   r   Ï  r   z1StatisticalFeatures.transform.<locals>.<listcomp>r   Nr4   r∏   )
r   ⁄
ValueErrorr0   r   r*   r   rπ   r   rG   r∫   rª   r   r   r   rπ   €  s    


 zStatisticalFeatures.transform)N)⁄__name__⁄
__module__⁄__qualname__⁄__doc__r   r0   r   r    r!   r"   r#   r$   r%   r&   r®   r'   rº   rπ   r   r   r   r   r      s   
#3PT3L8/NM"r   )r¡   ⁄pandasrE   ⁄numpyr7   ⁄scipy.statsrI   Zscipy.spatial.distancer   Zsklearn.preprocessingr   Zsklearn.decompositionr   ⁄warnings⁄logging⁄typingr   r   r   r   ⁄filterwarnings⁄basicConfig⁄INFO⁄	getLoggerræ   r(   r   r   r   r   r   ⁄<module>   s   



=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\features\__pycache__\timeseries_features.cpython-39.pyc ===
a
    èöhµ|  „                   @   s§   d Z ddlZddlZddlmZ ddlmZ ddl	m
Z
 ddlmZmZ ddlZddlZddlmZmZmZmZ e†d° ejejd	ç e†e°ZG d
dÑ dÉZdS )zf
Time Series Features Module
Implements time series feature extraction techniques for fraud detection
È    N)⁄stats)⁄
find_peaks)⁄seasonal_decompose)⁄acf⁄pacf)⁄Dict⁄List⁄Tuple⁄Union⁄ignore)⁄levelc                   @   sb   e Zd ZdZdddÑZddÑ ZddÑ Zd	d
Ñ ZddÑ ZddÑ Z	ddÑ Z
ddÑ ZddÑ ZddÑ ZdS )⁄TimeSeriesFeatureszê
    Class for extracting time series features from transaction data
    Implements techniques like burstiness analysis, gap analysis, etc.
    Nc                 C   s   |pi | _ g | _d| _dS )zÑ
        Initialize TimeSeriesFeatures
        
        Args:
            config (dict, optional): Configuration parameters
        FN)⁄config⁄feature_names⁄fitted)⁄selfr   © r   ˙rC:\Users\Tranquil Abyss\fraud-detection-platform\app\..\src\fraud_detection_engine\features\timeseries_features.py⁄__init__   s    
zTimeSeriesFeatures.__init__c              
      sÊ   z¶à † ° }dà jv r8tjj†à d °s8t†à d °|d< | †|°}| †|°}| †	|°}| †
|°}| †|°}| †|°}á fddÑ|jD É| _t†dt| jÉõ dù° |W S  ty‡ } z"t†dt|Éõ ù° Ç W Y d}~n
d}~0 0 dS )zÊ
        Extract all time series features from the dataframe
        
        Args:
            df (DataFrame): Input transaction data
            
        Returns:
            DataFrame: DataFrame with extracted features
        ⁄	timestampc                    s   g | ]}|à j vr|ëqS r   ©⁄columns©⁄.0⁄col©⁄dfr   r   ⁄
<listcomp>@   Û    z7TimeSeriesFeatures.extract_features.<locals>.<listcomp>z
Extracted z time series featuresz'Error extracting time series features: N)⁄copyr   ⁄pd⁄api⁄types⁄is_datetime64_any_dtype⁄to_datetime⁄_extract_temporal_features⁄_extract_frequency_features⁄_extract_burstiness_features⁄_extract_gap_features⁄_extract_seasonal_features⁄!_extract_autocorrelation_featuresr   ⁄logger⁄info⁄len⁄	Exception⁄error⁄str©r   r   ⁄	result_df⁄er   r   r   ⁄extract_features%   s     






z#TimeSeriesFeatures.extract_featuresc              
   C   sB  êz¸|† ° }d|jvr&t†d° |W S |d jj|d< |d jj|d< |d jj|d< |d jj|d< |d jj	|d< |d jj
|d< |d j†° j|d	< |d jj|d
< |d dk†t°|d< |d jj†t°|d< |d jj†t°|d< |d jj†t°|d< |d jj†t°|d< |d jj†t°|d< |d jj†t°|d< |d dk|d dk B †t°|d< |d dk|d dk @ †t°|d< |d dk|d dk @ †t°|d< |d dk|d dk @ †t°|d< |d dk|d dk @ |d dk @ †t°|d< |W S  têy< } z$t†dt|Éõ ù° |W  Y d}~S d}~0 0 dS ) zƒ
        Extract temporal features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with temporal features
        r   z7Timestamp column not found. Skipping temporal features.⁄hour⁄day⁄month⁄year⁄	dayofweek⁄	dayofyear⁄
weekofyear⁄quarterÈ   Z
is_weekend⁄is_month_start⁄is_month_end⁄is_quarter_start⁄is_quarter_end⁄is_year_start⁄is_year_endÈ   È   Zis_nightÈ   Z
is_morningÈ   Zis_afternoonZ
is_eveningÈ	   È   Zis_business_hoursz$Error extracting temporal features: N)r   r   r+   ⁄warning⁄dtr5   r6   r7   r8   r9   r:   ⁄isocalendar⁄weekr<   ⁄astype⁄intr>   r?   r@   rA   rB   rC   r.   r/   r0   r1   r   r   r   r%   I   sH    


""""

ˇ
˛ˇ¸z-TimeSeriesFeatures._extract_temporal_featuresc              
   C   s&  êz‡|† ° }d|jvr&t†d° |W S |†d°}g d¢}|D ]p}g }|†° D ]P\}}|d }	|	t†|° }
|	}||d |
k|d |k @  }t|É}|†	|° qL||d|õ ù< q<d|jv êrDdD ]Ñ}g }|†° D ]d\}}|d }|d }	|	t†|° }
|	}||d |
k|d |k @ |d |k@  }t|É}|†	|° qŒ||d|õ ù< qæd|jv êrﬁdD ]à}g }|†° D ]f\}}|d }|d }	|	t†|° }
|	}||d |
k|d |k @ |d |k@  }t|É}|†	|° êqd||d	|õ ù< êqT|W S  t
êy  } z$t†d
t|Éõ ù° |W  Y d}~S d}~0 0 dS )zÃ
        Extract frequency-based features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with frequency features
        r   z8Timestamp column not found. Skipping frequency features.)⁄1H⁄6H⁄24H⁄7DZ30DZtransaction_frequency_⁄	sender_id)rP   rQ   rR   Zsender_frequency_⁄receiver_idZreceiver_frequency_z%Error extracting frequency features: N)r   r   r+   rJ   ⁄sort_values⁄iterrowsr    ⁄	Timedeltar-   ⁄appendr.   r/   r0   )r   r   r2   ⁄	df_sorted⁄time_windows⁄windowZwindow_counts⁄_⁄rowr   ⁄window_start⁄
window_end⁄window_transactions⁄countZsender_window_counts⁄senderZreceiver_window_counts⁄receiverr3   r   r   r   r&   Ä   sz    





ˇˇ

ˇ
˛ˇ

ˇ
˛ˇz.TimeSeriesFeatures._extract_frequency_featuresc              
   C   st  êz.|† ° }d|jvr&t†d° |W S |†d°}|d †° j†° †d°}t	|ÉdkrÇ|†
° dkrÇ|†
° |†°  |†
° |†°   }nd}||d< g d¢}|D ]Æ}g }tt	|ÉÉD ]ä}	td|	|d  É}
tt	|É|	|d  d É}|j|
|Ö }t	|Édkêr*|†
° dkêr*|†
° |†°  |†
° |†°   }nd}|†|° qÆ||d|õ ù< qö|†° }|†
° }td|d|  É}||k †t°|d	< g }d}|d	 D ]$}|êr¢|d7 }n|†|° d}êqé|†|° g }d}t|d	 ÉD ]P\}	}|êr|†|| ° n0|†d° |	dkêr“|d	 j|	d  ês“|d7 }êq“||d
< |W S  têyn } z$t†dt|Éõ ù° |W  Y d}~S d}~0 0 dS )z—
        Extract burstiness analysis features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with burstiness features
        r   z9Timestamp column not found. Skipping burstiness features.r   È   Zburstiness_coefficient)È
   È2   Èd   È   Zlocal_burstiness_Zis_in_burstZburst_durationz&Error extracting burstiness features: N)r   r   r+   rJ   rV   ⁄diffrK   ⁄total_seconds⁄fillnar-   ⁄std⁄mean⁄range⁄max⁄min⁄ilocrY   rN   rO   ⁄	enumerater.   r/   r0   )r   r   r2   rZ   ⁄inter_timesZ
burstinessZwindow_sizes⁄window_sizeZlocal_burstiness⁄i⁄	start_idx⁄end_idxZwindow_inter_timesZlocal_bZavg_inter_timeZstd_inter_timeZburst_thresholdZburst_durationsZcurrent_durationZis_burstZburst_duration_map⁄idxr3   r   r   r   r'   Â   sd    



"ˇ



z/TimeSeriesFeatures._extract_burstiness_featuresc              
   C   s¯  êz≤|† ° }d|jvr&t†d° |W S |†d°}|d †° j†° †d°}||d< |d †d°j†° †	° †d°}||d< t
|ÉdkrÃ|†° dkrÃ|†° d|†°   }||k†t°|d	< t†||k|d°|d
< nd|d	< d|d
< d|jv êrDg }i }|d †° D ]T}	||d |	k †d°}
t
|
Édkr¸|
d †° j†° †d°}|†° |†° dú||	< q¸|†° D ]∆\}}|d }	|	|v êr
||	 }||d |	k|d |d k @  }
t
|
Édkêr»|
d †° }|d | †° }ntdÉ}|d dkêrÙ||d  |d  }nd}|†||dú° n|†tdÉddú° êqZt†|°}|d |d< |d |d< d|jv êr∞g }i }|d †° D ]X}||d |k †d°}t
|Édkêrd|d †° j†° †d°}|†° |†° dú||< êqd|†° D ]∆\}}|d }||v êrv|| }||d |k|d |d k @  }t
|Édkêr4|d †° }|d | †° }ntdÉ}|d dkêr`||d  |d  }nd}|†||dú° n|†tdÉddú° êq∆t†|°}|d |d< |d |d< |W S  têyÚ } z$t†dt|Éõ ù° |W  Y d}~S d}~0 0 dS )z√
        Extract gap analysis features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with gap features
        r   z2Timestamp column not found. Skipping gap features.r   Ztime_since_last_transactionÈˇˇˇˇZtime_until_next_transactionre   ri   Zis_after_gap⁄gap_sizerT   )rn   rm   ⁄infrm   rn   )⁄sender_time_since_last⁄sender_gap_z_scorer}   r~   rU   )⁄receiver_time_since_last⁄receiver_gap_z_scorer   rÄ   zError extracting gap features: N)r   r   r+   rJ   rV   rj   rK   rk   rl   ⁄absr-   rm   rn   rN   rO   ⁄np⁄where⁄uniquerW   rp   ⁄floatrY   r    ⁄	DataFramer.   r/   r0   )r   r   r2   rZ   rt   Ztime_until_nextZgap_thresholdZsender_gapsZsender_gap_statsrc   Zsender_transactionsZsender_inter_timesr]   r^   r   Zlast_sender_timeZsender_inter_timeZgap_z_scoreZgap_dfZreceiver_gapsZreceiver_gap_statsrd   Zreceiver_transactionsZreceiver_inter_timesZlast_receiver_timeZreceiver_inter_timer3   r   r   r   r(   I  sº    



˛

ˇˇ˛
˛
˛

ˇˇ˛
˛
z(TimeSeriesFeatures._extract_gap_featuresc           "      C   sH  êz|† ° }d|jvr&t†d° |W S |†d°}g d¢}|D ê]¥}|†d°†|°†° }t|Édkr<êzLt	|dt
dt|Éd Édç}|j}|j}	|j}
i }i }i }|d D ]V}|†|°}||jv rÊ|| ||< |	| ||< |
| ||< q®d	||< d	||< d	||< q®|d †|°†d	°|d
|õ ù< |d †|°†d	°|d|õ ù< |d †|°†d	°|d|õ ù< |
†° }|
†° }|d	kêr†|d †|°| | }|†d	°|d|õ ù< nd	|d|õ ù< W q< têy } z&t†d|õ dt|Éõ ù° W Y d}~q<d}~0 0 q<i }d|jv êrn|†d°†° }||†°  }tddÑ |D ÉÉ }t†d°}|d	kêrN|| nd	}d| |d< |†° }||d< d|jv êr‰|†d°†° }||†°  }tddÑ |D ÉÉ }t†d°}|d	kêrƒ|| nd	}d| |d< |†° }||d< |†° D ]\} }!|!|| < êqÏ|W S  têyB } z$t†dt|Éõ ù° |W  Y d}~S d}~0 0 dS )zƒ
        Extract seasonal features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with seasonal features
        r   z7Timestamp column not found. Skipping seasonal features.)rP   rQ   ⁄1DZ1Wrf   ⁄additiveÈ   ri   )⁄model⁄periodr   Ztrend_Z	seasonal_Z	residual_Zseasonal_anomaly_z$Error in seasonal decomposition for ˙: Nr5   c                 s   s$   | ]}|d kr|t †|° V  qdS ©r   N©rÇ   ⁄log2©r   ⁄pr   r   r   ⁄	<genexpr><  r   z@TimeSeriesFeatures._extract_seasonal_features.<locals>.<genexpr>re   Zdaily_pattern_strength⁄	peak_hourr9   c                 s   s$   | ]}|d kr|t †|° V  qdS rç   ré   rê   r   r   r   rí   K  r   È   Zweekly_pattern_strength⁄peak_dayz$Error extracting seasonal features: )r   r   r+   rJ   rV   ⁄	set_index⁄resample⁄sizer-   r   rq   ⁄trend⁄seasonal⁄resid⁄floor⁄index⁄maprl   rn   rm   r.   r0   ⁄groupby⁄sumrÇ   rè   ⁄idxmax⁄itemsr/   )"r   r   r2   rZ   ⁄frequencies⁄freq⁄ts⁄decompositionrô   rö   ⁄residualZ	trend_mapZseasonal_mapZresidual_mapr   Zperiod_startZresidual_meanZresidual_stdZanomaly_scoresr3   Zperiodicity_featuresZhourly_countsZhourly_probsZdaily_entropy⁄max_entropyZdaily_uniformityrì   Zweekly_countsZweekly_probsZweekly_entropyZweekly_uniformityrï   ⁄feature⁄valuer   r   r   r)   È  s~    








2

z-TimeSeriesFeatures._extract_seasonal_featuresc                 C   sÙ  êzÆ|† ° }d|jvr&t†d° |W S |†d°}g d¢}|D ê]l}|†d°†|°†° }t|Édkr<êzt	dt|Éd É}t
||ddç}tdt	d	t|ÉÉÉD ]}	||	 |d
|õ d|	õ ù< qöt||dç}
tdt	d	t|
ÉÉÉD ]}	|
|	 |d|õ d|	õ ù< qÿt|ddÖ ddç\}}t|ÉdkêrJ|d d }||d|õ ù< || |d|õ ù< nd|d|õ ù< d|d|õ ù< W q< têy® } z&t†d|õ dt|Éõ ù° W Y d}~q<d}~0 0 q<|W S  têyÓ } z$t†dt|Éõ ù° |W  Y d}~S d}~0 0 dS )z“
        Extract autocorrelation features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with autocorrelation features
        r   z>Timestamp column not found. Skipping autocorrelation features.)rP   rQ   rá   rf   ri   T)⁄nlags⁄fftre   rE   Z	autocorr_Z_lag_)r´   Zpacf_Ngöôôôôô…?)⁄heightr   Zperiodicity_Zperiodicity_strength_z&Error in autocorrelation analysis for rå   z+Error extracting autocorrelation features: )r   r   r+   rJ   rV   rñ   ró   rò   r-   rq   r   ro   r   r   r.   r0   r/   )r   r   r2   rZ   r£   r§   r•   r´   ⁄autocorrrv   Zpacf_valuesZpeaksr]   Zpeak_lagr3   r   r   r   r*   _  s>    




2z4TimeSeriesFeatures._extract_autocorrelation_featuresc                    s4   | † à °}á fddÑ|jD É}t|Édkr0d| _|S )z‘
        Fit the feature extractor and transform the data
        
        Args:
            df (DataFrame): Input data
            
        Returns:
            DataFrame: Transformed data with features
        c                    s   g | ]}|à j vr|ëqS r   r   r   r   r   r   r   ∞  r   z4TimeSeriesFeatures.fit_transform.<locals>.<listcomp>r   T)r4   r   r-   r   )r   r   r2   ⁄feature_colsr   r   r   ⁄fit_transform¢  s
    
z TimeSeriesFeatures.fit_transformc                 C   s   | j stdÉÇ| †|°}|S )z’
        Transform new data using fitted feature extractor
        
        Args:
            df (DataFrame): Input data
            
        Returns:
            DataFrame: Transformed data with features
        z7Feature extractor not fitted. Call fit_transform first.)r   ⁄
ValueErrorr4   )r   r   r2   r   r   r   ⁄	transform∑  s    

zTimeSeriesFeatures.transform)N)⁄__name__⁄
__module__⁄__qualname__⁄__doc__r   r4   r%   r&   r'   r(   r)   r*   r∞   r≤   r   r   r   r   r      s   
$7ed !vCr   )r∂   ⁄pandasr    ⁄numpyrÇ   ⁄scipyr   Zscipy.signalr   Zstatsmodels.tsa.seasonalr   Zstatsmodels.tsa.stattoolsr   r   ⁄warnings⁄logging⁄typingr   r   r	   r
   ⁄filterwarnings⁄basicConfig⁄INFO⁄	getLoggerr≥   r+   r   r   r   r   r   ⁄<module>   s   



=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\features\__pycache__\__init__.cpython-313.pyc ===
Û
    dÄöh    „                   Û   ï g )N© r   Û    ⁄gC:\Users\Tranquil Abyss\fraud-detection-platform\app\..\src\fraud_detection_engine\features\__init__.py⁄<module>r      s   Òr   

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\features\__pycache__\__init__.cpython-39.pyc ===
a
    èöh    „                   @   s   d S )N© r   r   r   ˙gC:\Users\Tranquil Abyss\fraud-detection-platform\app\..\src\fraud_detection_engine\features\__init__.py⁄<module>   Û    

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\ingestion\column_mapper.py ===
"""
Column Mapper Module
Handles intelligent mapping of user columns to expected format
"""
import pandas as pd
import numpy as np
import re
import logging
from collections import defaultdict
from difflib import SequenceMatcher
import yaml
import os
# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ColumnMapper:
    """
    Class for mapping user columns to expected format
    Uses AI-powered techniques to intelligently match columns
    """
    
    def __init__(self, config_path=None):
        """
        Initialize ColumnMapper
        
        Args:
            config_path (str, optional): Path to configuration file
        """
        self.expected_columns = self._get_expected_columns()
        self.synonyms = self._load_synonyms(config_path)
        self.patterns = self._load_patterns(config_path)
        self.mapping_history = []

    def get_expected_columns(self):
        """
        Return the list of expected column names
        
        Returns:
            list: Expected column names
        """
        return list(self.expected_columns.keys())

    
    def _get_expected_columns(self):
        """
        Get the expected columns for the fraud detection system
        
        Returns:
            dict: Expected columns with descriptions
        """
        return {
            'transaction_id': {
                'description': 'Unique identifier for the transaction',
                'data_type': 'string',
                'required': True
            },
            'timestamp': {
                'description': 'Date and time of the transaction',
                'data_type': 'datetime',
                'required': True
            },
            'amount': {
                'description': 'Transaction amount',
                'data_type': 'float',
                'required': True
            },
            'currency': {
                'description': 'Currency code (e.g., USD, EUR)',
                'data_type': 'string',
                'required': False
            },
            'sender_id': {
                'description': 'Identifier of the sender',
                'data_type': 'string',
                'required': True
            },
            'receiver_id': {
                'description': 'Identifier of the receiver',
                'data_type': 'string',
                'required': True
            },
            'sender_account_type': {
                'description': 'Type of sender account (e.g., personal, business)',
                'data_type': 'string',
                'required': False
            },
            'receiver_account_type': {
                'description': 'Type of receiver account (e.g., personal, business)',
                'data_type': 'string',
                'required': False
            },
            'sender_bank': {
                'description': 'Name of sender bank',
                'data_type': 'string',
                'required': False
            },
            'receiver_bank': {
                'description': 'Name of receiver bank',
                'data_type': 'string',
                'required': False
            },
            'sender_location': {
                'description': 'Location of sender (country, state, city)',
                'data_type': 'string',
                'required': False
            },
            'receiver_location': {
                'description': 'Location of receiver (country, state, city)',
                'data_type': 'string',
                'required': False
            },
            'transaction_type': {
                'description': 'Type of transaction (e.g., transfer, payment)',
                'data_type': 'string',
                'required': False
            },
            'transaction_category': {
                'description': 'Category of transaction (e.g., retail, services)',
                'data_type': 'string',
                'required': False
            },
            'merchant_id': {
                'description': 'Identifier of the merchant',
                'data_type': 'string',
                'required': False
            },
            'merchant_category': {
                'description': 'Category of the merchant',
                'data_type': 'string',
                'required': False
            },
            'ip_address': {
                'description': 'IP address used for the transaction',
                'data_type': 'string',
                'required': False
            },
            'device_id': {
                'description': 'Identifier of the device used',
                'data_type': 'string',
                'required': False
            },
            'description': {
                'description': 'Description of the transaction',
                'data_type': 'string',
                'required': False
            },
            'notes': {
                'description': 'Additional notes',
                'data_type': 'string',
                'required': False
            },
            'authorization_status': {
                'description': 'Authorization status (e.g., approved, declined)',
                'data_type': 'string',
                'required': False
            },
            'chargeback_flag': {
                'description': 'Whether the transaction was charged back',
                'data_type': 'boolean',
                'required': False
            },
            'fraud_flag': {
                'description': 'Whether the transaction is fraudulent (for supervised learning)',
                'data_type': 'boolean',
                'required': False
            }
        }
    
    def _load_synonyms(self, config_path=None):
        """
        Load synonyms for column names
        
        Args:
            config_path (str, optional): Path to configuration file
            
        Returns:
            dict: Synonyms for expected columns
        """
        # Default synonyms
        synonyms = {
            'transaction_id': ['id', 'transaction_id', 'tx_id', 'trans_id', 'reference', 'ref_no', 'transaction_no'],
            'timestamp': ['timestamp', 'date', 'time', 'datetime', 'trans_date', 'trans_time', 'transaction_date', 'transaction_time'],
            'amount': ['amount', 'value', 'sum', 'total', 'transaction_amount', 'amt', 'tx_amount'],
            'currency': ['currency', 'curr', 'ccy', 'currency_code'],
            'sender_id': ['sender_id', 'from_id', 'payer_id', 'source_id', 'originator_id'],
            'receiver_id': ['receiver_id', 'to_id', 'payee_id', 'destination_id', 'beneficiary_id'],
            'sender_account_type': ['sender_account_type', 'from_account_type', 'payer_account_type', 'source_account_type'],
            'receiver_account_type': ['receiver_account_type', 'to_account_type', 'payee_account_type', 'destination_account_type'],
            'sender_bank': ['sender_bank', 'from_bank', 'payer_bank', 'source_bank'],
            'receiver_bank': ['receiver_bank', 'to_bank', 'payee_bank', 'destination_bank'],
            'sender_location': ['sender_location', 'from_location', 'payer_location', 'source_location', 'sender_country', 'from_country'],
            'receiver_location': ['receiver_location', 'to_location', 'payee_location', 'destination_location', 'receiver_country', 'to_country'],
            'transaction_type': ['transaction_type', 'trans_type', 'type', 'tx_type'],
            'transaction_category': ['transaction_category', 'trans_category', 'category', 'tx_category'],
            'merchant_id': ['merchant_id', 'merchant', 'retailer_id', 'vendor_id'],
            'merchant_category': ['merchant_category', 'merchant_type', 'retailer_category', 'vendor_category'],
            'ip_address': ['ip_address', 'ip', 'ip_addr'],
            'device_id': ['device_id', 'device', 'device_identifier'],
            'description': ['description', 'desc', 'details', 'narrative'],
            'notes': ['notes', 'note', 'comments', 'remark'],
            'authorization_status': ['authorization_status', 'auth_status', 'status', 'approval_status'],
            'chargeback_flag': ['chargeback_flag', 'chargeback', 'is_chargeback'],
            'fraud_flag': ['fraud_flag', 'fraud', 'is_fraud', 'fraudulent']
        }
        
        # Try to load from config file
        if config_path and os.path.exists(config_path):
            try:
                with open(config_path, 'r') as f:
                    config = yaml.safe_load(f)
                    if 'column_synonyms' in config:
                        # Update with config synonyms
                        for col, syn_list in config['column_synonyms'].items():
                            if col in synonyms:
                                synonyms[col].extend(syn_list)
                            else:
                                synonyms[col] = syn_list
            except Exception as e:
                logger.warning(f"Error loading synonyms from config: {str(e)}")
        
        return synonyms
    
    def _load_patterns(self, config_path=None):
        """
        Load regex patterns for column names
        
        Args:
            config_path (str, optional): Path to configuration file
            
        Returns:
            dict: Regex patterns for expected columns
        """
        # Default patterns
        patterns = {
            'transaction_id': [r'transaction.?id', r'tx.?id', r'trans.?id', r'reference', r'ref.?(no|num)'],
            'timestamp': [r'timestamp', r'date.?time', r'trans.?date', r'trans.?time'],
            'amount': [r'amount', r'value', r'sum', r'total'],
            'currency': [r'currency', r'ccy', r'curr'],
            'sender_id': [r'sender.?id', r'from.?id', r'payer.?id', r'source.?id', r'originator.?id'],
            'receiver_id': [r'receiver.?id', r'to.?id', r'payee.?id', r'destination.?id', r'beneficiary.?id'],
            'sender_account_type': [r'sender.?account.?type', r'from.?account.?type', r'payer.?account.?type'],
            'receiver_account_type': [r'receiver.?account.?type', r'to.?account.?type', r'payee.?account.?type'],
            'sender_bank': [r'sender.?bank', r'from.?bank', r'payer.?bank', r'source.?bank'],
            'receiver_bank': [r'receiver.?bank', r'to.?bank', r'payee.?bank', r'destination.?bank'],
            'sender_location': [r'sender.?location', r'from.?location', r'payer.?location', r'source.?location', r'sender.?country'],
            'receiver_location': [r'receiver.?location', r'to.?location', r'payee.?location', r'destination.?location', r'receiver.?country'],
            'transaction_type': [r'transaction.?type', r'trans.?type', r'tx.?type'],
            'transaction_category': [r'transaction.?category', r'trans.?category', r'tx.?category'],
            'merchant_id': [r'merchant.?id', r'merchant', r'retailer.?id', r'vendor.?id'],
            'merchant_category': [r'merchant.?category', r'merchant.?type', r'retailer.?category'],
            'ip_address': [r'ip.?address', r'ip'],
            'device_id': [r'device.?id', r'device'],
            'description': [r'description', r'desc', r'details', r'narrative'],
            'notes': [r'notes?', r'comments?', r'remarks?'],
            'authorization_status': [r'authorization.?status', r'auth.?status', r'approval.?status'],
            'chargeback_flag': [r'chargeback.?(flag|is)'],
            'fraud_flag': [r'fraud.?(flag|is)', r'is.?fraud']
        }
        
        # Try to load from config file
        if config_path and os.path.exists(config_path):
            try:
                with open(config_path, 'r') as f:
                    config = yaml.safe_load(f)
                    if 'column_patterns' in config:
                        # Update with config patterns
                        for col, pat_list in config['column_patterns'].items():
                            if col in patterns:
                                patterns[col].extend(pat_list)
                            else:
                                patterns[col] = pat_list
            except Exception as e:
                logger.warning(f"Error loading patterns from config: {str(e)}")
        
        return patterns
    
    def auto_map_columns(self, user_columns, expected_columns=None):
        """
        Automatically map user columns to expected columns
        
        Args:
            user_columns (list): List of user column names
            expected_columns (list, optional): List of expected column names
            
        Returns:
            dict: Mapping from user columns to expected columns
        """
        if expected_columns is None:
            expected_columns = list(self.expected_columns.keys())
        
        mapping = {}
        used_columns = set()
        
        # First, try exact matches
        for user_col in user_columns:
            user_col_clean = self._clean_column_name(user_col)
            if user_col_clean in expected_columns and user_col_clean not in used_columns:
                mapping[user_col] = user_col_clean
                used_columns.add(user_col_clean)
        
        # Then, try synonym matches
        for user_col in user_columns:
            if user_col in mapping:
                continue
                
            user_col_clean = self._clean_column_name(user_col)
            
            for expected_col in expected_columns:
                if expected_col in used_columns:
                    continue
                    
                if user_col_clean in self.synonyms.get(expected_col, []):
                    mapping[user_col] = expected_col
                    used_columns.add(expected_col)
                    break
        
        # Then, try pattern matches
        for user_col in user_columns:
            if user_col in mapping:
                continue
                
            user_col_clean = self._clean_column_name(user_col)
            
            for expected_col in expected_columns:
                if expected_col in used_columns:
                    continue
                    
                for pattern in self.patterns.get(expected_col, []):
                    if re.search(pattern, user_col_clean, re.IGNORECASE):
                        mapping[user_col] = expected_col
                        used_columns.add(expected_col)
                        break
                else:
                    continue
                break
        
        # Finally, try fuzzy matching for remaining columns
        for user_col in user_columns:
            if user_col in mapping:
                continue
                
            user_col_clean = self._clean_column_name(user_col)
            
            best_match = None
            best_score = 0
            
            for expected_col in expected_columns:
                if expected_col in used_columns:
                    continue
                    
                # Calculate similarity score
                score = self._calculate_similarity(user_col_clean, expected_col)
                
                if score > best_score and score > 0.6:  # Threshold for fuzzy matching
                    best_score = score
                    best_match = expected_col
            
            if best_match:
                mapping[user_col] = best_match
                used_columns.add(best_match)
        
        # Store mapping history
        self.mapping_history.append({
            'timestamp': pd.Timestamp.now(),
            'user_columns': user_columns,
            'mapping': mapping
        })
        
        return mapping
    
    def _clean_column_name(self, column_name):
        """
        Clean column name for matching
        
        Args:
            column_name (str): Column name to clean
            
        Returns:
            str: Cleaned column name
        """
        # Convert to lowercase
        cleaned = column_name.lower()
        
        # Remove special characters and spaces
        cleaned = re.sub(r'[^a-z0-9]', '_', cleaned)
        
        # Remove consecutive underscores
        cleaned = re.sub(r'_+', '_', cleaned)
        
        # Remove leading and trailing underscores
        cleaned = cleaned.strip('_')
        
        return cleaned
    
    def _calculate_similarity(self, str1, str2):
        """
        Calculate similarity between two strings
        
        Args:
            str1 (str): First string
            str2 (str): Second string
            
        Returns:
            float: Similarity score (0-1)
        """
        # Use SequenceMatcher for similarity
        return SequenceMatcher(None, str1, str2).ratio()
    
    def apply_mapping(self, df, mapping):
        """
        Apply column mapping to a DataFrame
        
        Args:
            df (DataFrame): Input DataFrame
            mapping (dict): Column mapping
            
        Returns:
            DataFrame: DataFrame with mapped columns
        """
        try:
            # Create a copy of the DataFrame
            result_df = df.copy()
            
            # Create a new DataFrame with mapped columns
            mapped_df = pd.DataFrame()
            
            # Map columns
            for user_col, expected_col in mapping.items():
                if user_col in df.columns:
                    mapped_df[expected_col] = df[user_col]
            
            # Add unmapped columns with original names
            for col in df.columns:
                if col not in mapping:
                    mapped_df[col] = df[col]
            
            logger.info(f"Column mapping applied successfully. Mapped {len(mapping)} columns.")
            return mapped_df
            
        except Exception as e:
            logger.error(f"Error applying column mapping: {str(e)}")
            raise
    
    def validate_mapping(self, mapping, required_columns=None):
        """
        Validate a column mapping
        
        Args:
            mapping (dict): Column mapping
            required_columns (list, optional): List of required columns
            
        Returns:
            dict: Validation results
        """
        if required_columns is None:
            required_columns = [col for col, info in self.expected_columns.items() if info['required']]
        
        validation_result = {
            "valid": True,
            "errors": [],
            "warnings": [],
            "missing_required": [],
            "missing_optional": []
        }
        
        # Check for required columns
        mapped_columns = set(mapping.values())
        
        for col in required_columns:
            if col not in mapped_columns:
                validation_result["valid"] = False
                validation_result["missing_required"].append(col)
        
        # Check for optional columns
        optional_columns = [col for col in self.expected_columns if col not in required_columns]
        
        for col in optional_columns:
            if col not in mapped_columns:
                validation_result["missing_optional"].append(col)
        
        # Generate error messages
        if validation_result["missing_required"]:
            validation_result["errors"].append(
                f"Missing required columns: {', '.join(validation_result['missing_required'])}"
            )
        
        # Generate warning messages
        if validation_result["missing_optional"]:
            validation_result["warnings"].append(
                f"Missing optional columns: {', '.join(validation_result['missing_optional'])}"
            )
        
        return validation_result
    
    def save_mapping_template(self, file_path, mapping=None):
        """
        Save a mapping template to file
        
        Args:
            file_path (str): Path to save the template
            mapping (dict, optional): Mapping to save
        """
        if mapping is None:
            mapping = {}
        
        try:
            template = {
                "expected_columns": self.expected_columns,
                "synonyms": self.synonyms,
                "patterns": self.patterns,
                "current_mapping": mapping
            }
            
            with open(file_path, 'w') as f:
                yaml.dump(template, f, default_flow_style=False)
            
            logger.info(f"Mapping template saved to {file_path}")
            
        except Exception as e:
            logger.error(f"Error saving mapping template: {str(e)}")
            raise
    
    def load_mapping_template(self, file_path):
        """
        Load a mapping template from file
        
        Args:
            file_path (str): Path to the template file
            
        Returns:
            dict: Loaded mapping
        """
        try:
            with open(file_path, 'r') as f:
                template = yaml.safe_load(f)
            
            # Update instance variables
            if 'expected_columns' in template:
                self.expected_columns = template['expected_columns']
            
            if 'synonyms' in template:
                self.synonyms = template['synonyms']
            
            if 'patterns' in template:
                self.patterns = template['patterns']
            
            logger.info(f"Mapping template loaded from {file_path}")
            
            return template.get('current_mapping', {})
            
        except Exception as e:
            logger.error(f"Error loading mapping template: {str(e)}")
            raise
    
    def get_mapping_suggestions(self, user_columns, expected_columns=None):
        """
        Get mapping suggestions with confidence scores
        
        Args:
            user_columns (list): List of user column names
            expected_columns (list, optional): List of expected column names
            
        Returns:
            dict: Mapping suggestions with confidence scores
        """
        if expected_columns is None:
            expected_columns = list(self.expected_columns.keys())
        
        suggestions = {}
        
        for user_col in user_columns:
            user_col_clean = self._clean_column_name(user_col)
            
            col_suggestions = []
            
            # Check for exact matches
            if user_col_clean in expected_columns:
                col_suggestions.append({
                    "column": user_col_clean,
                    "confidence": 1.0,
                    "method": "exact_match"
                })
            
            # Check for synonyms
            for expected_col in expected_columns:
                if user_col_clean in self.synonyms.get(expected_col, []):
                    col_suggestions.append({
                        "column": expected_col,
                        "confidence": 0.9,
                        "method": "synonym_match"
                    })
            
            # Check for pattern matches
            for expected_col in expected_columns:
                for pattern in self.patterns.get(expected_col, []):
                    if re.search(pattern, user_col_clean, re.IGNORECASE):
                        col_suggestions.append({
                            "column": expected_col,
                            "confidence": 0.8,
                            "method": "pattern_match"
                        })
            
            # Check for fuzzy matches
            for expected_col in expected_columns:
                similarity = self._calculate_similarity(user_col_clean, expected_col)
                if similarity > 0.6:
                    col_suggestions.append({
                        "column": expected_col,
                        "confidence": similarity,
                        "method": "fuzzy_match"
                    })
            
            # Sort by confidence
            col_suggestions.sort(key=lambda x: x["confidence"], reverse=True)
            
            suggestions[user_col] = col_suggestions
        
        return suggestions

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\ingestion\data_loader.py ===
"""
Data Loader Module
Handles loading and preprocessing of transaction data
"""

import pandas as pd
import numpy as np
import dask.dataframe as dd
import os
import logging
from datetime import datetime
import chardet
import warnings
warnings.filterwarnings('ignore')

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class DataLoader:
    """
    Class for loading and preprocessing transaction data
    Supports CSV, Excel, and Parquet files
    Uses Dask for handling large files
    """
    
    def __init__(self, use_dask=True, chunk_size=100000):
        """
        Initialize DataLoader
        
        Args:
            use_dask (bool): Whether to use Dask for large files
            chunk_size (int): Chunk size for Dask processing
        """
        self.use_dask = use_dask
        self.chunk_size = chunk_size
        self.data = None
        self.original_data = None
        
    def load_data(self, file_path, file_type=None, **kwargs):
        """
        Load data from file
        
        Args:
            file_path (str): Path to the data file
            file_type (str, optional): Type of file (csv, excel, parquet)
            **kwargs: Additional parameters for pandas/dask read functions
            
        Returns:
            DataFrame: Loaded data
        """
        try:
            # Determine file type if not provided
            if file_type is None:
                file_type = os.path.splitext(file_path)[1][1:].lower()
            
            # Get file size to decide whether to use Dask
            file_size = os.path.getsize(file_path) / (1024 * 1024)  # Size in MB
            
            # Use Dask for large files
            if self.use_dask and file_size > 100:  # Use Dask for files > 100MB
                logger.info(f"Loading large file ({file_size:.2f} MB) using Dask")
                return self._load_with_dask(file_path, file_type, **kwargs)
            else:
                logger.info(f"Loading file ({file_size:.2f} MB) using pandas")
                return self._load_with_pandas(file_path, file_type, **kwargs)
                
        except Exception as e:
            logger.error(f"Error loading data: {str(e)}")
            raise
    
    def _load_with_pandas(self, file_path, file_type, **kwargs):
        """
        Load data using pandas
        
        Args:
            file_path (str): Path to the data file
            file_type (str): Type of file
            **kwargs: Additional parameters for pandas read functions
            
        Returns:
            DataFrame: Loaded data
        """
        try:
            # Detect encoding for CSV files
            if file_type == 'csv':
                with open(file_path, 'rb') as f:
                    result = chardet.detect(f.read())
                encoding = result['encoding']
                kwargs.setdefault('encoding', encoding)
            
            # Load data based on file type
            if file_type in ['csv', 'txt']:
                df = pd.read_csv(file_path, **kwargs)
            elif file_type in ['xlsx', 'xls']:
                df = pd.read_excel(file_path, **kwargs)
            elif file_type == 'parquet':
                df = pd.read_parquet(file_path, **kwargs)
            else:
                raise ValueError(f"Unsupported file type: {file_type}")
            
            # Store original data
            self.original_data = df.copy()
            
            # Basic preprocessing
            df = self._preprocess_data(df)
            
            self.data = df
            logger.info(f"Data loaded successfully. Shape: {df.shape}")
            return df
            
        except Exception as e:
            logger.error(f"Error loading data with pandas: {str(e)}")
            raise
    
    def _load_with_dask(self, file_path, file_type, **kwargs):
        """
        Load data using Dask
        
        Args:
            file_path (str): Path to the data file
            file_type (str): Type of file
            **kwargs: Additional parameters for dask read functions
            
        Returns:
            DataFrame: Loaded data
        """
        try:
            # Load data based on file type
            if file_type in ['csv', 'txt']:
                df = dd.read_csv(file_path, blocksize=self.chunk_size, **kwargs)
            elif file_type == 'parquet':
                df = dd.read_parquet(file_path, **kwargs)
            else:
                # For Excel files, we need to use pandas
                logger.warning("Dask does not support Excel files directly. Using pandas instead.")
                return self._load_with_pandas(file_path, file_type, **kwargs)
            
            # Store original data (compute a sample for inspection)
            self.original_data = df.head(1000)
            
            # Basic preprocessing
            df = df.map_partitions(self._preprocess_data)
            
            self.data = df
            logger.info(f"Data loaded successfully with Dask. Partitions: {df.npartitions}")
            return df
            
        except Exception as e:
            logger.error(f"Error loading data with Dask: {str(e)}")
            raise
    
    def _preprocess_data(self, df):
        """
        Basic preprocessing of the data
        
        Args:
            df (DataFrame): Input data
            
        Returns:
            DataFrame: Preprocessed data
        """
        try:
            # Remove duplicate rows
            initial_rows = len(df)
            df = df.drop_duplicates()
            removed_rows = initial_rows - len(df)
            if removed_rows > 0:
                logger.info(f"Removed {removed_rows} duplicate rows")
            
            # Handle missing values
            for col in df.columns:
                # For numeric columns, fill with median
                if pd.api.types.is_numeric_dtype(df[col]):
                    median_val = df[col].median()
                    df[col] = df[col].fillna(median_val)
                # For categorical columns, fill with mode or 'Unknown'
                elif pd.api.types.is_string_dtype(df[col]) or pd.api.types.is_categorical_dtype(df[col]):
                    mode_val = df[col].mode()
                    if len(mode_val) > 0:
                        df[col] = df[col].fillna(mode_val[0])
                    else:
                        df[col] = df[col].fillna('Unknown')
            
            # Convert timestamp columns to datetime
            for col in df.columns:
                if 'time' in col.lower() or 'date' in col.lower():
                    try:
                        df[col] = pd.to_datetime(df[col], errors='coerce')
                    except:
                        logger.warning(f"Could not convert {col} to datetime")
            
            # Convert amount columns to numeric
            for col in df.columns:
                if 'amount' in col.lower() or 'value' in col.lower() or 'sum' in col.lower():
                    try:
                        # Remove currency symbols and commas
                        if pd.api.types.is_string_dtype(df[col]):
                            df[col] = df[col].str.replace('[\$,‚Ç¨,¬£,¬•]', '', regex=True)
                            df[col] = df[col].str.replace(',', '')
                        df[col] = pd.to_numeric(df[col], errors='coerce')
                    except:
                        logger.warning(f"Could not convert {col} to numeric")
            
            # Convert ID columns to string
            for col in df.columns:
                if 'id' in col.lower():
                    df[col] = df[col].astype(str)
            
            # Convert boolean columns
            for col in df.columns:
                if df[col].dtype == 'object':
                    unique_vals = df[col].dropna().unique()
                    if len(unique_vals) <= 2 and all(val.lower() in ['true', 'false', 'yes', 'no', 'y', 'n', '0', '1'] for val in unique_vals):
                        df[col] = df[col].replace({
                            'true': True, 'yes': True, 'y': True, '1': True,
                            'false': False, 'no': False, 'n': False, '0': False
                        })
            
            # Convert categorical columns with many unique values to 'category' dtype
            for col in df.columns:
                if (pd.api.types.is_string_dtype(df[col]) and 
                    df[col].nunique() > 10 and 
                    df[col].nunique() < len(df) * 0.5):
                    df[col] = df[col].astype('category')
            
            logger.info("Data preprocessing completed")
            return df
            
        except Exception as e:
            logger.error(f"Error preprocessing data: {str(e)}")
            raise
    
    def get_data_info(self):
        """
        Get information about the loaded data
        
        Returns:
            dict: Data information
        """
        if self.data is None:
            return {"error": "No data loaded"}
        
        info = {
            "shape": self.data.shape,
            "columns": list(self.data.columns),
            "dtypes": dict(self.data.dtypes),
            "missing_values": dict(self.data.isnull().sum()),
            "memory_usage": dict(self.data.memory_usage(deep=True))
        }
        
        # Add statistics for numeric columns
        numeric_stats = {}
        for col in self.data.select_dtypes(include=[np.number]).columns:
            numeric_stats[col] = {
                "min": self.data[col].min(),
                "max": self.data[col].max(),
                "mean": self.data[col].mean(),
                "median": self.data[col].median(),
                "std": self.data[col].std()
            }
        
        info["numeric_stats"] = numeric_stats
        
        # Add unique counts for categorical columns
        categorical_stats = {}
        for col in self.data.select_dtypes(include=['object', 'category']).columns:
            categorical_stats[col] = {
                "unique_count": self.data[col].nunique(),
                "top_values": self.data[col].value_counts().head(5).to_dict()
            }
        
        info["categorical_stats"] = categorical_stats
        
        return info
    
    def get_sample(self, n=5):
        """
        Get a sample of the data
        
        Args:
            n (int): Number of rows to return
            
        Returns:
            DataFrame: Sample data
        """
        if self.data is None:
            return None
        
        if isinstance(self.data, dd.DataFrame):
            return self.data.head(n)
        else:
            return self.data.sample(n) if len(self.data) > n else self.data
    
    def validate_data(self, required_columns=None):
        """
        Validate the data against required columns
        
        Args:
            required_columns (list, optional): List of required columns
            
        Returns:
            dict: Validation results
        """
        if self.data is None:
            return {"valid": False, "error": "No data loaded"}
        
        if required_columns is None:
            required_columns = ['transaction_id', 'timestamp', 'amount', 'sender_id', 'receiver_id']
        
        validation_result = {
            "valid": True,
            "errors": [],
            "warnings": []
        }
        
        # Check for required columns
        missing_columns = [col for col in required_columns if col not in self.data.columns]
        if missing_columns:
            validation_result["valid"] = False
            validation_result["errors"].append(f"Missing required columns: {', '.join(missing_columns)}")
        
        # Check for duplicate transaction IDs
        if 'transaction_id' in self.data.columns:
            duplicate_ids = self.data['transaction_id'].duplicated().sum()
            if duplicate_ids > 0:
                validation_result["warnings"].append(f"Found {duplicate_ids} duplicate transaction IDs")
        
        # Check for missing values in critical columns
        critical_columns = ['transaction_id', 'timestamp', 'amount']
        for col in critical_columns:
            if col in self.data.columns:
                missing_count = self.data[col].isnull().sum()
                if missing_count > 0:
                    validation_result["warnings"].append(f"Found {missing_count} missing values in {col}")
        
        # Check for negative amounts
        if 'amount' in self.data.columns:
            negative_amounts = (self.data['amount'] < 0).sum()
            if negative_amounts > 0:
                validation_result["warnings"].append(f"Found {negative_amounts} transactions with negative amounts")
        
        # Check for future timestamps
        if 'timestamp' in self.data.columns and pd.api.types.is_datetime64_any_dtype(self.data['timestamp']):
            future_dates = (self.data['timestamp'] > datetime.now()).sum()
            if future_dates > 0:
                validation_result["warnings"].append(f"Found {future_dates} transactions with future timestamps")
        
        return validation_result
    
    def save_data(self, file_path, file_type=None, **kwargs):
        """
        Save the processed data to a file
        
        Args:
            file_path (str): Path to save the file
            file_type (str, optional): Type of file (csv, excel, parquet)
            **kwargs: Additional parameters for pandas save functions
        """
        if self.data is None:
            logger.error("No data to save")
            return
        
        try:
            # Determine file type if not provided
            if file_type is None:
                file_type = os.path.splitext(file_path)[1][1:].lower()
            
            # Save data based on file type
            if file_type == 'csv':
                self.data.to_csv(file_path, index=False, **kwargs)
            elif file_type in ['xlsx', 'xls']:
                self.data.to_excel(file_path, index=False, **kwargs)
            elif file_type == 'parquet':
                self.data.to_parquet(file_path, index=False, **kwargs)
            else:
                raise ValueError(f"Unsupported file type: {file_type}")
            
            logger.info(f"Data saved successfully to {file_path}")
            
        except Exception as e:
            logger.error(f"Error saving data: {str(e)}")
            raise

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\ingestion\__init__.py ===

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\ingestion\.ipynb_checkpoints\column_mapper-checkpoint.py ===
"""
Column Mapper Module
Handles intelligent mapping of user columns to expected format
"""
import pandas as pd
import numpy as np
import re
import logging
from collections import defaultdict
from difflib import SequenceMatcher
import yaml
import os
# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ColumnMapper:
    """
    Class for mapping user columns to expected format
    Uses AI-powered techniques to intelligently match columns
    """
    
    def __init__(self, config_path=None):
        """
        Initialize ColumnMapper
        
        Args:
            config_path (str, optional): Path to configuration file
        """
        self.expected_columns = self._get_expected_columns()
        self.synonyms = self._load_synonyms(config_path)
        self.patterns = self._load_patterns(config_path)
        self.mapping_history = []

    def get_expected_columns(self):
        """
        Return the list of expected column names
        
        Returns:
            list: Expected column names
        """
        return list(self.expected_columns.keys())

    
    def _get_expected_columns(self):
        """
        Get the expected columns for the fraud detection system
        
        Returns:
            dict: Expected columns with descriptions
        """
        return {
            'transaction_id': {
                'description': 'Unique identifier for the transaction',
                'data_type': 'string',
                'required': True
            },
            'timestamp': {
                'description': 'Date and time of the transaction',
                'data_type': 'datetime',
                'required': True
            },
            'amount': {
                'description': 'Transaction amount',
                'data_type': 'float',
                'required': True
            },
            'currency': {
                'description': 'Currency code (e.g., USD, EUR)',
                'data_type': 'string',
                'required': False
            },
            'sender_id': {
                'description': 'Identifier of the sender',
                'data_type': 'string',
                'required': True
            },
            'receiver_id': {
                'description': 'Identifier of the receiver',
                'data_type': 'string',
                'required': True
            },
            'sender_account_type': {
                'description': 'Type of sender account (e.g., personal, business)',
                'data_type': 'string',
                'required': False
            },
            'receiver_account_type': {
                'description': 'Type of receiver account (e.g., personal, business)',
                'data_type': 'string',
                'required': False
            },
            'sender_bank': {
                'description': 'Name of sender bank',
                'data_type': 'string',
                'required': False
            },
            'receiver_bank': {
                'description': 'Name of receiver bank',
                'data_type': 'string',
                'required': False
            },
            'sender_location': {
                'description': 'Location of sender (country, state, city)',
                'data_type': 'string',
                'required': False
            },
            'receiver_location': {
                'description': 'Location of receiver (country, state, city)',
                'data_type': 'string',
                'required': False
            },
            'transaction_type': {
                'description': 'Type of transaction (e.g., transfer, payment)',
                'data_type': 'string',
                'required': False
            },
            'transaction_category': {
                'description': 'Category of transaction (e.g., retail, services)',
                'data_type': 'string',
                'required': False
            },
            'merchant_id': {
                'description': 'Identifier of the merchant',
                'data_type': 'string',
                'required': False
            },
            'merchant_category': {
                'description': 'Category of the merchant',
                'data_type': 'string',
                'required': False
            },
            'ip_address': {
                'description': 'IP address used for the transaction',
                'data_type': 'string',
                'required': False
            },
            'device_id': {
                'description': 'Identifier of the device used',
                'data_type': 'string',
                'required': False
            },
            'description': {
                'description': 'Description of the transaction',
                'data_type': 'string',
                'required': False
            },
            'notes': {
                'description': 'Additional notes',
                'data_type': 'string',
                'required': False
            },
            'authorization_status': {
                'description': 'Authorization status (e.g., approved, declined)',
                'data_type': 'string',
                'required': False
            },
            'chargeback_flag': {
                'description': 'Whether the transaction was charged back',
                'data_type': 'boolean',
                'required': False
            },
            'fraud_flag': {
                'description': 'Whether the transaction is fraudulent (for supervised learning)',
                'data_type': 'boolean',
                'required': False
            }
        }
    
    def _load_synonyms(self, config_path=None):
        """
        Load synonyms for column names
        
        Args:
            config_path (str, optional): Path to configuration file
            
        Returns:
            dict: Synonyms for expected columns
        """
        # Default synonyms
        synonyms = {
            'transaction_id': ['id', 'transaction_id', 'tx_id', 'trans_id', 'reference', 'ref_no', 'transaction_no'],
            'timestamp': ['timestamp', 'date', 'time', 'datetime', 'trans_date', 'trans_time', 'transaction_date', 'transaction_time'],
            'amount': ['amount', 'value', 'sum', 'total', 'transaction_amount', 'amt', 'tx_amount'],
            'currency': ['currency', 'curr', 'ccy', 'currency_code'],
            'sender_id': ['sender_id', 'from_id', 'payer_id', 'source_id', 'originator_id'],
            'receiver_id': ['receiver_id', 'to_id', 'payee_id', 'destination_id', 'beneficiary_id'],
            'sender_account_type': ['sender_account_type', 'from_account_type', 'payer_account_type', 'source_account_type'],
            'receiver_account_type': ['receiver_account_type', 'to_account_type', 'payee_account_type', 'destination_account_type'],
            'sender_bank': ['sender_bank', 'from_bank', 'payer_bank', 'source_bank'],
            'receiver_bank': ['receiver_bank', 'to_bank', 'payee_bank', 'destination_bank'],
            'sender_location': ['sender_location', 'from_location', 'payer_location', 'source_location', 'sender_country', 'from_country'],
            'receiver_location': ['receiver_location', 'to_location', 'payee_location', 'destination_location', 'receiver_country', 'to_country'],
            'transaction_type': ['transaction_type', 'trans_type', 'type', 'tx_type'],
            'transaction_category': ['transaction_category', 'trans_category', 'category', 'tx_category'],
            'merchant_id': ['merchant_id', 'merchant', 'retailer_id', 'vendor_id'],
            'merchant_category': ['merchant_category', 'merchant_type', 'retailer_category', 'vendor_category'],
            'ip_address': ['ip_address', 'ip', 'ip_addr'],
            'device_id': ['device_id', 'device', 'device_identifier'],
            'description': ['description', 'desc', 'details', 'narrative'],
            'notes': ['notes', 'note', 'comments', 'remark'],
            'authorization_status': ['authorization_status', 'auth_status', 'status', 'approval_status'],
            'chargeback_flag': ['chargeback_flag', 'chargeback', 'is_chargeback'],
            'fraud_flag': ['fraud_flag', 'fraud', 'is_fraud', 'fraudulent']
        }
        
        # Try to load from config file
        if config_path and os.path.exists(config_path):
            try:
                with open(config_path, 'r') as f:
                    config = yaml.safe_load(f)
                    if 'column_synonyms' in config:
                        # Update with config synonyms
                        for col, syn_list in config['column_synonyms'].items():
                            if col in synonyms:
                                synonyms[col].extend(syn_list)
                            else:
                                synonyms[col] = syn_list
            except Exception as e:
                logger.warning(f"Error loading synonyms from config: {str(e)}")
        
        return synonyms
    
    def _load_patterns(self, config_path=None):
        """
        Load regex patterns for column names
        
        Args:
            config_path (str, optional): Path to configuration file
            
        Returns:
            dict: Regex patterns for expected columns
        """
        # Default patterns
        patterns = {
            'transaction_id': [r'transaction.?id', r'tx.?id', r'trans.?id', r'reference', r'ref.?(no|num)'],
            'timestamp': [r'timestamp', r'date.?time', r'trans.?date', r'trans.?time'],
            'amount': [r'amount', r'value', r'sum', r'total'],
            'currency': [r'currency', r'ccy', r'curr'],
            'sender_id': [r'sender.?id', r'from.?id', r'payer.?id', r'source.?id', r'originator.?id'],
            'receiver_id': [r'receiver.?id', r'to.?id', r'payee.?id', r'destination.?id', r'beneficiary.?id'],
            'sender_account_type': [r'sender.?account.?type', r'from.?account.?type', r'payer.?account.?type'],
            'receiver_account_type': [r'receiver.?account.?type', r'to.?account.?type', r'payee.?account.?type'],
            'sender_bank': [r'sender.?bank', r'from.?bank', r'payer.?bank', r'source.?bank'],
            'receiver_bank': [r'receiver.?bank', r'to.?bank', r'payee.?bank', r'destination.?bank'],
            'sender_location': [r'sender.?location', r'from.?location', r'payer.?location', r'source.?location', r'sender.?country'],
            'receiver_location': [r'receiver.?location', r'to.?location', r'payee.?location', r'destination.?location', r'receiver.?country'],
            'transaction_type': [r'transaction.?type', r'trans.?type', r'tx.?type'],
            'transaction_category': [r'transaction.?category', r'trans.?category', r'tx.?category'],
            'merchant_id': [r'merchant.?id', r'merchant', r'retailer.?id', r'vendor.?id'],
            'merchant_category': [r'merchant.?category', r'merchant.?type', r'retailer.?category'],
            'ip_address': [r'ip.?address', r'ip'],
            'device_id': [r'device.?id', r'device'],
            'description': [r'description', r'desc', r'details', r'narrative'],
            'notes': [r'notes?', r'comments?', r'remarks?'],
            'authorization_status': [r'authorization.?status', r'auth.?status', r'approval.?status'],
            'chargeback_flag': [r'chargeback.?(flag|is)'],
            'fraud_flag': [r'fraud.?(flag|is)', r'is.?fraud']
        }
        
        # Try to load from config file
        if config_path and os.path.exists(config_path):
            try:
                with open(config_path, 'r') as f:
                    config = yaml.safe_load(f)
                    if 'column_patterns' in config:
                        # Update with config patterns
                        for col, pat_list in config['column_patterns'].items():
                            if col in patterns:
                                patterns[col].extend(pat_list)
                            else:
                                patterns[col] = pat_list
            except Exception as e:
                logger.warning(f"Error loading patterns from config: {str(e)}")
        
        return patterns
    
    def auto_map_columns(self, user_columns, expected_columns=None):
        """
        Automatically map user columns to expected columns
        
        Args:
            user_columns (list): List of user column names
            expected_columns (list, optional): List of expected column names
            
        Returns:
            dict: Mapping from user columns to expected columns
        """
        if expected_columns is None:
            expected_columns = list(self.expected_columns.keys())
        
        mapping = {}
        used_columns = set()
        
        # First, try exact matches
        for user_col in user_columns:
            user_col_clean = self._clean_column_name(user_col)
            if user_col_clean in expected_columns and user_col_clean not in used_columns:
                mapping[user_col] = user_col_clean
                used_columns.add(user_col_clean)
        
        # Then, try synonym matches
        for user_col in user_columns:
            if user_col in mapping:
                continue
                
            user_col_clean = self._clean_column_name(user_col)
            
            for expected_col in expected_columns:
                if expected_col in used_columns:
                    continue
                    
                if user_col_clean in self.synonyms.get(expected_col, []):
                    mapping[user_col] = expected_col
                    used_columns.add(expected_col)
                    break
        
        # Then, try pattern matches
        for user_col in user_columns:
            if user_col in mapping:
                continue
                
            user_col_clean = self._clean_column_name(user_col)
            
            for expected_col in expected_columns:
                if expected_col in used_columns:
                    continue
                    
                for pattern in self.patterns.get(expected_col, []):
                    if re.search(pattern, user_col_clean, re.IGNORECASE):
                        mapping[user_col] = expected_col
                        used_columns.add(expected_col)
                        break
                else:
                    continue
                break
        
        # Finally, try fuzzy matching for remaining columns
        for user_col in user_columns:
            if user_col in mapping:
                continue
                
            user_col_clean = self._clean_column_name(user_col)
            
            best_match = None
            best_score = 0
            
            for expected_col in expected_columns:
                if expected_col in used_columns:
                    continue
                    
                # Calculate similarity score
                score = self._calculate_similarity(user_col_clean, expected_col)
                
                if score > best_score and score > 0.6:  # Threshold for fuzzy matching
                    best_score = score
                    best_match = expected_col
            
            if best_match:
                mapping[user_col] = best_match
                used_columns.add(best_match)
        
        # Store mapping history
        self.mapping_history.append({
            'timestamp': pd.Timestamp.now(),
            'user_columns': user_columns,
            'mapping': mapping
        })
        
        return mapping
    
    def _clean_column_name(self, column_name):
        """
        Clean column name for matching
        
        Args:
            column_name (str): Column name to clean
            
        Returns:
            str: Cleaned column name
        """
        # Convert to lowercase
        cleaned = column_name.lower()
        
        # Remove special characters and spaces
        cleaned = re.sub(r'[^a-z0-9]', '_', cleaned)
        
        # Remove consecutive underscores
        cleaned = re.sub(r'_+', '_', cleaned)
        
        # Remove leading and trailing underscores
        cleaned = cleaned.strip('_')
        
        return cleaned
    
    def _calculate_similarity(self, str1, str2):
        """
        Calculate similarity between two strings
        
        Args:
            str1 (str): First string
            str2 (str): Second string
            
        Returns:
            float: Similarity score (0-1)
        """
        # Use SequenceMatcher for similarity
        return SequenceMatcher(None, str1, str2).ratio()
    
    def apply_mapping(self, df, mapping):
        """
        Apply column mapping to a DataFrame
        
        Args:
            df (DataFrame): Input DataFrame
            mapping (dict): Column mapping
            
        Returns:
            DataFrame: DataFrame with mapped columns
        """
        try:
            # Create a copy of the DataFrame
            result_df = df.copy()
            
            # Create a new DataFrame with mapped columns
            mapped_df = pd.DataFrame()
            
            # Map columns
            for user_col, expected_col in mapping.items():
                if user_col in df.columns:
                    mapped_df[expected_col] = df[user_col]
            
            # Add unmapped columns with original names
            for col in df.columns:
                if col not in mapping:
                    mapped_df[col] = df[col]
            
            logger.info(f"Column mapping applied successfully. Mapped {len(mapping)} columns.")
            return mapped_df
            
        except Exception as e:
            logger.error(f"Error applying column mapping: {str(e)}")
            raise
    
    def validate_mapping(self, mapping, required_columns=None):
        """
        Validate a column mapping
        
        Args:
            mapping (dict): Column mapping
            required_columns (list, optional): List of required columns
            
        Returns:
            dict: Validation results
        """
        if required_columns is None:
            required_columns = [col for col, info in self.expected_columns.items() if info['required']]
        
        validation_result = {
            "valid": True,
            "errors": [],
            "warnings": [],
            "missing_required": [],
            "missing_optional": []
        }
        
        # Check for required columns
        mapped_columns = set(mapping.values())
        
        for col in required_columns:
            if col not in mapped_columns:
                validation_result["valid"] = False
                validation_result["missing_required"].append(col)
        
        # Check for optional columns
        optional_columns = [col for col in self.expected_columns if col not in required_columns]
        
        for col in optional_columns:
            if col not in mapped_columns:
                validation_result["missing_optional"].append(col)
        
        # Generate error messages
        if validation_result["missing_required"]:
            validation_result["errors"].append(
                f"Missing required columns: {', '.join(validation_result['missing_required'])}"
            )
        
        # Generate warning messages
        if validation_result["missing_optional"]:
            validation_result["warnings"].append(
                f"Missing optional columns: {', '.join(validation_result['missing_optional'])}"
            )
        
        return validation_result
    
    def save_mapping_template(self, file_path, mapping=None):
        """
        Save a mapping template to file
        
        Args:
            file_path (str): Path to save the template
            mapping (dict, optional): Mapping to save
        """
        if mapping is None:
            mapping = {}
        
        try:
            template = {
                "expected_columns": self.expected_columns,
                "synonyms": self.synonyms,
                "patterns": self.patterns,
                "current_mapping": mapping
            }
            
            with open(file_path, 'w') as f:
                yaml.dump(template, f, default_flow_style=False)
            
            logger.info(f"Mapping template saved to {file_path}")
            
        except Exception as e:
            logger.error(f"Error saving mapping template: {str(e)}")
            raise
    
    def load_mapping_template(self, file_path):
        """
        Load a mapping template from file
        
        Args:
            file_path (str): Path to the template file
            
        Returns:
            dict: Loaded mapping
        """
        try:
            with open(file_path, 'r') as f:
                template = yaml.safe_load(f)
            
            # Update instance variables
            if 'expected_columns' in template:
                self.expected_columns = template['expected_columns']
            
            if 'synonyms' in template:
                self.synonyms = template['synonyms']
            
            if 'patterns' in template:
                self.patterns = template['patterns']
            
            logger.info(f"Mapping template loaded from {file_path}")
            
            return template.get('current_mapping', {})
            
        except Exception as e:
            logger.error(f"Error loading mapping template: {str(e)}")
            raise
    
    def get_mapping_suggestions(self, user_columns, expected_columns=None):
        """
        Get mapping suggestions with confidence scores
        
        Args:
            user_columns (list): List of user column names
            expected_columns (list, optional): List of expected column names
            
        Returns:
            dict: Mapping suggestions with confidence scores
        """
        if expected_columns is None:
            expected_columns = list(self.expected_columns.keys())
        
        suggestions = {}
        
        for user_col in user_columns:
            user_col_clean = self._clean_column_name(user_col)
            
            col_suggestions = []
            
            # Check for exact matches
            if user_col_clean in expected_columns:
                col_suggestions.append({
                    "column": user_col_clean,
                    "confidence": 1.0,
                    "method": "exact_match"
                })
            
            # Check for synonyms
            for expected_col in expected_columns:
                if user_col_clean in self.synonyms.get(expected_col, []):
                    col_suggestions.append({
                        "column": expected_col,
                        "confidence": 0.9,
                        "method": "synonym_match"
                    })
            
            # Check for pattern matches
            for expected_col in expected_columns:
                for pattern in self.patterns.get(expected_col, []):
                    if re.search(pattern, user_col_clean, re.IGNORECASE):
                        col_suggestions.append({
                            "column": expected_col,
                            "confidence": 0.8,
                            "method": "pattern_match"
                        })
            
            # Check for fuzzy matches
            for expected_col in expected_columns:
                similarity = self._calculate_similarity(user_col_clean, expected_col)
                if similarity > 0.6:
                    col_suggestions.append({
                        "column": expected_col,
                        "confidence": similarity,
                        "method": "fuzzy_match"
                    })
            
            # Sort by confidence
            col_suggestions.sort(key=lambda x: x["confidence"], reverse=True)
            
            suggestions[user_col] = col_suggestions
        
        return suggestions

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\ingestion\.ipynb_checkpoints\data_loader-checkpoint.py ===
"""
Data Loader Module
Handles loading and preprocessing of transaction data
"""

import pandas as pd
import numpy as np
import dask.dataframe as dd
import os
import logging
from datetime import datetime
import chardet
import warnings
warnings.filterwarnings('ignore')

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class DataLoader:
    """
    Class for loading and preprocessing transaction data
    Supports CSV, Excel, and Parquet files
    Uses Dask for handling large files
    """
    
    def __init__(self, use_dask=True, chunk_size=100000):
        """
        Initialize DataLoader
        
        Args:
            use_dask (bool): Whether to use Dask for large files
            chunk_size (int): Chunk size for Dask processing
        """
        self.use_dask = use_dask
        self.chunk_size = chunk_size
        self.data = None
        self.original_data = None
        
    def load_data(self, file_path, file_type=None, **kwargs):
        """
        Load data from file
        
        Args:
            file_path (str): Path to the data file
            file_type (str, optional): Type of file (csv, excel, parquet)
            **kwargs: Additional parameters for pandas/dask read functions
            
        Returns:
            DataFrame: Loaded data
        """
        try:
            # Determine file type if not provided
            if file_type is None:
                file_type = os.path.splitext(file_path)[1][1:].lower()
            
            # Get file size to decide whether to use Dask
            file_size = os.path.getsize(file_path) / (1024 * 1024)  # Size in MB
            
            # Use Dask for large files
            if self.use_dask and file_size > 100:  # Use Dask for files > 100MB
                logger.info(f"Loading large file ({file_size:.2f} MB) using Dask")
                return self._load_with_dask(file_path, file_type, **kwargs)
            else:
                logger.info(f"Loading file ({file_size:.2f} MB) using pandas")
                return self._load_with_pandas(file_path, file_type, **kwargs)
                
        except Exception as e:
            logger.error(f"Error loading data: {str(e)}")
            raise
    
    def _load_with_pandas(self, file_path, file_type, **kwargs):
        """
        Load data using pandas
        
        Args:
            file_path (str): Path to the data file
            file_type (str): Type of file
            **kwargs: Additional parameters for pandas read functions
            
        Returns:
            DataFrame: Loaded data
        """
        try:
            # Detect encoding for CSV files
            if file_type == 'csv':
                with open(file_path, 'rb') as f:
                    result = chardet.detect(f.read())
                encoding = result['encoding']
                kwargs.setdefault('encoding', encoding)
            
            # Load data based on file type
            if file_type in ['csv', 'txt']:
                df = pd.read_csv(file_path, **kwargs)
            elif file_type in ['xlsx', 'xls']:
                df = pd.read_excel(file_path, **kwargs)
            elif file_type == 'parquet':
                df = pd.read_parquet(file_path, **kwargs)
            else:
                raise ValueError(f"Unsupported file type: {file_type}")
            
            # Store original data
            self.original_data = df.copy()
            
            # Basic preprocessing
            df = self._preprocess_data(df)
            
            self.data = df
            logger.info(f"Data loaded successfully. Shape: {df.shape}")
            return df
            
        except Exception as e:
            logger.error(f"Error loading data with pandas: {str(e)}")
            raise
    
    def _load_with_dask(self, file_path, file_type, **kwargs):
        """
        Load data using Dask
        
        Args:
            file_path (str): Path to the data file
            file_type (str): Type of file
            **kwargs: Additional parameters for dask read functions
            
        Returns:
            DataFrame: Loaded data
        """
        try:
            # Load data based on file type
            if file_type in ['csv', 'txt']:
                df = dd.read_csv(file_path, blocksize=self.chunk_size, **kwargs)
            elif file_type == 'parquet':
                df = dd.read_parquet(file_path, **kwargs)
            else:
                # For Excel files, we need to use pandas
                logger.warning("Dask does not support Excel files directly. Using pandas instead.")
                return self._load_with_pandas(file_path, file_type, **kwargs)
            
            # Store original data (compute a sample for inspection)
            self.original_data = df.head(1000)
            
            # Basic preprocessing
            df = df.map_partitions(self._preprocess_data)
            
            self.data = df
            logger.info(f"Data loaded successfully with Dask. Partitions: {df.npartitions}")
            return df
            
        except Exception as e:
            logger.error(f"Error loading data with Dask: {str(e)}")
            raise
    
    def _preprocess_data(self, df):
        """
        Basic preprocessing of the data
        
        Args:
            df (DataFrame): Input data
            
        Returns:
            DataFrame: Preprocessed data
        """
        try:
            # Remove duplicate rows
            initial_rows = len(df)
            df = df.drop_duplicates()
            removed_rows = initial_rows - len(df)
            if removed_rows > 0:
                logger.info(f"Removed {removed_rows} duplicate rows")
            
            # Handle missing values
            for col in df.columns:
                # For numeric columns, fill with median
                if pd.api.types.is_numeric_dtype(df[col]):
                    median_val = df[col].median()
                    df[col] = df[col].fillna(median_val)
                # For categorical columns, fill with mode or 'Unknown'
                elif pd.api.types.is_string_dtype(df[col]) or pd.api.types.is_categorical_dtype(df[col]):
                    mode_val = df[col].mode()
                    if len(mode_val) > 0:
                        df[col] = df[col].fillna(mode_val[0])
                    else:
                        df[col] = df[col].fillna('Unknown')
            
            # Convert timestamp columns to datetime
            for col in df.columns:
                if 'time' in col.lower() or 'date' in col.lower():
                    try:
                        df[col] = pd.to_datetime(df[col], errors='coerce')
                    except:
                        logger.warning(f"Could not convert {col} to datetime")
            
            # Convert amount columns to numeric
            for col in df.columns:
                if 'amount' in col.lower() or 'value' in col.lower() or 'sum' in col.lower():
                    try:
                        # Remove currency symbols and commas
                        if pd.api.types.is_string_dtype(df[col]):
                            df[col] = df[col].str.replace('[\$,‚Ç¨,¬£,¬•]', '', regex=True)
                            df[col] = df[col].str.replace(',', '')
                        df[col] = pd.to_numeric(df[col], errors='coerce')
                    except:
                        logger.warning(f"Could not convert {col} to numeric")
            
            # Convert ID columns to string
            for col in df.columns:
                if 'id' in col.lower():
                    df[col] = df[col].astype(str)
            
            # Convert boolean columns
            for col in df.columns:
                if df[col].dtype == 'object':
                    unique_vals = df[col].dropna().unique()
                    if len(unique_vals) <= 2 and all(val.lower() in ['true', 'false', 'yes', 'no', 'y', 'n', '0', '1'] for val in unique_vals):
                        df[col] = df[col].replace({
                            'true': True, 'yes': True, 'y': True, '1': True,
                            'false': False, 'no': False, 'n': False, '0': False
                        })
            
            # Convert categorical columns with many unique values to 'category' dtype
            for col in df.columns:
                if (pd.api.types.is_string_dtype(df[col]) and 
                    df[col].nunique() > 10 and 
                    df[col].nunique() < len(df) * 0.5):
                    df[col] = df[col].astype('category')
            
            logger.info("Data preprocessing completed")
            return df
            
        except Exception as e:
            logger.error(f"Error preprocessing data: {str(e)}")
            raise
    
    def get_data_info(self):
        """
        Get information about the loaded data
        
        Returns:
            dict: Data information
        """
        if self.data is None:
            return {"error": "No data loaded"}
        
        info = {
            "shape": self.data.shape,
            "columns": list(self.data.columns),
            "dtypes": dict(self.data.dtypes),
            "missing_values": dict(self.data.isnull().sum()),
            "memory_usage": dict(self.data.memory_usage(deep=True))
        }
        
        # Add statistics for numeric columns
        numeric_stats = {}
        for col in self.data.select_dtypes(include=[np.number]).columns:
            numeric_stats[col] = {
                "min": self.data[col].min(),
                "max": self.data[col].max(),
                "mean": self.data[col].mean(),
                "median": self.data[col].median(),
                "std": self.data[col].std()
            }
        
        info["numeric_stats"] = numeric_stats
        
        # Add unique counts for categorical columns
        categorical_stats = {}
        for col in self.data.select_dtypes(include=['object', 'category']).columns:
            categorical_stats[col] = {
                "unique_count": self.data[col].nunique(),
                "top_values": self.data[col].value_counts().head(5).to_dict()
            }
        
        info["categorical_stats"] = categorical_stats
        
        return info
    
    def get_sample(self, n=5):
        """
        Get a sample of the data
        
        Args:
            n (int): Number of rows to return
            
        Returns:
            DataFrame: Sample data
        """
        if self.data is None:
            return None
        
        if isinstance(self.data, dd.DataFrame):
            return self.data.head(n)
        else:
            return self.data.sample(n) if len(self.data) > n else self.data
    
    def validate_data(self, required_columns=None):
        """
        Validate the data against required columns
        
        Args:
            required_columns (list, optional): List of required columns
            
        Returns:
            dict: Validation results
        """
        if self.data is None:
            return {"valid": False, "error": "No data loaded"}
        
        if required_columns is None:
            required_columns = ['transaction_id', 'timestamp', 'amount', 'sender_id', 'receiver_id']
        
        validation_result = {
            "valid": True,
            "errors": [],
            "warnings": []
        }
        
        # Check for required columns
        missing_columns = [col for col in required_columns if col not in self.data.columns]
        if missing_columns:
            validation_result["valid"] = False
            validation_result["errors"].append(f"Missing required columns: {', '.join(missing_columns)}")
        
        # Check for duplicate transaction IDs
        if 'transaction_id' in self.data.columns:
            duplicate_ids = self.data['transaction_id'].duplicated().sum()
            if duplicate_ids > 0:
                validation_result["warnings"].append(f"Found {duplicate_ids} duplicate transaction IDs")
        
        # Check for missing values in critical columns
        critical_columns = ['transaction_id', 'timestamp', 'amount']
        for col in critical_columns:
            if col in self.data.columns:
                missing_count = self.data[col].isnull().sum()
                if missing_count > 0:
                    validation_result["warnings"].append(f"Found {missing_count} missing values in {col}")
        
        # Check for negative amounts
        if 'amount' in self.data.columns:
            negative_amounts = (self.data['amount'] < 0).sum()
            if negative_amounts > 0:
                validation_result["warnings"].append(f"Found {negative_amounts} transactions with negative amounts")
        
        # Check for future timestamps
        if 'timestamp' in self.data.columns and pd.api.types.is_datetime64_any_dtype(self.data['timestamp']):
            future_dates = (self.data['timestamp'] > datetime.now()).sum()
            if future_dates > 0:
                validation_result["warnings"].append(f"Found {future_dates} transactions with future timestamps")
        
        return validation_result
    
    def save_data(self, file_path, file_type=None, **kwargs):
        """
        Save the processed data to a file
        
        Args:
            file_path (str): Path to save the file
            file_type (str, optional): Type of file (csv, excel, parquet)
            **kwargs: Additional parameters for pandas save functions
        """
        if self.data is None:
            logger.error("No data to save")
            return
        
        try:
            # Determine file type if not provided
            if file_type is None:
                file_type = os.path.splitext(file_path)[1][1:].lower()
            
            # Save data based on file type
            if file_type == 'csv':
                self.data.to_csv(file_path, index=False, **kwargs)
            elif file_type in ['xlsx', 'xls']:
                self.data.to_excel(file_path, index=False, **kwargs)
            elif file_type == 'parquet':
                self.data.to_parquet(file_path, index=False, **kwargs)
            else:
                raise ValueError(f"Unsupported file type: {file_type}")
            
            logger.info(f"Data saved successfully to {file_path}")
            
        except Exception as e:
            logger.error(f"Error saving data: {str(e)}")
            raise

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\ingestion\.ipynb_checkpoints\__init__-checkpoint.py ===

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\ingestion\__pycache__\column_mapper.cpython-313.pyc ===
Û
    dÄöhü`  „                   Ûº   ï S r SSKrSSKrSSKrSSKrSSKJr  SSK	J
r
  SSKrSSKr\R                  " \R                  S9  \R                  " \5      r " S S5      rg)zU
Column Mapper Module
Handles intelligent mapping of user columns to expected format
È    N)⁄defaultdict)⁄SequenceMatcher)⁄levelc                   ÛÇ   ï \ rS rSrSrSS jrS rS rSS jrSS jr	SS	 jr
S
 rS rS rSS jrSS jrS rSS jrSrg)⁄ColumnMapperÈ   zm
Class for mapping user columns to expected format
Uses AI-powered techniques to intelligently match columns
Nc                 Ûî   ï U R                  5       U l        U R                  U5      U l        U R	                  U5      U l        / U l        g)z\
Initialize ColumnMapper

Args:
    config_path (str, optional): Path to configuration file
N)⁄_get_expected_columns⁄expected_columns⁄_load_synonyms⁄synonyms⁄_load_patterns⁄patterns⁄mapping_history)⁄self⁄config_paths     ⁄mC:\Users\Tranquil Abyss\fraud-detection-platform\app\..\src\fraud_detection_engine\ingestion\column_mapper.py⁄__init__⁄ColumnMapper.__init__   sA   Ä  !%◊ :— :” <à‘ÿ◊+—+®K”8àåÿ◊+—+®K”8àåÿ!à’Û    c                 ÛH   ï [        U R                  R                  5       5      $ )zT
Return the list of expected column names

Returns:
    list: Expected column names
)⁄listr   ⁄keys©r   s    r   ⁄get_expected_columns⁄!ColumnMapper.get_expected_columns#   s   Ä Ù êD◊)—)◊.—.”0”1–1r   c           
      Û6  ï 0 SSSSS._SSSSS._S	S
SSS._SSSSS._SSSSS._SSSSS._SSSSS._SSSSS._SSSSS._SSSSS._SSSSS._SSSSS._SS SSS._S!S"SSS._S#S$SSS._S%S&SSS._S'S(SSS._S)SSS.S*SSS.S+SSS.S,SSS.S-S.SS.S/S.SS.S0.E$ )1zp
Get the expected columns for the fraud detection system

Returns:
    dict: Expected columns with descriptions
⁄transaction_idz%Unique identifier for the transaction⁄stringT)⁄description⁄	data_type⁄required⁄	timestampz Date and time of the transaction⁄datetime⁄amountzTransaction amount⁄float⁄currencyzCurrency code (e.g., USD, EUR)F⁄	sender_idzIdentifier of the sender⁄receiver_idzIdentifier of the receiver⁄sender_account_typez1Type of sender account (e.g., personal, business)⁄receiver_account_typez3Type of receiver account (e.g., personal, business)⁄sender_bankzName of sender bank⁄receiver_bankzName of receiver bank⁄sender_locationz)Location of sender (country, state, city)⁄receiver_locationz+Location of receiver (country, state, city)⁄transaction_typez-Type of transaction (e.g., transfer, payment)⁄transaction_categoryz0Category of transaction (e.g., retail, services)⁄merchant_idzIdentifier of the merchant⁄merchant_categoryzCategory of the merchant⁄
ip_addressz#IP address used for the transactionzIdentifier of the device usedzDescription of the transactionzAdditional notesz/Authorization status (e.g., approved, declined)z(Whether the transaction was charged back⁄booleanz?Whether the transaction is fraudulent (for supervised learning)©⁄	device_idr    ⁄notes⁄authorization_status⁄chargeback_flag⁄
fraud_flag© r   s    r   r
   ⁄"ColumnMapper._get_expected_columns-   s5  Ä t
ÿÿFÿ%ÿ Òt
 ÿAÿ'ÿ Òt
 ÿ3ÿ$ÿ Òt
  ÿ?ÿ%ÿ!Ò!t
* ÿ9ÿ%ÿ Ò+t
4 ÿ;ÿ%ÿ Ò5t
> "ÿRÿ%ÿ!Ò$?t
H $ÿTÿ%ÿ!Ò&It
R ÿ4ÿ%ÿ!ÒSt
\ ÿ6ÿ%ÿ!Ò]t
f ÿJÿ%ÿ!Ò gt
p  ÿLÿ%ÿ!Ò"qt
z ÿNÿ%ÿ!Ò!{t
D #ÿQÿ%ÿ!Ò%Et
N ÿ;ÿ%ÿ!ÒOt
X  ÿ9ÿ%ÿ!Ò"Yt
b ÿDÿ%ÿ!Òct
n  ?ÿ%ÿ!Ò  @ÿ%ÿ!Ò  2ÿ%ÿ!Ò  Qÿ%ÿ!Ò%  Jÿ&ÿ!Ò   aÿ&ÿ!ÒÚ_t
 t	
r   c                 Û¶  ï 0 S/ SQ_S/ SQ_S/ SQ_S/ SQ_S	/ S
Q_S/ SQ_S/ SQ_S/ SQ_S/ SQ_S/ SQ_S/ SQ_S/ SQ_S/ SQ_S/ SQ_S/ SQ_S/ S Q_S!/ S"Q_/ S#Q/ S$Q/ S%Q/ S&Q/ S'Q/ S(QS).EnU(       aí  [         R                  R                  U5      (       an   [        US*5       n[        R
                  " U5      nS+U;   a:  US+   R                  5        H#  u  pVXR;   a  X%   R                  U5        M  XbU'   M%     S,S,S,5        U$ U$ ! , (       d  f       U$ = f! [         a,  n[        R                  S-[        U5       35         S,nAU$ S,nAff = f).zô
Load synonyms for column names

Args:
    config_path (str, optional): Path to configuration file
    
Returns:
    dict: Synonyms for expected columns
r   )⁄idr   ⁄tx_id⁄trans_id⁄	reference⁄ref_no⁄transaction_nor#   )r#   ⁄date⁄timer$   ⁄
trans_date⁄
trans_time⁄transaction_date⁄transaction_timer%   )r%   ⁄value⁄sum⁄total⁄transaction_amount⁄amt⁄	tx_amountr'   )r'   ⁄curr⁄ccy⁄currency_coder(   )r(   ⁄from_id⁄payer_id⁄	source_id⁄originator_idr)   )r)   ⁄to_id⁄payee_id⁄destination_id⁄beneficiary_idr*   )r*   ⁄from_account_type⁄payer_account_type⁄source_account_typer+   )r+   ⁄to_account_type⁄payee_account_type⁄destination_account_typer,   )r,   ⁄	from_bank⁄
payer_bank⁄source_bankr-   )r-   ⁄to_bank⁄
payee_bank⁄destination_bankr.   )r.   ⁄from_location⁄payer_location⁄source_location⁄sender_country⁄from_countryr/   )r/   ⁄to_location⁄payee_location⁄destination_location⁄receiver_country⁄
to_countryr0   )r0   ⁄
trans_type⁄type⁄tx_typer1   )r1   ⁄trans_category⁄category⁄tx_categoryr2   )r2   ⁄merchant⁄retailer_id⁄	vendor_idr3   )r3   ⁄merchant_type⁄retailer_category⁄vendor_categoryr4   )r4   ⁄ip⁄ip_addr)r7   ⁄device⁄device_identifier©r    ⁄desc⁄details⁄	narrative)r8   ⁄note⁄comments⁄remark)r9   ⁄auth_status⁄status⁄approval_status)r:   ⁄
chargeback⁄is_chargeback)r;   ⁄fraud⁄is_fraud⁄
fraudulentr6   ⁄r⁄column_synonymsNz$Error loading synonyms from config: ©⁄os⁄path⁄exists⁄open⁄yaml⁄	safe_load⁄items⁄extend⁄	Exception⁄logger⁄warning⁄str)r   r   r   ⁄f⁄config⁄col⁄syn_list⁄es           r   r   ⁄ColumnMapper._load_synonyms™   s“  Ä 
ÿ“t
‡Ú  G
 “c
 “D	

 “[
 “c
 "“#|
 $Ú  &D
 “T
 “[
 Ú   K
  Ú  "R
 “ U
 #“$i
 “R
   “!o!
" “9#
Ú$ F⁄J⁄<⁄$h⁄Q⁄KÚ/
àˆ6 ú2ü7ô7ü>ô>®+◊6—6P‹ò+†s‘+®q‹!ü^ö^®A”.êFÿ(®F”2‡-3–4E—-F◊-L—-L÷-NôMòCÿ"õÿ (°◊ 4— 4∞X÷ >‡08®£Ò	 .O˜	 , ààxà˜ ,‘+ à˚Ù Û P‹óë–!Eƒc»!√f¿X–N◊O–O‡à˚P˙s7   ¬D ¬%AD√<D ƒ
DƒD ƒD ƒ
Eƒ$!E≈Ec                 Û§  ï 0 S/ SQ_S/ SQ_S/ SQ_S/ SQ_S	/ S
Q_S/ SQ_S/ SQ_S/ SQ_S/ SQ_S/ SQ_S/ SQ_S/ SQ_S/ SQ_S/ SQ_S/ SQ_S/ S Q_S!S"S#/_S$S%// S&Q/ S'Q/ S(QS)/S*S+/S,.EnU(       aí  [         R                  R                  U5      (       an   [        US-5       n[        R
                  " U5      nS.U;   a:  US.   R                  5        H#  u  pVXR;   a  X%   R                  U5        M  XbU'   M%     S/S/S/5        U$ U$ ! , (       d  f       U$ = f! [         a,  n[        R                  S0[        U5       35         S/nAU$ S/nAff = f)1z•
Load regex patterns for column names

Args:
    config_path (str, optional): Path to configuration file
    
Returns:
    dict: Regex patterns for expected columns
r   )ztransaction.?idztx.?idz	trans.?idrB   zref.?(no|num)r#   )r#   z
date.?timeztrans.?dateztrans.?timer%   )r%   rK   rL   rM   r'   )r'   rR   rQ   r(   )z
sender.?idzfrom.?idz	payer.?idz
source.?idzoriginator.?idr)   )zreceiver.?idzto.?idz	payee.?idzdestination.?idzbeneficiary.?idr*   )zsender.?account.?typezfrom.?account.?typezpayer.?account.?typer+   )zreceiver.?account.?typezto.?account.?typezpayee.?account.?typer,   )zsender.?bankz
from.?bankzpayer.?bankzsource.?bankr-   )zreceiver.?bankzto.?bankzpayee.?bankzdestination.?bankr.   )zsender.?locationzfrom.?locationzpayer.?locationzsource.?locationzsender.?countryr/   )zreceiver.?locationzto.?locationzpayee.?locationzdestination.?locationzreceiver.?countryr0   )ztransaction.?typeztrans.?typeztx.?typer1   )ztransaction.?categoryztrans.?categoryztx.?categoryr2   )zmerchant.?idrx   zretailer.?idz
vendor.?idr3   )zmerchant.?categoryzmerchant.?typezretailer.?categoryr4   zip.?addressr~   z
device.?idrÄ   rÇ   )znotes?z	comments?zremarks?)zauthorization.?statuszauth.?statuszapproval.?statuszchargeback.?(flag|is)zfraud.?(flag|is)z	is.?fraudr6   rë   ⁄column_patternsNz$Error loading patterns from config: rì   )r   r   r   r†   r°   r¢   ⁄pat_listr§   s           r   r   ⁄ColumnMapper._load_patterns‡   s‹  Ä 
ÿ“k
‡“V
 “=
 “6	

 “e
 “m
 "“#n
 $“%p
 “\
 “c
 Ú   E
  Ú  "N
 “ S
 #“$c
 “Y
   “!b!
" ò>®5–1#
$ (®–3⁄N⁄;⁄$dÿ 8–9ÿ.∞–=Ú/
àˆ6 ú2ü7ô7ü>ô>®+◊6—6P‹ò+†s‘+®q‹!ü^ö^®A”.êFÿ(®F”2‡-3–4E—-F◊-L—-L÷-NôMòCÿ"õÿ (°◊ 4— 4∞X÷ >‡08®£Ò	 .O˜	 , ààxà˜ ,‘+ à˚Ù Û P‹óë–!Eƒc»!√f¿X–N◊O–O‡à˚P˙s7   ¬D ¬$AD√;D ƒ
DƒD ƒD ƒ
Eƒ#!E
≈
Ec                 ÛH  ï Uc#  [        U R                  R                  5       5      n0 n[        5       nU H7  nU R	                  U5      nXb;   d  M  Xd;  d  M"  XcU'   UR                  U5        M9     U Hb  nXS;   a  M
  U R	                  U5      nU HA  nXt;   a  M
  X`R                  R                  U/ 5      ;   d  M+  XsU'   UR                  U5          M`     Md     U Hí  nXS;   a  M
  U R	                  U5      nU Hq  nXt;   a  M
  U R                  R                  U/ 5       HD  n[        R                  " XÜ[        R                  5      (       d  M/  XsU'   UR                  U5          O   Mp    Mê     Mî     U Hq  nXS;   a  M
  U R	                  U5      nSn	Sn
U H.  nXt;   a  M
  U R                  Xg5      nX∫:î  d  M"  US:î  d  M*  Un
Un	M0     U	(       d  M\  XìU'   UR                  U	5        Ms     U R                  R                  [        R                   R#                  5       UUS.5        U$ )z˘
Automatically map user columns to expected columns

Args:
    user_columns (list): List of user column names
    expected_columns (list, optional): List of expected column names
    
Returns:
    dict: Mapping from user columns to expected columns
Nr   Á333333„?)r#   ⁄user_columns⁄mapping)r   r   r   ⁄set⁄_clean_column_name⁄addr   ⁄getr   ⁄re⁄search⁄
IGNORECASE⁄_calculate_similarityr   ⁄append⁄pd⁄	Timestamp⁄now)r   r¨   r   r≠   ⁄used_columns⁄user_col⁄user_col_clean⁄expected_col⁄pattern⁄
best_match⁄
best_score⁄scores               r   ⁄auto_map_columns⁄ColumnMapper.auto_map_columns  s  Ä  —#‹#†D◊$9—$9◊$>—$>”$@”A–‡à‹ìuàÛ %àHÿ!◊4—4∞X”>àNÿ’1∞n’6Xÿ$2ò—!ÿ◊ — †÷0Ò	 %Û %àHÿ”"Ÿ‡!◊4—4∞X”>àN„ 0êÿ”/Ÿ‡!ß]°]◊%6—%6∞|¿R”%H’Hÿ(4òH—%ÿ ◊$—$†\‘2⁄Û !1Ò %Û  %àHÿ”"Ÿ‡!◊4—4∞X”>àN„ 0êÿ”/Ÿ‡#ü}ô}◊0—0∞∏r÷BêG‹óyíy†º"ø-π-◊H”Hÿ,8†—)ÿ$◊(—(®‘6ŸÒ	  CÒ ⁄Û !1Ò %Û( %àHÿ”"Ÿ‡!◊4—4∞X”>àN‡àJÿàJ„ 0êÿ”/Ÿ ◊2—2∞>”Pê‡’%®%∞#≠+ÿ!&êJÿ!-íJÒ !1˜ àzÿ$.ò—!ÿ◊ — †÷,Ò- %2 	◊—◊#—#‹üô◊)—)”+ÿ(ÿÒ%
Ù 	 àr   c                 Û®   ï UR                  5       n[        R                  " SSU5      n[        R                  " SSU5      nUR                  S5      nU$ )z~
Clean column name for matching

Args:
    column_name (str): Column name to clean
    
Returns:
    str: Cleaned column name
z	[^a-z0-9]⁄_z_+)⁄lowerr≤   ⁄sub⁄strip)r   ⁄column_name⁄cleaneds      r   rØ   ⁄ColumnMapper._clean_column_namet  sO   Ä  ◊#—#”%àÙ ó&í&ò†s®G”4àÙ ó&í&ò††W”-à ó-ë-†”$à‡àr   c                 Û6   ï [        SX5      R                  5       $ )zú
Calculate similarity between two strings

Args:
    str1 (str): First string
    str2 (str): Second string
    
Returns:
    float: Similarity score (0-1)
N)r   ⁄ratio)r   ⁄str1⁄str2s      r   rµ   ⁄"ColumnMapper._calculate_similarityå  s   Ä Ù òt†T”0◊6—6”8–8r   c                 Ûû  ï  UR                  5       n[        R                  " 5       nUR                  5        H  u  pVXQR                  ;   d  M  X   XF'   M     UR                   H  nXr;  d  M
  X   XG'   M     [
        R                  S[        U5       S35        U$ ! [         a'  n[
        R                  S[        U5       35        e SnAff = f)zÆ
Apply column mapping to a DataFrame

Args:
    df (DataFrame): Input DataFrame
    mapping (dict): Column mapping
    
Returns:
    DataFrame: DataFrame with mapped columns
z,Column mapping applied successfully. Mapped z	 columns.zError applying column mapping: N)⁄copyr∑   ⁄	DataFramerö   ⁄columnsrù   ⁄info⁄lenrú   ⁄errorrü   )	r   ⁄dfr≠   ⁄	result_df⁄	mapped_dfrª   rΩ   r¢   r§   s	            r   ⁄apply_mapping⁄ColumnMapper.apply_mappingö  sµ   Ä 	‡üôõ	àIÙ üöõàI +2Ø-©-Æ/—&êÿüzôz’)ÿ.0©lêI”+Ò +:
 ózîzêÿ’%ÿ%'°WêIìNÒ "Ù èKâK–Fƒs»7√|¿n–T]–^‘_ÿ–¯‰Û 	‹èLâL–:º3∏qª6∏(–C‘Dÿ˚	˙s$   ÇAB ¡B ¡--B ¬
C¬%"C√Cc                 ÛÑ  ï Uc:  U R                   R                  5        VVs/ s H  u  p4US   (       d  M  UPM     nnnS/ / / / S.n[        UR                  5       5      nU H#  nX6;  d  M
  SUS'   US   R	                  U5        M%     U R                    Vs/ s H  o3U;  d  M
  UPM     nnU H  nX6;  d  M
  US   R	                  U5        M      US   (       a)  US   R	                  S	S
R                  US   5       35        US   (       a)  US   R	                  SS
R                  US   5       35        U$ s  snnf s  snf )z∞
Validate a column mapping

Args:
    mapping (dict): Column mapping
    required_columns (list, optional): List of required columns
    
Returns:
    dict: Validation results
r"   T)⁄valid⁄errors⁄warnings⁄missing_required⁄missing_optionalFrﬁ   r·   r‚   rﬂ   zMissing required columns: z, r‡   zMissing optional columns: )r   rö   rÆ   ⁄valuesr∂   ⁄join)r   r≠   ⁄required_columnsr¢   r’   ⁄validation_result⁄mapped_columns⁄optional_columnss           r   ⁄validate_mapping⁄ColumnMapper.validate_mappingΩ  sf  Ä  —#ÿ59◊5J—5J◊5P—5P‘5R‘g“5R©	®–VZ–[e’Vfß—5R–—g ÿÿÿ "ÿ "Ò
–Ù òWü^ô^”-”.à„#àCÿ’(ÿ-2–!†'—*ÿ!–"4—5◊<—<∏S÷AÒ $ ,0◊+@“+@”`“+@†C–O_—D_üC—+@––`„#àCÿ’(ÿ!–"4—5◊<—<∏S÷AÒ $
 –/◊0ÿòh—'◊.—.ÿ,®TØY©Y–7H–I[—7\”-]–,^–_Ù
 –/◊0ÿòj—)◊0—0ÿ,®TØY©Y–7H–I[—7\”-]–,^–_Ù !– ˘ÛK  h˘Ú& as   °D7µD7¬	D=¬#D=c                 Ûh  ï Uc  0 n U R                   U R                  U R                  US.n[        US5       n[        R
                  " X4SS9  SSS5        [        R                  SU 35        g! , (       d  f       N'= f! [         a'  n[        R                  S[        U5       35        e SnAff = f)zÖ
Save a mapping template to file

Args:
    file_path (str): Path to save the template
    mapping (dict, optional): Mapping to save
N)r   r   r   ⁄current_mapping⁄wF)⁄default_flow_stylezMapping template saved to zError saving mapping template: )r   r   r   ró   rò   ⁄dumprù   r’   rú   r◊   rü   )r   ⁄	file_pathr≠   ⁄templater†   r§   s         r   ⁄save_mapping_template⁄"ColumnMapper.save_mapping_template  s†   Ä  â?ÿàG	‡$(◊$9—$9ÿ üMôMÿ üMôMÿ#*Ò	àHÙ êi†‘%®‹ó	í	ò(∏%“@˜ &Ù èKâK–4∞Y∞K–@’A˜ &’%˚Ù
 Û 	‹èLâL–:º3∏qª6∏(–C‘Dÿ˚	˙s.   á1B  ∏A/¡ B  ¡/
A=¡9B  ¬ 
B1¬
"B,¬,B1c                 Ûò  ï  [        US5       n[        R                  " U5      nSSS5        SW;   a
  US   U l        SU;   a
  US   U l        SU;   a
  US   U l        [        R                  SU 35        UR                  S0 5      $ ! , (       d  f       Nh= f! [         a'  n[        R                  S[        U5       35        e SnAff = f)	zÄ
Load a mapping template from file

Args:
    file_path (str): Path to the template file
    
Returns:
    dict: Loaded mapping
rë   Nr   r   r   zMapping template loaded from rÏ   z Error loading mapping template: )ró   rò   rô   r   r   r   rù   r’   r±   rú   r◊   rü   )r   r   r†   rÒ   r§   s        r   ⁄load_mapping_template⁄"ColumnMapper.load_mapping_template  s¡   Ä 	‹êi†‘%®‹ü>ö>®!”,ê˜ & "†X”-ÿ(0–1C—(Dê‘%‡òX”%ÿ (®— 4êî‡òX”%ÿ (®— 4êî‰èKâK–7∏	∞{–C‘D‡ó<ë<– 1∞2”6–6˜ &’%˚Ù" Û 	‹èLâL–;ºC¿ªF∏8–D‘Eÿ˚	˙s.   ÇB éB•A!B ¬
B¬B ¬
C	¬""C√C	c           	      Û∂  ï Uc#  [        U R                  R                  5       5      n0 nU GH)  nU R                  U5      n/ nXR;   a  UR	                  USSS.5        U H9  nXPR
                  R                  U/ 5      ;   d  M$  UR	                  USSS.5        M;     U Hg  nU R                  R                  U/ 5       HD  n[        R                  " XÖ[        R                  5      (       d  M/  UR	                  USSS.5        MF     Mi     U H1  nU R                  XW5      n	U	S:î  d  M  UR	                  UU	S	S.5        M3     UR                  S
 SS9  XcU'   GM,     U$ )zÚ
Get mapping suggestions with confidence scores

Args:
    user_columns (list): List of user column names
    expected_columns (list, optional): List of expected column names
    
Returns:
    dict: Mapping suggestions with confidence scores
g      ?⁄exact_match)⁄column⁄
confidence⁄methodgÕÃÃÃÃÃÏ?⁄synonym_matchgöôôôôôÈ?⁄pattern_matchr´   ⁄fuzzy_matchc                 Û   ï U S   $ )Nr˙   r<   )⁄xs    r   ⁄<lambda>⁄6ColumnMapper.get_mapping_suggestions.<locals>.<lambda>g  s   Ä ®q∞™r   T)⁄key⁄reverse)r   r   r   rØ   r∂   r   r±   r   r≤   r≥   r¥   rµ   ⁄sort)
r   r¨   r   ⁄suggestionsrª   rº   ⁄col_suggestionsrΩ   ræ   ⁄
similaritys
             r   ⁄get_mapping_suggestions⁄$ColumnMapper.get_mapping_suggestions,  sl  Ä  —#‹#†D◊$9—$9◊$>—$>”$@”A–‡à‰$àHÿ!◊4—4∞X”>àN‡ àO ”1ÿ◊&—&ÿ,ÿ"%ÿ+Ò(Ù Û !1êÿ!ß]°]◊%6—%6∞|¿R”%H’Hÿ#◊*—*ÿ".ÿ&)ÿ"1Ò,ˆ Ò !1Û !1êÿ#ü}ô}◊0—0∞∏r÷BêG‹óyíy†º"ø-π-◊H”Hÿ'◊.—.ÿ&2ÿ*-ÿ&5Ò0ˆ Û  CÒ !1Û !1êÿ!◊7—7∏”Uê
ÿ†’#ÿ#◊*—*ÿ".ÿ&0ÿ"/Ò,ˆ Ò !1 ◊ — —%>»– —M‡$3ò‘!Ò[ %^ –r   )r   r   r   r   )N)⁄__name__⁄
__module__⁄__qualname__⁄__firstlineno__⁄__doc__r   r   r
   r   r   r¬   rØ   rµ   r€   rÈ   rÚ   rı   r	  ⁄__static_attributes__r<   r   r   r   r      sQ   Ü ÒÙ

"Ú2Ú{
Ùz4Ùl4Ùl\Ú|Ú09Ú!ÙF1!ÙfÚ8˜@?r   r   )r  ⁄pandasr∑   ⁄numpy⁄npr≤   ⁄logging⁄collectionsr   ⁄difflibr   rò   rî   ⁄basicConfig⁄INFO⁄	getLoggerr  rù   r   r<   r   r   ⁄<module>r     sR   ÒÛ € € 	€ › #› #€ € 	‡ ◊ “ ò'ü,ô,“ 'ÿ	◊	“	ò8”	$Ä˜Z	Ú Z	r   

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\ingestion\__pycache__\column_mapper.cpython-39.pyc ===
a
    èöhü`  „                   @   sv   d Z ddlZddlZddlZddlZddlmZ ddl	m
Z
 ddlZddlZejejdç e†e°ZG ddÑ dÉZdS )zU
Column Mapper Module
Handles intelligent mapping of user columns to expected format
È    N)⁄defaultdict)⁄SequenceMatcher)⁄levelc                   @   sÜ   e Zd ZdZdddÑZddÑ ZddÑ Zdd	d
ÑZdddÑZd ddÑZ	ddÑ Z
ddÑ ZddÑ Zd!ddÑZd"ddÑZddÑ Zd#ddÑZdS )$⁄ColumnMapperzy
    Class for mapping user columns to expected format
    Uses AI-powered techniques to intelligently match columns
    Nc                 C   s,   | † ° | _| †|°| _| †|°| _g | _dS )zÑ
        Initialize ColumnMapper
        
        Args:
            config_path (str, optional): Path to configuration file
        N)⁄_get_expected_columns⁄expected_columns⁄_load_synonyms⁄synonyms⁄_load_patterns⁄patterns⁄mapping_history)⁄self⁄config_path© r   ˙mC:\Users\Tranquil Abyss\fraud-detection-platform\app\..\src\fraud_detection_engine\ingestion\column_mapper.py⁄__init__   s    
zColumnMapper.__init__c                 C   s   t | j†° ÉS )z|
        Return the list of expected column names
        
        Returns:
            list: Expected column names
        )⁄listr   ⁄keys©r   r   r   r   ⁄get_expected_columns#   s    z!ColumnMapper.get_expected_columnsc                 C   sÏ   ddddúddddúddddúd	dd
dúddddúddddúddd
dúddd
dúddd
dúddd
dúddd
dúddd
dúddd
dúddd
dúddd
dúddd
dúddd
dúddd
dúddd
dúddd
dúddd
dúddd
dúddd
dúdúS ) zò
        Get the expected columns for the fraud detection system
        
        Returns:
            dict: Expected columns with descriptions
        z%Unique identifier for the transaction⁄stringT)⁄description⁄	data_type⁄requiredz Date and time of the transaction⁄datetimezTransaction amount⁄floatzCurrency code (e.g., USD, EUR)FzIdentifier of the senderzIdentifier of the receiverz1Type of sender account (e.g., personal, business)z3Type of receiver account (e.g., personal, business)zName of sender bankzName of receiver bankz)Location of sender (country, state, city)z+Location of receiver (country, state, city)z-Type of transaction (e.g., transfer, payment)z0Category of transaction (e.g., retail, services)zIdentifier of the merchantzCategory of the merchantz#IP address used for the transactionzIdentifier of the device usedzDescription of the transactionzAdditional notesz/Authorization status (e.g., approved, declined)z(Whether the transaction was charged back⁄booleanz?Whether the transaction is fraudulent (for supervised learning)©⁄transaction_id⁄	timestamp⁄amount⁄currency⁄	sender_id⁄receiver_id⁄sender_account_type⁄receiver_account_type⁄sender_bank⁄receiver_bank⁄sender_location⁄receiver_location⁄transaction_type⁄transaction_category⁄merchant_id⁄merchant_category⁄
ip_address⁄	device_idr   ⁄notes⁄authorization_status⁄chargeback_flag⁄
fraud_flagr   r   r   r   r   r   -   s∫    	˝˝˝˝˝˝˝˝˝˝˝˝˝˝˝˝˝˝˝˝˝˝˝ëz"ColumnMapper._get_expected_columnsc                 C   s\  g d¢g d¢g d¢g d¢g d¢g d¢g d¢g d¢g d	¢g d
¢g d¢g d¢g d¢g d¢g d¢g d¢g d¢g d¢g d¢g d¢g d¢g d¢g d¢dú}|êrXt j†|°êrXzxt|dÉèX}t†|°}d|v r˙|d †° D ](\}}||v r|| †|° q–|||< q–W d  É n1 ês0    Y  W n: têyV } z t	†
dt|Éõ ù° W Y d}~n
d}~0 0 |S )zŸ
        Load synonyms for column names
        
        Args:
            config_path (str, optional): Path to configuration file
            
        Returns:
            dict: Synonyms for expected columns
        )⁄idr   Ztx_idZtrans_id⁄	referenceZref_noZtransaction_no)r   ⁄date⁄timer   Z
trans_dateZ
trans_timeZtransaction_dateZtransaction_time)r    ⁄value⁄sum⁄totalZtransaction_amount⁄amtZ	tx_amount)r!   ⁄curr⁄ccyZcurrency_code)r"   Zfrom_idZpayer_idZ	source_idZoriginator_id)r#   Zto_idZpayee_idZdestination_idZbeneficiary_id)r$   Zfrom_account_typeZpayer_account_typeZsource_account_type)r%   Zto_account_typeZpayee_account_typeZdestination_account_type)r&   Z	from_bankZ
payer_bankZsource_bank)r'   Zto_bankZ
payee_bankZdestination_bank)r(   Zfrom_locationZpayer_locationZsource_locationZsender_countryZfrom_country)r)   Zto_locationZpayee_locationZdestination_locationZreceiver_countryZ
to_country)r*   Z
trans_type⁄typeZtx_type)r+   Ztrans_category⁄categoryZtx_category)r,   ⁄merchantZretailer_idZ	vendor_id)r-   Zmerchant_typeZretailer_categoryZvendor_category)r.   ⁄ip⁄ip_addr)r/   ⁄deviceZdevice_identifier©r   ⁄desc⁄detailsZ	narrative)r0   ⁄note⁄commentsZremark)r1   Zauth_status⁄statusZapproval_status)r2   Z
chargebackZis_chargeback)r3   Zfraud⁄is_fraudZ
fraudulentr   ⁄rZcolumn_synonymsNz$Error loading synonyms from config: ©⁄os⁄path⁄exists⁄open⁄yaml⁄	safe_load⁄items⁄extend⁄	Exception⁄logger⁄warning⁄str)r   r   r	   ⁄f⁄config⁄colZsyn_list⁄er   r   r   r   ™   sH    È
.*zColumnMapper._load_synonymsc                 C   sZ  g d¢g d¢g d¢g d¢g d¢g d¢g d¢g d¢g d	¢g d
¢g d¢g d¢g d¢g d¢g d¢g d¢ddgddgg d¢g d¢g d¢dgddgdú}|êrVt j†|°êrVzxt|dÉèX}t†|°}d|v r¯|d †° D ](\}}||v rÓ|| †|° qŒ|||< qŒW d  É n1 ês0    Y  W n: têyT } z t	†
dt|Éõ ù° W Y d}~n
d}~0 0 |S ) zÂ
        Load regex patterns for column names
        
        Args:
            config_path (str, optional): Path to configuration file
            
        Returns:
            dict: Regex patterns for expected columns
        )ztransaction.?idztx.?idz	trans.?idr5   zref.?(no|num))r   z
date.?timeztrans.?dateztrans.?time)r    r8   r9   r:   )r!   r=   r<   )z
sender.?idzfrom.?idz	payer.?idz
source.?idzoriginator.?id)zreceiver.?idzto.?idz	payee.?idzdestination.?idzbeneficiary.?id)zsender.?account.?typezfrom.?account.?typezpayer.?account.?type)zreceiver.?account.?typezto.?account.?typezpayee.?account.?type)zsender.?bankz
from.?bankzpayer.?bankzsource.?bank)zreceiver.?bankzto.?bankzpayee.?bankzdestination.?bank)zsender.?locationzfrom.?locationzpayer.?locationzsource.?locationzsender.?country)zreceiver.?locationzto.?locationzpayee.?locationzdestination.?locationzreceiver.?country)ztransaction.?typeztrans.?typeztx.?type)ztransaction.?categoryztrans.?categoryztx.?category)zmerchant.?idr@   zretailer.?idz
vendor.?id)zmerchant.?categoryzmerchant.?typezretailer.?categoryzip.?addressrA   z
device.?idrC   rD   )znotes?z	comments?zremarks?)zauthorization.?statuszauth.?statuszapproval.?statuszchargeback.?(flag|is)zfraud.?(flag|is)z	is.?fraudr   rK   Zcolumn_patternsNz$Error loading patterns from config: rL   )r   r   r   rY   rZ   r[   Zpat_listr\   r   r   r   r
   ‡   sH    È
.*zColumnMapper._load_patternsc                 C   s»  |du rt | j†° É}i }tÉ }|D ]0}| †|°}||v r$||vr$|||< |†|° q$|D ]T}||v rhqZ| †|°}|D ]6}||v rÑqv|| j†|g °v rv|||< |†|°  qZqvqZ|D ]n}||v r¬q¥| †|°}|D ]P}||v rﬁq–| j†|g °D ],}t	†
||t	j°rÏ|||< |†|°  êqqÏq– q¥q–q¥|D ]Ä}||v êr:êq(| †|°}d}	d}
|D ]<}||v êrbêqP| †||°}||
kêrP|dkêrP|}
|}	êqP|	êr(|	||< |†|	° êq(| j†tj†° ||dú° |S )aA  
        Automatically map user columns to expected columns
        
        Args:
            user_columns (list): List of user column names
            expected_columns (list, optional): List of expected column names
            
        Returns:
            dict: Mapping from user columns to expected columns
        Nr   Á333333„?)r   ⁄user_columns⁄mapping)r   r   r   ⁄set⁄_clean_column_name⁄addr	   ⁄getr   ⁄re⁄search⁄
IGNORECASE⁄_calculate_similarityr   ⁄append⁄pd⁄	Timestamp⁄now)r   r^   r   r_   ⁄used_columns⁄user_col⁄user_col_clean⁄expected_col⁄patternZ
best_matchZ
best_score⁄scorer   r   r   ⁄auto_map_columns  sp    







˝zColumnMapper.auto_map_columnsc                 C   s2   |† ° }t†dd|°}t†dd|°}|†d°}|S )zæ
        Clean column name for matching
        
        Args:
            column_name (str): Column name to clean
            
        Returns:
            str: Cleaned column name
        z	[^a-z0-9]⁄_z_+)⁄lowerrd   ⁄sub⁄strip)r   ⁄column_name⁄cleanedr   r   r   ra   t  s
    
zColumnMapper._clean_column_namec                 C   s   t d||É†° S )z‰
        Calculate similarity between two strings
        
        Args:
            str1 (str): First string
            str2 (str): Second string
            
        Returns:
            float: Similarity score (0-1)
        N)r   ⁄ratio)r   ⁄str1⁄str2r   r   r   rg   å  s    z"ColumnMapper._calculate_similarityc           	   
   C   s¥   zt|† ° }t†° }|†° D ]\}}||jv r|| ||< q|jD ]}||vr@|| ||< q@t†dt|Éõ dù° |W S  tyÆ } z"t†	dt
|Éõ ù° Ç W Y d}~n
d}~0 0 dS )zˆ
        Apply column mapping to a DataFrame
        
        Args:
            df (DataFrame): Input DataFrame
            mapping (dict): Column mapping
            
        Returns:
            DataFrame: DataFrame with mapped columns
        z,Column mapping applied successfully. Mapped z	 columns.zError applying column mapping: N)⁄copyri   ⁄	DataFramerS   ⁄columnsrV   ⁄info⁄lenrU   ⁄errorrX   )	r   ⁄dfr_   Z	result_dfZ	mapped_dfrm   ro   r[   r\   r   r   r   ⁄apply_mappingö  s    

zColumnMapper.apply_mappingc                    s‰   à du rddÑ | j †° D Éâ dg g g g dú}t|†° É}à D ]"}||vr<d|d< |d †|° q<á fd	dÑ| j D É}|D ]}||vrx|d
 †|° qx|d r∫|d †dd†|d °õ ù° |d
 r‡|d †dd†|d
 °õ ù° |S )z¯
        Validate a column mapping
        
        Args:
            mapping (dict): Column mapping
            required_columns (list, optional): List of required columns
            
        Returns:
            dict: Validation results
        Nc                 S   s   g | ]\}}|d  r|ëqS )r   r   )⁄.0r[   r   r   r   r   ⁄
<listcomp>…  Û    z1ColumnMapper.validate_mapping.<locals>.<listcomp>T)⁄valid⁄errors⁄warnings⁄missing_required⁄missing_optionalFrá   rä   c                    s   g | ]}|à vr|ëqS r   r   )rÑ   r[   ©⁄required_columnsr   r   rÖ   ‹  rÜ   rã   rà   zMissing required columns: z, râ   zMissing optional columns: )r   rS   r`   ⁄valuesrh   ⁄join)r   r_   rç   ⁄validation_resultZmapped_columnsr[   Zoptional_columnsr   rå   r   ⁄validate_mappingΩ  s4    ˚	ˇˇzColumnMapper.validate_mappingc              
   C   sÆ   |du ri }zb| j | j| j|dú}t|dÉè }tj||ddç W d  É n1 sR0    Y  t†d|õ ù° W n: ty® } z"t†	dt
|Éõ ù° Ç W Y d}~n
d}~0 0 dS )zµ
        Save a mapping template to file
        
        Args:
            file_path (str): Path to save the template
            mapping (dict, optional): Mapping to save
        N)r   r	   r   ⁄current_mapping⁄wF)⁄default_flow_stylezMapping template saved to zError saving mapping template: )r   r	   r   rP   rQ   ⁄dumprV   r   rU   rÅ   rX   )r   ⁄	file_pathr_   ⁄templaterY   r\   r   r   r   ⁄save_mapping_template  s    ¸.z"ColumnMapper.save_mapping_templatec              
   C   s»   zàt |dÉè}t†|°}W d  É n1 s,0    Y  d|v rH|d | _d|v rZ|d | _d|v rl|d | _t†d|õ ù° |†di °W S  t	y¬ } z"t†
dt|Éõ ù° Ç W Y d}~n
d}~0 0 dS )	z¿
        Load a mapping template from file
        
        Args:
            file_path (str): Path to the template file
            
        Returns:
            dict: Loaded mapping
        rK   Nr   r	   r   zMapping template loaded from rí   z Error loading mapping template: )rP   rQ   rR   r   r	   r   rV   r   rc   rU   rÅ   rX   )r   rñ   rY   ró   r\   r   r   r   ⁄load_mapping_template  s    
(


z"ColumnMapper.load_mapping_templatec           
   	   C   s  |du rt | j†° É}i }|D ]‰}| †|°}g }||v rJ|†|dddú° |D ](}|| j†|g °v rN|†|dddú° qN|D ]:}| j†|g °D ]&}t†	||tj
°ré|†|dddú° qéq||D ]*}| †||°}	|	d	krº|†||	d
dú° qº|jddÑ ddç |||< q|S )a:  
        Get mapping suggestions with confidence scores
        
        Args:
            user_columns (list): List of user column names
            expected_columns (list, optional): List of expected column names
            
        Returns:
            dict: Mapping suggestions with confidence scores
        Ng      ?Zexact_match)⁄column⁄
confidence⁄methodgÕÃÃÃÃÃÏ?Zsynonym_matchgöôôôôôÈ?Zpattern_matchr]   Zfuzzy_matchc                 S   s   | d S )Nrõ   r   )⁄xr   r   r   ⁄<lambda>g  rÜ   z6ColumnMapper.get_mapping_suggestions.<locals>.<lambda>T)⁄key⁄reverse)r   r   r   ra   rh   r	   rc   r   rd   re   rf   rg   ⁄sort)
r   r^   r   Zsuggestionsrm   rn   Zcol_suggestionsro   rp   Z
similarityr   r   r   ⁄get_mapping_suggestions,  sL    
˝˝
˝˝

z$ColumnMapper.get_mapping_suggestions)N)N)N)N)N)N)N)⁄__name__⁄
__module__⁄__qualname__⁄__doc__r   r   r   r   r
   rr   ra   rg   rÉ   rë   rò   rô   r¢   r   r   r   r   r      s   

}
6
6
^#
3
 r   )r¶   ⁄pandasri   ⁄numpy⁄nprd   ⁄logging⁄collectionsr   ⁄difflibr   rQ   rM   ⁄basicConfig⁄INFO⁄	getLoggerr£   rV   r   r   r   r   r   ⁄<module>   s   


=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\ingestion\__pycache__\data_loader.cpython-313.pyc ===
Û
    dÄöhQ:  „                   Û‡   ï S r SSKrSSKrSSKJr  SSKrSSK	r	SSK
J
r
  SSKrSSKr\R                  " S5        \	R                  " \	R                  S9  \	R                   " \5      r " S S5      rg)zJ
Data Loader Module
Handles loading and preprocessing of transaction data
È    N)⁄datetime⁄ignore)⁄levelc                   Ûb   ï \ rS rSrSrSS jrSS jrS rS rS r	S	 r
SS
 jrSS jrSS jrSrg)⁄
DataLoaderÈ   zÄ
Class for loading and preprocessing transaction data
Supports CSV, Excel, and Parquet files
Uses Dask for handling large files
c                 Û8   ï Xl         X l        SU l        SU l        g)zå
Initialize DataLoader

Args:
    use_dask (bool): Whether to use Dask for large files
    chunk_size (int): Chunk size for Dask processing
N)⁄use_dask⁄
chunk_size⁄data⁄original_data)⁄selfr
   r   s      ⁄kC:\Users\Tranquil Abyss\fraud-detection-platform\app\..\src\fraud_detection_engine\ingestion\data_loader.py⁄__init__⁄DataLoader.__init__   s   Ä  !åÿ$åÿàå	ÿ!à’Û    Nc                 Û˛  ï  Uc3  [         R                  R                  U5      S   SS R                  5       n[         R                  R	                  U5      S-  nU R
                  (       a3  US:î  a-  [        R                  SUS S35        U R                  " X40 UD6$ [        R                  SUS S	35        U R                  " X40 UD6$ ! [         a'  n[        R                  S
[        U5       35        e SnAff = f)zı
Load data from file

Args:
    file_path (str): Path to the data file
    file_type (str, optional): Type of file (csv, excel, parquet)
    **kwargs: Additional parameters for pandas/dask read functions
    
Returns:
    DataFrame: Loaded data
NÈ   i   Èd   zLoading large file (z.2fz MB) using DaskzLoading file (z MB) using pandaszError loading data: )⁄os⁄path⁄splitext⁄lower⁄getsizer
   ⁄logger⁄info⁄_load_with_dask⁄_load_with_pandas⁄	Exception⁄error⁄str)r   ⁄	file_path⁄	file_type⁄kwargs⁄	file_size⁄es         r   ⁄	load_data⁄DataLoader.load_data(   sÂ   Ä 	‡— ‹üGôG◊,—,®Y”7∏—:∏1∏2–>◊D—D”Fê	Ù üôüô®	”2∞k—BàI è}è}†®S£‹óë–2∞9∏S∞/¿–Q‘Rÿ◊+“+®I—K¿F—K–K‰óëòn®Y∞s®O–;L–M‘Nÿ◊-“-®i—M¿f—M–M¯‰Û 	‹èLâL–/¥∞A≥®x–8‘9ÿ˚	˙s   ÇBC ¬,C √
C<√"C7√7C<c                 Û™  ï  US:X  aP  [        US5       n[        R                  " UR                  5       5      nSSS5        WS   nUR	                  SU5        US;   a  [
        R                  " U40 UD6nOJUS;   a  [
        R                  " U40 UD6nO,US:X  a  [
        R                  " U40 UD6nO[        SU 35      eUR                  5       U l        U R                  U5      nXpl        [        R                  S	UR                    35        U$ ! , (       d  f       N›= f! ["         a'  n[        R%                  S
['        U5       35        e SnAff = f)z”
Load data using pandas

Args:
    file_path (str): Path to the data file
    file_type (str): Type of file
    **kwargs: Additional parameters for pandas read functions
    
Returns:
    DataFrame: Loaded data
⁄csv⁄rbN⁄encoding©r*   ⁄txt©⁄xlsx⁄xls⁄parquet˙Unsupported file type: z!Data loaded successfully. Shape: z Error loading data with pandas: )⁄open⁄chardet⁄detect⁄read⁄
setdefault⁄pd⁄read_csv⁄
read_excel⁄read_parquet⁄
ValueError⁄copyr   ⁄_preprocess_datar   r   r   ⁄shaper   r    r!   )	r   r"   r#   r$   ⁄f⁄resultr,   ⁄dfr&   s	            r   r   ⁄DataLoader._load_with_pandasH   s-  Ä 	‡òE”!‹ò)†T‘*®a‹$ü^ö^®AØF©F´H”5êF˜ +‡!†*—-êÿ◊!—!†*®h‘7 òN”*‹ó[í[†—5®f—5ëÿòo”-‹ó]í]†9—7∞—7ëÿòi”'‹ó_í_†Y—9∞&—9ë‰ –#:∏9∏+–!F”G–G "$ß°£àD‘ ◊&—&†r”*àB‡åI‹èKâK–;∏BøHπH∏:–F‘GÿàI˜/ +’*˚Ù2 Û 	‹èLâL–;ºC¿ªF∏8–D‘Eÿ˚	˙s.   ÇD! î%DπCD! ƒ
DƒD! ƒ!
Eƒ+"E≈Ec                 Û  ï  US;   a$  [         R                  " U4SU R                  0UD6nOFUS:X  a  [         R                  " U40 UD6nO([        R                  S5        U R                  " X40 UD6$ UR                  S5      U l        UR                  U R                  5      nX@l        [        R                  SUR                   35        U$ ! [         a'  n[        R                  S[!        U5       35        e SnAff = f)	zœ
Load data using Dask

Args:
    file_path (str): Path to the data file
    file_type (str): Type of file
    **kwargs: Additional parameters for dask read functions
    
Returns:
    DataFrame: Loaded data
r-   ⁄	blocksizer2   zADask does not support Excel files directly. Using pandas instead.iË  z0Data loaded successfully with Dask. Partitions: zError loading data with Dask: N)⁄ddr:   r   r<   r   ⁄warningr   ⁄headr   ⁄map_partitionsr?   r   r   ⁄npartitionsr   r    r!   )r   r"   r#   r$   rC   r&   s         r   r   ⁄DataLoader._load_with_daskt   s„   Ä 	‡òN”*‹ó[í[†—P∞d∑o±o–P»—Pëÿòi”'‹ó_í_†Y—9∞&—9ëÙ óë–b‘cÿ◊-“-®i—M¿f—M–M "$ß°®£àD‘ ◊"—"†4◊#8—#8”9àB‡åI‹èKâK–J»2œ>…>–JZ–[‘\ÿàI¯‰Û 	‹èLâL–9º#∏aª&∏–B‘Cÿ˚	˙s   ÇA/C ¡2AC √
C>√"C9√9C>c                 Û"	  ï  [        U5      nUR                  5       nU[        U5      -
  nUS:î  a  [        R                  SU S35        UR                   GH  n[
        R                  R                  R                  X   5      (       a)  X   R                  5       nX   R                  U5      X'   M]  [
        R                  R                  R                  X   5      (       d2  [
        R                  R                  R                  X   5      (       d  Mø  X   R                  5       n[        U5      S:î  a  X   R                  US   5      X'   M˙  X   R                  S5      X'   GM     UR                   HG  nSUR                  5       ;   d  SUR                  5       ;   d  M-   [
        R                  " X   SS9X'   MI     UR                   H   nSUR                  5       ;   d*  SUR                  5       ;   d  SUR                  5       ;   d  MA   [
        R                  R                  R                  X   5      (       a?  X   R"                  R%                  SSSS9X'   X   R"                  R%                  SS5      X'   [
        R&                  " X   SS9X'   MÃ     UR                   H2  nSUR                  5       ;   d  M  X   R)                  ["        5      X'   M4     UR                   H  nX   R*                  S:X  d  M  X   R-                  5       R/                  5       n[        U5      S::  d  MH  [1        S U 5       5      (       d  Ma  X   R%                  SSSSSSSSS.5      X'   MÅ     UR                   HÜ  n[
        R                  R                  R                  X   5      (       d  M5  X   R3                  5       S:î  d  MM  X   R3                  5       [        U5      S-  :  d  Mq  X   R)                  S5      X'   Mà     [        R                  S5        U$ !   [        R!                  S	U S
35         GM¬  = f!   [        R!                  S	U S35         GMç  = f! [4         a'  n[        R7                  S[#        U5       35        e SnAff = f) zv
Basic preprocessing of the data

Args:
    df (DataFrame): Input data
    
Returns:
    DataFrame: Preprocessed data
r   zRemoved z duplicate rows⁄Unknown⁄time⁄date⁄coerce)⁄errorszCould not convert z to datetime⁄amount⁄value⁄sumu   [\$,‚Ç¨,¬£,¬•]⁄ T)⁄regex⁄,z to numeric⁄id⁄objectÈ   c              3   ÛF   #   ï U  H  oR                  5       S ;   v ï  M     g7f))⁄true⁄false⁄yes⁄no⁄y⁄n⁄0⁄1N)r   )⁄.0⁄vals     r   ⁄	<genexpr>⁄.DataLoader._preprocess_data.<locals>.<genexpr>÷   s(   È Ä   5OÚ  CN–{~∑Y±Y≥[–Dv÷5vÚ  CN˘s   Ç!F)r]   r_   ra   rd   r^   r`   rb   rc   È
   g      ‡?⁄categoryzData preprocessing completedzError preprocessing data: N)⁄len⁄drop_duplicatesr   r   ⁄columnsr9   ⁄api⁄types⁄is_numeric_dtype⁄median⁄fillna⁄is_string_dtype⁄is_categorical_dtype⁄moder   ⁄to_datetimerH   r!   ⁄replace⁄
to_numeric⁄astype⁄dtype⁄dropna⁄unique⁄all⁄nuniquer   r    )	r   rC   ⁄initial_rows⁄removed_rows⁄col⁄
median_val⁄mode_val⁄unique_valsr&   s	            r   r?   ⁄DataLoader._preprocess_dataô   s£  Ä E	‰òrõ7àLÿ◊#—#”%àBÿ'¨#®b´'—1àLÿòa”‹óëòh†|†n∞O–D‘E ózïzê‰ó6ë6ó<ë<◊0—0∞±◊9—9ÿ!#°ß°”!1êJÿ ôgünôn®Z”8êBìG‰óVëVó\ë\◊1—1∞"±'◊:—:ºbøfπfølπl◊>_—>_–`b—`g◊>h”>hÿ!ôwü|ô|õ~êH‹ò8ì}†q”(ÿ"$°'ß.°.∞∏!±”"=òõ‡"$°'ß.°.∞”";òúÒ " ózîzêÿòSüYôYõ[”(®F∞c∑i±i≥k’,AO‹"$ß.¢.∞±¿—"JòõÒ " ózîzêÿòsüyôyõ{”*®g∏øπª”.D»–QT◊QZ—QZ”Q\’H\N‰ü6ô6ü<ô<◊7—7∏π◊@—@ÿ&(°gßk°k◊&9—&9–:J»B–VZ–&9–&[òBôGÿ&(°gßk°k◊&9—&9∏#∏r”&BòBôG‹"$ß-¢-∞±¿—"IòõÒ " ózîzêÿò3ü9ô9õ;’&ÿ ôgünôn¨S”1êBìGÒ "
 ózîzêÿë7ó=ë=†H’,ÿ"$°'ß.°.”"2◊"9—"9”";êK‹ò;”'®1’,¥Ò  5OÒ  CNÛ  5O˜  2OÛ  2Oÿ"$°'ß/°/ÿ$(∞∏D¿tÿ%*∞%∏e»%Ò3Û #òõÒ	 " ózîzê‹óFëFóLëL◊0—0∞±◊9”9ÿëGóOëO”%®’*ÿëGóOëO”%¨®B´∞#©’5ÿ ôgünôn®Z”8êBìGÒ	 "Ù èKâK–6‘7ÿàI¯MO‹üô–);∏C∏5¿–'M◊N–N˚N‹üô–);∏C∏5¿–'L◊M–M˚Ù6 Û 	‹èLâL–5¥c∏!≥f∞X–>‘?ÿ˚	˙s|   ÇDQ ƒB
Q ∆$P∆=AQ »BP; %Q À <Q Ã -Q Ã1Q Õ
AQ Œ-Q œ Q œ)/Q –P8–4Q –;Q—Q —
R—'"R	“	Rc           
      Û*  ï U R                   c  SS0$ U R                   R                  [        U R                   R                  5      [	        U R                   R
                  5      [	        U R                   R                  5       R                  5       5      [	        U R                   R                  SS95      S.n0 nU R                   R                  [        R                  /S9R                   Hî  nU R                   U   R                  5       U R                   U   R                  5       U R                   U   R                  5       U R                   U   R                  5       U R                   U   R!                  5       S.X#'   Mñ     X!S'   0 nU R                   R                  S	S
/S9R                   H]  nU R                   U   R#                  5       U R                   U   R%                  5       R'                  S5      R)                  5       S.XC'   M_     XAS'   U$ )zL
Get information about the loaded data

Returns:
    dict: Data information
r    ˙No data loadedT)⁄deep)r@   rm   ⁄dtypes⁄missing_values⁄memory_usage)⁄include)⁄min⁄max⁄meanrq   ⁄std⁄numeric_statsrZ   rj   È   )⁄unique_count⁄
top_values⁄categorical_stats)r   r@   ⁄listrm   ⁄dictrâ   ⁄isnullrU   rã   ⁄select_dtypes⁄np⁄numberrç   ré   rè   rq   rê   r~   ⁄value_countsrI   ⁄to_dict)r   r   rë   rÅ   rï   s        r   ⁄get_data_info⁄DataLoader.get_data_infoÍ   sπ  Ä  è9â9—ÿ–-–.–. óYëYó_ë_‹òDüIôI◊-—-”.‹ò4ü9ô9◊+—+”,‹"†4ß9°9◊#3—#3”#5◊#9—#9”#;”<‹ †ß°◊!7—!7∏T–!7–!B”CÒ
à àÿó9ë9◊*—*¥B∑I±I∞;–*–?◊G‘GàC‡óyëy†ë~◊)—)”+ÿóyëy†ë~◊)—)”+ÿü	ô	†#ô◊+—+”-ÿü)ô)†Cô.◊/—/”1ÿóyëy†ë~◊)—)”+Ò"àM”Ò H !.à_— –ÿó9ë9◊*—*∞H∏j–3I–*–J◊R‘RàC‡ $ß	°	®#°◊ 6— 6” 8ÿ"üiôi®ôn◊9—9”;◊@—@¿”C◊K—K”MÒ&–”"Ò S %6– —!‡àr   c                 Û&  ï U R                   c  g[        U R                   [        R                  5      (       a  U R                   R	                  U5      $ [        U R                   5      U:î  a  U R                   R                  U5      $ U R                   $ )zp
Get a sample of the data

Args:
    n (int): Number of rows to return
    
Returns:
    DataFrame: Sample data
N)r   ⁄
isinstancerG   ⁄	DataFramerI   rk   ⁄sample)r   rb   s     r   ⁄
get_sample⁄DataLoader.get_sample  sf   Ä  è9â9—ÿ‰êdóiëi§ß°◊.—.ÿó9ë9ó>ë>†!”$–$‰*-®dØi©i´.∏1”*<ê4ó9ë9◊#—#†A”&–K¿$«)¡)–Kr   c                 Û∏  ï U R                   c  SSS.$ Uc  / SQnS/ / S.nU Vs/ s H   o3U R                   R                  ;  d  M  UPM"     nnU(       a+  SUS'   US   R                  S	S
R                  U5       35        SU R                   R                  ;   aI  U R                   S   R	                  5       R                  5       nUS:î  a  US   R                  SU S35        / SQnU Hk  nX0R                   R                  ;   d  M  U R                   U   R                  5       R                  5       nUS:î  d  MQ  US   R                  SU SU 35        Mm     SU R                   R                  ;   a>  U R                   S   S:  R                  5       nUS:î  a  US   R                  SU S35        SU R                   R                  ;   aå  [        R                  R                  R                  U R                   S   5      (       aQ  U R                   S   [        R                  " 5       :Ñ  R                  5       n	U	S:î  a  US   R                  SU	 S35        U$ s  snf )zû
Validate the data against required columns

Args:
    required_columns (list, optional): List of required columns
    
Returns:
    dict: Validation results
Frá   )⁄validr    )⁄transaction_id⁄	timestamprS   ⁄	sender_id⁄receiver_idT)rß   rR   ⁄warningsrß   rR   zMissing required columns: z, r®   r   r¨   zFound z duplicate transaction IDs)r®   r©   rS   z missing values in rS   z# transactions with negative amountsr©   z$ transactions with future timestamps)r   rm   ⁄append⁄join⁄
duplicatedrU   rò   r9   rn   ro   ⁄is_datetime64_any_dtyper   ⁄now)
r   ⁄required_columns⁄validation_resultrÅ   ⁄missing_columns⁄duplicate_ids⁄critical_columns⁄missing_count⁄negative_amounts⁄future_datess
             r   ⁄validate_data⁄DataLoader.validate_data'  s?  Ä  è9â9—ÿ"–-=—>–>‡—#⁄d– ÿÿÒ
–Ò +;”[“*:†3»œ…◊IZ—IZ—>Zü3—*:à–[ﬁÿ).–òg—&ÿòh—'◊.—.–1K»DœI…I–Ve”Lf–Kg–/h‘i òtüyôy◊0—0”0ÿ üIôI–&6—7◊B—B”D◊H—H”JàMÿòq” ÿ!†*—-◊4—4∞v∏m∏_–Lf–5g‘hÚ E–€#àCÿóiëi◊'—'’'ÿ $ß	°	®#°◊ 5— 5” 7◊ ;— ;” =êÿ †1’$ÿ%†j—1◊8—8∏6¿-¿–Pc–dg–ch–9i÷jÒ	 $ êtóyëy◊(—(”(ÿ $ß	°	®(— 3∞a— 7◊<—<”>–ÿ†!”#ÿ!†*—-◊4—4∞v–>N–=O–Or–5s‘t ò$ü)ô)◊+—+”+¥∑±∑±◊0T—0T–UY◊U^—U^–_j—Uk◊0l—0lÿ üIôI†k—2¥X∑\≤\≥^—C◊H—H”JàLÿòa”ÿ!†*—-◊4—4∞v∏l∏^–Ko–5p‘q‡ – ˘Ú? \s   §I¡Ic                 ÛP  ï U R                   c  [        R                  S5        g Uc3  [        R                  R                  U5      S   SS R                  5       nUS:X  a   U R                   R                  " U4SS0UD6  OZUS;   a   U R                   R                  " U4SS0UD6  O4US:X  a   U R                   R                  " U4SS0UD6  O[        S	U 35      e[        R                  S
U 35        g! [         a'  n[        R                  S[        U5       35        e SnAff = f)z’
Save the processed data to a file

Args:
    file_path (str): Path to save the file
    file_type (str, optional): Type of file (csv, excel, parquet)
    **kwargs: Additional parameters for pandas save functions
NzNo data to saver   r*   ⁄indexFr/   r2   r3   zData saved successfully to zError saving data: )r   r   r    r   r   r   r   ⁄to_csv⁄to_excel⁄
to_parquetr=   r   r   r!   )r   r"   r#   r$   r&   s        r   ⁄	save_data⁄DataLoader.save_data_  s  Ä  è9â9—‹èLâL–*‘+ÿ	‡— ‹üGôG◊,—,®Y”7∏—:∏1∏2–>◊D—D”Fê	 òE”!ÿó	ë	◊ “ †—B∞%–B∏6”Bÿòo”-ÿó	ë	◊"“"†9—D∞E–D∏V”Dÿòi”'ÿó	ë	◊$“$†Y—F∞e–F∏v”F‰ –#:∏9∏+–!F”G–G‰èKâK–5∞i∞[–A’B¯‰Û 	‹èLâL–.¨s∞1´v®h–7‘8ÿ˚	˙s   •CC4 √4
D%√>"D ƒ D%)r   r   r   r
   )Ti†Ü )N)rí   )⁄__name__⁄
__module__⁄__qualname__⁄__firstlineno__⁄__doc__r   r'   r   r   r?   rû   r§   r∫   r¡   ⁄__static_attributes__© r   r   r   r      s<   Ü ÒÙ"ÙÚ@*ÚX#ÚJOÚb)ÙVLÙ$6!˜p r   r   )r«   ⁄pandasr9   ⁄numpyrö   ⁄dask.dataframe⁄	dataframerG   r   ⁄loggingr   r5   r¨   ⁄filterwarnings⁄basicConfig⁄INFO⁄	getLoggerr√   r   r   r…   r   r   ⁄<module>r”      sb   ÒÛ
 € › € 	€ › € € ÿ ◊ “ ò‘ ! ◊ “ ò'ü,ô,“ 'ÿ	◊	“	ò8”	$Ä˜kÚ kr   

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\ingestion\__pycache__\data_loader.cpython-39.pyc ===
a
    èöhQ:  „                   @   sÄ   d Z ddlZddlZddlmZ ddlZddl	Z	ddl
m
Z
 ddlZddlZe†d° e	je	jdç e	†e°ZG ddÑ dÉZdS )zJ
Data Loader Module
Handles loading and preprocessing of transaction data
È    N)⁄datetime⁄ignore)⁄levelc                   @   sb   e Zd ZdZdddÑZdddÑZd	d
Ñ ZddÑ ZddÑ ZddÑ Z	dddÑZ
dddÑZdddÑZdS )⁄
DataLoaderzê
    Class for loading and preprocessing transaction data
    Supports CSV, Excel, and Parquet files
    Uses Dask for handling large files
    TÈ†Ü c                 C   s   || _ || _d| _d| _dS )zº
        Initialize DataLoader
        
        Args:
            use_dask (bool): Whether to use Dask for large files
            chunk_size (int): Chunk size for Dask processing
        N)⁄use_dask⁄
chunk_size⁄data⁄original_data)⁄selfr   r   © r   ˙kC:\Users\Tranquil Abyss\fraud-detection-platform\app\..\src\fraud_detection_engine\ingestion\data_loader.py⁄__init__   s    zDataLoader.__init__Nc              
   K   s⁄   zö|du r&t j†|°d ddÖ †° }t j†|°d }| jrn|dkrnt†d|dõdù° | j||fi |§éW S t†d|dõd	ù° | j	||fi |§éW S W n: t
y‘ } z"t†d
t|Éõ ù° Ç W Y d}~n
d}~0 0 dS )aE  
        Load data from file
        
        Args:
            file_path (str): Path to the data file
            file_type (str, optional): Type of file (csv, excel, parquet)
            **kwargs: Additional parameters for pandas/dask read functions
            
        Returns:
            DataFrame: Loaded data
        NÈ   i   Èd   zLoading large file (z.2fz MB) using DaskzLoading file (z MB) using pandaszError loading data: )⁄os⁄path⁄splitext⁄lower⁄getsizer   ⁄logger⁄info⁄_load_with_dask⁄_load_with_pandas⁄	Exception⁄error⁄str)r   ⁄	file_path⁄	file_type⁄kwargs⁄	file_size⁄er   r   r   ⁄	load_data(   s    zDataLoader.load_datac           	   
   K   s*  zË|dkrVt |dÉè}t†|†° °}W d  É n1 s80    Y  |d }|†d|° |dv rrtj|fi |§é}nF|dv rétj|fi |§é}n*|dkr™tj|fi |§é}nt	d|õ ùÉÇ|†
° | _| †|°}|| _t†d	|jõ ù° |W S  têy$ } z"t†d
t|Éõ ù° Ç W Y d}~n
d}~0 0 dS )a#  
        Load data using pandas
        
        Args:
            file_path (str): Path to the data file
            file_type (str): Type of file
            **kwargs: Additional parameters for pandas read functions
            
        Returns:
            DataFrame: Loaded data
        ⁄csv⁄rbN⁄encoding©r#   ⁄txt©⁄xlsx⁄xls⁄parquet˙Unsupported file type: z!Data loaded successfully. Shape: z Error loading data with pandas: )⁄open⁄chardet⁄detect⁄read⁄
setdefault⁄pd⁄read_csv⁄
read_excel⁄read_parquet⁄
ValueError⁄copyr
   ⁄_preprocess_datar	   r   r   ⁄shaper   r   r   )	r   r   r   r   ⁄f⁄resultr%   ⁄dfr!   r   r   r   r   H   s*    ,

zDataLoader._load_with_pandasc              
   K   s‘   zî|dv r$t j|fd| ji|§é}n<|dkr@t j|fi |§é}n t†d° | j||fi |§éW S |†d°| _|†	| j
°}|| _t†d|jõ ù° |W S  tyŒ } z"t†dt|Éõ ù° Ç W Y d}~n
d}~0 0 dS )	a  
        Load data using Dask
        
        Args:
            file_path (str): Path to the data file
            file_type (str): Type of file
            **kwargs: Additional parameters for dask read functions
            
        Returns:
            DataFrame: Loaded data
        r&   ⁄	blocksizer+   zADask does not support Excel files directly. Using pandas instead.iË  z0Data loaded successfully with Dask. Partitions: zError loading data with Dask: N)⁄ddr3   r   r5   r   ⁄warningr   ⁄headr
   Zmap_partitionsr8   r	   r   Znpartitionsr   r   r   )r   r   r   r   r<   r!   r   r   r   r   t   s    
zDataLoader._load_with_daskc           	      C   sF  êzt |É}|†° }|t |É }|dkr:t†d|õ dù° |jD ]ú}tjj†|| °rv|| †	° }|| †
|°||< q@tjj†|| °sötjj†|| °r@|| †° }t |Édkr || †
|d °||< q@|| †
d°||< q@|jD ]X}d|†° v êsd|†° v r‰ztj|| ddç||< W q‰   t†d	|õ d
ù° Y q‰0 q‰|jD ]Æ}d|†° v êsrd|†° v êsrd|†° v êrDz^tjj†|| °êr∏|| jjddddç||< || j†dd°||< tj|| ddç||< W n   t†d	|õ dù° Y n0 êqD|jD ]&}d|†° v êr˙|| †t°||< êq˙|jD ]l}|| jdkêr(|| †° †° }t |Édkêr(tddÑ |D ÉÉêr(|| †dddddddddú°||< êq(|jD ]X}tjj†|| °êrú|| †° dkêrú|| †° t |Éd k êrú|| †d°||< êqút†d° |W S  têy@ } z"t†dt|Éõ ù° Ç W Y d }~n
d }~0 0 d S )!z∂
        Basic preprocessing of the data
        
        Args:
            df (DataFrame): Input data
            
        Returns:
            DataFrame: Preprocessed data
        r   zRemoved z duplicate rows⁄Unknown⁄time⁄date⁄coerce)⁄errorszCould not convert z to datetime⁄amount⁄value⁄sumu   [\$,‚Ç¨,¬£,¬•]⁄ T)⁄regex˙,z to numeric⁄id⁄objectÈ   c                 s   s   | ]}|† ° d v V  qdS ))⁄true⁄false⁄yes⁄no⁄y⁄n⁄0⁄1N)r   )⁄.0⁄valr   r   r   ⁄	<genexpr>÷   Û    z.DataLoader._preprocess_data.<locals>.<genexpr>F)rO   rQ   rS   rV   rP   rR   rT   rU   È
   g      ‡?⁄categoryzData preprocessing completedzError preprocessing data: N)⁄len⁄drop_duplicatesr   r   ⁄columnsr2   ⁄api⁄types⁄is_numeric_dtype⁄median⁄fillna⁄is_string_dtype⁄is_categorical_dtype⁄moder   ⁄to_datetimer?   r   ⁄replace⁄
to_numeric⁄astype⁄dtype⁄dropna⁄unique⁄all⁄nuniquer   r   )	r   r<   Zinitial_rowsZremoved_rows⁄col⁄
median_val⁄mode_val⁄unique_valsr!   r   r   r   r8   ô   sj    

$

*

"˛
ˇ˛
zDataLoader._preprocess_datac                 C   s  | j du rddiS | j jt| j jÉt| j jÉt| j †° †° Ét| j jddçÉdú}i }| j j	t
jgdçjD ]J}| j | †° | j | †° | j | †° | j | †° | j | †° dú||< qj||d	< i }| j j	d
dgdçjD ]0}| j | †° | j | †° †d°†° dú||< q÷||d< |S )zt
        Get information about the loaded data
        
        Returns:
            dict: Data information
        Nr   ˙No data loadedT)⁄deep)r9   r_   ⁄dtypes⁄missing_values⁄memory_usage)⁄include)⁄min⁄max⁄meanrc   ⁄std⁄numeric_statsrM   r\   È   )Zunique_countZ
top_values⁄categorical_stats)r	   r9   ⁄listr_   ⁄dictrw   ⁄isnullrH   ry   ⁄select_dtypes⁄np⁄numberr{   r|   r}   rc   r~   rp   ⁄value_countsr@   ⁄to_dict)r   r   r   rq   rÅ   r   r   r   ⁄get_data_infoÍ   s0    


˚	˚˛zDataLoader.get_data_inforÄ   c                 C   sL   | j du rdS t| j tjÉr(| j †|°S t| j É|krB| j †|°S | j S dS )z∞
        Get a sample of the data
        
        Args:
            n (int): Number of rows to return
            
        Returns:
            DataFrame: Sample data
        N)r	   ⁄
isinstancer>   ⁄	DataFramer@   r]   ⁄sample)r   rT   r   r   r   ⁄
get_sample  s
    

zDataLoader.get_samplec           
         sî  à j du rdddúS |du r$g d¢}dg g dú}á fdd	Ñ|D É}|rhd|d
< |d †dd†|°õ ù° dà j jv r§à j d †° †° }|dkr§|d †d|õ dù° g d¢}|D ]D}|à j jv r∞à j | †° †° }|dkr∞|d †d|õ d|õ ù° q∞dà j jv êr6à j d dk †° }|dkêr6|d †d|õ dù° dà j jv êrêtjj	†
à j d °êrêà j d t†° k†° }	|	dkêrê|d †d|	õ dù° |S )zﬁ
        Validate the data against required columns
        
        Args:
            required_columns (list, optional): List of required columns
            
        Returns:
            dict: Validation results
        NFru   )⁄validr   )⁄transaction_id⁄	timestamprF   ⁄	sender_id⁄receiver_idT)rè   rE   ⁄warningsc                    s   g | ]}|à j jvr|ëqS r   )r	   r_   )rW   rq   ©r   r   r   ⁄
<listcomp>>  rZ   z,DataLoader.validate_data.<locals>.<listcomp>rè   rE   zMissing required columns: z, rê   r   rî   zFound z duplicate transaction IDs)rê   rë   rF   z missing values in rF   z# transactions with negative amountsrë   z$ transactions with future timestamps)r	   ⁄append⁄joinr_   ⁄
duplicatedrH   rÑ   r2   r`   ra   ⁄is_datetime64_any_dtyper   ⁄now)
r   Zrequired_columnsZvalidation_result⁄missing_columnsZduplicate_idsZcritical_columnsrq   Zmissing_countZnegative_amountsZfuture_datesr   rï   r   ⁄validate_data'  s>    


˝
$
zDataLoader.validate_datac              
   K   s  | j du rt†d° dS z¨|du r>tj†|°d ddÖ †° }|dkr`| j j|fddi|§é nR|dv rÇ| j j|fddi|§é n0|dkr§| j j	|fddi|§é nt
d	|õ ùÉÇt†d
|õ ù° W n: ty˛ } z"t†dt|Éõ ù° Ç W Y d}~n
d}~0 0 dS )a  
        Save the processed data to a file
        
        Args:
            file_path (str): Path to save the file
            file_type (str, optional): Type of file (csv, excel, parquet)
            **kwargs: Additional parameters for pandas save functions
        NzNo data to saver   r#   ⁄indexFr(   r+   r,   zData saved successfully to zError saving data: )r	   r   r   r   r   r   r   ⁄to_csv⁄to_excel⁄
to_parquetr6   r   r   r   )r   r   r   r   r!   r   r   r   ⁄	save_data_  s"    	

zDataLoader.save_data)Tr   )N)rÄ   )N)N)⁄__name__⁄
__module__⁄__qualname__⁄__doc__r   r"   r   r   r8   rä   ré   rù   r¢   r   r   r   r   r      s   

 ,%Q+

8r   )r¶   ⁄pandasr2   ⁄numpyrÜ   Zdask.dataframe⁄	dataframer>   r   ⁄loggingr   r.   rî   ⁄filterwarnings⁄basicConfig⁄INFO⁄	getLoggerr£   r   r   r   r   r   r   ⁄<module>   s   



=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\ingestion\__pycache__\__init__.cpython-313.pyc ===
Û
    dÄöh    „                   Û   ï g )N© r   Û    ⁄hC:\Users\Tranquil Abyss\fraud-detection-platform\app\..\src\fraud_detection_engine\ingestion\__init__.py⁄<module>r      s   Òr   

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\ingestion\__pycache__\__init__.cpython-39.pyc ===
a
    èöh    „                   @   s   d S )N© r   r   r   ˙hC:\Users\Tranquil Abyss\fraud-detection-platform\app\..\src\fraud_detection_engine\ingestion\__init__.py⁄<module>   Û    

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\models\rule_based.py ===
"""
Rule-based Models Module
Implements rule-based fraud detection
"""

import pandas as pd
import numpy as np
import re
from datetime import datetime, timedelta
import yaml
import os
import warnings
import logging
from typing import Dict, List, Tuple, Union, Callable

from fraud_detection_engine.utils.api_utils import is_api_available, get_demo_sanctions_data, get_demo_tax_compliance_data, get_demo_bank_verification_data, get_demo_identity_verification_data

warnings.filterwarnings('ignore')
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class RuleEngine:
    """
    Class for rule-based fraud detection
    Implements configurable rules with weights
    """
    
    def __init__(self, config_path=None, threshold=0.7):
        """
        Initialize RuleEngine
        
        Args:
            config_path (str, optional): Path to configuration file
            threshold (float): Threshold for rule violation
        """
        self.threshold = threshold
        self.rules = {}
        self.rule_weights = {}
        self.rule_descriptions = {}
        self.fitted = False
        self.api_available = {}
        
        # Load configuration
        if config_path and os.path.exists(config_path):
            self._load_config(config_path)
        else:
            self._load_default_rules()
    
    def _load_config(self, config_path):
        """
        Load configuration from YAML file
        
        Args:
            config_path (str): Path to configuration file
        """
        try:
            with open(config_path, 'r') as f:
                config = yaml.safe_load(f)
            
            # Load rules
            if 'rules' in config:
                for rule_name, rule_config in config['rules'].items():
                    if rule_config.get('enabled', True):
                        self.rules[rule_name] = self._create_rule_function(rule_config)
                        self.rule_weights[rule_name] = rule_config.get('weight', 1.0)
                        self.rule_descriptions[rule_name] = rule_config.get('description', '')
            
            # Check API availability for rules that depend on external services
            self.api_available = {
                'sanctions': is_api_available('sanctions'),
                'tax_compliance': is_api_available('tax_compliance'),
                'bank_verification': is_api_available('bank_verification'),
                'identity_verification': is_api_available('identity_verification'),
                'geolocation': is_api_available('geolocation')
            }
            
            logger.info(f"API availability: {self.api_available}")
            
        except Exception as e:
            logger.error(f"Error loading configuration: {str(e)}")
            self._load_default_rules()
    
    def _load_default_rules(self):
        """
        Load default rules
        """
        try:
            # Amount-based rules
            self.rules['high_amount'] = self._high_amount_rule
            self.rule_weights['high_amount'] = 0.3
            self.rule_descriptions['high_amount'] = "Transaction amount exceeds threshold"
            
            self.rules['unusual_amount_for_sender'] = self._unusual_amount_for_sender_rule
            self.rule_weights['unusual_amount_for_sender'] = 0.2
            self.rule_descriptions['unusual_amount_for_sender'] = "Amount is unusual for the sender"
            
            self.rules['unusual_amount_for_receiver'] = self._unusual_amount_for_receiver_rule
            self.rule_weights['unusual_amount_for_receiver'] = 0.2
            self.rule_descriptions['unusual_amount_for_receiver'] = "Amount is unusual for the receiver"
            
            self.rules['round_amount'] = self._round_amount_rule
            self.rule_weights['round_amount'] = 0.1
            self.rule_descriptions['round_amount'] = "Transaction amount is suspiciously round"
            
            # Frequency-based rules
            self.rules['high_frequency_sender'] = self._high_frequency_sender_rule
            self.rule_weights['high_frequency_sender'] = 0.3
            self.rule_descriptions['high_frequency_sender'] = "High transaction frequency from sender"
            
            self.rules['high_frequency_receiver'] = self._high_frequency_receiver_rule
            self.rule_weights['high_frequency_receiver'] = 0.3
            self.rule_descriptions['high_frequency_receiver'] = "High transaction frequency to receiver"
            
            self.rules['rapid_succession'] = self._rapid_succession_rule
            self.rule_weights['rapid_succession'] = 0.4
            self.rule_descriptions['rapid_succession'] = "Multiple transactions in rapid succession"
            
            # Location-based rules
            self.rules['cross_border'] = self._cross_border_rule
            self.rule_weights['cross_border'] = 0.3
            self.rule_descriptions['cross_border'] = "Transaction crosses international borders"
            
            self.rules['high_risk_country'] = self._high_risk_country_rule
            self.rule_weights['high_risk_country'] = 0.5
            self.rule_descriptions['high_risk_country'] = "Transaction involves high-risk country"
            
            self.rules['unusual_location_for_sender'] = self._unusual_location_for_sender_rule
            self.rule_weights['unusual_location_for_sender'] = 0.3
            self.rule_descriptions['unusual_location_for_sender'] = "Transaction from unusual location for sender"
            
            # Time-based rules
            self.rules['unusual_hour'] = self._unusual_hour_rule
            self.rule_weights['unusual_hour'] = 0.2
            self.rule_descriptions['unusual_hour'] = "Transaction during unusual hours"
            
            self.rules['weekend'] = self._weekend_rule
            self.rule_weights['weekend'] = 0.1
            self.rule_descriptions['weekend'] = "Transaction on weekend"
            
            # Identity-based rules
            self.rules['new_sender'] = self._new_sender_rule
            self.rule_weights['new_sender'] = 0.3
            self.rule_descriptions['new_sender'] = "First transaction from new sender"
            
            self.rules['new_receiver'] = self._new_receiver_rule
            self.rule_weights['new_receiver'] = 0.3
            self.rule_descriptions['new_receiver'] = "First transaction to new receiver"
            
            # External API-based rules
            self.rules['sanctions_check'] = self._sanctions_check_rule
            self.rule_weights['sanctions_check'] = 0.5
            self.rule_descriptions['sanctions_check'] = "Transaction involves sanctioned entities"
            
            self.rules['tax_compliance'] = self._tax_compliance_rule
            self.rule_weights['tax_compliance'] = 0.3
            self.rule_descriptions['tax_compliance'] = "Transaction involves non-compliant entities"
            
            self.rules['bank_verification'] = self._bank_verification_rule
            self.rule_weights['bank_verification'] = 0.3
            self.rule_descriptions['bank_verification'] = "Transaction involves unverified bank accounts"
            
            self.rules['identity_verification'] = self._identity_verification_rule
            self.rule_weights['identity_verification'] = 0.3
            self.rule_descriptions['identity_verification'] = "Transaction involves unverified identities"
            
            # Check API availability
            self.api_available = {
                'sanctions': is_api_available('sanctions'),
                'tax_compliance': is_api_available('tax_compliance'),
                'bank_verification': is_api_available('bank_verification'),
                'identity_verification': is_api_available('identity_verification'),
                'geolocation': is_api_available('geolocation')
            }
            
            logger.info(f"API availability: {self.api_available}")
            
        except Exception as e:
            logger.error(f"Error loading default rules: {str(e)}")
            raise
    
    def _create_rule_function(self, rule_config):
        """
        Create a rule function from configuration
        
        Args:
            rule_config (dict): Rule configuration
            
        Returns:
            function: Rule function
        """
        rule_type = rule_config.get('type')
        
        if rule_type == 'amount_threshold':
            threshold = rule_config.get('threshold', 10000)
            return lambda row: row.get('amount', 0) > threshold
        
        elif rule_type == 'sender_amount_outlier':
            std_multiplier = rule_config.get('std_multiplier', 3)
            min_transactions = rule_config.get('min_transactions', 5)
            return lambda row: self._is_sender_amount_outlier(row, std_multiplier, min_transactions)
        
        elif rule_type == 'receiver_amount_outlier':
            std_multiplier = rule_config.get('std_multiplier', 3)
            min_transactions = rule_config.get('min_transactions', 5)
            return lambda row: self._is_receiver_amount_outlier(row, std_multiplier, min_transactions)
        
        elif rule_type == 'round_amount':
            threshold = rule_config.get('threshold', 1000)
            return lambda row: row.get('amount', 0) > threshold and row.get('amount', 0) % 1000 == 0
        
        elif rule_type == 'high_frequency_sender':
            time_window = rule_config.get('time_window', '1H')
            max_transactions = rule_config.get('max_transactions', 10)
            return lambda row: self._is_high_frequency_sender(row, time_window, max_transactions)
        
        elif rule_type == 'high_frequency_receiver':
            time_window = rule_config.get('time_window', '1H')
            max_transactions = rule_config.get('max_transactions', 10)
            return lambda row: self._is_high_frequency_receiver(row, time_window, max_transactions)
        
        elif rule_type == 'rapid_succession':
            time_window = rule_config.get('time_window', '5M')
            min_transactions = rule_config.get('min_transactions', 3)
            return lambda row: self._is_rapid_succession(row, time_window, min_transactions)
        
        elif rule_type == 'cross_border':
            return lambda row: self._is_cross_border(row)
        
        elif rule_type == 'high_risk_country':
            countries = rule_config.get('countries', ['North Korea', 'Iran', 'Syria', 'Cuba'])
            return lambda row: self._is_high_risk_country(row, countries)
        
        elif rule_type == 'unusual_location_for_sender':
            return lambda row: self._is_unusual_location_for_sender(row)
        
        elif rule_type == 'unusual_hour':
            start_hour = rule_config.get('start_hour', 23)
            end_hour = rule_config.get('end_hour', 5)
            return lambda row: self._is_unusual_hour(row, start_hour, end_hour)
        
        elif rule_type == 'weekend':
            return lambda row: self._is_weekend(row)
        
        elif rule_type == 'new_sender':
            time_window = rule_config.get('time_window', '7D')
            return lambda row: self._is_new_sender(row, time_window)
        
        elif rule_type == 'new_receiver':
            time_window = rule_config.get('time_window', '7D')
            return lambda row: self._is_new_receiver(row, time_window)
        
        else:
            logger.warning(f"Unknown rule type: {rule_type}")
            return lambda row: False
    
    def apply_rules(self, df):
        """
        Apply all rules to the dataframe
        
        Args:
            df (DataFrame): Input data
            
        Returns:
            dict: Rule results
        """
        try:
            # Sort by timestamp if available
            if 'timestamp' in df.columns and not pd.api.types.is_datetime64_any_dtype(df['timestamp']):
                df = df.copy()
                df['timestamp'] = pd.to_datetime(df['timestamp'])
            
            if 'timestamp' in df.columns:
                df = df.sort_values('timestamp')
            
            # Initialize results
            rule_results = {}
            rule_scores = {}
            violated_rules = {}
            
            # Apply each rule
            for rule_name, rule_func in self.rules.items():
                try:
                    # Apply rule to each row
                    results = []
                    for _, row in df.iterrows():
                        try:
                            result = rule_func(row)
                            results.append(result)
                        except Exception as e:
                            logger.warning(f"Error applying rule {rule_name} to row: {str(e)}")
                            results.append(False)
                    
                    rule_results[rule_name] = results
                    
                    # Calculate weighted score
                    weight = self.rule_weights.get(rule_name, 1.0)
                    rule_scores[rule_name] = [weight if result else 0 for result in results]
                    
                    # Track violated rules
                    violated_rules[rule_name] = [i for i, result in enumerate(results) if result]
                    
                except Exception as e:
                    logger.error(f"Error applying rule {rule_name}: {str(e)}")
                    rule_results[rule_name] = [False] * len(df)
                    rule_scores[rule_name] = [0] * len(df)
                    violated_rules[rule_name] = []
            
            # Calculate total rule score for each transaction
            total_scores = []
            for i in range(len(df)):
                score = sum(rule_scores[rule_name][i] for rule_name in rule_scores)
                total_scores.append(score)
            
            # Normalize scores
            max_possible_score = sum(self.rule_weights.values())
            normalized_scores = [score / max_possible_score for score in total_scores]
            
            # Determine if transaction violates rules based on threshold
            rule_violations = [score >= self.threshold for score in normalized_scores]
            
            # Get violated rule names for each transaction
            violated_rule_names = []
            for i in range(len(df)):
                names = []
                for rule_name in violated_rules:
                    if i in violated_rules[rule_name]:
                        names.append(rule_name)
                violated_rule_names.append(names)
            
            self.fitted = True
            
            return {
                'rule_results': rule_results,
                'rule_scores': rule_scores,
                'total_scores': total_scores,
                'normalized_scores': normalized_scores,
                'rule_violations': rule_violations,
                'violated_rule_names': violated_rule_names,
                'rules': self.rules,
                'rule_weights': self.rule_weights,
                'rule_descriptions': self.rule_descriptions
            }
            
        except Exception as e:
            logger.error(f"Error applying rules: {str(e)}")
            raise
    
    # Rule functions
    def _high_amount_rule(self, row):
        """Check if transaction amount is high"""
        return row.get('amount', 0) > 10000
    
    def _unusual_amount_for_sender_rule(self, row, std_multiplier=3, min_transactions=5):
        """Check if amount is unusual for the sender"""
        # This would require access to sender's transaction history
        # For simplicity, we'll use a placeholder
        return False
    
    def _unusual_amount_for_receiver_rule(self, row, std_multiplier=3, min_transactions=5):
        """Check if amount is unusual for the receiver"""
        # This would require access to receiver's transaction history
        # For simplicity, we'll use a placeholder
        return False
    
    def _round_amount_rule(self, row):
        """Check if transaction amount is suspiciously round"""
        amount = row.get('amount', 0)
        return amount > 1000 and amount % 1000 == 0
    
    def _high_frequency_sender_rule(self, row, time_window='1H', max_transactions=10):
        """Check if sender has high transaction frequency"""
        # This would require access to sender's transaction history
        # For simplicity, we'll use a placeholder
        return False
    
    def _high_frequency_receiver_rule(self, row, time_window='1H', max_transactions=10):
        """Check if receiver has high transaction frequency"""
        # This would require access to receiver's transaction history
        # For simplicity, we'll use a placeholder
        return False
    
    def _rapid_succession_rule(self, row, time_window='5M', min_transactions=3):
        """Check if there are multiple transactions in rapid succession"""
        # This would require access to transaction history
        # For simplicity, we'll use a placeholder
        return False
    
    def _cross_border_rule(self, row):
        """Check if transaction crosses international borders"""
        sender_location = row.get('sender_location', '')
        receiver_location = row.get('receiver_location', '')
        
        if not sender_location or not receiver_location:
            return False
        
        # Extract countries from locations
        sender_country = sender_location.split(',')[0].strip()
        receiver_country = receiver_location.split(',')[0].strip()
        
        return sender_country != receiver_country
    
    def _high_risk_country_rule(self, row, countries=None):
        """Check if transaction involves high-risk country"""
        if countries is None:
            countries = ['North Korea', 'Iran', 'Syria', 'Cuba']
        
        sender_location = row.get('sender_location', '')
        receiver_location = row.get('receiver_location', '')
        
        # Check if any high-risk country is involved
        for country in countries:
            if country in sender_location or country in receiver_location:
                return True
        
        return False
    
    def _unusual_location_for_sender_rule(self, row):
        """Check if transaction is from unusual location for sender"""
        # This would require access to sender's transaction history
        # For simplicity, we'll use a placeholder
        return False
    
    def _unusual_hour_rule(self, row, start_hour=23, end_hour=5):
        """Check if transaction is during unusual hours"""
        timestamp = row.get('timestamp')
        if timestamp is None:
            return False
        
        if not pd.api.types.is_datetime64_any_dtype(timestamp):
            timestamp = pd.to_datetime(timestamp)
        
        hour = timestamp.hour
        
        # Handle overnight range
        if start_hour > end_hour:
            return hour >= start_hour or hour < end_hour
        else:
            return start_hour <= hour < end_hour
    
    def _weekend_rule(self, row):
        """Check if transaction is on weekend"""
        timestamp = row.get('timestamp')
        if timestamp is None:
            return False
        
        if not pd.api.types.is_datetime64_any_dtype(timestamp):
            timestamp = pd.to_datetime(timestamp)
        
        # Saturday=5, Sunday=6
        return timestamp.dayofweek >= 5
    
    def _new_sender_rule(self, row, time_window='7D'):
        """Check if this is the first transaction from sender"""
        # This would require access to sender's transaction history
        # For simplicity, we'll use a placeholder
        return False
    
    def _new_receiver_rule(self, row, time_window='7D'):
        """Check if this is the first transaction to receiver"""
        # This would require access to receiver's transaction history
        # For simplicity, we'll use a placeholder
        return False
    
    def _sanctions_check_rule(self, row):
        """Check if transaction involves sanctioned entities"""
        try:
            if not self.api_available.get('sanctions', False):
                # Use demo data
                sanctions_data = get_demo_sanctions_data()
                
                # Simple demo check
                sender_id = row.get('sender_id', '')
                receiver_id = row.get('receiver_id', '')
                
                # Check if sender or receiver is in demo sanctions list
                is_sanctioned = (
                    sanctions_data['entity_id'].str.contains(sender_id, na=False).any() or
                    sanctions_data['entity_id'].str.contains(receiver_id, na=False).any()
                )
                
                return is_sanctioned
            else:
                # Here you would implement the actual API call
                # For now, return False
                return False
        except Exception as e:
            logger.warning(f"Error in sanctions check: {str(e)}")
            return False
    
    def _tax_compliance_rule(self, row):
        """Check if entities are tax compliant"""
        try:
            if not self.api_available.get('tax_compliance', False):
                # Use demo data
                tax_data = get_demo_tax_compliance_data()
                
                # Simple demo check
                sender_id = row.get('sender_id', '')
                receiver_id = row.get('receiver_id', '')
                
                # Check if sender or receiver is non-compliant
                is_non_compliant = False
                
                for _, entity in tax_data.iterrows():
                    if entity['entity_id'] in [sender_id, receiver_id] and entity['compliance_status'] == 'Non-compliant':
                        is_non_compliant = True
                        break
                
                return is_non_compliant
            else:
                # Here you would implement the actual API call
                # For now, return False
                return False
        except Exception as e:
            logger.warning(f"Error in tax compliance check: {str(e)}")
            return False
    
    def _bank_verification_rule(self, row):
        """Check if bank accounts are verified"""
        try:
            if not self.api_available.get('bank_verification', False):
                # Use demo data
                bank_data = get_demo_bank_verification_data()
                
                # Simple demo check
                sender_id = row.get('sender_id', '')
                receiver_id = row.get('receiver_id', '')
                
                # Check if sender or receiver account is not verified
                is_unverified = False
                
                for _, account in bank_data.iterrows():
                    if account['account_number'] in [sender_id, receiver_id] and account['verification_status'] == 'Not Verified':
                        is_unverified = True
                        break
                
                return is_unverified
            else:
                # Here you would implement the actual API call
                # For now, return False
                return False
        except Exception as e:
            logger.warning(f"Error in bank verification check: {str(e)}")
            return False
    
    def _identity_verification_rule(self, row):
        """Check if identities are verified"""
        try:
            if not self.api_available.get('identity_verification', False):
                # Use demo data
                identity_data = get_demo_identity_verification_data()
                
                # Simple demo check
                sender_id = row.get('sender_id', '')
                receiver_id = row.get('receiver_id', '')
                
                # Check if sender or receiver identity is not verified
                is_unverified = False
                
                for _, identity in identity_data.iterrows():
                    if identity['id_number'] in [sender_id, receiver_id] and identity['verification_status'] == 'Not Verified':
                        is_unverified = True
                        break
                
                return is_unverified
            else:
                # Here you would implement the actual API call
                # For now, return False
                return False
        except Exception as e:
            logger.warning(f"Error in identity verification check: {str(e)}")
            return False
    
    def add_rule(self, rule_name, rule_func, weight=1.0, description=''):
        """
        Add a custom rule
        
        Args:
            rule_name (str): Name of the rule
            rule_func (function): Rule function
            weight (float): Weight of the rule
            description (str): Description of the rule
        """
        self.rules[rule_name] = rule_func
        self.rule_weights[rule_name] = weight
        self.rule_descriptions[rule_name] = description
        logger.info(f"Added rule: {rule_name}")
    
    def remove_rule(self, rule_name):
        """
        Remove a rule
        
        Args:
            rule_name (str): Name of the rule to remove
        """
        if rule_name in self.rules:
            del self.rules[rule_name]
            del self.rule_weights[rule_name]
            del self.rule_descriptions[rule_name]
            logger.info(f"Removed rule: {rule_name}")
        else:
            logger.warning(f"Rule not found: {rule_name}")
    
    def update_rule_weight(self, rule_name, weight):
        """
        Update the weight of a rule
        
        Args:
            rule_name (str): Name of the rule
            weight (float): New weight
        """
        if rule_name in self.rule_weights:
            self.rule_weights[rule_name] = weight
            logger.info(f"Updated weight for rule {rule_name}: {weight}")
        else:
            logger.warning(f"Rule not found: {rule_name}")
    
    def get_rules(self):
        """
        Get all rules
        
        Returns:
            dict: Rules information
        """
        return {
            'rules': list(self.rules.keys()),
            'weights': self.rule_weights,
            'descriptions': self.rule_descriptions
        }
    
    def save_config(self, config_path):
        """
        Save current configuration to file
        
        Args:
            config_path (str): Path to save configuration
        """
        try:
            config = {
                'rules': {}
            }
            
            for rule_name in self.rules:
                config['rules'][rule_name] = {
                    'enabled': True,
                    'weight': self.rule_weights[rule_name],
                    'description': self.rule_descriptions[rule_name]
                }
            
            with open(config_path, 'w') as f:
                yaml.dump(config, f, default_flow_style=False)
            
            logger.info(f"Configuration saved to {config_path}")
            
        except Exception as e:
            logger.error(f"Error saving configuration: {str(e)}")
            raise

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\models\supervised.py ===
"""
Supervised Models Module
Implements supervised learning models for fraud detection
"""
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score, 
    roc_auc_score, confusion_matrix, classification_report, 
    precision_recall_curve, average_precision_score, roc_curve
)
from sklearn.preprocessing import StandardScaler, LabelEncoder, RobustScaler
from sklearn.feature_selection import SelectKBest, f_classif, RFE
from imblearn.over_sampling import SMOTE, ADASYN, BorderlineSMOTE
from imblearn.under_sampling import RandomUnderSampler, TomekLinks
from imblearn.combine import SMOTEENN, SMOTETomek
import xgboost as xgb
import lightgbm as lgb
import shap
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
import logging
from typing import Dict, List, Tuple, Union
warnings.filterwarnings('ignore')
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class SupervisedModels:
    """
    Class for supervised fraud detection models
    Implements Random Forest, XGBoost, Logistic Regression, etc.
    """
    
    def __init__(self, test_size=0.2, random_state=42, handle_imbalance=True):
        """
        Initialize SupervisedModels
        
        Args:
            test_size (float): Proportion of data for testing
            random_state (int): Random seed
            handle_imbalance (bool): Whether to handle class imbalance
        """
        self.test_size = test_size
        self.random_state = random_state
        self.handle_imbalance = handle_imbalance
        self.models = {}
        self.feature_names = {}
        self.scalers = {}
        self.label_encoders = {}
        self.feature_selectors = {}
        self.resamplers = {}
        self.performance = {}
        self.feature_importance = {}
        self.shap_values = {}
        self.fitted = False
        
    def run_models(self, df):
        """
        Run all supervised models
        
        Args:
            df (DataFrame): Input data
            
        Returns:
            dict: Model results
        """
        try:
            # Check if fraud_flag column exists
            if 'fraud_flag' not in df.columns:
                logger.warning("No fraud_flag column found. Using synthetic labels for demonstration.")
                # Create synthetic labels based on statistical outliers
                numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
                if len(numeric_cols) > 0:
                    # Use Z-score to identify outliers as potential fraud
                    X = df[numeric_cols].fillna(0)
                    scaler = StandardScaler()
                    X_scaled = scaler.fit_transform(X)
                    
                    # Calculate Z-scores
                    z_scores = np.abs(X_scaled).max(axis=1)
                    
                    # Label top 5% as fraud
                    threshold = np.percentile(z_scores, 95)
                    y = (z_scores > threshold).astype(int)
                    
                    # Add synthetic labels to dataframe
                    df = df.copy()
                    df['fraud_flag'] = y
                else:
                    raise ValueError("No numeric columns found for creating synthetic labels")
            else:
                y = df['fraud_flag'].astype(int)
            
            # Get feature columns (exclude target and ID columns)
            exclude_cols = ['fraud_flag', 'transaction_id', 'sender_id', 'receiver_id']
            feature_cols = [col for col in df.columns if col not in exclude_cols]
            
            # Get numeric and categorical features
            numeric_features = df[feature_cols].select_dtypes(include=[np.number]).columns.tolist()
            categorical_features = df[feature_cols].select_dtypes(include=['object', 'category']).columns.tolist()
            
            # Prepare data
            X = df[feature_cols].copy()
            
            # Clean data before processing
            X = self._clean_data(X)
            
            # Handle categorical features
            for col in categorical_features:
                if col in X.columns:
                    # Label encode categorical features
                    le = LabelEncoder()
                    X[col] = le.fit_transform(X[col].astype(str))
                    self.label_encoders[col] = le
            
            # Split data
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=self.test_size, random_state=self.random_state, stratify=y
            )
            
            # Scale numeric features using RobustScaler
            if numeric_features:
                scaler = RobustScaler()
                X_train[numeric_features] = scaler.fit_transform(X_train[numeric_features])
                X_test[numeric_features] = scaler.transform(X_test[numeric_features])
                self.scalers['numeric'] = scaler
            
            # Handle class imbalance
            if self.handle_imbalance and y_train.sum() / len(y_train) < 0.1:  # If minority class < 10%
                # Try different resampling techniques
                resampling_methods = {
                    'smote': SMOTE(random_state=self.random_state),
                    'adasyn': ADASYN(random_state=self.random_state),
                    'borderline_smote': BorderlineSMOTE(random_state=self.random_state),
                    'smote_enn': SMOTEENN(random_state=self.random_state),
                    'smote_tomek': SMOTETomek(random_state=self.random_state)
                }
                
                # Evaluate each method using cross-validation
                best_method = None
                best_score = 0
                
                for method_name, resampler in resampling_methods.items():
                    try:
                        # Apply resampling
                        X_resampled, y_resampled = resampler.fit_resample(X_train, y_train)
                        
                        # Evaluate with a simple model
                        rf = RandomForestClassifier(n_estimators=50, random_state=self.random_state, n_jobs=-1)
                        scores = cross_val_score(rf, X_resampled, y_resampled, cv=3, scoring='f1')
                        avg_score = scores.mean()
                        
                        if avg_score > best_score:
                            best_score = avg_score
                            best_method = method_name
                            self.resamplers['best'] = resampler
                            X_train, y_train = X_resampled, y_resampled
                    except Exception as e:
                        logger.warning(f"Error with {method_name}: {str(e)}")
                
                if best_method:
                    logger.info(f"Using {best_method} for handling class imbalance")
                else:
                    logger.warning("No suitable resampling method found")
            
            # Store feature names
            all_feature_names = feature_cols
            
            # Run different models
            results = {}
            
            # Random Forest
            try:
                rf_model = RandomForestClassifier(
                    n_estimators=100,
                    max_depth=10,
                    min_samples_split=5,
                    min_samples_leaf=2,
                    class_weight='balanced',
                    random_state=self.random_state,
                    n_jobs=-1
                )
                
                rf_model.fit(X_train, y_train)
                rf_pred = rf_model.predict(X_test)
                rf_proba = rf_model.predict_proba(X_test)[:, 1]
                
                # Calculate performance metrics
                rf_performance = self._calculate_performance_metrics(y_test, rf_pred, rf_proba)
                
                # Get feature importance
                rf_importance = pd.DataFrame({
                    'feature': all_feature_names,
                    'importance': rf_model.feature_importances_
                }).sort_values('importance', ascending=False)
                
                # Calculate SHAP values
                explainer = shap.TreeExplainer(rf_model)
                rf_shap_values = explainer.shap_values(X_test)
                
                results['random_forest'] = {
                    'model': rf_model,
                    'predictions': rf_pred,
                    'probabilities': rf_proba,
                    'performance': rf_performance,
                    'feature_importance': rf_importance,
                    'shap_values': rf_shap_values,
                    'feature_names': all_feature_names
                }
                
                self.models['random_forest'] = rf_model
                self.feature_names['random_forest'] = all_feature_names
                self.performance['random_forest'] = rf_performance
                self.feature_importance['random_forest'] = rf_importance
                self.shap_values['random_forest'] = rf_shap_values
                
                logger.info("Random Forest model completed")
            except Exception as e:
                logger.error(f"Error running Random Forest: {str(e)}")
            
            # XGBoost
            try:
                xgb_model = xgb.XGBClassifier(
                    n_estimators=100,
                    max_depth=6,
                    learning_rate=0.1,
                    subsample=0.8,
                    colsample_bytree=0.8,
                    scale_pos_weight=len(y_train[y_train==0]) / len(y_train[y_train==1]),
                    random_state=self.random_state,
                    use_label_encoder=False,
                    eval_metric='logloss'
                )
                
                xgb_model.fit(X_train, y_train)
                xgb_pred = xgb_model.predict(X_test)
                xgb_proba = xgb_model.predict_proba(X_test)[:, 1]
                
                # Calculate performance metrics
                xgb_performance = self._calculate_performance_metrics(y_test, xgb_pred, xgb_proba)
                
                # Get feature importance
                xgb_importance = pd.DataFrame({
                    'feature': all_feature_names,
                    'importance': xgb_model.feature_importances_
                }).sort_values('importance', ascending=False)
                
                # Calculate SHAP values
                explainer = shap.TreeExplainer(xgb_model)
                xgb_shap_values = explainer.shap_values(X_test)
                
                results['xgboost'] = {
                    'model': xgb_model,
                    'predictions': xgb_pred,
                    'probabilities': xgb_proba,
                    'performance': xgb_performance,
                    'feature_importance': xgb_importance,
                    'shap_values': xgb_shap_values,
                    'feature_names': all_feature_names
                }
                
                self.models['xgboost'] = xgb_model
                self.feature_names['xgboost'] = all_feature_names
                self.performance['xgboost'] = xgb_performance
                self.feature_importance['xgboost'] = xgb_importance
                self.shap_values['xgboost'] = xgb_shap_values
                
                logger.info("XGBoost model completed")
            except Exception as e:
                logger.error(f"Error running XGBoost: {str(e)}")
            
            # LightGBM
            try:
                lgb_model = lgb.LGBMClassifier(
                    n_estimators=100,
                    max_depth=6,
                    learning_rate=0.1,
                    subsample=0.8,
                    colsample_bytree=0.8,
                    scale_pos_weight=len(y_train[y_train==0]) / len(y_train[y_train==1]),
                    random_state=self.random_state
                )
                
                lgb_model.fit(X_train, y_train)
                lgb_pred = lgb_model.predict(X_test)
                lgb_proba = lgb_model.predict_proba(X_test)[:, 1]
                
                # Calculate performance metrics
                lgb_performance = self._calculate_performance_metrics(y_test, lgb_pred, lgb_proba)
                
                # Get feature importance
                lgb_importance = pd.DataFrame({
                    'feature': all_feature_names,
                    'importance': lgb_model.feature_importances_
                }).sort_values('importance', ascending=False)
                
                # Calculate SHAP values
                explainer = shap.TreeExplainer(lgb_model)
                lgb_shap_values = explainer.shap_values(X_test)
                
                results['lightgbm'] = {
                    'model': lgb_model,
                    'predictions': lgb_pred,
                    'probabilities': lgb_proba,
                    'performance': lgb_performance,
                    'feature_importance': lgb_importance,
                    'shap_values': lgb_shap_values,
                    'feature_names': all_feature_names
                }
                
                self.models['lightgbm'] = lgb_model
                self.feature_names['lightgbm'] = all_feature_names
                self.performance['lightgbm'] = lgb_performance
                self.feature_importance['lightgbm'] = lgb_importance
                self.shap_values['lightgbm'] = lgb_shap_values
                
                logger.info("LightGBM model completed")
            except Exception as e:
                logger.error(f"Error running LightGBM: {str(e)}")
            
            # Logistic Regression
            try:
                lr_model = LogisticRegression(
                    C=1.0,
                    class_weight='balanced',
                    random_state=self.random_state,
                    max_iter=1000
                )
                
                lr_model.fit(X_train, y_train)
                lr_pred = lr_model.predict(X_test)
                lr_proba = lr_model.predict_proba(X_test)[:, 1]
                
                # Calculate performance metrics
                lr_performance = self._calculate_performance_metrics(y_test, lr_pred, lr_proba)
                
                # Get feature importance (coefficients)
                lr_importance = pd.DataFrame({
                    'feature': all_feature_names,
                    'importance': np.abs(lr_model.coef_[0])
                }).sort_values('importance', ascending=False)
                
                # Calculate SHAP values
                explainer = shap.LinearExplainer(lr_model, X_train)
                lr_shap_values = explainer.shap_values(X_test)
                
                results['logistic_regression'] = {
                    'model': lr_model,
                    'predictions': lr_pred,
                    'probabilities': lr_proba,
                    'performance': lr_performance,
                    'feature_importance': lr_importance,
                    'shap_values': lr_shap_values,
                    'feature_names': all_feature_names
                }
                
                self.models['logistic_regression'] = lr_model
                self.feature_names['logistic_regression'] = all_feature_names
                self.performance['logistic_regression'] = lr_performance
                self.feature_importance['logistic_regression'] = lr_importance
                self.shap_values['logistic_regression'] = lr_shap_values
                
                logger.info("Logistic Regression model completed")
            except Exception as e:
                logger.error(f"Error running Logistic Regression: {str(e)}")
            
            # Gradient Boosting
            try:
                gb_model = GradientBoostingClassifier(
                    n_estimators=100,
                    max_depth=6,
                    learning_rate=0.1,
                    subsample=0.8,
                    random_state=self.random_state
                )
                
                gb_model.fit(X_train, y_train)
                gb_pred = gb_model.predict(X_test)
                gb_proba = gb_model.predict_proba(X_test)[:, 1]
                
                # Calculate performance metrics
                gb_performance = self._calculate_performance_metrics(y_test, gb_pred, gb_proba)
                
                # Get feature importance
                gb_importance = pd.DataFrame({
                    'feature': all_feature_names,
                    'importance': gb_model.feature_importances_
                }).sort_values('importance', ascending=False)
                
                # Calculate SHAP values
                explainer = shap.TreeExplainer(gb_model)
                gb_shap_values = explainer.shap_values(X_test)
                
                results['gradient_boosting'] = {
                    'model': gb_model,
                    'predictions': gb_pred,
                    'probabilities': gb_proba,
                    'performance': gb_performance,
                    'feature_importance': gb_importance,
                    'shap_values': gb_shap_values,
                    'feature_names': all_feature_names
                }
                
                self.models['gradient_boosting'] = gb_model
                self.feature_names['gradient_boosting'] = all_feature_names
                self.performance['gradient_boosting'] = gb_performance
                self.feature_importance['gradient_boosting'] = gb_importance
                self.shap_values['gradient_boosting'] = gb_shap_values
                
                logger.info("Gradient Boosting model completed")
            except Exception as e:
                logger.error(f"Error running Gradient Boosting: {str(e)}")
            
            # Neural Network
            try:
                nn_model = MLPClassifier(
                    hidden_layer_sizes=(100, 50),
                    activation='relu',
                    solver='adam',
                    alpha=0.0001,
                    batch_size=32,
                    learning_rate='adaptive',
                    max_iter=200,
                    random_state=self.random_state
                )
                
                nn_model.fit(X_train, y_train)
                nn_pred = nn_model.predict(X_test)
                nn_proba = nn_model.predict_proba(X_test)[:, 1]
                
                # Calculate performance metrics
                nn_performance = self._calculate_performance_metrics(y_test, nn_pred, nn_proba)
                
                # For neural networks, we don't have direct feature importance
                # Use permutation importance instead
                from sklearn.inspection import permutation_importance
                
                perm_importance = permutation_importance(
                    nn_model, X_test, y_test, n_repeats=5, random_state=self.random_state
                )
                
                nn_importance = pd.DataFrame({
                    'feature': all_feature_names,
                    'importance': perm_importance.importances_mean
                }).sort_values('importance', ascending=False)
                
                # Calculate SHAP values (using KernelExplainer for black-box models)
                explainer = shap.KernelExplainer(nn_model.predict_proba, X_train[:100])  # Use subset for speed
                nn_shap_values = explainer.shap_values(X_test[:100])  # Use subset for speed
                
                results['neural_network'] = {
                    'model': nn_model,
                    'predictions': nn_pred,
                    'probabilities': nn_proba,
                    'performance': nn_performance,
                    'feature_importance': nn_importance,
                    'shap_values': nn_shap_values,
                    'feature_names': all_feature_names
                }
                
                self.models['neural_network'] = nn_model
                self.feature_names['neural_network'] = all_feature_names
                self.performance['neural_network'] = nn_performance
                self.feature_importance['neural_network'] = nn_importance
                self.shap_values['neural_network'] = nn_shap_values
                
                logger.info("Neural Network model completed")
            except Exception as e:
                logger.error(f"Error running Neural Network: {str(e)}")
            
            # Ensemble model (combine predictions from all models)
            try:
                # Get predictions from all models
                all_predictions = []
                all_probabilities = []
                
                for model_name in results:
                    all_predictions.append(results[model_name]['predictions'])
                    all_probabilities.append(results[model_name]['probabilities'])
                
                # Majority voting for predictions
                ensemble_pred = np.array(all_predictions).mean(axis=0) > 0.5
                
                # Average probabilities
                ensemble_proba = np.array(all_probabilities).mean(axis=0)
                
                # Calculate performance metrics
                ensemble_performance = self._calculate_performance_metrics(y_test, ensemble_pred, ensemble_proba)
                
                # For ensemble, average feature importances
                ensemble_importance = pd.DataFrame({
                    'feature': all_feature_names,
                    'importance': np.zeros(len(all_feature_names))
                })
                
                for model_name in results:
                    if model_name in self.feature_importance:
                        model_importance = self.feature_importance[model_name]
                        for i, feature in enumerate(all_feature_names):
                            if feature in model_importance['feature'].values:
                                idx = model_importance[model_importance['feature'] == feature].index[0]
                                ensemble_importance.loc[i, 'importance'] += model_importance.loc[idx, 'importance']
                
                # Normalize importance
                ensemble_importance['importance'] = ensemble_importance['importance'] / len(results)
                ensemble_importance = ensemble_importance.sort_values('importance', ascending=False)
                
                results['ensemble'] = {
                    'predictions': ensemble_pred,
                    'probabilities': ensemble_proba,
                    'performance': ensemble_performance,
                    'feature_importance': ensemble_importance,
                    'feature_names': all_feature_names
                }
                
                self.performance['ensemble'] = ensemble_performance
                self.feature_importance['ensemble'] = ensemble_importance
                
                logger.info("Ensemble model completed")
            except Exception as e:
                logger.error(f"Error running Ensemble model: {str(e)}")
            
            self.fitted = True
            return results
            
        except Exception as e:
            logger.error(f"Error running supervised models: {str(e)}")
            raise
    
    def _clean_data(self, X):
        """
        Clean data by handling infinity, NaN, and extreme values
        
        Args:
            X (DataFrame): Input data
            
        Returns:
            DataFrame: Cleaned data
        """
        try:
            # Replace infinity with NaN
            X = X.replace([np.inf, -np.inf], np.nan)
            
            # Replace extremely large values with a more reasonable maximum
            for col in X.columns:
                if X[col].dtype in ['float64', 'int64']:
                    # Calculate 99th percentile as a reasonable maximum
                    percentile_99 = np.nanpercentile(X[col], 99)
                    if not np.isnan(percentile_99):
                        # Cap values at 10 times the 99th percentile
                        max_val = percentile_99 * 10
                        X[col] = np.where(X[col] > max_val, max_val, X[col])
                        
                        # Similarly, handle extremely negative values
                        percentile_1 = np.nanpercentile(X[col], 1)
                        if not np.isnan(percentile_1):
                            min_val = percentile_1 * 10
                            X[col] = np.where(X[col] < min_val, min_val, X[col])
            
            return X
            
        except Exception as e:
            logger.error(f"Error cleaning data: {str(e)}")
            return X
    
    def _calculate_performance_metrics(self, y_true, y_pred, y_proba):
        """
        Calculate performance metrics for a model
        
        Args:
            y_true (array): True labels
            y_pred (array): Predicted labels
            y_proba (array): Predicted probabilities
            
        Returns:
            dict: Performance metrics
        """
        try:
            # Basic metrics
            accuracy = accuracy_score(y_true, y_pred)
            precision = precision_score(y_true, y_pred, zero_division=0)
            recall = recall_score(y_true, y_pred, zero_division=0)
            f1 = f1_score(y_true, y_pred, zero_division=0)
            
            # ROC AUC
            roc_auc = roc_auc_score(y_true, y_proba)
            
            # Average precision
            avg_precision = average_precision_score(y_true, y_proba)
            
            # Confusion matrix
            cm = confusion_matrix(y_true, y_pred)
            
            # Classification report
            class_report = classification_report(y_true, y_pred, output_dict=True)
            
            # Precision-Recall curve
            precision_curve, recall_curve, _ = precision_recall_curve(y_true, y_proba)
            
            # ROC curve
            fpr, tpr, _ = roc_curve(y_true, y_proba)
            
            return {
                'accuracy': accuracy,
                'precision': precision,
                'recall': recall,
                'f1': f1,
                'roc_auc': roc_auc,
                'avg_precision': avg_precision,
                'confusion_matrix': cm,
                'classification_report': class_report,
                'precision_curve': precision_curve,
                'recall_curve': recall_curve,
                'fpr': fpr,
                'tpr': tpr
            }
            
        except Exception as e:
            logger.error(f"Error calculating performance metrics: {str(e)}")
            raise
    
    def predict(self, df, model_name):
        """
        Make predictions using a fitted model
        
        Args:
            df (DataFrame): Input data
            model_name (str): Name of the model to use
            
        Returns:
            dict: Predictions and probabilities
        """
        if not self.fitted:
            raise ValueError("Models not fitted. Call run_models first.")
        
        if model_name not in self.models and model_name != 'ensemble':
            raise ValueError(f"Model {model_name} not found.")
        
        try:
            # Get feature names for this model
            if model_name == 'ensemble':
                # For ensemble, use all feature names
                feature_names = []
                for name in self.feature_names:
                    feature_names.extend(self.feature_names[name])
                feature_names = list(set(feature_names))  # Remove duplicates
            else:
                feature_names = self.feature_names[model_name]
            
            # Prepare data
            X = df[feature_names].copy()
            
            # Clean the data
            X = self._clean_data(X)
            
            # Handle categorical features
            for col in X.columns:
                if col in self.label_encoders:
                    le = self.label_encoders[col]
                    # Handle unseen categories
                    X[col] = X[col].astype(str).map(
                        lambda x: le.transform([x])[0] if x in le.classes_ else -1
                    )
            
            # Scale numeric features if scaler exists
            if 'numeric' in self.scalers:
                numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()
                if numeric_features:
                    X[numeric_features] = self.scalers['numeric'].transform(X[numeric_features])
            
            # Make predictions
            if model_name == 'ensemble':
                # Get predictions from all models
                all_predictions = []
                all_probabilities = []
                
                for name in self.models:
                    model = self.models[name]
                    pred = model.predict(X)
                    proba = model.predict_proba(X)[:, 1]
                    all_predictions.append(pred)
                    all_probabilities.append(proba)
                
                # Majority voting for predictions
                predictions = np.array(all_predictions).mean(axis=0) > 0.5
                
                # Average probabilities
                probabilities = np.array(all_probabilities).mean(axis=0)
                
            else:
                # Get model
                model = self.models[model_name]
                
                # Make predictions
                predictions = model.predict(X)
                probabilities = model.predict_proba(X)[:, 1]
            
            return {
                'predictions': predictions,
                'probabilities': probabilities
            }
            
        except Exception as e:
            logger.error(f"Error making predictions with {model_name}: {str(e)}")
            raise
    
    def get_feature_importance(self, model_name):
        """
        Get feature importance for a model
        
        Args:
            model_name (str): Name of the model
            
        Returns:
            DataFrame: Feature importance
        """
        if not self.fitted:
            raise ValueError("Models not fitted. Call run_models first.")
        
        if model_name not in self.feature_importance:
            raise ValueError(f"Feature importance for {model_name} not found.")
        
        return self.feature_importance[model_name]
    
    def get_performance(self, model_name):
        """
        Get performance metrics for a model
        
        Args:
            model_name (str): Name of the model
            
        Returns:
            dict: Performance metrics
        """
        if not self.fitted:
            raise ValueError("Models not fitted. Call run_models first.")
        
        if model_name not in self.performance:
            raise ValueError(f"Performance for {model_name} not found.")
        
        return self.performance[model_name]
    
    def get_shap_values(self, model_name):
        """
        Get SHAP values for a model
        
        Args:
            model_name (str): Name of the model
            
        Returns:
            array: SHAP values
        """
        if not self.fitted:
            raise ValueError("Models not fitted. Call run_models first.")
        
        if model_name not in self.shap_values:
            raise ValueError(f"SHAP values for {model_name} not found.")
        
        return self.shap_values[model_name]
    
    def plot_feature_importance(self, model_name, top_n=20):
        """
        Plot feature importance for a model
        
        Args:
            model_name (str): Name of the model
            top_n (int): Number of top features to show
        """
        try:
            # Get feature importance
            importance_df = self.get_feature_importance(model_name)
            
            # Get top features
            top_features = importance_df.head(top_n)
            
            # Create plot
            plt.figure(figsize=(10, 8))
            sns.barplot(x='importance', y='feature', data=top_features)
            plt.title(f'Top {top_n} Feature Importance - {model_name}')
            plt.tight_layout()
            
            return plt.gcf()
            
        except Exception as e:
            logger.error(f"Error plotting feature importance for {model_name}: {str(e)}")
            raise
    
    def plot_confusion_matrix(self, model_name):
        """
        Plot confusion matrix for a model
        
        Args:
            model_name (str): Name of the model
        """
        try:
            # Get performance metrics
            performance = self.get_performance(model_name)
            cm = performance['confusion_matrix']
            
            # Create plot
            plt.figure(figsize=(8, 6))
            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                       xticklabels=['Not Fraud', 'Fraud'], 
                       yticklabels=['Not Fraud', 'Fraud'])
            plt.title(f'Confusion Matrix - {model_name}')
            plt.ylabel('Actual')
            plt.xlabel('Predicted')
            plt.tight_layout()
            
            return plt.gcf()
            
        except Exception as e:
            logger.error(f"Error plotting confusion matrix for {model_name}: {str(e)}")
            raise
    
    def plot_roc_curve(self, model_name):
        """
        Plot ROC curve for a model
        
        Args:
            model_name (str): Name of the model
        """
        try:
            # Get performance metrics
            performance = self.get_performance(model_name)
            fpr = performance['fpr']
            tpr = performance['tpr']
            roc_auc = performance['roc_auc']
            
            # Create plot
            plt.figure(figsize=(8, 6))
            plt.plot(fpr, tpr, label=f'{model_name} (AUC = {roc_auc:.3f})')
            plt.plot([0, 1], [0, 1], 'k--')
            plt.xlim([0.0, 1.0])
            plt.ylim([0.0, 1.05])
            plt.xlabel('False Positive Rate')
            plt.ylabel('True Positive Rate')
            plt.title(f'ROC Curve - {model_name}')
            plt.legend(loc="lower right")
            plt.tight_layout()
            
            return plt.gcf()
            
        except Exception as e:
            logger.error(f"Error plotting ROC curve for {model_name}: {str(e)}")
            raise
    
    def plot_precision_recall_curve(self, model_name):
        """
        Plot precision-recall curve for a model
        
        Args:
            model_name (str): Name of the model
        """
        try:
            # Get performance metrics
            performance = self.get_performance(model_name)
            precision_curve = performance['precision_curve']
            recall_curve = performance['recall_curve']
            avg_precision = performance['avg_precision']
            
            # Create plot
            plt.figure(figsize=(8, 6))
            plt.plot(recall_curve, precision_curve, label=f'{model_name} (AP = {avg_precision:.3f})')
            plt.xlim([0.0, 1.0])
            plt.ylim([0.0, 1.05])
            plt.xlabel('Recall')
            plt.ylabel('Precision')
            plt.title(f'Precision-Recall Curve - {model_name}')
            plt.legend(loc="lower left")
            plt.tight_layout()
            
            return plt.gcf()
            
        except Exception as e:
            logger.error(f"Error plotting precision-recall curve for {model_name}: {str(e)}")
            raise

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\models\unsupervised.py ===
"""
Unsupervised Models Module
Implements unsupervised learning models for fraud detection
"""
import pandas as pd
import numpy as np
from sklearn.ensemble import IsolationForest
from sklearn.neighbors import LocalOutlierFactor
from sklearn.svm import OneClassSVM
from sklearn.cluster import DBSCAN, KMeans
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.impute import SimpleImputer
from sklearn.metrics import silhouette_score
import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.callbacks import EarlyStopping
import warnings
import logging
from typing import Dict, List, Tuple, Union
warnings.filterwarnings('ignore')
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class UnsupervisedModels:
    """
    Class for unsupervised fraud detection models
    Implements Isolation Forest, Local Outlier Factor, Autoencoders, etc.
    """
    
    def __init__(self, contamination=0.01, random_state=42):
        """
        Initialize UnsupervisedModels
        
        Args:
            contamination (float): Expected proportion of outliers
            random_state (int): Random seed
        """
        self.contamination = contamination
        self.random_state = random_state
        self.models = {}
        self.feature_names = {}
        self.scalers = {}
        self.imputers = {}
        self.fitted = False
        
    def run_models(self, df):
        """
        Run all unsupervised models
        
        Args:
            df (DataFrame): Input data
            
        Returns:
            dict: Model results
        """
        try:
            # Get numeric columns
            numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
            
            if len(numeric_cols) < 2:
                logger.warning("Not enough numeric columns for unsupervised models")
                return {}
            
            # Prepare data
            X = df[numeric_cols].copy()
            
            # Clean the data - handle infinity and very large values
            X = self._clean_data(X)
            
            # Scale data
            X_scaled, scaler, imputer = self._scale_data(X)
            
            # Store scaler and imputer
            self.scalers['global'] = scaler
            self.imputers['global'] = imputer
            
            # Run different models
            results = {}
            
            # Isolation Forest
            try:
                if_model = IsolationForest(
                    contamination=self.contamination,
                    random_state=self.random_state,
                    n_jobs=-1,
                    max_samples='auto'
                )
                if_predictions = if_model.fit_predict(X_scaled)
                if_scores = if_model.decision_function(X_scaled)
                
                # Normalize scores to 0-1 range
                if_scores_normalized = self._normalize_scores(if_scores)
                
                results['isolation_forest'] = {
                    'predictions': if_predictions,
                    'scores': if_scores_normalized,
                    'model': if_model,
                    'feature_names': numeric_cols
                }
                
                self.models['isolation_forest'] = if_model
                self.feature_names['isolation_forest'] = numeric_cols
                
                logger.info("Isolation Forest model completed")
            except Exception as e:
                logger.error(f"Error running Isolation Forest: {str(e)}")
            
            # Local Outlier Factor
            try:
                lof_model = LocalOutlierFactor(
                    contamination=self.contamination,
                    n_neighbors=min(20, len(X_scaled) - 1),  # Ensure n_neighbors < n_samples
                    novelty=True,
                    n_jobs=-1
                )
                lof_predictions = lof_model.fit_predict(X_scaled)
                lof_scores = lof_model.decision_function(X_scaled)
                
                # Normalize scores to 0-1 range
                lof_scores_normalized = self._normalize_scores(lof_scores)
                
                results['local_outlier_factor'] = {
                    'predictions': lof_predictions,
                    'scores': lof_scores_normalized,
                    'model': lof_model,
                    'feature_names': numeric_cols
                }
                
                self.models['local_outlier_factor'] = lof_model
                self.feature_names['local_outlier_factor'] = numeric_cols
                
                logger.info("Local Outlier Factor model completed")
            except Exception as e:
                logger.error(f"Error running Local Outlier Factor: {str(e)}")
            
            # One-Class SVM
            try:
                ocsvm_model = OneClassSVM(
                    nu=self.contamination,
                    kernel='rbf',
                    gamma='scale'
                )
                ocsvm_predictions = ocsvm_model.fit_predict(X_scaled)
                ocsvm_scores = ocsvm_model.decision_function(X_scaled)
                
                # Normalize scores to 0-1 range
                ocsvm_scores_normalized = self._normalize_scores(ocsvm_scores)
                
                results['one_class_svm'] = {
                    'predictions': ocsvm_predictions,
                    'scores': ocsvm_scores_normalized,
                    'model': ocsvm_model,
                    'feature_names': numeric_cols
                }
                
                self.models['one_class_svm'] = ocsvm_model
                self.feature_names['one_class_svm'] = numeric_cols
                
                logger.info("One-Class SVM model completed")
            except Exception as e:
                logger.error(f"Error running One-Class SVM: {str(e)}")
            
            # DBSCAN Clustering
            try:
                # Use a subset of data for DBSCAN if it's too large
                if len(X_scaled) > 10000:
                    X_subset = X_scaled[np.random.choice(len(X_scaled), 10000, replace=False)]
                else:
                    X_subset = X_scaled
                
                dbscan_model = DBSCAN(eps=0.5, min_samples=5)
                dbscan_labels = dbscan_model.fit_predict(X_subset)
                
                # Convert to outlier predictions (-1 is outlier in DBSCAN)
                dbscan_predictions = np.where(dbscan_labels == -1, -1, 1)
                
                # Calculate distance to nearest core point as anomaly score
                dbscan_scores = np.zeros(len(X_subset))
                for i in range(len(X_subset)):
                    if dbscan_labels[i] == -1:  # Outlier
                        # Find distance to nearest core point
                        core_points = np.where(dbscan_labels != -1)[0]
                        if len(core_points) > 0:
                            distances = np.linalg.norm(X_subset[i] - X_subset[core_points], axis=1)
                            dbscan_scores[i] = distances.min()
                        else:
                            dbscan_scores[i] = np.max(np.linalg.norm(X_subset - X_subset.mean(axis=0), axis=1))
                
                # Normalize scores to 0-1 range
                dbscan_scores_normalized = self._normalize_scores(dbscan_scores)
                
                # For the full dataset, use the model to predict
                if len(X_scaled) > 10000:
                    full_predictions = dbscan_model.fit_predict(X_scaled)
                    full_predictions = np.where(full_predictions == -1, -1, 1)
                    
                    # Calculate scores for full dataset
                    full_scores = np.zeros(len(X_scaled))
                    for i in range(len(X_scaled)):
                        if full_predictions[i] == -1:  # Outlier
                            # Find distance to nearest core point
                            core_points = np.where(full_predictions != -1)[0]
                            if len(core_points) > 0:
                                distances = np.linalg.norm(X_scaled[i] - X_scaled[core_points], axis=1)
                                full_scores[i] = distances.min()
                            else:
                                full_scores[i] = np.max(np.linalg.norm(X_scaled - X_scaled.mean(axis=0), axis=1))
                    
                    # Normalize scores
                    full_scores_normalized = self._normalize_scores(full_scores)
                else:
                    full_predictions = dbscan_predictions
                    full_scores_normalized = dbscan_scores_normalized
                
                results['dbscan'] = {
                    'predictions': full_predictions,
                    'scores': full_scores_normalized,
                    'model': dbscan_model,
                    'feature_names': numeric_cols
                }
                
                self.models['dbscan'] = dbscan_model
                self.feature_names['dbscan'] = numeric_cols
                
                logger.info("DBSCAN model completed")
            except Exception as e:
                logger.error(f"Error running DBSCAN: {str(e)}")
            
            # K-Means Clustering
            try:
                # Determine optimal number of clusters
                silhouette_scores = []
                k_range = range(2, min(11, len(X_scaled) // 2))
                
                for k in k_range:
                    kmeans = KMeans(n_clusters=k, random_state=self.random_state, n_init=10)
                    labels = kmeans.fit_predict(X_scaled)
                    silhouette_scores.append(silhouette_score(X_scaled, labels))
                
                # Find optimal k
                optimal_k = k_range[np.argmax(silhouette_scores)]
                
                # Fit K-Means with optimal k
                kmeans_model = KMeans(n_clusters=optimal_k, random_state=self.random_state, n_init=10)
                kmeans_labels = kmeans_model.fit_predict(X_scaled)
                
                # Calculate distance to nearest cluster center as anomaly score
                distances = kmeans_model.transform(X_scaled)
                min_distances = distances.min(axis=1)
                
                # Normalize scores to 0-1 range
                kmeans_scores_normalized = self._normalize_scores(min_distances)
                
                # Convert to outlier predictions (top contamination% are outliers)
                threshold = np.percentile(kmeans_scores_normalized, (1 - self.contamination) * 100)
                kmeans_predictions = np.where(kmeans_scores_normalized > threshold, -1, 1)
                
                results['kmeans'] = {
                    'predictions': kmeans_predictions,
                    'scores': kmeans_scores_normalized,
                    'model': kmeans_model,
                    'feature_names': numeric_cols,
                    'optimal_k': optimal_k,
                    'silhouette_scores': silhouette_scores
                }
                
                self.models['kmeans'] = kmeans_model
                self.feature_names['kmeans'] = numeric_cols
                
                logger.info(f"K-Means model completed with {optimal_k} clusters")
            except Exception as e:
                logger.error(f"Error running K-Means: {str(e)}")
            
            # Autoencoder
            try:
                # Build autoencoder model
                input_dim = X_scaled.shape[1]
                encoding_dim = max(2, input_dim // 2)  # At least 2 dimensions
                
                autoencoder = self._build_autoencoder(input_dim, encoding_dim)
                
                # Train autoencoder
                early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
                
                history = autoencoder.fit(
                    X_scaled, X_scaled,
                    epochs=50,
                    batch_size=32,
                    validation_split=0.2,
                    callbacks=[early_stopping],
                    verbose=0
                )
                
                # Get reconstruction errors
                reconstructions = autoencoder.predict(X_scaled)
                mse = np.mean(np.power(X_scaled - reconstructions, 2), axis=1)
                
                # Handle any infinity or NaN values in MSE
                mse = np.nan_to_num(mse)
                mse = np.clip(mse, 0, np.finfo(np.float64).max)  # Clip to valid range
                
                # Normalize scores to 0-1 range
                mse_normalized = self._normalize_scores(mse)
                
                # Convert to outlier predictions (top contamination% are outliers)
                threshold = np.percentile(mse_normalized, (1 - self.contamination) * 100)
                autoencoder_predictions = np.where(mse_normalized > threshold, -1, 1)
                
                results['autoencoder'] = {
                    'predictions': autoencoder_predictions,
                    'scores': mse_normalized,
                    'model': autoencoder,
                    'feature_names': numeric_cols,
                    'history': history.history
                }
                
                self.models['autoencoder'] = autoencoder
                self.feature_names['autoencoder'] = numeric_cols
                
                logger.info("Autoencoder model completed")
            except Exception as e:
                logger.error(f"Error running Autoencoder: {str(e)}")
            
            # PCA-based anomaly detection
            try:
                # Fit PCA
                pca = PCA(n_components=min(10, X_scaled.shape[1] - 1), random_state=self.random_state)
                pca_transformed = pca.fit_transform(X_scaled)
                
                # Calculate reconstruction error
                X_reconstructed = pca.inverse_transform(pca_transformed)
                pca_errors = np.mean(np.power(X_scaled - X_reconstructed, 2), axis=1)
                
                # Handle any infinity or NaN values in errors
                pca_errors = np.nan_to_num(pca_errors)
                pca_errors = np.clip(pca_errors, 0, np.finfo(np.float64).max)  # Clip to valid range
                
                # Normalize scores to 0-1 range
                pca_errors_normalized = self._normalize_scores(pca_errors)
                
                # Convert to outlier predictions (top contamination% are outliers)
                threshold = np.percentile(pca_errors_normalized, (1 - self.contamination) * 100)
                pca_predictions = np.where(pca_errors_normalized > threshold, -1, 1)
                
                results['pca'] = {
                    'predictions': pca_predictions,
                    'scores': pca_errors_normalized,
                    'model': pca,
                    'feature_names': numeric_cols,
                    'explained_variance': pca.explained_variance_ratio_
                }
                
                self.models['pca'] = pca
                self.feature_names['pca'] = numeric_cols
                
                logger.info("PCA-based anomaly detection completed")
            except Exception as e:
                logger.error(f"Error running PCA-based anomaly detection: {str(e)}")
            
            self.fitted = True
            return results
            
        except Exception as e:
            logger.error(f"Error running unsupervised models: {str(e)}")
            raise
    
    def _clean_data(self, X):
        """
        Clean data by handling infinity, NaN, and extreme values
        
        Args:
            X (DataFrame): Input data
            
        Returns:
            DataFrame: Cleaned data
        """
        try:
            # Replace infinity with NaN
            X = X.replace([np.inf, -np.inf], np.nan)
            
            # Replace extremely large values with a more reasonable maximum
            for col in X.columns:
                if X[col].dtype in ['float64', 'int64']:
                    # Calculate 99th percentile as a reasonable maximum
                    percentile_99 = np.nanpercentile(X[col], 99)
                    if not np.isnan(percentile_99):
                        # Cap values at 10 times the 99th percentile
                        max_val = percentile_99 * 10
                        X[col] = np.where(X[col] > max_val, max_val, X[col])
                        
                        # Similarly, handle extremely negative values
                        percentile_1 = np.nanpercentile(X[col], 1)
                        if not np.isnan(percentile_1):
                            min_val = percentile_1 * 10
                            X[col] = np.where(X[col] < min_val, min_val, X[col])
            
            return X
            
        except Exception as e:
            logger.error(f"Error cleaning data: {str(e)}")
            return X
    
    def _scale_data(self, X):
        """
        Scale data using RobustScaler (more resistant to outliers)
        
        Args:
            X (DataFrame): Input data
            
        Returns:
            tuple: (scaled data, scaler, imputer)
        """
        try:
            # First impute missing values
            imputer = SimpleImputer(strategy='median')
            X_imputed = imputer.fit_transform(X)
            
            # Use RobustScaler instead of StandardScaler to handle outliers better
            scaler = RobustScaler()
            X_scaled = scaler.fit_transform(X_imputed)
            
            # Check for any remaining infinity or NaN values
            X_scaled = np.nan_to_num(X_scaled)
            X_scaled = np.clip(X_scaled, -1e10, 1e10)  # Clip to reasonable range
            
            return X_scaled, scaler, imputer
            
        except Exception as e:
            logger.error(f"Error scaling data: {str(e)}")
            # Fallback to simple normalization
            X_normalized = (X - X.min()) / (X.max() - X.min())
            return X_normalized.values, None, None
    
    def _normalize_scores(self, scores):
        """
        Normalize scores to 0-1 range
        
        Args:
            scores (array): Input scores
            
        Returns:
            array: Normalized scores
        """
        try:
            # Handle NaN and infinity
            scores = np.nan_to_num(scores)
            scores = np.clip(scores, -1e10, 1e10)  # Clip to reasonable range
            
            # Min-max normalization
            min_score = scores.min()
            max_score = scores.max()
            
            if max_score > min_score:
                normalized_scores = (scores - min_score) / (max_score - min_score)
            else:
                normalized_scores = np.zeros_like(scores)
            
            return normalized_scores
            
        except Exception as e:
            logger.error(f"Error normalizing scores: {str(e)}")
            return np.zeros_like(scores)
    
    def _build_autoencoder(self, input_dim, encoding_dim):
        """
        Build autoencoder model
        
        Args:
            input_dim (int): Input dimension
            encoding_dim (int): Encoding dimension
            
        Returns:
            Model: Autoencoder model
        """
        try:
            # Encoder
            input_layer = layers.Input(shape=(input_dim,))
            encoded = layers.Dense(encoding_dim * 2, activation='relu')(input_layer)
            encoded = layers.Dense(encoding_dim, activation='relu')(encoded)
            
            # Decoder
            decoded = layers.Dense(encoding_dim * 2, activation='relu')(encoded)
            decoded = layers.Dense(input_dim, activation='linear')(decoded)
            
            # Autoencoder model
            autoencoder = models.Model(input_layer, decoded)
            
            # Compile model
            autoencoder.compile(optimizer='adam', loss='mean_squared_error')
            
            return autoencoder
            
        except Exception as e:
            logger.error(f"Error building autoencoder: {str(e)}")
            raise
    
    def predict(self, df, model_name):
        """
        Make predictions using a fitted model
        
        Args:
            df (DataFrame): Input data
            model_name (str): Name of the model to use
            
        Returns:
            dict: Predictions and scores
        """
        if not self.fitted:
            raise ValueError("Models not fitted. Call run_models first.")
        
        if model_name not in self.models:
            raise ValueError(f"Model {model_name} not found.")
        
        try:
            # Get feature names for this model
            feature_names = self.feature_names[model_name]
            
            # Prepare data
            X = df[feature_names].copy()
            
            # Clean the data
            X = self._clean_data(X)
            
            # Scale data using fitted scaler and imputer
            if 'global' in self.scalers and 'global' in self.imputers:
                X_imputed = self.imputers['global'].transform(X)
                X_scaled = self.scalers['global'].transform(X_imputed)
            else:
                # Fallback to simple normalization
                X_scaled = (X - X.min()) / (X.max() - X.min())
                X_scaled = np.nan_to_num(X_scaled)
                X_scaled = np.clip(X_scaled, -1e10, 1e10)
            
            # Get model
            model = self.models[model_name]
            
            # Make predictions
            if model_name == 'isolation_forest':
                predictions = model.predict(X_scaled)
                scores = model.decision_function(X_scaled)
                # Normalize scores to 0-1 range
                scores_normalized = self._normalize_scores(scores)
                
            elif model_name == 'local_outlier_factor':
                predictions = model.predict(X_scaled)
                scores = model.decision_function(X_scaled)
                # Normalize scores to 0-1 range
                scores_normalized = self._normalize_scores(scores)
                
            elif model_name == 'one_class_svm':
                predictions = model.predict(X_scaled)
                scores = model.decision_function(X_scaled)
                # Normalize scores to 0-1 range
                scores_normalized = self._normalize_scores(scores)
                
            elif model_name == 'dbscan':
                predictions = model.fit_predict(X_scaled)
                # Convert to outlier predictions (-1 is outlier in DBSCAN)
                predictions = np.where(predictions == -1, -1, 1)
                
                # Calculate distance to nearest core point as anomaly score
                scores = np.zeros(len(X_scaled))
                for i in range(len(X_scaled)):
                    if predictions[i] == -1:  # Outlier
                        # Find distance to nearest core point
                        core_points = np.where(predictions != -1)[0]
                        if len(core_points) > 0:
                            distances = np.linalg.norm(X_scaled[i] - X_scaled[core_points], axis=1)
                            scores[i] = distances.min()
                        else:
                            scores[i] = np.max(np.linalg.norm(X_scaled - X_scaled.mean(axis=0), axis=1))
                
                # Normalize scores to 0-1 range
                scores_normalized = self._normalize_scores(scores)
                
            elif model_name == 'kmeans':
                # Transform data
                distances = model.transform(X_scaled)
                min_distances = distances.min(axis=1)
                
                # Normalize scores to 0-1 range
                scores_normalized = self._normalize_scores(min_distances)
                
                # Convert to outlier predictions (top contamination% are outliers)
                threshold = np.percentile(scores_normalized, (1 - self.contamination) * 100)
                predictions = np.where(scores_normalized > threshold, -1, 1)
                
            elif model_name == 'autoencoder':
                # Get reconstructions
                reconstructions = model.predict(X_scaled)
                mse = np.mean(np.power(X_scaled - reconstructions, 2), axis=1)
                
                # Handle any infinity or NaN values in MSE
                mse = np.nan_to_num(mse)
                mse = np.clip(mse, 0, np.finfo(np.float64).max)
                
                # Normalize scores to 0-1 range
                scores_normalized = self._normalize_scores(mse)
                
                # Convert to outlier predictions (top contamination% are outliers)
                threshold = np.percentile(scores_normalized, (1 - self.contamination) * 100)
                predictions = np.where(scores_normalized > threshold, -1, 1)
                
            elif model_name == 'pca':
                # Transform data
                pca_transformed = model.transform(X_scaled)
                
                # Calculate reconstruction error
                X_reconstructed = model.inverse_transform(pca_transformed)
                mse = np.mean(np.power(X_scaled - X_reconstructed, 2), axis=1)
                
                # Handle any infinity or NaN values in errors
                mse = np.nan_to_num(mse)
                mse = np.clip(mse, 0, np.finfo(np.float64).max)
                
                # Normalize scores to 0-1 range
                scores_normalized = self._normalize_scores(mse)
                
                # Convert to outlier predictions (top contamination% are outliers)
                threshold = np.percentile(scores_normalized, (1 - self.contamination) * 100)
                predictions = np.where(scores_normalized > threshold, -1, 1)
                
            else:
                raise ValueError(f"Unknown model: {model_name}")
            
            return {
                'predictions': predictions,
                'scores': scores_normalized
            }
            
        except Exception as e:
            logger.error(f"Error making predictions with {model_name}: {str(e)}")
            raise
    
    def get_feature_importance(self, model_name):
        """
        Get feature importance for a model
        
        Args:
            model_name (str): Name of the model
            
        Returns:
            DataFrame: Feature importance
        """
        if not self.fitted:
            raise ValueError("Models not fitted. Call run_models first.")
        
        if model_name not in self.models:
            raise ValueError(f"Model {model_name} not found.")
        
        try:
            # Get feature names
            feature_names = self.feature_names[model_name]
            
            if model_name == 'isolation_forest':
                # Get feature importance from the model
                importance = self.models[model_name].feature_importances_
                
                # Create DataFrame
                importance_df = pd.DataFrame({
                    'feature': feature_names,
                    'importance': importance
                }).sort_values('importance', ascending=False)
                
                return importance_df
                
            elif model_name == 'pca':
                # Get explained variance ratio
                explained_variance = self.models[model_name].explained_variance_ratio_
                
                # Create DataFrame
                importance_df = pd.DataFrame({
                    'feature': [f'PC{i+1}' for i in range(len(explained_variance))],
                    'importance': explained_variance
                }).sort_values('importance', ascending=False)
                
                return importance_df
                
            else:
                # For other models, return equal importance
                importance = np.ones(len(feature_names)) / len(feature_names)
                
                # Create DataFrame
                importance_df = pd.DataFrame({
                    'feature': feature_names,
                    'importance': importance
                }).sort_values('importance', ascending=False)
                
                return importance_df
                
        except Exception as e:
            logger.error(f"Error getting feature importance for {model_name}: {str(e)}")
            raise

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\models\__init__.py ===

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\models\.ipynb_checkpoints\rule_based-checkpoint.py ===
"""
Rule-based Models Module
Implements rule-based fraud detection
"""

import pandas as pd
import numpy as np
import re
from datetime import datetime, timedelta
import yaml
import os
import warnings
import logging
from typing import Dict, List, Tuple, Union, Callable

from fraud_detection_engine.utils.api_utils import is_api_available, get_demo_sanctions_data, get_demo_tax_compliance_data, get_demo_bank_verification_data, get_demo_identity_verification_data

warnings.filterwarnings('ignore')
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class RuleEngine:
    """
    Class for rule-based fraud detection
    Implements configurable rules with weights
    """
    
    def __init__(self, config_path=None, threshold=0.7):
        """
        Initialize RuleEngine
        
        Args:
            config_path (str, optional): Path to configuration file
            threshold (float): Threshold for rule violation
        """
        self.threshold = threshold
        self.rules = {}
        self.rule_weights = {}
        self.rule_descriptions = {}
        self.fitted = False
        self.api_available = {}
        
        # Load configuration
        if config_path and os.path.exists(config_path):
            self._load_config(config_path)
        else:
            self._load_default_rules()
    
    def _load_config(self, config_path):
        """
        Load configuration from YAML file
        
        Args:
            config_path (str): Path to configuration file
        """
        try:
            with open(config_path, 'r') as f:
                config = yaml.safe_load(f)
            
            # Load rules
            if 'rules' in config:
                for rule_name, rule_config in config['rules'].items():
                    if rule_config.get('enabled', True):
                        self.rules[rule_name] = self._create_rule_function(rule_config)
                        self.rule_weights[rule_name] = rule_config.get('weight', 1.0)
                        self.rule_descriptions[rule_name] = rule_config.get('description', '')
            
            # Check API availability for rules that depend on external services
            self.api_available = {
                'sanctions': is_api_available('sanctions'),
                'tax_compliance': is_api_available('tax_compliance'),
                'bank_verification': is_api_available('bank_verification'),
                'identity_verification': is_api_available('identity_verification'),
                'geolocation': is_api_available('geolocation')
            }
            
            logger.info(f"API availability: {self.api_available}")
            
        except Exception as e:
            logger.error(f"Error loading configuration: {str(e)}")
            self._load_default_rules()
    
    def _load_default_rules(self):
        """
        Load default rules
        """
        try:
            # Amount-based rules
            self.rules['high_amount'] = self._high_amount_rule
            self.rule_weights['high_amount'] = 0.3
            self.rule_descriptions['high_amount'] = "Transaction amount exceeds threshold"
            
            self.rules['unusual_amount_for_sender'] = self._unusual_amount_for_sender_rule
            self.rule_weights['unusual_amount_for_sender'] = 0.2
            self.rule_descriptions['unusual_amount_for_sender'] = "Amount is unusual for the sender"
            
            self.rules['unusual_amount_for_receiver'] = self._unusual_amount_for_receiver_rule
            self.rule_weights['unusual_amount_for_receiver'] = 0.2
            self.rule_descriptions['unusual_amount_for_receiver'] = "Amount is unusual for the receiver"
            
            self.rules['round_amount'] = self._round_amount_rule
            self.rule_weights['round_amount'] = 0.1
            self.rule_descriptions['round_amount'] = "Transaction amount is suspiciously round"
            
            # Frequency-based rules
            self.rules['high_frequency_sender'] = self._high_frequency_sender_rule
            self.rule_weights['high_frequency_sender'] = 0.3
            self.rule_descriptions['high_frequency_sender'] = "High transaction frequency from sender"
            
            self.rules['high_frequency_receiver'] = self._high_frequency_receiver_rule
            self.rule_weights['high_frequency_receiver'] = 0.3
            self.rule_descriptions['high_frequency_receiver'] = "High transaction frequency to receiver"
            
            self.rules['rapid_succession'] = self._rapid_succession_rule
            self.rule_weights['rapid_succession'] = 0.4
            self.rule_descriptions['rapid_succession'] = "Multiple transactions in rapid succession"
            
            # Location-based rules
            self.rules['cross_border'] = self._cross_border_rule
            self.rule_weights['cross_border'] = 0.3
            self.rule_descriptions['cross_border'] = "Transaction crosses international borders"
            
            self.rules['high_risk_country'] = self._high_risk_country_rule
            self.rule_weights['high_risk_country'] = 0.5
            self.rule_descriptions['high_risk_country'] = "Transaction involves high-risk country"
            
            self.rules['unusual_location_for_sender'] = self._unusual_location_for_sender_rule
            self.rule_weights['unusual_location_for_sender'] = 0.3
            self.rule_descriptions['unusual_location_for_sender'] = "Transaction from unusual location for sender"
            
            # Time-based rules
            self.rules['unusual_hour'] = self._unusual_hour_rule
            self.rule_weights['unusual_hour'] = 0.2
            self.rule_descriptions['unusual_hour'] = "Transaction during unusual hours"
            
            self.rules['weekend'] = self._weekend_rule
            self.rule_weights['weekend'] = 0.1
            self.rule_descriptions['weekend'] = "Transaction on weekend"
            
            # Identity-based rules
            self.rules['new_sender'] = self._new_sender_rule
            self.rule_weights['new_sender'] = 0.3
            self.rule_descriptions['new_sender'] = "First transaction from new sender"
            
            self.rules['new_receiver'] = self._new_receiver_rule
            self.rule_weights['new_receiver'] = 0.3
            self.rule_descriptions['new_receiver'] = "First transaction to new receiver"
            
            # External API-based rules
            self.rules['sanctions_check'] = self._sanctions_check_rule
            self.rule_weights['sanctions_check'] = 0.5
            self.rule_descriptions['sanctions_check'] = "Transaction involves sanctioned entities"
            
            self.rules['tax_compliance'] = self._tax_compliance_rule
            self.rule_weights['tax_compliance'] = 0.3
            self.rule_descriptions['tax_compliance'] = "Transaction involves non-compliant entities"
            
            self.rules['bank_verification'] = self._bank_verification_rule
            self.rule_weights['bank_verification'] = 0.3
            self.rule_descriptions['bank_verification'] = "Transaction involves unverified bank accounts"
            
            self.rules['identity_verification'] = self._identity_verification_rule
            self.rule_weights['identity_verification'] = 0.3
            self.rule_descriptions['identity_verification'] = "Transaction involves unverified identities"
            
            # Check API availability
            self.api_available = {
                'sanctions': is_api_available('sanctions'),
                'tax_compliance': is_api_available('tax_compliance'),
                'bank_verification': is_api_available('bank_verification'),
                'identity_verification': is_api_available('identity_verification'),
                'geolocation': is_api_available('geolocation')
            }
            
            logger.info(f"API availability: {self.api_available}")
            
        except Exception as e:
            logger.error(f"Error loading default rules: {str(e)}")
            raise
    
    def _create_rule_function(self, rule_config):
        """
        Create a rule function from configuration
        
        Args:
            rule_config (dict): Rule configuration
            
        Returns:
            function: Rule function
        """
        rule_type = rule_config.get('type')
        
        if rule_type == 'amount_threshold':
            threshold = rule_config.get('threshold', 10000)
            return lambda row: row.get('amount', 0) > threshold
        
        elif rule_type == 'sender_amount_outlier':
            std_multiplier = rule_config.get('std_multiplier', 3)
            min_transactions = rule_config.get('min_transactions', 5)
            return lambda row: self._is_sender_amount_outlier(row, std_multiplier, min_transactions)
        
        elif rule_type == 'receiver_amount_outlier':
            std_multiplier = rule_config.get('std_multiplier', 3)
            min_transactions = rule_config.get('min_transactions', 5)
            return lambda row: self._is_receiver_amount_outlier(row, std_multiplier, min_transactions)
        
        elif rule_type == 'round_amount':
            threshold = rule_config.get('threshold', 1000)
            return lambda row: row.get('amount', 0) > threshold and row.get('amount', 0) % 1000 == 0
        
        elif rule_type == 'high_frequency_sender':
            time_window = rule_config.get('time_window', '1H')
            max_transactions = rule_config.get('max_transactions', 10)
            return lambda row: self._is_high_frequency_sender(row, time_window, max_transactions)
        
        elif rule_type == 'high_frequency_receiver':
            time_window = rule_config.get('time_window', '1H')
            max_transactions = rule_config.get('max_transactions', 10)
            return lambda row: self._is_high_frequency_receiver(row, time_window, max_transactions)
        
        elif rule_type == 'rapid_succession':
            time_window = rule_config.get('time_window', '5M')
            min_transactions = rule_config.get('min_transactions', 3)
            return lambda row: self._is_rapid_succession(row, time_window, min_transactions)
        
        elif rule_type == 'cross_border':
            return lambda row: self._is_cross_border(row)
        
        elif rule_type == 'high_risk_country':
            countries = rule_config.get('countries', ['North Korea', 'Iran', 'Syria', 'Cuba'])
            return lambda row: self._is_high_risk_country(row, countries)
        
        elif rule_type == 'unusual_location_for_sender':
            return lambda row: self._is_unusual_location_for_sender(row)
        
        elif rule_type == 'unusual_hour':
            start_hour = rule_config.get('start_hour', 23)
            end_hour = rule_config.get('end_hour', 5)
            return lambda row: self._is_unusual_hour(row, start_hour, end_hour)
        
        elif rule_type == 'weekend':
            return lambda row: self._is_weekend(row)
        
        elif rule_type == 'new_sender':
            time_window = rule_config.get('time_window', '7D')
            return lambda row: self._is_new_sender(row, time_window)
        
        elif rule_type == 'new_receiver':
            time_window = rule_config.get('time_window', '7D')
            return lambda row: self._is_new_receiver(row, time_window)
        
        else:
            logger.warning(f"Unknown rule type: {rule_type}")
            return lambda row: False
    
    def apply_rules(self, df):
        """
        Apply all rules to the dataframe
        
        Args:
            df (DataFrame): Input data
            
        Returns:
            dict: Rule results
        """
        try:
            # Sort by timestamp if available
            if 'timestamp' in df.columns and not pd.api.types.is_datetime64_any_dtype(df['timestamp']):
                df = df.copy()
                df['timestamp'] = pd.to_datetime(df['timestamp'])
            
            if 'timestamp' in df.columns:
                df = df.sort_values('timestamp')
            
            # Initialize results
            rule_results = {}
            rule_scores = {}
            violated_rules = {}
            
            # Apply each rule
            for rule_name, rule_func in self.rules.items():
                try:
                    # Apply rule to each row
                    results = []
                    for _, row in df.iterrows():
                        try:
                            result = rule_func(row)
                            results.append(result)
                        except Exception as e:
                            logger.warning(f"Error applying rule {rule_name} to row: {str(e)}")
                            results.append(False)
                    
                    rule_results[rule_name] = results
                    
                    # Calculate weighted score
                    weight = self.rule_weights.get(rule_name, 1.0)
                    rule_scores[rule_name] = [weight if result else 0 for result in results]
                    
                    # Track violated rules
                    violated_rules[rule_name] = [i for i, result in enumerate(results) if result]
                    
                except Exception as e:
                    logger.error(f"Error applying rule {rule_name}: {str(e)}")
                    rule_results[rule_name] = [False] * len(df)
                    rule_scores[rule_name] = [0] * len(df)
                    violated_rules[rule_name] = []
            
            # Calculate total rule score for each transaction
            total_scores = []
            for i in range(len(df)):
                score = sum(rule_scores[rule_name][i] for rule_name in rule_scores)
                total_scores.append(score)
            
            # Normalize scores
            max_possible_score = sum(self.rule_weights.values())
            normalized_scores = [score / max_possible_score for score in total_scores]
            
            # Determine if transaction violates rules based on threshold
            rule_violations = [score >= self.threshold for score in normalized_scores]
            
            # Get violated rule names for each transaction
            violated_rule_names = []
            for i in range(len(df)):
                names = []
                for rule_name in violated_rules:
                    if i in violated_rules[rule_name]:
                        names.append(rule_name)
                violated_rule_names.append(names)
            
            self.fitted = True
            
            return {
                'rule_results': rule_results,
                'rule_scores': rule_scores,
                'total_scores': total_scores,
                'normalized_scores': normalized_scores,
                'rule_violations': rule_violations,
                'violated_rule_names': violated_rule_names,
                'rules': self.rules,
                'rule_weights': self.rule_weights,
                'rule_descriptions': self.rule_descriptions
            }
            
        except Exception as e:
            logger.error(f"Error applying rules: {str(e)}")
            raise
    
    # Rule functions
    def _high_amount_rule(self, row):
        """Check if transaction amount is high"""
        return row.get('amount', 0) > 10000
    
    def _unusual_amount_for_sender_rule(self, row, std_multiplier=3, min_transactions=5):
        """Check if amount is unusual for the sender"""
        # This would require access to sender's transaction history
        # For simplicity, we'll use a placeholder
        return False
    
    def _unusual_amount_for_receiver_rule(self, row, std_multiplier=3, min_transactions=5):
        """Check if amount is unusual for the receiver"""
        # This would require access to receiver's transaction history
        # For simplicity, we'll use a placeholder
        return False
    
    def _round_amount_rule(self, row):
        """Check if transaction amount is suspiciously round"""
        amount = row.get('amount', 0)
        return amount > 1000 and amount % 1000 == 0
    
    def _high_frequency_sender_rule(self, row, time_window='1H', max_transactions=10):
        """Check if sender has high transaction frequency"""
        # This would require access to sender's transaction history
        # For simplicity, we'll use a placeholder
        return False
    
    def _high_frequency_receiver_rule(self, row, time_window='1H', max_transactions=10):
        """Check if receiver has high transaction frequency"""
        # This would require access to receiver's transaction history
        # For simplicity, we'll use a placeholder
        return False
    
    def _rapid_succession_rule(self, row, time_window='5M', min_transactions=3):
        """Check if there are multiple transactions in rapid succession"""
        # This would require access to transaction history
        # For simplicity, we'll use a placeholder
        return False
    
    def _cross_border_rule(self, row):
        """Check if transaction crosses international borders"""
        sender_location = row.get('sender_location', '')
        receiver_location = row.get('receiver_location', '')
        
        if not sender_location or not receiver_location:
            return False
        
        # Extract countries from locations
        sender_country = sender_location.split(',')[0].strip()
        receiver_country = receiver_location.split(',')[0].strip()
        
        return sender_country != receiver_country
    
    def _high_risk_country_rule(self, row, countries=None):
        """Check if transaction involves high-risk country"""
        if countries is None:
            countries = ['North Korea', 'Iran', 'Syria', 'Cuba']
        
        sender_location = row.get('sender_location', '')
        receiver_location = row.get('receiver_location', '')
        
        # Check if any high-risk country is involved
        for country in countries:
            if country in sender_location or country in receiver_location:
                return True
        
        return False
    
    def _unusual_location_for_sender_rule(self, row):
        """Check if transaction is from unusual location for sender"""
        # This would require access to sender's transaction history
        # For simplicity, we'll use a placeholder
        return False
    
    def _unusual_hour_rule(self, row, start_hour=23, end_hour=5):
        """Check if transaction is during unusual hours"""
        timestamp = row.get('timestamp')
        if timestamp is None:
            return False
        
        if not pd.api.types.is_datetime64_any_dtype(timestamp):
            timestamp = pd.to_datetime(timestamp)
        
        hour = timestamp.hour
        
        # Handle overnight range
        if start_hour > end_hour:
            return hour >= start_hour or hour < end_hour
        else:
            return start_hour <= hour < end_hour
    
    def _weekend_rule(self, row):
        """Check if transaction is on weekend"""
        timestamp = row.get('timestamp')
        if timestamp is None:
            return False
        
        if not pd.api.types.is_datetime64_any_dtype(timestamp):
            timestamp = pd.to_datetime(timestamp)
        
        # Saturday=5, Sunday=6
        return timestamp.dayofweek >= 5
    
    def _new_sender_rule(self, row, time_window='7D'):
        """Check if this is the first transaction from sender"""
        # This would require access to sender's transaction history
        # For simplicity, we'll use a placeholder
        return False
    
    def _new_receiver_rule(self, row, time_window='7D'):
        """Check if this is the first transaction to receiver"""
        # This would require access to receiver's transaction history
        # For simplicity, we'll use a placeholder
        return False
    
    def _sanctions_check_rule(self, row):
        """Check if transaction involves sanctioned entities"""
        try:
            if not self.api_available.get('sanctions', False):
                # Use demo data
                sanctions_data = get_demo_sanctions_data()
                
                # Simple demo check
                sender_id = row.get('sender_id', '')
                receiver_id = row.get('receiver_id', '')
                
                # Check if sender or receiver is in demo sanctions list
                is_sanctioned = (
                    sanctions_data['entity_id'].str.contains(sender_id, na=False).any() or
                    sanctions_data['entity_id'].str.contains(receiver_id, na=False).any()
                )
                
                return is_sanctioned
            else:
                # Here you would implement the actual API call
                # For now, return False
                return False
        except Exception as e:
            logger.warning(f"Error in sanctions check: {str(e)}")
            return False
    
    def _tax_compliance_rule(self, row):
        """Check if entities are tax compliant"""
        try:
            if not self.api_available.get('tax_compliance', False):
                # Use demo data
                tax_data = get_demo_tax_compliance_data()
                
                # Simple demo check
                sender_id = row.get('sender_id', '')
                receiver_id = row.get('receiver_id', '')
                
                # Check if sender or receiver is non-compliant
                is_non_compliant = False
                
                for _, entity in tax_data.iterrows():
                    if entity['entity_id'] in [sender_id, receiver_id] and entity['compliance_status'] == 'Non-compliant':
                        is_non_compliant = True
                        break
                
                return is_non_compliant
            else:
                # Here you would implement the actual API call
                # For now, return False
                return False
        except Exception as e:
            logger.warning(f"Error in tax compliance check: {str(e)}")
            return False
    
    def _bank_verification_rule(self, row):
        """Check if bank accounts are verified"""
        try:
            if not self.api_available.get('bank_verification', False):
                # Use demo data
                bank_data = get_demo_bank_verification_data()
                
                # Simple demo check
                sender_id = row.get('sender_id', '')
                receiver_id = row.get('receiver_id', '')
                
                # Check if sender or receiver account is not verified
                is_unverified = False
                
                for _, account in bank_data.iterrows():
                    if account['account_number'] in [sender_id, receiver_id] and account['verification_status'] == 'Not Verified':
                        is_unverified = True
                        break
                
                return is_unverified
            else:
                # Here you would implement the actual API call
                # For now, return False
                return False
        except Exception as e:
            logger.warning(f"Error in bank verification check: {str(e)}")
            return False
    
    def _identity_verification_rule(self, row):
        """Check if identities are verified"""
        try:
            if not self.api_available.get('identity_verification', False):
                # Use demo data
                identity_data = get_demo_identity_verification_data()
                
                # Simple demo check
                sender_id = row.get('sender_id', '')
                receiver_id = row.get('receiver_id', '')
                
                # Check if sender or receiver identity is not verified
                is_unverified = False
                
                for _, identity in identity_data.iterrows():
                    if identity['id_number'] in [sender_id, receiver_id] and identity['verification_status'] == 'Not Verified':
                        is_unverified = True
                        break
                
                return is_unverified
            else:
                # Here you would implement the actual API call
                # For now, return False
                return False
        except Exception as e:
            logger.warning(f"Error in identity verification check: {str(e)}")
            return False
    
    def add_rule(self, rule_name, rule_func, weight=1.0, description=''):
        """
        Add a custom rule
        
        Args:
            rule_name (str): Name of the rule
            rule_func (function): Rule function
            weight (float): Weight of the rule
            description (str): Description of the rule
        """
        self.rules[rule_name] = rule_func
        self.rule_weights[rule_name] = weight
        self.rule_descriptions[rule_name] = description
        logger.info(f"Added rule: {rule_name}")
    
    def remove_rule(self, rule_name):
        """
        Remove a rule
        
        Args:
            rule_name (str): Name of the rule to remove
        """
        if rule_name in self.rules:
            del self.rules[rule_name]
            del self.rule_weights[rule_name]
            del self.rule_descriptions[rule_name]
            logger.info(f"Removed rule: {rule_name}")
        else:
            logger.warning(f"Rule not found: {rule_name}")
    
    def update_rule_weight(self, rule_name, weight):
        """
        Update the weight of a rule
        
        Args:
            rule_name (str): Name of the rule
            weight (float): New weight
        """
        if rule_name in self.rule_weights:
            self.rule_weights[rule_name] = weight
            logger.info(f"Updated weight for rule {rule_name}: {weight}")
        else:
            logger.warning(f"Rule not found: {rule_name}")
    
    def get_rules(self):
        """
        Get all rules
        
        Returns:
            dict: Rules information
        """
        return {
            'rules': list(self.rules.keys()),
            'weights': self.rule_weights,
            'descriptions': self.rule_descriptions
        }
    
    def save_config(self, config_path):
        """
        Save current configuration to file
        
        Args:
            config_path (str): Path to save configuration
        """
        try:
            config = {
                'rules': {}
            }
            
            for rule_name in self.rules:
                config['rules'][rule_name] = {
                    'enabled': True,
                    'weight': self.rule_weights[rule_name],
                    'description': self.rule_descriptions[rule_name]
                }
            
            with open(config_path, 'w') as f:
                yaml.dump(config, f, default_flow_style=False)
            
            logger.info(f"Configuration saved to {config_path}")
            
        except Exception as e:
            logger.error(f"Error saving configuration: {str(e)}")
            raise

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\models\.ipynb_checkpoints\supervised-checkpoint.py ===
"""
Supervised Models Module
Implements supervised learning models for fraud detection
"""
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score, 
    roc_auc_score, confusion_matrix, classification_report, 
    precision_recall_curve, average_precision_score, roc_curve
)
from sklearn.preprocessing import StandardScaler, LabelEncoder, RobustScaler
from sklearn.feature_selection import SelectKBest, f_classif, RFE
from imblearn.over_sampling import SMOTE, ADASYN, BorderlineSMOTE
from imblearn.under_sampling import RandomUnderSampler, TomekLinks
from imblearn.combine import SMOTEENN, SMOTETomek
import xgboost as xgb
import lightgbm as lgb
import shap
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
import logging
from typing import Dict, List, Tuple, Union
warnings.filterwarnings('ignore')
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class SupervisedModels:
    """
    Class for supervised fraud detection models
    Implements Random Forest, XGBoost, Logistic Regression, etc.
    """
    
    def __init__(self, test_size=0.2, random_state=42, handle_imbalance=True):
        """
        Initialize SupervisedModels
        
        Args:
            test_size (float): Proportion of data for testing
            random_state (int): Random seed
            handle_imbalance (bool): Whether to handle class imbalance
        """
        self.test_size = test_size
        self.random_state = random_state
        self.handle_imbalance = handle_imbalance
        self.models = {}
        self.feature_names = {}
        self.scalers = {}
        self.label_encoders = {}
        self.feature_selectors = {}
        self.resamplers = {}
        self.performance = {}
        self.feature_importance = {}
        self.shap_values = {}
        self.fitted = False
        
    def run_models(self, df):
        """
        Run all supervised models
        
        Args:
            df (DataFrame): Input data
            
        Returns:
            dict: Model results
        """
        try:
            # Check if fraud_flag column exists
            if 'fraud_flag' not in df.columns:
                logger.warning("No fraud_flag column found. Using synthetic labels for demonstration.")
                # Create synthetic labels based on statistical outliers
                numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
                if len(numeric_cols) > 0:
                    # Use Z-score to identify outliers as potential fraud
                    X = df[numeric_cols].fillna(0)
                    scaler = StandardScaler()
                    X_scaled = scaler.fit_transform(X)
                    
                    # Calculate Z-scores
                    z_scores = np.abs(X_scaled).max(axis=1)
                    
                    # Label top 5% as fraud
                    threshold = np.percentile(z_scores, 95)
                    y = (z_scores > threshold).astype(int)
                    
                    # Add synthetic labels to dataframe
                    df = df.copy()
                    df['fraud_flag'] = y
                else:
                    raise ValueError("No numeric columns found for creating synthetic labels")
            else:
                y = df['fraud_flag'].astype(int)
            
            # Get feature columns (exclude target and ID columns)
            exclude_cols = ['fraud_flag', 'transaction_id', 'sender_id', 'receiver_id']
            feature_cols = [col for col in df.columns if col not in exclude_cols]
            
            # Get numeric and categorical features
            numeric_features = df[feature_cols].select_dtypes(include=[np.number]).columns.tolist()
            categorical_features = df[feature_cols].select_dtypes(include=['object', 'category']).columns.tolist()
            
            # Prepare data
            X = df[feature_cols].copy()
            
            # Clean data before processing
            X = self._clean_data(X)
            
            # Handle categorical features
            for col in categorical_features:
                if col in X.columns:
                    # Label encode categorical features
                    le = LabelEncoder()
                    X[col] = le.fit_transform(X[col].astype(str))
                    self.label_encoders[col] = le
            
            # Split data
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=self.test_size, random_state=self.random_state, stratify=y
            )
            
            # Scale numeric features using RobustScaler
            if numeric_features:
                scaler = RobustScaler()
                X_train[numeric_features] = scaler.fit_transform(X_train[numeric_features])
                X_test[numeric_features] = scaler.transform(X_test[numeric_features])
                self.scalers['numeric'] = scaler
            
            # Handle class imbalance
            if self.handle_imbalance and y_train.sum() / len(y_train) < 0.1:  # If minority class < 10%
                # Try different resampling techniques
                resampling_methods = {
                    'smote': SMOTE(random_state=self.random_state),
                    'adasyn': ADASYN(random_state=self.random_state),
                    'borderline_smote': BorderlineSMOTE(random_state=self.random_state),
                    'smote_enn': SMOTEENN(random_state=self.random_state),
                    'smote_tomek': SMOTETomek(random_state=self.random_state)
                }
                
                # Evaluate each method using cross-validation
                best_method = None
                best_score = 0
                
                for method_name, resampler in resampling_methods.items():
                    try:
                        # Apply resampling
                        X_resampled, y_resampled = resampler.fit_resample(X_train, y_train)
                        
                        # Evaluate with a simple model
                        rf = RandomForestClassifier(n_estimators=50, random_state=self.random_state, n_jobs=-1)
                        scores = cross_val_score(rf, X_resampled, y_resampled, cv=3, scoring='f1')
                        avg_score = scores.mean()
                        
                        if avg_score > best_score:
                            best_score = avg_score
                            best_method = method_name
                            self.resamplers['best'] = resampler
                            X_train, y_train = X_resampled, y_resampled
                    except Exception as e:
                        logger.warning(f"Error with {method_name}: {str(e)}")
                
                if best_method:
                    logger.info(f"Using {best_method} for handling class imbalance")
                else:
                    logger.warning("No suitable resampling method found")
            
            # Store feature names
            all_feature_names = feature_cols
            
            # Run different models
            results = {}
            
            # Random Forest
            try:
                rf_model = RandomForestClassifier(
                    n_estimators=100,
                    max_depth=10,
                    min_samples_split=5,
                    min_samples_leaf=2,
                    class_weight='balanced',
                    random_state=self.random_state,
                    n_jobs=-1
                )
                
                rf_model.fit(X_train, y_train)
                rf_pred = rf_model.predict(X_test)
                rf_proba = rf_model.predict_proba(X_test)[:, 1]
                
                # Calculate performance metrics
                rf_performance = self._calculate_performance_metrics(y_test, rf_pred, rf_proba)
                
                # Get feature importance
                rf_importance = pd.DataFrame({
                    'feature': all_feature_names,
                    'importance': rf_model.feature_importances_
                }).sort_values('importance', ascending=False)
                
                # Calculate SHAP values
                explainer = shap.TreeExplainer(rf_model)
                rf_shap_values = explainer.shap_values(X_test)
                
                results['random_forest'] = {
                    'model': rf_model,
                    'predictions': rf_pred,
                    'probabilities': rf_proba,
                    'performance': rf_performance,
                    'feature_importance': rf_importance,
                    'shap_values': rf_shap_values,
                    'feature_names': all_feature_names
                }
                
                self.models['random_forest'] = rf_model
                self.feature_names['random_forest'] = all_feature_names
                self.performance['random_forest'] = rf_performance
                self.feature_importance['random_forest'] = rf_importance
                self.shap_values['random_forest'] = rf_shap_values
                
                logger.info("Random Forest model completed")
            except Exception as e:
                logger.error(f"Error running Random Forest: {str(e)}")
            
            # XGBoost
            try:
                xgb_model = xgb.XGBClassifier(
                    n_estimators=100,
                    max_depth=6,
                    learning_rate=0.1,
                    subsample=0.8,
                    colsample_bytree=0.8,
                    scale_pos_weight=len(y_train[y_train==0]) / len(y_train[y_train==1]),
                    random_state=self.random_state,
                    use_label_encoder=False,
                    eval_metric='logloss'
                )
                
                xgb_model.fit(X_train, y_train)
                xgb_pred = xgb_model.predict(X_test)
                xgb_proba = xgb_model.predict_proba(X_test)[:, 1]
                
                # Calculate performance metrics
                xgb_performance = self._calculate_performance_metrics(y_test, xgb_pred, xgb_proba)
                
                # Get feature importance
                xgb_importance = pd.DataFrame({
                    'feature': all_feature_names,
                    'importance': xgb_model.feature_importances_
                }).sort_values('importance', ascending=False)
                
                # Calculate SHAP values
                explainer = shap.TreeExplainer(xgb_model)
                xgb_shap_values = explainer.shap_values(X_test)
                
                results['xgboost'] = {
                    'model': xgb_model,
                    'predictions': xgb_pred,
                    'probabilities': xgb_proba,
                    'performance': xgb_performance,
                    'feature_importance': xgb_importance,
                    'shap_values': xgb_shap_values,
                    'feature_names': all_feature_names
                }
                
                self.models['xgboost'] = xgb_model
                self.feature_names['xgboost'] = all_feature_names
                self.performance['xgboost'] = xgb_performance
                self.feature_importance['xgboost'] = xgb_importance
                self.shap_values['xgboost'] = xgb_shap_values
                
                logger.info("XGBoost model completed")
            except Exception as e:
                logger.error(f"Error running XGBoost: {str(e)}")
            
            # LightGBM
            try:
                lgb_model = lgb.LGBMClassifier(
                    n_estimators=100,
                    max_depth=6,
                    learning_rate=0.1,
                    subsample=0.8,
                    colsample_bytree=0.8,
                    scale_pos_weight=len(y_train[y_train==0]) / len(y_train[y_train==1]),
                    random_state=self.random_state
                )
                
                lgb_model.fit(X_train, y_train)
                lgb_pred = lgb_model.predict(X_test)
                lgb_proba = lgb_model.predict_proba(X_test)[:, 1]
                
                # Calculate performance metrics
                lgb_performance = self._calculate_performance_metrics(y_test, lgb_pred, lgb_proba)
                
                # Get feature importance
                lgb_importance = pd.DataFrame({
                    'feature': all_feature_names,
                    'importance': lgb_model.feature_importances_
                }).sort_values('importance', ascending=False)
                
                # Calculate SHAP values
                explainer = shap.TreeExplainer(lgb_model)
                lgb_shap_values = explainer.shap_values(X_test)
                
                results['lightgbm'] = {
                    'model': lgb_model,
                    'predictions': lgb_pred,
                    'probabilities': lgb_proba,
                    'performance': lgb_performance,
                    'feature_importance': lgb_importance,
                    'shap_values': lgb_shap_values,
                    'feature_names': all_feature_names
                }
                
                self.models['lightgbm'] = lgb_model
                self.feature_names['lightgbm'] = all_feature_names
                self.performance['lightgbm'] = lgb_performance
                self.feature_importance['lightgbm'] = lgb_importance
                self.shap_values['lightgbm'] = lgb_shap_values
                
                logger.info("LightGBM model completed")
            except Exception as e:
                logger.error(f"Error running LightGBM: {str(e)}")
            
            # Logistic Regression
            try:
                lr_model = LogisticRegression(
                    C=1.0,
                    class_weight='balanced',
                    random_state=self.random_state,
                    max_iter=1000
                )
                
                lr_model.fit(X_train, y_train)
                lr_pred = lr_model.predict(X_test)
                lr_proba = lr_model.predict_proba(X_test)[:, 1]
                
                # Calculate performance metrics
                lr_performance = self._calculate_performance_metrics(y_test, lr_pred, lr_proba)
                
                # Get feature importance (coefficients)
                lr_importance = pd.DataFrame({
                    'feature': all_feature_names,
                    'importance': np.abs(lr_model.coef_[0])
                }).sort_values('importance', ascending=False)
                
                # Calculate SHAP values
                explainer = shap.LinearExplainer(lr_model, X_train)
                lr_shap_values = explainer.shap_values(X_test)
                
                results['logistic_regression'] = {
                    'model': lr_model,
                    'predictions': lr_pred,
                    'probabilities': lr_proba,
                    'performance': lr_performance,
                    'feature_importance': lr_importance,
                    'shap_values': lr_shap_values,
                    'feature_names': all_feature_names
                }
                
                self.models['logistic_regression'] = lr_model
                self.feature_names['logistic_regression'] = all_feature_names
                self.performance['logistic_regression'] = lr_performance
                self.feature_importance['logistic_regression'] = lr_importance
                self.shap_values['logistic_regression'] = lr_shap_values
                
                logger.info("Logistic Regression model completed")
            except Exception as e:
                logger.error(f"Error running Logistic Regression: {str(e)}")
            
            # Gradient Boosting
            try:
                gb_model = GradientBoostingClassifier(
                    n_estimators=100,
                    max_depth=6,
                    learning_rate=0.1,
                    subsample=0.8,
                    random_state=self.random_state
                )
                
                gb_model.fit(X_train, y_train)
                gb_pred = gb_model.predict(X_test)
                gb_proba = gb_model.predict_proba(X_test)[:, 1]
                
                # Calculate performance metrics
                gb_performance = self._calculate_performance_metrics(y_test, gb_pred, gb_proba)
                
                # Get feature importance
                gb_importance = pd.DataFrame({
                    'feature': all_feature_names,
                    'importance': gb_model.feature_importances_
                }).sort_values('importance', ascending=False)
                
                # Calculate SHAP values
                explainer = shap.TreeExplainer(gb_model)
                gb_shap_values = explainer.shap_values(X_test)
                
                results['gradient_boosting'] = {
                    'model': gb_model,
                    'predictions': gb_pred,
                    'probabilities': gb_proba,
                    'performance': gb_performance,
                    'feature_importance': gb_importance,
                    'shap_values': gb_shap_values,
                    'feature_names': all_feature_names
                }
                
                self.models['gradient_boosting'] = gb_model
                self.feature_names['gradient_boosting'] = all_feature_names
                self.performance['gradient_boosting'] = gb_performance
                self.feature_importance['gradient_boosting'] = gb_importance
                self.shap_values['gradient_boosting'] = gb_shap_values
                
                logger.info("Gradient Boosting model completed")
            except Exception as e:
                logger.error(f"Error running Gradient Boosting: {str(e)}")
            
            # Neural Network
            try:
                nn_model = MLPClassifier(
                    hidden_layer_sizes=(100, 50),
                    activation='relu',
                    solver='adam',
                    alpha=0.0001,
                    batch_size=32,
                    learning_rate='adaptive',
                    max_iter=200,
                    random_state=self.random_state
                )
                
                nn_model.fit(X_train, y_train)
                nn_pred = nn_model.predict(X_test)
                nn_proba = nn_model.predict_proba(X_test)[:, 1]
                
                # Calculate performance metrics
                nn_performance = self._calculate_performance_metrics(y_test, nn_pred, nn_proba)
                
                # For neural networks, we don't have direct feature importance
                # Use permutation importance instead
                from sklearn.inspection import permutation_importance
                
                perm_importance = permutation_importance(
                    nn_model, X_test, y_test, n_repeats=5, random_state=self.random_state
                )
                
                nn_importance = pd.DataFrame({
                    'feature': all_feature_names,
                    'importance': perm_importance.importances_mean
                }).sort_values('importance', ascending=False)
                
                # Calculate SHAP values (using KernelExplainer for black-box models)
                explainer = shap.KernelExplainer(nn_model.predict_proba, X_train[:100])  # Use subset for speed
                nn_shap_values = explainer.shap_values(X_test[:100])  # Use subset for speed
                
                results['neural_network'] = {
                    'model': nn_model,
                    'predictions': nn_pred,
                    'probabilities': nn_proba,
                    'performance': nn_performance,
                    'feature_importance': nn_importance,
                    'shap_values': nn_shap_values,
                    'feature_names': all_feature_names
                }
                
                self.models['neural_network'] = nn_model
                self.feature_names['neural_network'] = all_feature_names
                self.performance['neural_network'] = nn_performance
                self.feature_importance['neural_network'] = nn_importance
                self.shap_values['neural_network'] = nn_shap_values
                
                logger.info("Neural Network model completed")
            except Exception as e:
                logger.error(f"Error running Neural Network: {str(e)}")
            
            # Ensemble model (combine predictions from all models)
            try:
                # Get predictions from all models
                all_predictions = []
                all_probabilities = []
                
                for model_name in results:
                    all_predictions.append(results[model_name]['predictions'])
                    all_probabilities.append(results[model_name]['probabilities'])
                
                # Majority voting for predictions
                ensemble_pred = np.array(all_predictions).mean(axis=0) > 0.5
                
                # Average probabilities
                ensemble_proba = np.array(all_probabilities).mean(axis=0)
                
                # Calculate performance metrics
                ensemble_performance = self._calculate_performance_metrics(y_test, ensemble_pred, ensemble_proba)
                
                # For ensemble, average feature importances
                ensemble_importance = pd.DataFrame({
                    'feature': all_feature_names,
                    'importance': np.zeros(len(all_feature_names))
                })
                
                for model_name in results:
                    if model_name in self.feature_importance:
                        model_importance = self.feature_importance[model_name]
                        for i, feature in enumerate(all_feature_names):
                            if feature in model_importance['feature'].values:
                                idx = model_importance[model_importance['feature'] == feature].index[0]
                                ensemble_importance.loc[i, 'importance'] += model_importance.loc[idx, 'importance']
                
                # Normalize importance
                ensemble_importance['importance'] = ensemble_importance['importance'] / len(results)
                ensemble_importance = ensemble_importance.sort_values('importance', ascending=False)
                
                results['ensemble'] = {
                    'predictions': ensemble_pred,
                    'probabilities': ensemble_proba,
                    'performance': ensemble_performance,
                    'feature_importance': ensemble_importance,
                    'feature_names': all_feature_names
                }
                
                self.performance['ensemble'] = ensemble_performance
                self.feature_importance['ensemble'] = ensemble_importance
                
                logger.info("Ensemble model completed")
            except Exception as e:
                logger.error(f"Error running Ensemble model: {str(e)}")
            
            self.fitted = True
            return results
            
        except Exception as e:
            logger.error(f"Error running supervised models: {str(e)}")
            raise
    
    def _clean_data(self, X):
        """
        Clean data by handling infinity, NaN, and extreme values
        
        Args:
            X (DataFrame): Input data
            
        Returns:
            DataFrame: Cleaned data
        """
        try:
            # Replace infinity with NaN
            X = X.replace([np.inf, -np.inf], np.nan)
            
            # Replace extremely large values with a more reasonable maximum
            for col in X.columns:
                if X[col].dtype in ['float64', 'int64']:
                    # Calculate 99th percentile as a reasonable maximum
                    percentile_99 = np.nanpercentile(X[col], 99)
                    if not np.isnan(percentile_99):
                        # Cap values at 10 times the 99th percentile
                        max_val = percentile_99 * 10
                        X[col] = np.where(X[col] > max_val, max_val, X[col])
                        
                        # Similarly, handle extremely negative values
                        percentile_1 = np.nanpercentile(X[col], 1)
                        if not np.isnan(percentile_1):
                            min_val = percentile_1 * 10
                            X[col] = np.where(X[col] < min_val, min_val, X[col])
            
            return X
            
        except Exception as e:
            logger.error(f"Error cleaning data: {str(e)}")
            return X
    
    def _calculate_performance_metrics(self, y_true, y_pred, y_proba):
        """
        Calculate performance metrics for a model
        
        Args:
            y_true (array): True labels
            y_pred (array): Predicted labels
            y_proba (array): Predicted probabilities
            
        Returns:
            dict: Performance metrics
        """
        try:
            # Basic metrics
            accuracy = accuracy_score(y_true, y_pred)
            precision = precision_score(y_true, y_pred, zero_division=0)
            recall = recall_score(y_true, y_pred, zero_division=0)
            f1 = f1_score(y_true, y_pred, zero_division=0)
            
            # ROC AUC
            roc_auc = roc_auc_score(y_true, y_proba)
            
            # Average precision
            avg_precision = average_precision_score(y_true, y_proba)
            
            # Confusion matrix
            cm = confusion_matrix(y_true, y_pred)
            
            # Classification report
            class_report = classification_report(y_true, y_pred, output_dict=True)
            
            # Precision-Recall curve
            precision_curve, recall_curve, _ = precision_recall_curve(y_true, y_proba)
            
            # ROC curve
            fpr, tpr, _ = roc_curve(y_true, y_proba)
            
            return {
                'accuracy': accuracy,
                'precision': precision,
                'recall': recall,
                'f1': f1,
                'roc_auc': roc_auc,
                'avg_precision': avg_precision,
                'confusion_matrix': cm,
                'classification_report': class_report,
                'precision_curve': precision_curve,
                'recall_curve': recall_curve,
                'fpr': fpr,
                'tpr': tpr
            }
            
        except Exception as e:
            logger.error(f"Error calculating performance metrics: {str(e)}")
            raise
    
    def predict(self, df, model_name):
        """
        Make predictions using a fitted model
        
        Args:
            df (DataFrame): Input data
            model_name (str): Name of the model to use
            
        Returns:
            dict: Predictions and probabilities
        """
        if not self.fitted:
            raise ValueError("Models not fitted. Call run_models first.")
        
        if model_name not in self.models and model_name != 'ensemble':
            raise ValueError(f"Model {model_name} not found.")
        
        try:
            # Get feature names for this model
            if model_name == 'ensemble':
                # For ensemble, use all feature names
                feature_names = []
                for name in self.feature_names:
                    feature_names.extend(self.feature_names[name])
                feature_names = list(set(feature_names))  # Remove duplicates
            else:
                feature_names = self.feature_names[model_name]
            
            # Prepare data
            X = df[feature_names].copy()
            
            # Clean the data
            X = self._clean_data(X)
            
            # Handle categorical features
            for col in X.columns:
                if col in self.label_encoders:
                    le = self.label_encoders[col]
                    # Handle unseen categories
                    X[col] = X[col].astype(str).map(
                        lambda x: le.transform([x])[0] if x in le.classes_ else -1
                    )
            
            # Scale numeric features if scaler exists
            if 'numeric' in self.scalers:
                numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()
                if numeric_features:
                    X[numeric_features] = self.scalers['numeric'].transform(X[numeric_features])
            
            # Make predictions
            if model_name == 'ensemble':
                # Get predictions from all models
                all_predictions = []
                all_probabilities = []
                
                for name in self.models:
                    model = self.models[name]
                    pred = model.predict(X)
                    proba = model.predict_proba(X)[:, 1]
                    all_predictions.append(pred)
                    all_probabilities.append(proba)
                
                # Majority voting for predictions
                predictions = np.array(all_predictions).mean(axis=0) > 0.5
                
                # Average probabilities
                probabilities = np.array(all_probabilities).mean(axis=0)
                
            else:
                # Get model
                model = self.models[model_name]
                
                # Make predictions
                predictions = model.predict(X)
                probabilities = model.predict_proba(X)[:, 1]
            
            return {
                'predictions': predictions,
                'probabilities': probabilities
            }
            
        except Exception as e:
            logger.error(f"Error making predictions with {model_name}: {str(e)}")
            raise
    
    def get_feature_importance(self, model_name):
        """
        Get feature importance for a model
        
        Args:
            model_name (str): Name of the model
            
        Returns:
            DataFrame: Feature importance
        """
        if not self.fitted:
            raise ValueError("Models not fitted. Call run_models first.")
        
        if model_name not in self.feature_importance:
            raise ValueError(f"Feature importance for {model_name} not found.")
        
        return self.feature_importance[model_name]
    
    def get_performance(self, model_name):
        """
        Get performance metrics for a model
        
        Args:
            model_name (str): Name of the model
            
        Returns:
            dict: Performance metrics
        """
        if not self.fitted:
            raise ValueError("Models not fitted. Call run_models first.")
        
        if model_name not in self.performance:
            raise ValueError(f"Performance for {model_name} not found.")
        
        return self.performance[model_name]
    
    def get_shap_values(self, model_name):
        """
        Get SHAP values for a model
        
        Args:
            model_name (str): Name of the model
            
        Returns:
            array: SHAP values
        """
        if not self.fitted:
            raise ValueError("Models not fitted. Call run_models first.")
        
        if model_name not in self.shap_values:
            raise ValueError(f"SHAP values for {model_name} not found.")
        
        return self.shap_values[model_name]
    
    def plot_feature_importance(self, model_name, top_n=20):
        """
        Plot feature importance for a model
        
        Args:
            model_name (str): Name of the model
            top_n (int): Number of top features to show
        """
        try:
            # Get feature importance
            importance_df = self.get_feature_importance(model_name)
            
            # Get top features
            top_features = importance_df.head(top_n)
            
            # Create plot
            plt.figure(figsize=(10, 8))
            sns.barplot(x='importance', y='feature', data=top_features)
            plt.title(f'Top {top_n} Feature Importance - {model_name}')
            plt.tight_layout()
            
            return plt.gcf()
            
        except Exception as e:
            logger.error(f"Error plotting feature importance for {model_name}: {str(e)}")
            raise
    
    def plot_confusion_matrix(self, model_name):
        """
        Plot confusion matrix for a model
        
        Args:
            model_name (str): Name of the model
        """
        try:
            # Get performance metrics
            performance = self.get_performance(model_name)
            cm = performance['confusion_matrix']
            
            # Create plot
            plt.figure(figsize=(8, 6))
            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                       xticklabels=['Not Fraud', 'Fraud'], 
                       yticklabels=['Not Fraud', 'Fraud'])
            plt.title(f'Confusion Matrix - {model_name}')
            plt.ylabel('Actual')
            plt.xlabel('Predicted')
            plt.tight_layout()
            
            return plt.gcf()
            
        except Exception as e:
            logger.error(f"Error plotting confusion matrix for {model_name}: {str(e)}")
            raise
    
    def plot_roc_curve(self, model_name):
        """
        Plot ROC curve for a model
        
        Args:
            model_name (str): Name of the model
        """
        try:
            # Get performance metrics
            performance = self.get_performance(model_name)
            fpr = performance['fpr']
            tpr = performance['tpr']
            roc_auc = performance['roc_auc']
            
            # Create plot
            plt.figure(figsize=(8, 6))
            plt.plot(fpr, tpr, label=f'{model_name} (AUC = {roc_auc:.3f})')
            plt.plot([0, 1], [0, 1], 'k--')
            plt.xlim([0.0, 1.0])
            plt.ylim([0.0, 1.05])
            plt.xlabel('False Positive Rate')
            plt.ylabel('True Positive Rate')
            plt.title(f'ROC Curve - {model_name}')
            plt.legend(loc="lower right")
            plt.tight_layout()
            
            return plt.gcf()
            
        except Exception as e:
            logger.error(f"Error plotting ROC curve for {model_name}: {str(e)}")
            raise
    
    def plot_precision_recall_curve(self, model_name):
        """
        Plot precision-recall curve for a model
        
        Args:
            model_name (str): Name of the model
        """
        try:
            # Get performance metrics
            performance = self.get_performance(model_name)
            precision_curve = performance['precision_curve']
            recall_curve = performance['recall_curve']
            avg_precision = performance['avg_precision']
            
            # Create plot
            plt.figure(figsize=(8, 6))
            plt.plot(recall_curve, precision_curve, label=f'{model_name} (AP = {avg_precision:.3f})')
            plt.xlim([0.0, 1.0])
            plt.ylim([0.0, 1.05])
            plt.xlabel('Recall')
            plt.ylabel('Precision')
            plt.title(f'Precision-Recall Curve - {model_name}')
            plt.legend(loc="lower left")
            plt.tight_layout()
            
            return plt.gcf()
            
        except Exception as e:
            logger.error(f"Error plotting precision-recall curve for {model_name}: {str(e)}")
            raise

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\models\.ipynb_checkpoints\unsupervised-checkpoint.py ===
"""
Unsupervised Models Module
Implements unsupervised learning models for fraud detection
"""
import pandas as pd
import numpy as np
from sklearn.ensemble import IsolationForest
from sklearn.neighbors import LocalOutlierFactor
from sklearn.svm import OneClassSVM
from sklearn.cluster import DBSCAN, KMeans
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.impute import SimpleImputer
from sklearn.metrics import silhouette_score
import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.callbacks import EarlyStopping
import warnings
import logging
from typing import Dict, List, Tuple, Union
warnings.filterwarnings('ignore')
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class UnsupervisedModels:
    """
    Class for unsupervised fraud detection models
    Implements Isolation Forest, Local Outlier Factor, Autoencoders, etc.
    """
    
    def __init__(self, contamination=0.01, random_state=42):
        """
        Initialize UnsupervisedModels
        
        Args:
            contamination (float): Expected proportion of outliers
            random_state (int): Random seed
        """
        self.contamination = contamination
        self.random_state = random_state
        self.models = {}
        self.feature_names = {}
        self.scalers = {}
        self.imputers = {}
        self.fitted = False
        
    def run_models(self, df):
        """
        Run all unsupervised models
        
        Args:
            df (DataFrame): Input data
            
        Returns:
            dict: Model results
        """
        try:
            # Get numeric columns
            numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
            
            if len(numeric_cols) < 2:
                logger.warning("Not enough numeric columns for unsupervised models")
                return {}
            
            # Prepare data
            X = df[numeric_cols].copy()
            
            # Clean the data - handle infinity and very large values
            X = self._clean_data(X)
            
            # Scale data
            X_scaled, scaler, imputer = self._scale_data(X)
            
            # Store scaler and imputer
            self.scalers['global'] = scaler
            self.imputers['global'] = imputer
            
            # Run different models
            results = {}
            
            # Isolation Forest
            try:
                if_model = IsolationForest(
                    contamination=self.contamination,
                    random_state=self.random_state,
                    n_jobs=-1,
                    max_samples='auto'
                )
                if_predictions = if_model.fit_predict(X_scaled)
                if_scores = if_model.decision_function(X_scaled)
                
                # Normalize scores to 0-1 range
                if_scores_normalized = self._normalize_scores(if_scores)
                
                results['isolation_forest'] = {
                    'predictions': if_predictions,
                    'scores': if_scores_normalized,
                    'model': if_model,
                    'feature_names': numeric_cols
                }
                
                self.models['isolation_forest'] = if_model
                self.feature_names['isolation_forest'] = numeric_cols
                
                logger.info("Isolation Forest model completed")
            except Exception as e:
                logger.error(f"Error running Isolation Forest: {str(e)}")
            
            # Local Outlier Factor
            try:
                lof_model = LocalOutlierFactor(
                    contamination=self.contamination,
                    n_neighbors=min(20, len(X_scaled) - 1),  # Ensure n_neighbors < n_samples
                    novelty=True,
                    n_jobs=-1
                )
                lof_predictions = lof_model.fit_predict(X_scaled)
                lof_scores = lof_model.decision_function(X_scaled)
                
                # Normalize scores to 0-1 range
                lof_scores_normalized = self._normalize_scores(lof_scores)
                
                results['local_outlier_factor'] = {
                    'predictions': lof_predictions,
                    'scores': lof_scores_normalized,
                    'model': lof_model,
                    'feature_names': numeric_cols
                }
                
                self.models['local_outlier_factor'] = lof_model
                self.feature_names['local_outlier_factor'] = numeric_cols
                
                logger.info("Local Outlier Factor model completed")
            except Exception as e:
                logger.error(f"Error running Local Outlier Factor: {str(e)}")
            
            # One-Class SVM
            try:
                ocsvm_model = OneClassSVM(
                    nu=self.contamination,
                    kernel='rbf',
                    gamma='scale'
                )
                ocsvm_predictions = ocsvm_model.fit_predict(X_scaled)
                ocsvm_scores = ocsvm_model.decision_function(X_scaled)
                
                # Normalize scores to 0-1 range
                ocsvm_scores_normalized = self._normalize_scores(ocsvm_scores)
                
                results['one_class_svm'] = {
                    'predictions': ocsvm_predictions,
                    'scores': ocsvm_scores_normalized,
                    'model': ocsvm_model,
                    'feature_names': numeric_cols
                }
                
                self.models['one_class_svm'] = ocsvm_model
                self.feature_names['one_class_svm'] = numeric_cols
                
                logger.info("One-Class SVM model completed")
            except Exception as e:
                logger.error(f"Error running One-Class SVM: {str(e)}")
            
            # DBSCAN Clustering
            try:
                # Use a subset of data for DBSCAN if it's too large
                if len(X_scaled) > 10000:
                    X_subset = X_scaled[np.random.choice(len(X_scaled), 10000, replace=False)]
                else:
                    X_subset = X_scaled
                
                dbscan_model = DBSCAN(eps=0.5, min_samples=5)
                dbscan_labels = dbscan_model.fit_predict(X_subset)
                
                # Convert to outlier predictions (-1 is outlier in DBSCAN)
                dbscan_predictions = np.where(dbscan_labels == -1, -1, 1)
                
                # Calculate distance to nearest core point as anomaly score
                dbscan_scores = np.zeros(len(X_subset))
                for i in range(len(X_subset)):
                    if dbscan_labels[i] == -1:  # Outlier
                        # Find distance to nearest core point
                        core_points = np.where(dbscan_labels != -1)[0]
                        if len(core_points) > 0:
                            distances = np.linalg.norm(X_subset[i] - X_subset[core_points], axis=1)
                            dbscan_scores[i] = distances.min()
                        else:
                            dbscan_scores[i] = np.max(np.linalg.norm(X_subset - X_subset.mean(axis=0), axis=1))
                
                # Normalize scores to 0-1 range
                dbscan_scores_normalized = self._normalize_scores(dbscan_scores)
                
                # For the full dataset, use the model to predict
                if len(X_scaled) > 10000:
                    full_predictions = dbscan_model.fit_predict(X_scaled)
                    full_predictions = np.where(full_predictions == -1, -1, 1)
                    
                    # Calculate scores for full dataset
                    full_scores = np.zeros(len(X_scaled))
                    for i in range(len(X_scaled)):
                        if full_predictions[i] == -1:  # Outlier
                            # Find distance to nearest core point
                            core_points = np.where(full_predictions != -1)[0]
                            if len(core_points) > 0:
                                distances = np.linalg.norm(X_scaled[i] - X_scaled[core_points], axis=1)
                                full_scores[i] = distances.min()
                            else:
                                full_scores[i] = np.max(np.linalg.norm(X_scaled - X_scaled.mean(axis=0), axis=1))
                    
                    # Normalize scores
                    full_scores_normalized = self._normalize_scores(full_scores)
                else:
                    full_predictions = dbscan_predictions
                    full_scores_normalized = dbscan_scores_normalized
                
                results['dbscan'] = {
                    'predictions': full_predictions,
                    'scores': full_scores_normalized,
                    'model': dbscan_model,
                    'feature_names': numeric_cols
                }
                
                self.models['dbscan'] = dbscan_model
                self.feature_names['dbscan'] = numeric_cols
                
                logger.info("DBSCAN model completed")
            except Exception as e:
                logger.error(f"Error running DBSCAN: {str(e)}")
            
            # K-Means Clustering
            try:
                # Determine optimal number of clusters
                silhouette_scores = []
                k_range = range(2, min(11, len(X_scaled) // 2))
                
                for k in k_range:
                    kmeans = KMeans(n_clusters=k, random_state=self.random_state, n_init=10)
                    labels = kmeans.fit_predict(X_scaled)
                    silhouette_scores.append(silhouette_score(X_scaled, labels))
                
                # Find optimal k
                optimal_k = k_range[np.argmax(silhouette_scores)]
                
                # Fit K-Means with optimal k
                kmeans_model = KMeans(n_clusters=optimal_k, random_state=self.random_state, n_init=10)
                kmeans_labels = kmeans_model.fit_predict(X_scaled)
                
                # Calculate distance to nearest cluster center as anomaly score
                distances = kmeans_model.transform(X_scaled)
                min_distances = distances.min(axis=1)
                
                # Normalize scores to 0-1 range
                kmeans_scores_normalized = self._normalize_scores(min_distances)
                
                # Convert to outlier predictions (top contamination% are outliers)
                threshold = np.percentile(kmeans_scores_normalized, (1 - self.contamination) * 100)
                kmeans_predictions = np.where(kmeans_scores_normalized > threshold, -1, 1)
                
                results['kmeans'] = {
                    'predictions': kmeans_predictions,
                    'scores': kmeans_scores_normalized,
                    'model': kmeans_model,
                    'feature_names': numeric_cols,
                    'optimal_k': optimal_k,
                    'silhouette_scores': silhouette_scores
                }
                
                self.models['kmeans'] = kmeans_model
                self.feature_names['kmeans'] = numeric_cols
                
                logger.info(f"K-Means model completed with {optimal_k} clusters")
            except Exception as e:
                logger.error(f"Error running K-Means: {str(e)}")
            
            # Autoencoder
            try:
                # Build autoencoder model
                input_dim = X_scaled.shape[1]
                encoding_dim = max(2, input_dim // 2)  # At least 2 dimensions
                
                autoencoder = self._build_autoencoder(input_dim, encoding_dim)
                
                # Train autoencoder
                early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
                
                history = autoencoder.fit(
                    X_scaled, X_scaled,
                    epochs=50,
                    batch_size=32,
                    validation_split=0.2,
                    callbacks=[early_stopping],
                    verbose=0
                )
                
                # Get reconstruction errors
                reconstructions = autoencoder.predict(X_scaled)
                mse = np.mean(np.power(X_scaled - reconstructions, 2), axis=1)
                
                # Handle any infinity or NaN values in MSE
                mse = np.nan_to_num(mse)
                mse = np.clip(mse, 0, np.finfo(np.float64).max)  # Clip to valid range
                
                # Normalize scores to 0-1 range
                mse_normalized = self._normalize_scores(mse)
                
                # Convert to outlier predictions (top contamination% are outliers)
                threshold = np.percentile(mse_normalized, (1 - self.contamination) * 100)
                autoencoder_predictions = np.where(mse_normalized > threshold, -1, 1)
                
                results['autoencoder'] = {
                    'predictions': autoencoder_predictions,
                    'scores': mse_normalized,
                    'model': autoencoder,
                    'feature_names': numeric_cols,
                    'history': history.history
                }
                
                self.models['autoencoder'] = autoencoder
                self.feature_names['autoencoder'] = numeric_cols
                
                logger.info("Autoencoder model completed")
            except Exception as e:
                logger.error(f"Error running Autoencoder: {str(e)}")
            
            # PCA-based anomaly detection
            try:
                # Fit PCA
                pca = PCA(n_components=min(10, X_scaled.shape[1] - 1), random_state=self.random_state)
                pca_transformed = pca.fit_transform(X_scaled)
                
                # Calculate reconstruction error
                X_reconstructed = pca.inverse_transform(pca_transformed)
                pca_errors = np.mean(np.power(X_scaled - X_reconstructed, 2), axis=1)
                
                # Handle any infinity or NaN values in errors
                pca_errors = np.nan_to_num(pca_errors)
                pca_errors = np.clip(pca_errors, 0, np.finfo(np.float64).max)  # Clip to valid range
                
                # Normalize scores to 0-1 range
                pca_errors_normalized = self._normalize_scores(pca_errors)
                
                # Convert to outlier predictions (top contamination% are outliers)
                threshold = np.percentile(pca_errors_normalized, (1 - self.contamination) * 100)
                pca_predictions = np.where(pca_errors_normalized > threshold, -1, 1)
                
                results['pca'] = {
                    'predictions': pca_predictions,
                    'scores': pca_errors_normalized,
                    'model': pca,
                    'feature_names': numeric_cols,
                    'explained_variance': pca.explained_variance_ratio_
                }
                
                self.models['pca'] = pca
                self.feature_names['pca'] = numeric_cols
                
                logger.info("PCA-based anomaly detection completed")
            except Exception as e:
                logger.error(f"Error running PCA-based anomaly detection: {str(e)}")
            
            self.fitted = True
            return results
            
        except Exception as e:
            logger.error(f"Error running unsupervised models: {str(e)}")
            raise
    
    def _clean_data(self, X):
        """
        Clean data by handling infinity, NaN, and extreme values
        
        Args:
            X (DataFrame): Input data
            
        Returns:
            DataFrame: Cleaned data
        """
        try:
            # Replace infinity with NaN
            X = X.replace([np.inf, -np.inf], np.nan)
            
            # Replace extremely large values with a more reasonable maximum
            for col in X.columns:
                if X[col].dtype in ['float64', 'int64']:
                    # Calculate 99th percentile as a reasonable maximum
                    percentile_99 = np.nanpercentile(X[col], 99)
                    if not np.isnan(percentile_99):
                        # Cap values at 10 times the 99th percentile
                        max_val = percentile_99 * 10
                        X[col] = np.where(X[col] > max_val, max_val, X[col])
                        
                        # Similarly, handle extremely negative values
                        percentile_1 = np.nanpercentile(X[col], 1)
                        if not np.isnan(percentile_1):
                            min_val = percentile_1 * 10
                            X[col] = np.where(X[col] < min_val, min_val, X[col])
            
            return X
            
        except Exception as e:
            logger.error(f"Error cleaning data: {str(e)}")
            return X
    
    def _scale_data(self, X):
        """
        Scale data using RobustScaler (more resistant to outliers)
        
        Args:
            X (DataFrame): Input data
            
        Returns:
            tuple: (scaled data, scaler, imputer)
        """
        try:
            # First impute missing values
            imputer = SimpleImputer(strategy='median')
            X_imputed = imputer.fit_transform(X)
            
            # Use RobustScaler instead of StandardScaler to handle outliers better
            scaler = RobustScaler()
            X_scaled = scaler.fit_transform(X_imputed)
            
            # Check for any remaining infinity or NaN values
            X_scaled = np.nan_to_num(X_scaled)
            X_scaled = np.clip(X_scaled, -1e10, 1e10)  # Clip to reasonable range
            
            return X_scaled, scaler, imputer
            
        except Exception as e:
            logger.error(f"Error scaling data: {str(e)}")
            # Fallback to simple normalization
            X_normalized = (X - X.min()) / (X.max() - X.min())
            return X_normalized.values, None, None
    
    def _normalize_scores(self, scores):
        """
        Normalize scores to 0-1 range
        
        Args:
            scores (array): Input scores
            
        Returns:
            array: Normalized scores
        """
        try:
            # Handle NaN and infinity
            scores = np.nan_to_num(scores)
            scores = np.clip(scores, -1e10, 1e10)  # Clip to reasonable range
            
            # Min-max normalization
            min_score = scores.min()
            max_score = scores.max()
            
            if max_score > min_score:
                normalized_scores = (scores - min_score) / (max_score - min_score)
            else:
                normalized_scores = np.zeros_like(scores)
            
            return normalized_scores
            
        except Exception as e:
            logger.error(f"Error normalizing scores: {str(e)}")
            return np.zeros_like(scores)
    
    def _build_autoencoder(self, input_dim, encoding_dim):
        """
        Build autoencoder model
        
        Args:
            input_dim (int): Input dimension
            encoding_dim (int): Encoding dimension
            
        Returns:
            Model: Autoencoder model
        """
        try:
            # Encoder
            input_layer = layers.Input(shape=(input_dim,))
            encoded = layers.Dense(encoding_dim * 2, activation='relu')(input_layer)
            encoded = layers.Dense(encoding_dim, activation='relu')(encoded)
            
            # Decoder
            decoded = layers.Dense(encoding_dim * 2, activation='relu')(encoded)
            decoded = layers.Dense(input_dim, activation='linear')(decoded)
            
            # Autoencoder model
            autoencoder = models.Model(input_layer, decoded)
            
            # Compile model
            autoencoder.compile(optimizer='adam', loss='mean_squared_error')
            
            return autoencoder
            
        except Exception as e:
            logger.error(f"Error building autoencoder: {str(e)}")
            raise
    
    def predict(self, df, model_name):
        """
        Make predictions using a fitted model
        
        Args:
            df (DataFrame): Input data
            model_name (str): Name of the model to use
            
        Returns:
            dict: Predictions and scores
        """
        if not self.fitted:
            raise ValueError("Models not fitted. Call run_models first.")
        
        if model_name not in self.models:
            raise ValueError(f"Model {model_name} not found.")
        
        try:
            # Get feature names for this model
            feature_names = self.feature_names[model_name]
            
            # Prepare data
            X = df[feature_names].copy()
            
            # Clean the data
            X = self._clean_data(X)
            
            # Scale data using fitted scaler and imputer
            if 'global' in self.scalers and 'global' in self.imputers:
                X_imputed = self.imputers['global'].transform(X)
                X_scaled = self.scalers['global'].transform(X_imputed)
            else:
                # Fallback to simple normalization
                X_scaled = (X - X.min()) / (X.max() - X.min())
                X_scaled = np.nan_to_num(X_scaled)
                X_scaled = np.clip(X_scaled, -1e10, 1e10)
            
            # Get model
            model = self.models[model_name]
            
            # Make predictions
            if model_name == 'isolation_forest':
                predictions = model.predict(X_scaled)
                scores = model.decision_function(X_scaled)
                # Normalize scores to 0-1 range
                scores_normalized = self._normalize_scores(scores)
                
            elif model_name == 'local_outlier_factor':
                predictions = model.predict(X_scaled)
                scores = model.decision_function(X_scaled)
                # Normalize scores to 0-1 range
                scores_normalized = self._normalize_scores(scores)
                
            elif model_name == 'one_class_svm':
                predictions = model.predict(X_scaled)
                scores = model.decision_function(X_scaled)
                # Normalize scores to 0-1 range
                scores_normalized = self._normalize_scores(scores)
                
            elif model_name == 'dbscan':
                predictions = model.fit_predict(X_scaled)
                # Convert to outlier predictions (-1 is outlier in DBSCAN)
                predictions = np.where(predictions == -1, -1, 1)
                
                # Calculate distance to nearest core point as anomaly score
                scores = np.zeros(len(X_scaled))
                for i in range(len(X_scaled)):
                    if predictions[i] == -1:  # Outlier
                        # Find distance to nearest core point
                        core_points = np.where(predictions != -1)[0]
                        if len(core_points) > 0:
                            distances = np.linalg.norm(X_scaled[i] - X_scaled[core_points], axis=1)
                            scores[i] = distances.min()
                        else:
                            scores[i] = np.max(np.linalg.norm(X_scaled - X_scaled.mean(axis=0), axis=1))
                
                # Normalize scores to 0-1 range
                scores_normalized = self._normalize_scores(scores)
                
            elif model_name == 'kmeans':
                # Transform data
                distances = model.transform(X_scaled)
                min_distances = distances.min(axis=1)
                
                # Normalize scores to 0-1 range
                scores_normalized = self._normalize_scores(min_distances)
                
                # Convert to outlier predictions (top contamination% are outliers)
                threshold = np.percentile(scores_normalized, (1 - self.contamination) * 100)
                predictions = np.where(scores_normalized > threshold, -1, 1)
                
            elif model_name == 'autoencoder':
                # Get reconstructions
                reconstructions = model.predict(X_scaled)
                mse = np.mean(np.power(X_scaled - reconstructions, 2), axis=1)
                
                # Handle any infinity or NaN values in MSE
                mse = np.nan_to_num(mse)
                mse = np.clip(mse, 0, np.finfo(np.float64).max)
                
                # Normalize scores to 0-1 range
                scores_normalized = self._normalize_scores(mse)
                
                # Convert to outlier predictions (top contamination% are outliers)
                threshold = np.percentile(scores_normalized, (1 - self.contamination) * 100)
                predictions = np.where(scores_normalized > threshold, -1, 1)
                
            elif model_name == 'pca':
                # Transform data
                pca_transformed = model.transform(X_scaled)
                
                # Calculate reconstruction error
                X_reconstructed = model.inverse_transform(pca_transformed)
                mse = np.mean(np.power(X_scaled - X_reconstructed, 2), axis=1)
                
                # Handle any infinity or NaN values in errors
                mse = np.nan_to_num(mse)
                mse = np.clip(mse, 0, np.finfo(np.float64).max)
                
                # Normalize scores to 0-1 range
                scores_normalized = self._normalize_scores(mse)
                
                # Convert to outlier predictions (top contamination% are outliers)
                threshold = np.percentile(scores_normalized, (1 - self.contamination) * 100)
                predictions = np.where(scores_normalized > threshold, -1, 1)
                
            else:
                raise ValueError(f"Unknown model: {model_name}")
            
            return {
                'predictions': predictions,
                'scores': scores_normalized
            }
            
        except Exception as e:
            logger.error(f"Error making predictions with {model_name}: {str(e)}")
            raise
    
    def get_feature_importance(self, model_name):
        """
        Get feature importance for a model
        
        Args:
            model_name (str): Name of the model
            
        Returns:
            DataFrame: Feature importance
        """
        if not self.fitted:
            raise ValueError("Models not fitted. Call run_models first.")
        
        if model_name not in self.models:
            raise ValueError(f"Model {model_name} not found.")
        
        try:
            # Get feature names
            feature_names = self.feature_names[model_name]
            
            if model_name == 'isolation_forest':
                # Get feature importance from the model
                importance = self.models[model_name].feature_importances_
                
                # Create DataFrame
                importance_df = pd.DataFrame({
                    'feature': feature_names,
                    'importance': importance
                }).sort_values('importance', ascending=False)
                
                return importance_df
                
            elif model_name == 'pca':
                # Get explained variance ratio
                explained_variance = self.models[model_name].explained_variance_ratio_
                
                # Create DataFrame
                importance_df = pd.DataFrame({
                    'feature': [f'PC{i+1}' for i in range(len(explained_variance))],
                    'importance': explained_variance
                }).sort_values('importance', ascending=False)
                
                return importance_df
                
            else:
                # For other models, return equal importance
                importance = np.ones(len(feature_names)) / len(feature_names)
                
                # Create DataFrame
                importance_df = pd.DataFrame({
                    'feature': feature_names,
                    'importance': importance
                }).sort_values('importance', ascending=False)
                
                return importance_df
                
        except Exception as e:
            logger.error(f"Error getting feature importance for {model_name}: {str(e)}")
            raise

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\models\.ipynb_checkpoints\__init__-checkpoint.py ===

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\models\__pycache__\rule_based.cpython-39.pyc ===
a
    èöhám  „                   @   s∏   d Z ddlZddlZddlZddlmZmZ ddlZddl	Z	ddl
Z
ddlZddlmZmZmZmZmZ ddlmZmZmZmZmZ e
†d° ejejdç e†e°ZG dd	Ñ d	ÉZdS )
z@
Rule-based Models Module
Implements rule-based fraud detection
È    N)⁄datetime⁄	timedelta)⁄Dict⁄List⁄Tuple⁄Union⁄Callable)⁄is_api_available⁄get_demo_sanctions_data⁄get_demo_tax_compliance_data⁄get_demo_bank_verification_data⁄#get_demo_identity_verification_data⁄ignore)⁄levelc                   @   s  e Zd ZdZdEddÑZddÑ Zdd	Ñ Zd
dÑ ZddÑ ZddÑ Z	dFddÑZ
dGddÑZddÑ ZdHddÑZdIddÑZdJdd ÑZd!d"Ñ ZdKd#d$ÑZd%d&Ñ ZdLd(d)ÑZd*d+Ñ ZdMd-d.ÑZdNd/d0ÑZd1d2Ñ Zd3d4Ñ Zd5d6Ñ Zd7d8Ñ ZdOd;d<ÑZd=d>Ñ Zd?d@Ñ ZdAdBÑ ZdCdDÑ ZdS )P⁄
RuleEnginez]
    Class for rule-based fraud detection
    Implements configurable rules with weights
    NÁffffffÊ?c                 C   sL   || _ i | _i | _i | _d| _i | _|r@tj†|°r@| †	|° n| †
°  dS )zæ
        Initialize RuleEngine
        
        Args:
            config_path (str, optional): Path to configuration file
            threshold (float): Threshold for rule violation
        FN)⁄	threshold⁄rules⁄rule_weights⁄rule_descriptions⁄fitted⁄api_available⁄os⁄path⁄exists⁄_load_config⁄_load_default_rules)⁄self⁄config_pathr   © r   ˙gC:\Users\Tranquil Abyss\fraud-detection-platform\app\..\src\fraud_detection_engine\models\rule_based.py⁄__init__   s    zRuleEngine.__init__c              
   C   s  zŒt |dÉè}t†|°}W d  É n1 s,0    Y  d|v rî|d †° D ]H\}}|†dd°rJ| †|°| j|< |†dd°| j|< |†dd	°| j|< qJt	d
Ét	dÉt	dÉt	dÉt	dÉdú| _
t†d| j
õ ù° W nB têy } z(t†dt|Éõ ù° | †°  W Y d}~n
d}~0 0 dS )zÑ
        Load configuration from YAML file
        
        Args:
            config_path (str): Path to configuration file
        ⁄rNr   ⁄enabledT⁄weightÁ      ?⁄description⁄ ⁄	sanctions⁄tax_compliance⁄bank_verification⁄identity_verification⁄geolocation©r(   r)   r*   r+   r,   ˙API availability: zError loading configuration: )⁄open⁄yaml⁄	safe_load⁄items⁄get⁄_create_rule_functionr   r   r   r	   r   ⁄logger⁄info⁄	Exception⁄error⁄strr   )r   r   ⁄f⁄config⁄	rule_name⁄rule_config⁄er   r   r    r   1   s&    (˚zRuleEngine._load_configc              
   C   s¿  êz|| j | jd< d| jd< d| jd< | j| jd< d| jd< d| jd< | j| jd< d| jd< d| jd< | j| jd	< d
| jd	< d| jd	< | j| jd< d| jd< d| jd< | j| jd< d| jd< d| jd< | j	| jd< d| jd< d| jd< | j
| jd< d| jd< d| jd< | j| jd< d| jd< d| jd< | j| jd< d| jd< d| jd< | j| jd< d| jd< d| jd< | j| jd< d
| jd< d| jd< | j| jd< d| jd< d| jd< | j| jd < d| jd < d!| jd < | j| jd"< d| jd"< d#| jd"< | j| jd$< d| jd$< d%| jd$< | j| jd&< d| jd&< d'| jd&< | j| jd(< d| jd(< d)| jd(< td*Étd$Étd&Étd(Étd+Éd,ú| _t†d-| jõ ù° W n< têy∫ } z"t†d.t|Éõ ù° Ç W Y d/}~n
d/}~0 0 d/S )0z$
        Load default rules
        Zhigh_amountg333333”?z$Transaction amount exceeds thresholdZunusual_amount_for_sendergöôôôôô…?z Amount is unusual for the senderZunusual_amount_for_receiverz"Amount is unusual for the receiver⁄round_amountgöôôôôôπ?z(Transaction amount is suspiciously round⁄high_frequency_senderz&High transaction frequency from sender⁄high_frequency_receiverz&High transaction frequency to receiver⁄rapid_successiongöôôôôôŸ?z)Multiple transactions in rapid succession⁄cross_borderz)Transaction crosses international borders⁄high_risk_countryg      ‡?z&Transaction involves high-risk country⁄unusual_location_for_senderz,Transaction from unusual location for sender⁄unusual_hourz Transaction during unusual hours⁄weekendzTransaction on weekend⁄
new_senderz!First transaction from new sender⁄new_receiverz!First transaction to new receiverZsanctions_checkz(Transaction involves sanctioned entitiesr)   z+Transaction involves non-compliant entitiesr*   z-Transaction involves unverified bank accountsr+   z*Transaction involves unverified identitiesr(   r,   r-   r.   zError loading default rules: N)⁄_high_amount_ruler   r   r   ⁄_unusual_amount_for_sender_rule⁄!_unusual_amount_for_receiver_rule⁄_round_amount_rule⁄_high_frequency_sender_rule⁄_high_frequency_receiver_rule⁄_rapid_succession_rule⁄_cross_border_rule⁄_high_risk_country_rule⁄!_unusual_location_for_sender_rule⁄_unusual_hour_rule⁄_weekend_rule⁄_new_sender_rule⁄_new_receiver_rule⁄_sanctions_check_rule⁄_tax_compliance_rule⁄_bank_verification_rule⁄_identity_verification_ruler	   r   r5   r6   r7   r8   r9   )r   r>   r   r   r    r   S   sÇ    



































˚zRuleEngine._load_default_rulesc                    s>  |† d°}|dkr*|† dd°âáfddÑS |dkrZ|† dd	°â|† d
d°âáááfddÑS |dkrä|† dd	°â|† d
d°âáááfddÑS |dkr™|† dd°âáfddÑS |dkr⁄|† dd°â|† dd°âáááfddÑS |dkêr|† dd°â|† dd°âáááfddÑS |dkêr>|† dd°â|† d
d	°âáááfddÑS |dkêrTáfddÑS |dkêr||† d g d!¢°â á áfd"dÑS |d#kêríáfd$dÑS |d%kêrƒ|† d&d'°â|† d(d°âáááfd)dÑS |d*kêr⁄áfd+dÑS |d,kêr˛|† dd-°âááfd.dÑS |d/kêr"|† dd-°âááfd0dÑS t†d1|õ ù° d2dÑ S d3S )4z«
        Create a rule function from configuration
        
        Args:
            rule_config (dict): Rule configuration
            
        Returns:
            function: Rule function
        ⁄typeZamount_thresholdr   È'  c                    s   | † dd°à kS )N⁄amountr   ©r3   ©⁄row©r   r   r    ⁄<lambda>√   Û    z2RuleEngine._create_rule_function.<locals>.<lambda>Zsender_amount_outlier⁄std_multiplierÈ   ⁄min_transactionsÈ   c                    s   à† | àà °S ©N)Z_is_sender_amount_outlierr`   ©rg   r   re   r   r    rc   »   rd   Zreceiver_amount_outlierc                    s   à† | àà °S ri   )Z_is_receiver_amount_outlierr`   rj   r   r    rc   Õ   rd   r?   ÈË  c                    s$   | † dd°à ko"| † dd°d dkS )Nr^   r   rk   r_   r`   rb   r   r    rc   —   rd   r@   ⁄time_window⁄1H⁄max_transactionsÈ
   c                    s   à† | àà °S ri   )Z_is_high_frequency_senderr`   ©rn   r   rl   r   r    rc   ÷   rd   rA   c                    s   à† | àà °S ri   )Z_is_high_frequency_receiverr`   rp   r   r    rc   €   rd   rB   ⁄5Mc                    s   à† | àà °S ri   )Z_is_rapid_successionr`   )rg   r   rl   r   r    rc   ‡   rd   rC   c                    s
   à † | °S ri   )Z_is_cross_borderr`   ©r   r   r    rc   „   rd   rD   ⁄	countries©zNorth Korea⁄Iran⁄Syria⁄Cubac                    s   à† | à °S ri   )Z_is_high_risk_countryr`   )rs   r   r   r    rc   Á   rd   rE   c                    s
   à † | °S ri   )Z_is_unusual_location_for_senderr`   rr   r   r    rc   Í   rd   rF   ⁄
start_hourÈ   ⁄end_hourc                    s   à† | àà °S ri   )Z_is_unusual_hourr`   )rz   r   rx   r   r    rc   Ô   rd   rG   c                    s
   à † | °S ri   )Z_is_weekendr`   rr   r   r    rc   Ú   rd   rH   ⁄7Dc                    s   à † | à°S ri   )Z_is_new_senderr`   ©r   rl   r   r    rc   ˆ   rd   rI   c                    s   à † | à°S ri   )Z_is_new_receiverr`   r|   r   r    rc   ˙   rd   zUnknown rule type: c                 S   s   dS )NFr   r`   r   r   r    rc   ˛   rd   N)r3   r5   ⁄warning)r   r=   ⁄	rule_typer   )	rs   rz   rn   rg   r   rx   re   r   rl   r    r4   µ   s`    










z RuleEngine._create_rule_functionc                    s∂  êzrd|j v r:tjj†|d °s:|†° }t†|d °|d< d|j v rN|†d°}i }i âi }àj†	° D ê]2\}}zºg }|†
° D ]h\}}z||É}	|†|	° W q| ty‚ }
 z0t†d|õ dt|
Éõ ù° |†d° W Y d}
~
q|d}
~
0 0 q||||< àj†|d°âáfddÑ|D Éà|< d	dÑ t|ÉD É||< W qd têyñ }
 zRt†d|õ d
t|
Éõ ù° dgt|É ||< dgt|É à|< g ||< W Y d}
~
qdd}
~
0 0 qdg }tt|ÉÉD ](â tá áfddÑàD ÉÉ}|†|° êq™tàj†° ÉâáfddÑ|D É}áfddÑ|D É}g }tt|ÉÉD ]8â g }|D ]}à || v êr"|†|° êq"|†|° êqdà_|à||||àjàjàjdú	W S  têy∞ }
 z"t†dt|
Éõ ù° Ç W Y d}
~
n
d}
~
0 0 dS )z≠
        Apply all rules to the dataframe
        
        Args:
            df (DataFrame): Input data
            
        Returns:
            dict: Rule results
        ⁄	timestampzError applying rule z	 to row: FNr%   c                    s   g | ]}|rà nd ëqS )r   r   )⁄.0⁄result)r$   r   r    ⁄
<listcomp>)  rd   z*RuleEngine.apply_rules.<locals>.<listcomp>c                 S   s   g | ]\}}|r|ëqS r   r   )rÄ   ⁄irÅ   r   r   r    rÇ   ,  rd   ˙: r   c                 3   s   | ]}à| à  V  qd S ri   r   )rÄ   r<   )rÉ   ⁄rule_scoresr   r    ⁄	<genexpr>7  rd   z)RuleEngine.apply_rules.<locals>.<genexpr>c                    s   g | ]}|à  ëqS r   r   ©rÄ   ⁄score)⁄max_possible_scorer   r    rÇ   <  rd   c                    s   g | ]}|à j këqS r   rb   rá   rr   r   r    rÇ   ?  rd   T)	⁄rule_resultsrÖ   ⁄total_scores⁄normalized_scores⁄rule_violations⁄violated_rule_namesr   r   r   zError applying rules: )⁄columns⁄pd⁄api⁄types⁄is_datetime64_any_dtype⁄copy⁄to_datetime⁄sort_valuesr   r2   ⁄iterrows⁄appendr7   r5   r}   r9   r   r3   ⁄	enumerater8   ⁄len⁄range⁄sum⁄valuesr   r   )r   ⁄dfrä   ⁄violated_rulesr<   ⁄	rule_func⁄results⁄_ra   rÅ   r>   rã   rà   rå   rç   ré   ⁄namesr   )rÉ   râ   rÖ   r   r$   r    ⁄apply_rules   sp    


" ˜zRuleEngine.apply_rulesc                 C   s   |† dd°dkS )z#Check if transaction amount is highr^   r   r]   r_   ©r   ra   r   r   r    rJ   ]  s    zRuleEngine._high_amount_rulerf   rh   c                 C   s   dS )z)Check if amount is unusual for the senderFr   ©r   ra   re   rg   r   r   r    rK   a  s    z*RuleEngine._unusual_amount_for_sender_rulec                 C   s   dS )z+Check if amount is unusual for the receiverFr   r¶   r   r   r    rL   g  s    z,RuleEngine._unusual_amount_for_receiver_rulec                 C   s    |† dd°}|dko|d dkS )z1Check if transaction amount is suspiciously roundr^   r   rk   r_   )r   ra   r^   r   r   r    rM   m  s    zRuleEngine._round_amount_rulerm   ro   c                 C   s   dS )z.Check if sender has high transaction frequencyFr   ©r   ra   rl   rn   r   r   r    rN   r  s    z&RuleEngine._high_frequency_sender_rulec                 C   s   dS )z0Check if receiver has high transaction frequencyFr   rß   r   r   r    rO   x  s    z(RuleEngine._high_frequency_receiver_rulerq   c                 C   s   dS )z<Check if there are multiple transactions in rapid successionFr   )r   ra   rl   rg   r   r   r    rP   ~  s    z!RuleEngine._rapid_succession_rulec                 C   sP   |† dd°}|† dd°}|r |s$dS |†d°d †° }|†d°d †° }||kS )z2Check if transaction crosses international borders⁄sender_locationr'   ⁄receiver_locationF˙,r   )r3   ⁄split⁄strip)r   ra   r®   r©   ⁄sender_country⁄receiver_countryr   r   r    rQ   Ñ  s    zRuleEngine._cross_border_rulec                 C   sL   |du rg d¢}|† dd°}|† dd°}|D ]}||v s@||v r, dS q,dS )z/Check if transaction involves high-risk countryNrt   r®   r'   r©   TFr_   )r   ra   rs   r®   r©   ⁄countryr   r   r    rR   í  s    z"RuleEngine._high_risk_country_rulec                 C   s   dS )z8Check if transaction is from unusual location for senderFr   r•   r   r   r    rS   °  s    z,RuleEngine._unusual_location_for_sender_rulery   c                 C   sh   |† d°}|du rdS tjj†|°s.t†|°}|j}||krL||kpJ||k S ||  ko^|k S   S dS )z,Check if transaction is during unusual hoursr   NF)r3   rê   rë   rí   rì   rï   ⁄hour)r   ra   rx   rz   r   r∞   r   r   r    rT   ß  s    

zRuleEngine._unusual_hour_rulec                 C   s8   |† d°}|du rdS tjj†|°s.t†|°}|jdkS )z"Check if transaction is on weekendr   NFrh   )r3   rê   rë   rí   rì   rï   ⁄	dayofweek)r   ra   r   r   r   r    rU   ∏  s    

zRuleEngine._weekend_ruler{   c                 C   s   dS )z2Check if this is the first transaction from senderFr   ©r   ra   rl   r   r   r    rV   ƒ  s    zRuleEngine._new_sender_rulec                 C   s   dS )z2Check if this is the first transaction to receiverFr   r≤   r   r   r    rW      s    zRuleEngine._new_receiver_rulec              
   C   s¨   zl| j †dd°sdtÉ }|†dd°}|†dd°}|d jj|ddç†° p\|d jj|ddç†° }|W S W dS W n: ty¶ } z"t†dt|Éõ ù° W Y d	}~dS d	}~0 0 d	S )
z1Check if transaction involves sanctioned entitiesr(   F⁄	sender_idr'   ⁄receiver_id⁄	entity_id)⁄nazError in sanctions check: N)	r   r3   r
   r9   ⁄contains⁄anyr7   r5   r}   )r   ra   Zsanctions_datar≥   r¥   Zis_sanctionedr>   r   r   r    rX   –  s    ˛
z RuleEngine._sanctions_check_rulec           	   
   C   s∂   zv| j †dd°sntÉ }|†dd°}|†dd°}d}|†° D ],\}}|d ||fv r:|d dkr:d	} qhq:|W S W dS W n: ty∞ } z"t†d
t|Éõ ù° W Y d}~dS d}~0 0 dS )z#Check if entities are tax compliantr)   Fr≥   r'   r¥   rµ   ⁄compliance_statuszNon-compliantTzError in tax compliance check: N)r   r3   r   ró   r7   r5   r}   r9   )	r   ra   Ztax_datar≥   r¥   Zis_non_compliantr¢   ⁄entityr>   r   r   r    rY   Í  s    
zRuleEngine._tax_compliance_rulec           	   
   C   s∂   zv| j †dd°sntÉ }|†dd°}|†dd°}d}|†° D ],\}}|d ||fv r:|d dkr:d	} qhq:|W S W dS W n: ty∞ } z"t†d
t|Éõ ù° W Y d}~dS d}~0 0 dS )z#Check if bank accounts are verifiedr*   Fr≥   r'   r¥   ⁄account_number⁄verification_status˙Not VerifiedTz"Error in bank verification check: N)r   r3   r   ró   r7   r5   r}   r9   )	r   ra   Z	bank_datar≥   r¥   ⁄is_unverifiedr¢   ⁄accountr>   r   r   r    rZ     s    
z"RuleEngine._bank_verification_rulec           	   
   C   s∂   zv| j †dd°sntÉ }|†dd°}|†dd°}d}|†° D ],\}}|d ||fv r:|d dkr:d	} qhq:|W S W dS W n: ty∞ } z"t†d
t|Éõ ù° W Y d}~dS d}~0 0 dS )z Check if identities are verifiedr+   Fr≥   r'   r¥   ⁄	id_numberrº   rΩ   Tz&Error in identity verification check: N)r   r3   r   ró   r7   r5   r}   r9   )	r   ra   Zidentity_datar≥   r¥   ræ   r¢   ⁄identityr>   r   r   r    r[   "  s    
z&RuleEngine._identity_verification_ruler%   r'   c                 C   s2   || j |< || j|< || j|< t†d|õ ù° dS )z˛
        Add a custom rule
        
        Args:
            rule_name (str): Name of the rule
            rule_func (function): Rule function
            weight (float): Weight of the rule
            description (str): Description of the rule
        zAdded rule: N)r   r   r   r5   r6   )r   r<   r†   r$   r&   r   r   r    ⁄add_rule>  s    



zRuleEngine.add_rulec                 C   sH   || j v r4| j |= | j|= | j|= t†d|õ ù° nt†d|õ ù° dS )zn
        Remove a rule
        
        Args:
            rule_name (str): Name of the rule to remove
        zRemoved rule: ˙Rule not found: N)r   r   r   r5   r6   r}   )r   r<   r   r   r    ⁄remove_ruleM  s    
zRuleEngine.remove_rulec                 C   s@   || j v r,|| j |< t†d|õ d|õ ù° nt†d|õ ù° dS )zô
        Update the weight of a rule
        
        Args:
            rule_name (str): Name of the rule
            weight (float): New weight
        zUpdated weight for rule rÑ   r√   N)r   r5   r6   r}   )r   r<   r$   r   r   r    ⁄update_rule_weight\  s    

zRuleEngine.update_rule_weightc                 C   s   t | j†° É| j| jdúS )z]
        Get all rules
        
        Returns:
            dict: Rules information
        )r   ⁄weights⁄descriptions)⁄listr   ⁄keysr   r   rr   r   r   r    ⁄	get_rulesj  s    ˝zRuleEngine.get_rulesc              
   C   s¬   zÇdi i}| j D ]$}d| j| | j| dú|d |< qt|dÉè }tj||ddç W d  É n1 sf0    Y  t†d|õ ù° W n: tyº } z"t†	d	t
|Éõ ù° Ç W Y d}~n
d}~0 0 dS )
zÖ
        Save current configuration to file
        
        Args:
            config_path (str): Path to save configuration
        r   T)r#   r$   r&   ⁄wF)⁄default_flow_styleNzConfiguration saved to zError saving configuration: )r   r   r   r/   r0   ⁄dumpr5   r6   r7   r8   r9   )r   r   r;   r<   r:   r>   r   r   r    ⁄save_configw  s    ˇ
˝.zRuleEngine.save_config)Nr   )rf   rh   )rf   rh   )rm   ro   )rm   ro   )rq   rf   )N)ry   rh   )r{   )r{   )r%   r'   ) ⁄__name__⁄
__module__⁄__qualname__⁄__doc__r!   r   r   r4   r§   rJ   rK   rL   rM   rN   rO   rP   rQ   rR   rS   rT   rU   rV   rW   rX   rY   rZ   r[   r¬   rƒ   r≈   r    rŒ   r   r   r   r    r      s:   
"bK]









r   )r“   ⁄pandasrê   ⁄numpy⁄np⁄rer   r   r0   r   ⁄warnings⁄logging⁄typingr   r   r   r   r   ⁄&fraud_detection_engine.utils.api_utilsr	   r
   r   r   r   ⁄filterwarnings⁄basicConfig⁄INFO⁄	getLoggerrœ   r5   r   r   r   r   r    ⁄<module>   s   



=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\models\__pycache__\supervised.cpython-39.pyc ===
a
    èöh·ë  „                   @   sÑ  d Z ddlZddlZddlmZmZmZm	Z	 ddl
mZmZmZ ddlmZ ddlmZ ddlmZ ddlmZ dd	lmZmZmZmZmZmZmZmZmZm Z  dd
l!m"Z"m#Z#m$Z$ ddl%m&Z&m'Z'm(Z( ddl)m*Z*m+Z+m,Z, ddl-m.Z.m/Z/ ddl0m1Z1m2Z2 ddl3Z4ddl5Z6ddl7Z7ddl8m9Z: ddl;Z<ddl=Z=ddl>Z>ddl?m@Z@mAZAmBZBmCZC e=†Dd° e>jEe>jFdç e>†GeH°ZIG ddÑ dÉZJdS )zT
Supervised Models Module
Implements supervised learning models for fraud detection
È    N)⁄train_test_split⁄cross_val_score⁄GridSearchCV⁄StratifiedKFold)⁄RandomForestClassifier⁄GradientBoostingClassifier⁄ExtraTreesClassifier)⁄LogisticRegression)⁄SVC)⁄MLPClassifier)⁄
GaussianNB)
⁄accuracy_score⁄precision_score⁄recall_score⁄f1_score⁄roc_auc_score⁄confusion_matrix⁄classification_report⁄precision_recall_curve⁄average_precision_score⁄	roc_curve)⁄StandardScaler⁄LabelEncoder⁄RobustScaler)⁄SelectKBest⁄	f_classif⁄RFE)⁄SMOTE⁄ADASYN⁄BorderlineSMOTE)⁄RandomUnderSampler⁄
TomekLinks)⁄SMOTEENN⁄
SMOTETomek)⁄Dict⁄List⁄Tuple⁄Union⁄ignore)⁄levelc                   @   st   e Zd ZdZdddÑZddÑ Zd	d
Ñ ZddÑ ZddÑ ZddÑ Z	ddÑ Z
ddÑ Zd ddÑZddÑ ZddÑ ZddÑ ZdS )!⁄SupervisedModelszv
    Class for supervised fraud detection models
    Implements Random Forest, XGBoost, Logistic Regression, etc.
    Áöôôôôô…?È*   Tc                 C   sR   || _ || _|| _i | _i | _i | _i | _i | _i | _i | _	i | _
i | _d| _dS )zı
        Initialize SupervisedModels
        
        Args:
            test_size (float): Proportion of data for testing
            random_state (int): Random seed
            handle_imbalance (bool): Whether to handle class imbalance
        FN)⁄	test_size⁄random_state⁄handle_imbalance⁄models⁄feature_names⁄scalers⁄label_encodersZfeature_selectors⁄
resamplers⁄performance⁄feature_importance⁄shap_values⁄fitted)⁄selfr-   r.   r/   © r:   ˙gC:\Users\Tranquil Abyss\fraud-detection-platform\app\..\src\fraud_detection_engine\models\supervised.py⁄__init__)   s    	zSupervisedModels.__init__c           Q         s‘  êzêd|j vr†t†d° |jtjgdçj †° }t|Édkrñ|| †d°}t	É }|†
|°}t†|°jddç}t†|d°}||k†t°}|†° }||d< qÆtdÉÇn|d †t°}g d	¢â á fd
dÑ|j D É}	||	 jtjgdçj †° }
||	 jddgdçj †° }||	 †° }| †|°}|D ]:}||j v êrtÉ }|†
|| †t°°||< || j|< êqt||| j| j|dç\}}}}|
êr¨tÉ }|†
||
 °||
< |†||
 °||
< || jd< | jêrË|†° t|É dk êrËt| jdçt| jdçt | jdçt!| jdçt"| jdçdú}d}d}|†#° D ]∞\}}zd|†$||°\}}t%d| jddç}t&|||dddç}|†'° }||kêr||}|}|| j(d< || }}W n@ t)êyæ } z&t†d|õ dt|Éõ ù° W Y d}~n
d}~0 0 êq|êrﬁt†*d|õ dù° n
t†d° |	}i }z⁄t%d d!d"d#d$| jdd%ç}|†+||° |†,|°} |†-|°ddÖdf }!| †.|| |!°}"t/†0||j1d&ú°j2d'd(d)ç}#t3†4|°}$|$†5|°}%|| |!|"|#|%|d*ú|d+< || j6d+< || j7d+< |"| j8d+< |#| j9d+< |%| j5d+< t†*d,° W n: t)êy } z t†:d-t|Éõ ù° W Y d}~n
d}~0 0 z¸t;j<d d.dd/d/t||dk Ét||dk É | jd(d0d1ç	}&|&†+||° |&†,|°}'|&†-|°ddÖdf }(| †.||'|(°})t/†0||&j1d&ú°j2d'd(d)ç}*t3†4|&°}$|$†5|°}+|&|'|(|)|*|+|d*ú|d2< |&| j6d2< || j7d2< |)| j8d2< |*| j9d2< |+| j5d2< t†*d3° W n: t)êy< } z t†:d4t|Éõ ù° W Y d}~n
d}~0 0 z¯t=j>d d.dd/d/t||dk Ét||dk É | jd5ç},|,†+||° |,†,|°}-|,†-|°ddÖdf }.| †.||-|.°}/t/†0||,j1d&ú°j2d'd(d)ç}0t3†4|,°}$|$†5|°}1|,|-|.|/|0|1|d*ú|d6< |,| j6d6< || j7d6< |/| j8d6< |0| j9d6< |1| j5d6< t†*d7° W n: t)êyp } z t†:d8t|Éõ ù° W Y d}~n
d}~0 0 z‡t?d9d$| jd:d;ç}2|2†+||° |2†,|°}3|2†-|°ddÖdf }4| †.||3|4°}5t/†0|t†|2j@d °d&ú°j2d'd(d)ç}6t3†A|2|°}$|$†5|°}7|2|3|4|5|6|7|d*ú|d<< |2| j6d<< || j7d<< |5| j8d<< |6| j9d<< |7| j5d<< t†*d=° W n: t)êyå } z t†:d>t|Éõ ù° W Y d}~n
d}~0 0 z÷tBd d.dd/| jd?ç}8|8†+||° |8†,|°}9|8†-|°ddÖdf }:| †.||9|:°};t/†0||8j1d&ú°j2d'd(d)ç}<t3†4|8°}$|$†5|°}=|8|9|:|;|<|=|d*ú|d@< |8| j6d@< || j7d@< |;| j8d@< |<| j9d@< |=| j5d@< t†*dA° W n: t)êyû } z t†:dBt|Éõ ù° W Y d}~n
d}~0 0 êztCdCdDdEdFdGdHdI| jdJç}>|>†+||° |>†,|°}?|>†-|°ddÖdf }@| †.||?|@°}AddKlDmE}B |B|>||d"| jdLç}Ct/†0||CjFd&ú°j2d'd(d)ç}Dt3†G|>j-|dd Ö °}$|$†5|dd Ö °}E|>|?|@|A|D|E|d*ú|dM< |>| j6dM< || j7dM< |A| j8dM< |D| j9dM< |E| j5dM< t†*dN° W n: t)ê	yÏ } z t†:dOt|Éõ ù° W Y d}~n
d}~0 0 êz\g }Fg }G|D ]*}H|F†H||H dP ° |G†H||H dQ ° ê	q˛t†I|F°j'ddçdRk}It†I|G°j'ddç}J| †.||I|J°}Kt/†0|t†Jt|É°d&ú°}L|D ]v}H|H| j9v ê
r~| j9|H }MtK|ÉD ]P\}N}O|O|MdS jLv ê
r†|M|MdS |Ok jMd }P|LjN|Nd'f  |MjN|Pd'f 7  < ê
q†ê
q~|Ld' t|É |Ld'< |Lj2d'd(d)ç}L|I|J|K|L|dTú|dU< |K| j8dU< |L| j9dU< t†*dV° W n: t)êyÜ } z t†:dWt|Éõ ù° W Y d}~n
d}~0 0 dX| _O|W S  t)êyŒ } z"t†:dYt|Éõ ù° Ç W Y d}~n
d}~0 0 dS )Zzß
        Run all supervised models
        
        Args:
            df (DataFrame): Input data
            
        Returns:
            dict: Model results
        ⁄
fraud_flagzENo fraud_flag column found. Using synthetic labels for demonstration.©⁄includer   È   ©⁄axisÈ_   z6No numeric columns found for creating synthetic labels)r=   ⁄transaction_id⁄	sender_id⁄receiver_idc                    s   g | ]}|à vr|ëqS r:   r:   )⁄.0⁄col©⁄exclude_colsr:   r;   ⁄
<listcomp>g   Û    z/SupervisedModels.run_models.<locals>.<listcomp>⁄object⁄category)r-   r.   ⁄stratify⁄numericgöôôôôôπ?)r.   )ZsmoteZadasynZborderline_smoteZ	smote_ennZsmote_tomekNÈ2   Èˇˇˇˇ)⁄n_estimatorsr.   ⁄n_jobsÈ   ⁄f1)⁄cv⁄scoring⁄bestzError with ˙: zUsing z for handling class imbalancez#No suitable resampling method foundÈd   È
   È   È   ⁄balanced)rS   ⁄	max_depth⁄min_samples_split⁄min_samples_leaf⁄class_weightr.   rT   )⁄feature⁄
importancere   F)⁄	ascending)⁄model⁄predictions⁄probabilitiesr5   r6   r7   r1   Zrandom_forestzRandom Forest model completedzError running Random Forest: È   göôôôôôÈ?Zlogloss)	rS   r`   ⁄learning_rate⁄	subsample⁄colsample_bytree⁄scale_pos_weightr.   Zuse_label_encoderZeval_metric⁄xgboostzXGBoost model completedzError running XGBoost: )rS   r`   rk   rl   rm   rn   r.   ⁄lightgbmzLightGBM model completedzError running LightGBM: Á      ?iË  )⁄Crc   r.   ⁄max_iter⁄logistic_regressionz#Logistic Regression model completedz#Error running Logistic Regression: )rS   r`   rk   rl   r.   ⁄gradient_boostingz!Gradient Boosting model completedz!Error running Gradient Boosting: )r[   rQ   ⁄relu⁄adamg-CÎ‚6?È    ⁄adaptiveÈ»   )Zhidden_layer_sizes⁄
activation⁄solver⁄alpha⁄
batch_sizerk   rs   r.   )⁄permutation_importance)⁄	n_repeatsr.   ⁄neural_networkzNeural Network model completedzError running Neural Network: rh   ri   Á      ‡?rd   )rh   ri   r5   r6   r1   ⁄ensemblezEnsemble model completedzError running Ensemble model: Tz!Error running supervised models: )P⁄columns⁄logger⁄warning⁄select_dtypes⁄np⁄number⁄tolist⁄len⁄fillnar   ⁄fit_transform⁄abs⁄max⁄
percentile⁄astype⁄int⁄copy⁄
ValueError⁄_clean_datar   ⁄strr3   r   r-   r.   r   ⁄	transformr2   r/   ⁄sumr   r   r   r"   r#   ⁄itemsZfit_resampler   r   ⁄meanr4   ⁄	Exception⁄info⁄fit⁄predict⁄predict_proba⁄_calculate_performance_metrics⁄pd⁄	DataFrame⁄feature_importances_⁄sort_values⁄shapZTreeExplainerr7   r0   r1   r5   r6   ⁄error⁄xgbZXGBClassifier⁄lgbZLGBMClassifierr	   ⁄coef_ZLinearExplainerr   r   Zsklearn.inspectionr   Zimportances_meanZKernelExplainer⁄append⁄array⁄zeros⁄	enumerate⁄values⁄index⁄locr8   )Qr9   ⁄df⁄numeric_cols⁄X⁄scaler⁄X_scaled⁄z_scores⁄	threshold⁄y⁄feature_cols⁄numeric_features⁄categorical_featuresrH   ⁄le⁄X_train⁄X_test⁄y_train⁄y_testZresampling_methodsZbest_method⁄
best_score⁄method_name⁄	resamplerZX_resampledZy_resampled⁄rf⁄scores⁄	avg_score⁄e⁄all_feature_names⁄resultsZrf_modelZrf_predZrf_probaZrf_performanceZrf_importanceZ	explainerZrf_shap_valuesZ	xgb_modelZxgb_predZ	xgb_probaZxgb_performanceZxgb_importanceZxgb_shap_valuesZ	lgb_modelZlgb_predZ	lgb_probaZlgb_performanceZlgb_importanceZlgb_shap_valuesZlr_modelZlr_predZlr_probaZlr_performanceZlr_importanceZlr_shap_valuesZgb_modelZgb_predZgb_probaZgb_performanceZgb_importanceZgb_shap_valuesZnn_modelZnn_predZnn_probaZnn_performancer   Zperm_importanceZnn_importanceZnn_shap_values⁄all_predictions⁄all_probabilities⁄
model_nameZensemble_predZensemble_probaZensemble_performanceZensemble_importanceZmodel_importance⁄ird   ⁄idxr:   rI   r;   ⁄
run_models@   sí   






ˇ





˚	

4
˘

˛˝

˘






*˜
˛˝

˘






*˘

˛˝

˘






*¸
˛˝
˘






*˚
˛˝

˘






*¯
ˇ˛˝˘






*˛
(˚


*zSupervisedModels.run_modelsc              
   C   s¸   z∏|† tjtj gtj°}|jD ]í}|| jdv r t†|| d°}t†|°s |d }t†|| |k||| °||< t†|| d°}t†|°s |d }t†|| |k ||| °||< q |W S  t	ê yˆ } z$t
†dt|Éõ ù° |W  Y d}~S d}~0 0 dS )z…
        Clean data by handling infinity, NaN, and extreme values
        
        Args:
            X (DataFrame): Input data
            
        Returns:
            DataFrame: Cleaned data
        )⁄float64⁄int64Èc   r\   r@   zError cleaning data: N)⁄replacerà   ⁄inf⁄nanrÑ   ⁄dtype⁄nanpercentile⁄isnan⁄whererõ   rÖ   r¶   rñ   )r9   r≥   rH   ⁄percentile_99⁄max_val⁄percentile_1⁄min_valr«   r:   r:   r;   rï     s     



 zSupervisedModels._clean_datac                 C   s‡   z†t ||É}t||ddç}t||ddç}t||ddç}t||É}t||É}	t||É}
t||ddç}t||É\}}}t	||É\}}}||||||	|
|||||dúW S  t
y⁄ } z"t†dt|Éõ ù° Ç W Y d}~n
d}~0 0 dS )a   
        Calculate performance metrics for a model
        
        Args:
            y_true (array): True labels
            y_pred (array): Predicted labels
            y_proba (array): Predicted probabilities
            
        Returns:
            dict: Performance metrics
        r   )⁄zero_divisionT)⁄output_dict)⁄accuracy⁄	precision⁄recallrV   ⁄roc_auc⁄avg_precisionr   r   ⁄precision_curve⁄recall_curve⁄fpr⁄tprz'Error calculating performance metrics: N)r   r   r   r   r   r   r   r   r   r   rõ   rÖ   r¶   rñ   )r9   ⁄y_true⁄y_pred⁄y_probar‡   r·   r‚   rV   r„   r‰   ⁄cmZclass_reportrÂ   rÊ   ⁄_rÁ   rË   r«   r:   r:   r;   r†   =  s6    



Ùz/SupervisedModels._calculate_performance_metricsc              
      s  | j stdÉÇ|| jvr0|dkr0td|õ dùÉÇêzî|dkrjg }| jD ]}|†| j| ° qFtt|ÉÉ}n
| j| }|| †° }| †|°}|j	D ]8}|| j
v rê| j
| â || †t°†á fddÑ°||< qêd| jv êr
|jtjgdçj	†° }|êr
| jd †|| °||< |dkêríg }g }	| jD ]D}| j| }
|
†|°}|
†|°d	d	Öd
f }|†|° |	†|° êq"t†|°jddçdk}t†|	°jddç}n*| j| }
|
†|°}|
†|°d	d	Öd
f }||dúW S  têy } z(t†d|õ dt|Éõ ù° Ç W Y d	}~n
d	}~0 0 d	S )z˙
        Make predictions using a fitted model
        
        Args:
            df (DataFrame): Input data
            model_name (str): Name of the model to use
            
        Returns:
            dict: Predictions and probabilities
        ˙)Models not fitted. Call run_models first.rÉ   zModel ˙ not found.c                    s   | à j v rà †| g°d S dS )Nr   rR   )⁄classes_ró   )⁄x©rº   r:   r;   ⁄<lambda>ù  rL   z*SupervisedModels.predict.<locals>.<lambda>rP   r>   Nr@   r   rA   rÇ   )rh   ri   zError making predictions with rZ   )r8   rî   r0   r1   ⁄extend⁄list⁄setrì   rï   rÑ   r3   rë   rñ   ⁄mapr2   rá   rà   râ   rä   ró   rû   rü   r™   r´   rö   rõ   rÖ   r¶   )r9   r±   rÃ   r1   ⁄namer≥   rH   r∫   r    rÀ   rg   ⁄pred⁄probarh   ri   r«   r:   rÚ   r;   rû   u  sV    






ˇ







˛zSupervisedModels.predictc                 C   s2   | j stdÉÇ|| jvr(td|õ dùÉÇ| j| S )z√
        Get feature importance for a model
        
        Args:
            model_name (str): Name of the model
            
        Returns:
            DataFrame: Feature importance
        rÓ   zFeature importance for rÔ   )r8   rî   r6   ©r9   rÃ   r:   r:   r;   ⁄get_feature_importance   s
    

z'SupervisedModels.get_feature_importancec                 C   s2   | j stdÉÇ|| jvr(td|õ dùÉÇ| j| S )z¿
        Get performance metrics for a model
        
        Args:
            model_name (str): Name of the model
            
        Returns:
            dict: Performance metrics
        rÓ   zPerformance for rÔ   )r8   rî   r5   r˚   r:   r:   r;   ⁄get_performance‹  s
    

z SupervisedModels.get_performancec                 C   s2   | j stdÉÇ|| jvr(td|õ dùÉÇ| j| S )z±
        Get SHAP values for a model
        
        Args:
            model_name (str): Name of the model
            
        Returns:
            array: SHAP values
        rÓ   zSHAP values for rÔ   )r8   rî   r7   r˚   r:   r:   r;   ⁄get_shap_valuesÓ  s
    

z SupervisedModels.get_shap_valuesÈ   c              
   C   sû   zX| † |°}|†|°}tjddç tjdd|dç t†d|õ d|õ ù° t†°  t†° W S  t	yò } z(t
†d|õ d	t|Éõ ù° Ç W Y d
}~n
d
}~0 0 d
S )z¥
        Plot feature importance for a model
        
        Args:
            model_name (str): Name of the model
            top_n (int): Number of top features to show
        )r\   È   ©⁄figsizere   rd   )rÒ   r∏   ⁄datazTop z Feature Importance - z&Error plotting feature importance for rZ   N)r¸   ⁄head⁄plt⁄figure⁄snsZbarplot⁄title⁄tight_layout⁄gcfrõ   rÖ   r¶   rñ   )r9   rÃ   ⁄top_n⁄importance_df⁄top_featuresr«   r:   r:   r;   ⁄plot_feature_importance   s    


z(SupervisedModels.plot_feature_importancec              
   C   s∏   zr| † |°}|d }tjddç tj|dddddgddgd	ç t†d
|õ ù° t†d° t†d° t†°  t†	° W S  t
y≤ } z(t†d|õ dt|Éõ ù° Ç W Y d}~n
d}~0 0 dS )zz
        Plot confusion matrix for a model
        
        Args:
            model_name (str): Name of the model
        r   ©r   rj   r  T⁄d⁄Bluesz	Not Fraud⁄Fraud)⁄annot⁄fmt⁄cmap⁄xticklabels⁄yticklabelszConfusion Matrix - ⁄Actual⁄	Predictedz$Error plotting confusion matrix for rZ   N)r˝   r  r  r  ⁄heatmapr  ⁄ylabel⁄xlabelr	  r
  rõ   rÖ   r¶   rñ   )r9   rÃ   r5   rÏ   r«   r:   r:   r;   ⁄plot_confusion_matrix  s     
˛


z&SupervisedModels.plot_confusion_matrixc              
   C   s  z¿| † |°}|d }|d }|d }tjddç tj|||õ d|dõdùd	ç t†d
dgd
dgd° t†ddg° t†ddg° t†d° t†d° t†d|õ ù° tj	ddç t†
°  t†° W S  têy } z(t†d|õ dt|Éõ ù° Ç W Y d}~n
d}~0 0 dS )zs
        Plot ROC curve for a model
        
        Args:
            model_name (str): Name of the model
        rÁ   rË   r„   r  r  z (AUC = ˙.3f˙)©⁄labelr   r@   zk--Á        rq   ÁÕÃÃÃÃÃ?zFalse Positive RatezTrue Positive RatezROC Curve - zlower right©r∞   zError plotting ROC curve for rZ   N©r˝   r  r  ⁄plot⁄xlim⁄ylimr  r  r  ⁄legendr	  r
  rõ   rÖ   r¶   rñ   )r9   rÃ   r5   rÁ   rË   r„   r«   r:   r:   r;   ⁄plot_roc_curve7  s&    



zSupervisedModels.plot_roc_curvec              
   C   s   z™| † |°}|d }|d }|d }tjddç tj|||õ d|dõdùd	ç t†d
dg° t†d
dg° t†d° t†d° t†d|õ ù° tj	ddç t†
°  t†° W S  tyÍ } z(t†d|õ dt|Éõ ù° Ç W Y d}~n
d}~0 0 dS )zÄ
        Plot precision-recall curve for a model
        
        Args:
            model_name (str): Name of the model
        rÂ   rÊ   r‰   r  r  z (AP = r  r  r   r"  rq   r#  ⁄Recall⁄	PrecisionzPrecision-Recall Curve - z
lower leftr$  z*Error plotting precision-recall curve for rZ   Nr%  )r9   rÃ   r5   rÂ   rÊ   r‰   r«   r:   r:   r;   ⁄plot_precision_recall_curveW  s$    



z,SupervisedModels.plot_precision_recall_curveN)r+   r,   T)rˇ   )⁄__name__⁄
__module__⁄__qualname__⁄__doc__r<   rœ   rï   r†   rû   r¸   r˝   r˛   r  r  r*  r-  r:   r:   r:   r;   r*   #   s    
   \$8U
 r*   )Kr1  ⁄pandasr°   ⁄numpyrà   Zsklearn.model_selectionr   r   r   r   ⁄sklearn.ensembler   r   r   ⁄sklearn.linear_modelr	   Zsklearn.svmr
   Zsklearn.neural_networkr   Zsklearn.naive_bayesr   Zsklearn.metricsr   r   r   r   r   r   r   r   r   r   ⁄sklearn.preprocessingr   r   r   Zsklearn.feature_selectionr   r   r   Zimblearn.over_samplingr   r   r   Zimblearn.under_samplingr    r!   Zimblearn.combiner"   r#   ro   rß   rp   r®   r•   ⁄matplotlib.pyplot⁄pyplotr  ⁄seabornr  ⁄warnings⁄logging⁄typingr$   r%   r&   r'   ⁄filterwarnings⁄basicConfig⁄INFO⁄	getLoggerr.  rÖ   r*   r:   r:   r:   r;   ⁄<module>   s4   0



=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\models\__pycache__\unsupervised.cpython-39.pyc ===
a
    èöh9t  „                   @   s  d Z ddlZddlZddlmZ ddlmZ ddl	m
Z
 ddlmZmZ ddlmZ ddlmZ dd	lmZmZ dd
lmZ ddlmZ ddlZddlmZmZ ddlmZ ddl Z ddl!Z!ddl"m#Z#m$Z$m%Z%m&Z& e †'d° e!j(e!j)dç e!†*e+°Z,G ddÑ dÉZ-dS )zX
Unsupervised Models Module
Implements unsupervised learning models for fraud detection
È    N)⁄IsolationForest)⁄LocalOutlierFactor)⁄OneClassSVM)⁄DBSCAN⁄KMeans)⁄PCA)⁄TSNE)⁄StandardScaler⁄RobustScaler)⁄SimpleImputer)⁄silhouette_score)⁄layers⁄models)⁄EarlyStopping)⁄Dict⁄List⁄Tuple⁄Union⁄ignore)⁄levelc                   @   sR   e Zd ZdZdddÑZddÑ Zdd	Ñ Zd
dÑ ZddÑ ZddÑ Z	ddÑ Z
ddÑ ZdS )⁄UnsupervisedModelszÅ
    Class for unsupervised fraud detection models
    Implements Isolation Forest, Local Outlier Factor, Autoencoders, etc.
    Á{ÆG·zÑ?È*   c                 C   s.   || _ || _i | _i | _i | _i | _d| _dS )zµ
        Initialize UnsupervisedModels
        
        Args:
            contamination (float): Expected proportion of outliers
            random_state (int): Random seed
        FN)⁄contamination⁄random_stater   ⁄feature_names⁄scalers⁄imputers⁄fitted)⁄selfr   r   © r    ˙iC:\Users\Tranquil Abyss\fraud-detection-platform\app\..\src\fraud_detection_engine\models\unsupervised.py⁄__init__    s    zUnsupervisedModels.__init__c           <   
   C   sä  êzF|j tjgdçj†° }t|Édk r6t†d° i W S || †° }| †	|°}| †
|°\}}}|| jd< || jd< i }zft| j| jdddç}|†|°}	|†|°}
| †|
°}|	|||dú|d	< || jd	< || jd	< t†d
° W n: têy } z t†dt|Éõ ù° W Y d}~n
d}~0 0 zrt| jtdt|Éd Édddç}|†|°}|†|°}| †|°}||||dú|d< || jd< || jd< t†d° W n: têy¬ } z t†dt|Éõ ù° W Y d}~n
d}~0 0 zbt| jdddç}|†|°}|†|°}| †|°}||||dú|d< || jd< || jd< t†d° W n: têy` } z t†dt|Éõ ù° W Y d}~n
d}~0 0 êzt|Édkêrê|tjjt|Édddç }n|}tdddç}|†|°}t†|dkdd°}t† t|É°}t!t|ÉÉD ]Ñ}|| dkêr÷t†|dk°d  }t|Éd kêr2tj"j#|| ||  dd!ç}|†° ||< n&t†$tj"j#||j%d d!ç dd!ç°||< êq÷| †|°}t|Édkêr<|†|°}t†|dkdd°}t† t|É°}t!t|ÉÉD ]Ñ}|| dkêr™t†|dk°d  }t|Éd kêrtj"j#|| ||  dd!ç}|†° ||< n&t†$tj"j#||j%d d!ç dd!ç°||< êq™| †|°} n|}|} || ||dú|d"< || jd"< || jd"< t†d#° W n: têy∞ } z t†d$t|Éõ ù° W Y d}~n
d}~0 0 êzg }!t!dtd%t|Éd ÉÉ}"|"D ]0}#t&|#| jd&d'ç}$|$†|°}%|!†'t(||%É° êq÷|"t†)|!° }&t&|&| jd&d'ç}'|'†|°}(|'†*|°}|jdd!ç})| †|)°}*t†+|*d| j d( °}+t†|*|+kdd°},|,|*|'||&|!d)ú|d*< |'| jd*< || jd*< t†d+|&õ d,ù° W n: têy } z t†d-t|Éõ ù° W Y d}~n
d}~0 0 z¸|j,d }-t$d|-d É}.| †-|-|.°}/t.d.ddd/ç}0|/j/||d0d1d2|0gd d3ç}1|/†0|°}2tj%t†1||2 d°dd!ç}3t†2|3°}3t†3|3d t†4tj5°j$°}3| †|3°}4t†+|4d| j d( °}+t†|4|+kdd°}5|5|4|/||1j6d4ú|d5< |/| jd5< || jd5< t†d6° W n: têy( } z t†d7t|Éõ ù° W Y d}~n
d}~0 0 zÿt7td&|j,d d É| jd8ç}6|6†8|°}7|6†9|7°}8tj%t†1||8 d°dd!ç}9t†2|9°}9t†3|9d t†4tj5°j$°}9| †|9°}:t†+|:d| j d( °}+t†|:|+kdd°};|;|:|6||6j:d9ú|d:< |6| jd:< || jd:< t†d;° W n: têy< } z t†d<t|Éõ ù° W Y d}~n
d}~0 0 d| _;|W S  têyÑ } z"t†d=t|Éõ ù° Ç W Y d}~n
d}~0 0 dS )>z©
        Run all unsupervised models
        
        Args:
            df (DataFrame): Input data
            
        Returns:
            dict: Model results
        )⁄includeÈ   z2Not enough numeric columns for unsupervised models⁄globalÈˇˇˇˇ⁄auto)r   r   ⁄n_jobsZmax_samples)⁄predictions⁄scores⁄modelr   ⁄isolation_forestz Isolation Forest model completedz Error running Isolation Forest: NÈ   È   T)r   Zn_neighborsZnoveltyr(   ⁄local_outlier_factorz$Local Outlier Factor model completedz$Error running Local Outlier Factor: ⁄rbf⁄scale)⁄nu⁄kernel⁄gamma⁄one_class_svmzOne-Class SVM model completedzError running One-Class SVM: i'  F)⁄replaceg      ‡?È   )⁄eps⁄min_samplesr   ©⁄axis⁄dbscanzDBSCAN model completedzError running DBSCAN: È   È
   )⁄
n_clustersr   ⁄n_initÈd   )r)   r*   r+   r   ⁄	optimal_k⁄silhouette_scores⁄kmeanszK-Means model completed with z	 clusterszError running K-Means: Zval_loss)⁄monitor⁄patienceZrestore_best_weightsÈ2   È    göôôôôô…?)⁄epochs⁄
batch_sizeZvalidation_split⁄	callbacks⁄verbose)r)   r*   r+   r   ⁄history⁄autoencoderzAutoencoder model completedzError running Autoencoder: )⁄n_componentsr   )r)   r*   r+   r   ⁄explained_variance⁄pcaz%PCA-based anomaly detection completedz+Error running PCA-based anomaly detection: z#Error running unsupervised models: )<⁄select_dtypes⁄np⁄number⁄columns⁄tolist⁄len⁄logger⁄warning⁄copy⁄_clean_data⁄_scale_datar   r   r   r   r   ⁄fit_predict⁄decision_function⁄_normalize_scoresr   r   ⁄info⁄	Exception⁄error⁄strr   ⁄minr   ⁄random⁄choicer   ⁄where⁄zeros⁄range⁄linalg⁄norm⁄max⁄meanr   ⁄appendr   ⁄argmax⁄	transform⁄
percentile⁄shape⁄_build_autoencoderr   ⁄fit⁄predict⁄power⁄
nan_to_num⁄clip⁄finfo⁄float64rM   r   ⁄fit_transform⁄inverse_transform⁄explained_variance_ratio_r   )<r   ⁄df⁄numeric_cols⁄X⁄X_scaled⁄scaler⁄imputer⁄resultsZif_modelZif_predictionsZ	if_scoresZif_scores_normalized⁄eZ	lof_modelZlof_predictionsZ
lof_scoresZlof_scores_normalizedZocsvm_modelZocsvm_predictionsZocsvm_scoresZocsvm_scores_normalized⁄X_subsetZdbscan_modelZdbscan_labelsZdbscan_predictionsZdbscan_scores⁄i⁄core_points⁄	distancesZdbscan_scores_normalizedZfull_predictionsZfull_scoresZfull_scores_normalizedrC   Zk_range⁄krD   ⁄labelsrB   Zkmeans_modelZkmeans_labels⁄min_distancesZkmeans_scores_normalized⁄	thresholdZkmeans_predictions⁄	input_dim⁄encoding_dimrN   ⁄early_stoppingrM   ⁄reconstructions⁄mseZmse_normalizedZautoencoder_predictionsrQ   ⁄pca_transformed⁄X_reconstructedZ
pca_errorsZpca_errors_normalizedZpca_predictionsr    r    r!   ⁄
run_models0   sÑ   




¸


¸


*¸


¸


*˝


¸


*
*

*¸


*



˙
	

*
˙



˚


*



˚


*zUnsupervisedModels.run_modelsc              
   C   s¸   z∏|† tjtj gtj°}|jD ]í}|| jdv r t†|| d°}t†|°s |d }t†|| |k||| °||< t†|| d°}t†|°s |d }t†|| |k ||| °||< q |W S  t	ê yˆ } z$t
†dt|Éõ ù° |W  Y d}~S d}~0 0 dS )z…
        Clean data by handling infinity, NaN, and extreme values
        
        Args:
            X (DataFrame): Input data
            
        Returns:
            DataFrame: Cleaned data
        )rz   ⁄int64Èc   r>   r.   zError cleaning data: N)r6   rS   ⁄inf⁄nanrU   ⁄dtype⁄nanpercentile⁄isnanrg   ra   rX   rb   rc   )r   rÄ   ⁄colZpercentile_99⁄max_valZpercentile_1⁄min_valrÖ   r    r    r!   r[   q  s     



 zUnsupervisedModels._clean_datac              
   C   sÆ   zHt ddç}|†|°}tÉ }|†|°}t†|°}t†|dd°}|||fW S  ty® } zHt†dt	|Éõ ù° ||†
°  |†° |†
°   }|jddfW  Y d}~S d}~0 0 dS )zŸ
        Scale data using RobustScaler (more resistant to outliers)
        
        Args:
            X (DataFrame): Input data
            
        Returns:
            tuple: (scaled data, scaler, imputer)
        ⁄median)⁄strategyÁ    _†¬Á    _†BzError scaling data: N)r   r{   r
   rS   rw   rx   ra   rX   rb   rc   rd   rl   ⁄values)r   rÄ   rÉ   ⁄	X_imputedrÇ   rÅ   rÖ   ⁄X_normalizedr    r    r!   r\   ï  s    




zUnsupervisedModels._scale_datac              
   C   sö   zRt †|°}t †|dd°}|†° }|†° }||krD|| ||  }n
t †|°}|W S  tyî } z*t†dt	|Éõ ù° t †|°W  Y d}~S d}~0 0 dS )z≤
        Normalize scores to 0-1 range
        
        Args:
            scores (array): Input scores
            
        Returns:
            array: Normalized scores
        r¢   r£   zError normalizing scores: N)
rS   rw   rx   rd   rl   ⁄
zeros_likera   rX   rb   rc   )r   r*   ⁄	min_score⁄	max_scoreZnormalized_scoresrÖ   r    r    r!   r_   ¥  s    


z$UnsupervisedModels._normalize_scoresc              
   C   sæ   z~t j|fdç}t j|d ddç|É}t j|ddç|É}t j|d ddç|É}t j|ddç|É}t†||°}|jdddç |W S  ty∏ } z"t†d	t	|Éõ ù° Ç W Y d
}~n
d
}~0 0 d
S )z„
        Build autoencoder model
        
        Args:
            input_dim (int): Input dimension
            encoding_dim (int): Encoding dimension
            
        Returns:
            Model: Autoencoder model
        )rr   r$   Zrelu)⁄
activation⁄linear⁄adam⁄mean_squared_error)⁄	optimizer⁄losszError building autoencoder: N)
r   ⁄InputZDenser   ⁄Model⁄compilera   rX   rb   rc   )r   ré   rè   Zinput_layer⁄encoded⁄decodedrN   rÖ   r    r    r!   rs   “  s    z%UnsupervisedModels._build_autoencoderc              
   C   sﬁ  | j stdÉÇ|| jvr(td|õ dùÉÇêzl| j| }|| †° }| †|°}d| jv rÇd| jv rÇ| jd †|°}| jd †|°}n4||†	°  |†
° |†	°   }t†|°}t†|dd°}| j| }|dkrÍ|†|°}|†|°}	| †|	°}
ên¢|dkêr|†|°}|†|°}	| †|	°}
ênv|d	kêrB|†|°}|†|°}	| †|	°}
ênJ|d
kêr|†|°}t†|dkdd°}t†t|É°}	tt|ÉÉD ]Ñ}|| dkêrÇt†|dk°d }t|Édkêrﬁtjj|| ||  ddç}|†	° |	|< n&t†
tjj||jddç ddç°|	|< êqÇ| †|	°}
ênv|dkêrl|†|°}|j	ddç}| †|°}
t†|
d| j d °}t†|
|kdd°}ên |dkêr|†|°}tjt†|| d°ddç}t†|°}t†|dt†tj°j
°}| †|°}
t†|
d| j d °}t†|
|kdd°}nú|dkêr~|†|°}|†|°}tjt†|| d°ddç}t†|°}t†|dt†tj°j
°}| †|°}
t†|
d| j d °}t†|
|kdd°}ntd|õ ùÉÇ||
dúW S  têyÿ } z(t †!d|õ dt"|Éõ ù° Ç W Y d}~n
d}~0 0 dS )zÛ
        Make predictions using a fitted model
        
        Args:
            df (DataFrame): Input data
            model_name (str): Name of the model to use
            
        Returns:
            dict: Predictions and scores
        ˙)Models not fitted. Call run_models first.˙Model ˙ not found.r%   r¢   r£   r,   r/   r5   r<   r&   r.   r   r:   rD   rA   rN   r$   rQ   zUnknown model: )r)   r*   zError making predictions with ˙: N)#r   ⁄
ValueErrorr   r   rZ   r[   r   r   rp   rd   rl   rS   rw   rx   ru   r^   r_   r]   rg   rh   rW   ri   rj   rk   rm   rq   r   rv   ry   rz   r|   ra   rX   rb   rc   )r   r~   ⁄
model_namer   rÄ   r•   rÅ   r+   r)   r*   Zscores_normalizedrá   rà   râ   rå   rç   rë   rí   rì   rî   rÖ   r    r    r!   ru   Û  sä    














*











˛zUnsupervisedModels.predictc              
   C   s.  | j stdÉÇ|| jvr(td|õ dùÉÇzæ| j| }|dkrh| j| j}t†||dú°jdddç}|W S |d	krÆ| j| j}t†d
dÑ t	t
|ÉÉD É|dú°jdddç}|W S t†t
|É°t
|É }t†||dú°jdddç}|W S W nB têy( } z(t†d|õ dt|Éõ ù° Ç W Y d}~n
d}~0 0 dS )z√
        Get feature importance for a model
        
        Args:
            model_name (str): Name of the model
            
        Returns:
            DataFrame: Feature importance
        rµ   r∂   r∑   r,   )⁄feature⁄
importancerº   F)⁄	ascendingrQ   c                 S   s   g | ]}d |d õ ùëqS )⁄PCr.   r    )⁄.0rá   r    r    r!   ⁄
<listcomp>£  Û    z=UnsupervisedModels.get_feature_importance.<locals>.<listcomp>z%Error getting feature importance for r∏   N)r   rπ   r   r   Zfeature_importances_⁄pd⁄	DataFrame⁄sort_valuesr}   ri   rW   rS   ⁄onesra   rX   rb   rc   )r   r∫   r   rº   Zimportance_dfrP   rÖ   r    r    r!   ⁄get_feature_importance}  sF    


˛˝˛˝˛˝
z)UnsupervisedModels.get_feature_importanceN)r   r   )⁄__name__⁄
__module__⁄__qualname__⁄__doc__r"   rï   r[   r\   r_   rs   ru   r∆   r    r    r    r!   r      s   
  C$! r   ).r    ⁄pandasr¬   ⁄numpyrS   Zsklearn.ensembler   Zsklearn.neighborsr   Zsklearn.svmr   Zsklearn.clusterr   r   ⁄sklearn.decompositionr   Zsklearn.manifoldr   ⁄sklearn.preprocessingr	   r
   Zsklearn.imputer   Zsklearn.metricsr   ⁄
tensorflow⁄tfZtensorflow.kerasr   r   Ztensorflow.keras.callbacksr   ⁄warnings⁄logging⁄typingr   r   r   r   ⁄filterwarnings⁄basicConfig⁄INFO⁄	getLoggerr«   rX   r   r    r    r    r!   ⁄<module>   s*   



=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\models\__pycache__\__init__.cpython-39.pyc ===
a
    èöh    „                   @   s   d S )N© r   r   r   ˙eC:\Users\Tranquil Abyss\fraud-detection-platform\app\..\src\fraud_detection_engine\models\__init__.py⁄<module>   Û    

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\reporting\pdf_generator.py ===
"""
PDF Generator Module
Implements PDF report generation for fraud detection results
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import os
import io
import base64
from reportlab.lib.pagesizes import letter, A4
from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Table, TableStyle, Image, PageBreak
from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
from reportlab.lib.units import inch
from reportlab.lib import colors
from reportlab.lib.enums import TA_CENTER, TA_LEFT, TA_RIGHT
from reportlab.graphics.shapes import Drawing
from reportlab.graphics.charts.lineplots import LinePlot
from reportlab.graphics.charts.barcharts import VerticalBarChart
from reportlab.graphics.widgets import markers
import warnings
import logging
from typing import Dict, List, Tuple, Union

warnings.filterwarnings('ignore')
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class PDFGenerator:
    """
    Class for generating PDF reports for fraud detection results
    Implements professional audit report generation
    """
    
    def __init__(self, output_dir='../reports/generated'):
        """
        Initialize PDFGenerator
        
        Args:
            output_dir (str): Output directory for reports
        """
        self.output_dir = output_dir
        self.styles = getSampleStyleSheet()
        self.setup_custom_styles()
        
        # Create output directory if it doesn't exist
        os.makedirs(output_dir, exist_ok=True)
    
    def setup_custom_styles(self):
        """Setup custom styles for the report"""
        # Title style
        self.styles.add(ParagraphStyle(
            name='CustomTitle',
            parent=self.styles['Title'],
            fontSize=24,
            spaceAfter=30,
            alignment=TA_CENTER,
            textColor=colors.darkblue
        ))
        
        # Subtitle style
        self.styles.add(ParagraphStyle(
            name='CustomSubtitle',
            parent=self.styles['Heading1'],
            fontSize=16,
            spaceAfter=12,
            alignment=TA_CENTER,
            textColor=colors.darkblue
        ))
        
        # Section heading style
        self.styles.add(ParagraphStyle(
            name='SectionHeading',
            parent=self.styles['Heading2'],
            fontSize=14,
            spaceAfter=12,
            textColor=colors.darkblue
        ))
        
        # Subsection heading style
        self.styles.add(ParagraphStyle(
            name='SubsectionHeading',
            parent=self.styles['Heading3'],
            fontSize=12,
            spaceAfter=6,
            textColor=colors.darkblue
        ))
        
        # Body style
        self.styles.add(ParagraphStyle(
            name='Body',
            parent=self.styles['Normal'],
            fontSize=10,
            spaceAfter=6,
            alignment=TA_LEFT
        ))
        
        # Footer style
        self.styles.add(ParagraphStyle(
            name='Footer',
            parent=self.styles['Normal'],
            fontSize=8,
            alignment=TA_CENTER,
            textColor=colors.grey
        ))
    
    def generate_executive_summary(self, df, risk_scores, top_fraud, include_charts=True, include_recommendations=True):
        """
        Generate executive summary report
        
        Args:
            df (DataFrame): Transaction data
            risk_scores (DataFrame): Risk scores
            top_fraud (DataFrame): Top fraudulent transactions
            include_charts (bool): Whether to include charts
            include_recommendations (bool): Whether to include recommendations
            
        Returns:
            str: Path to generated PDF
        """
        try:
            # Create filename
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"executive_summary_{timestamp}.pdf"
            filepath = os.path.join(self.output_dir, filename)
            
            # Create document
            doc = SimpleDocTemplate(filepath, pagesize=A4)
            story = []
            
            # Add title
            story.append(Paragraph("Fraud Detection Executive Summary", self.styles['CustomTitle']))
            story.append(Spacer(1, 12))
            
            # Add subtitle with date
            report_date = datetime.now().strftime("%B %d, %Y")
            story.append(Paragraph(f"Report Date: {report_date}", self.styles['CustomSubtitle']))
            story.append(PageBreak())
            
            # Add summary statistics
            story.append(Paragraph("Summary Statistics", self.styles['SectionHeading']))
            
            # Calculate statistics
            total_transactions = len(df)
            fraud_transactions = risk_scores['is_fraud'].sum()
            fraud_percentage = (fraud_transactions / total_transactions) * 100
            total_amount = df['amount'].sum() if 'amount' in df.columns else 0
            fraud_amount = df.loc[risk_scores['is_fraud'], 'amount'].sum() if 'amount' in df.columns else 0
            
            # Create statistics table
            stats_data = [
                ['Metric', 'Value'],
                ['Total Transactions', f"{total_transactions:,}"],
                ['Fraudulent Transactions', f"{fraud_transactions:,}"],
                ['Fraud Percentage', f"{fraud_percentage:.2f}%"],
                ['Total Amount', f"${total_amount:,.2f}"],
                ['Fraud Amount', f"${fraud_amount:,.2f}"]
            ]
            
            stats_table = Table(stats_data, colWidths=[3*inch, 2*inch])
            stats_table.setStyle(TableStyle([
                ('BACKGROUND', (0, 0), (-1, 0), colors.darkblue),
                ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),
                ('ALIGN', (0, 0), (-1, -1), 'CENTER'),
                ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),
                ('FONTSIZE', (0, 0), (-1, 0), 12),
                ('BOTTOMPADDING', (0, 0), (-1, 0), 12),
                ('BACKGROUND', (0, 1), (-1, -1), colors.beige),
                ('GRID', (0, 0), (-1, -1), 1, colors.black)
            ]))
            
            story.append(stats_table)
            story.append(Spacer(1, 12))
            
            # Add risk distribution chart if requested
            if include_charts:
                story.append(Paragraph("Risk Distribution", self.styles['SectionHeading']))
                
                # Create risk distribution chart
                plt.figure(figsize=(8, 6))
                sns.histplot(risk_scores['risk_score'], bins=50, kde=True)
                plt.title('Distribution of Risk Scores')
                plt.xlabel('Risk Score')
                plt.ylabel('Frequency')
                
                # Convert plot to image
                img_buffer = io.BytesIO()
                plt.savefig(img_buffer, format='png', dpi=300, bbox_inches='tight')
                img_buffer.seek(0)
                
                # Add image to report
                risk_dist_img = Image(img_buffer, width=6*inch, height=4*inch)
                story.append(risk_dist_img)
                story.append(Spacer(1, 12))
                
                plt.close()
            
            # Add top fraudulent transactions
            story.append(Paragraph("Top Fraudulent Transactions", self.styles['SectionHeading']))
            
            # Create table for top fraud transactions
            if len(top_fraud) > 0:
                # Prepare data
                fraud_data = []
                fraud_data.append(['Transaction ID', 'Amount', 'Risk Score', 'Date'])
                
                for _, row in top_fraud.head(10).iterrows():
                    transaction_id = row.get('transaction_id', 'N/A')
                    amount = f"${row.get('amount', 0):,.2f}"
                    risk_score = f"{row.get('risk_score', 0):.3f}"
                    date = row.get('timestamp', 'N/A')
                    
                    if isinstance(date, pd.Timestamp):
                        date = date.strftime('%Y-%m-%d')
                    
                    fraud_data.append([transaction_id, amount, risk_score, date])
                
                fraud_table = Table(fraud_data, colWidths=[1.5*inch, 1*inch, 1*inch, 1.5*inch])
                fraud_table.setStyle(TableStyle([
                    ('BACKGROUND', (0, 0), (-1, 0), colors.darkblue),
                    ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),
                    ('ALIGN', (0, 0), (-1, -1), 'CENTER'),
                    ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),
                    ('FONTSIZE', (0, 0), (-1, 0), 10),
                    ('BOTTOMPADDING', (0, 0), (-1, 0), 12),
                    ('BACKGROUND', (0, 1), (-1, -1), colors.beige),
                    ('GRID', (0, 0), (-1, -1), 1, colors.black)
                ]))
                
                story.append(fraud_table)
                story.append(Spacer(1, 12))
            
            # Add recommendations if requested
            if include_recommendations:
                story.append(Paragraph("Recommendations", self.styles['SectionHeading']))
                
                recommendations = [
                    "1. Immediately review all high-risk transactions (risk score > 0.8).",
                    "2. Implement additional verification steps for transactions over $10,000.",
                    "3. Monitor patterns in fraudulent transactions to identify potential fraud rings.",
                    "4. Update fraud detection rules based on recent fraud patterns.",
                    "5. Conduct regular audits of the fraud detection system."
                ]
                
                for rec in recommendations:
                    story.append(Paragraph(rec, self.styles['Body']))
                
                story.append(Spacer(1, 12))
            
            # Add footer
            footer_text = f"Generated by Fraud Detection System on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
            story.append(Paragraph(footer_text, self.styles['Footer']))
            
            # Build PDF
            doc.build(story)
            
            logger.info(f"Executive summary report generated: {filepath}")
            return filepath
            
        except Exception as e:
            logger.error(f"Error generating executive summary: {str(e)}")
            raise
    
    def generate_detailed_fraud_analysis(self, df, risk_scores, top_fraud, explanations, 
                                        include_charts=True, include_explanations=True, 
                                        include_recommendations=True):
        """
        Generate detailed fraud analysis report
        
        Args:
            df (DataFrame): Transaction data
            risk_scores (DataFrame): Risk scores
            top_fraud (DataFrame): Top fraudulent transactions
            explanations (dict): Transaction explanations
            include_charts (bool): Whether to include charts
            include_explanations (bool): Whether to include explanations
            include_recommendations (bool): Whether to include recommendations
            
        Returns:
            str: Path to generated PDF
        """
        try:
            # Create filename
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"detailed_fraud_analysis_{timestamp}.pdf"
            filepath = os.path.join(self.output_dir, filename)
            
            # Create document
            doc = SimpleDocTemplate(filepath, pagesize=A4)
            story = []
            
            # Add title
            story.append(Paragraph("Detailed Fraud Analysis Report", self.styles['CustomTitle']))
            story.append(Spacer(1, 12))
            
            # Add subtitle with date
            report_date = datetime.now().strftime("%B %d, %Y")
            story.append(Paragraph(f"Report Date: {report_date}", self.styles['CustomSubtitle']))
            story.append(PageBreak())
            
            # Add table of contents
            story.append(Paragraph("Table of Contents", self.styles['SectionHeading']))
            
            toc_items = [
                "1. Executive Summary",
                "2. Methodology",
                "3. Analysis Results",
                "4. Detailed Transaction Analysis",
                "5. Risk Factors",
                "6. Recommendations",
                "7. Appendix"
            ]
            
            for item in toc_items:
                story.append(Paragraph(item, self.styles['Body']))
            
            story.append(PageBreak())
            
            # Add executive summary
            story.append(Paragraph("1. Executive Summary", self.styles['SectionHeading']))
            
            # Calculate statistics
            total_transactions = len(df)
            fraud_transactions = risk_scores['is_fraud'].sum()
            fraud_percentage = (fraud_transactions / total_transactions) * 100
            
            summary_text = f"""
            This report presents a detailed analysis of fraudulent transactions detected by the fraud detection system.
            The analysis identified {fraud_transactions:,} fraudulent transactions out of {total_transactions:,} total transactions,
            representing {fraud_percentage:.2f}% of all transactions. The total value of fraudulent transactions 
            amounts to ${df.loc[risk_scores['is_fraud'], 'amount'].sum():,.2f} if amount data is available.
            """
            
            story.append(Paragraph(summary_text, self.styles['Body']))
            story.append(Spacer(1, 12))
            
            # Add methodology
            story.append(Paragraph("2. Methodology", self.styles['SectionHeading']))
            
            methodology_text = """
            The fraud detection system employs a multi-layered approach combining unsupervised learning, 
            supervised learning, and rule-based methods to identify potentially fraudulent transactions. 
            The system analyzes various features including transaction amounts, frequency patterns, 
            geographic locations, temporal patterns, and behavioral characteristics to assign risk scores 
            to each transaction.
            """
            
            story.append(Paragraph(methodology_text, self.styles['Body']))
            story.append(Spacer(1, 12))
            
            # Add analysis results
            story.append(Paragraph("3. Analysis Results", self.styles['SectionHeading']))
            
            # Add risk distribution chart if requested
            if include_charts:
                story.append(Paragraph("3.1 Risk Distribution", self.styles['SubsectionHeading']))
                
                # Create risk distribution chart
                plt.figure(figsize=(8, 6))
                sns.histplot(risk_scores['risk_score'], bins=50, kde=True)
                plt.title('Distribution of Risk Scores')
                plt.xlabel('Risk Score')
                plt.ylabel('Frequency')
                
                # Convert plot to image
                img_buffer = io.BytesIO()
                plt.savefig(img_buffer, format='png', dpi=300, bbox_inches='tight')
                img_buffer.seek(0)
                
                # Add image to report
                risk_dist_img = Image(img_buffer, width=6*inch, height=4*inch)
                story.append(risk_dist_img)
                story.append(Spacer(1, 12))
                
                plt.close()
            
            # Add fraud by time chart if requested
            if include_charts and 'timestamp' in df.columns:
                story.append(Paragraph("3.2 Fraud by Time", self.styles['SubsectionHeading']))
                
                # Create fraud by time chart
                df_with_risk = df.copy()
                df_with_risk['is_fraud'] = risk_scores['is_fraud']
                df_with_risk['timestamp'] = pd.to_datetime(df_with_risk['timestamp'])
                df_with_risk['date'] = df_with_risk['timestamp'].dt.date
                
                fraud_by_date = df_with_risk.groupby('date')['is_fraud'].agg(['sum', 'count']).reset_index()
                fraud_by_date['fraud_rate'] = fraud_by_date['sum'] / fraud_by_date['count']
                
                plt.figure(figsize=(10, 6))
                plt.subplot(2, 1, 1)
                plt.plot(fraud_by_date['date'], fraud_by_date['sum'], marker='o')
                plt.title('Fraud Count by Date')
                plt.ylabel('Count')
                plt.xticks(rotation=45)
                
                plt.subplot(2, 1, 2)
                plt.plot(fraud_by_date['date'], fraud_by_date['fraud_rate'], marker='o', color='red')
                plt.title('Fraud Rate by Date')
                plt.ylabel('Rate')
                plt.xlabel('Date')
                plt.xticks(rotation=45)
                
                plt.tight_layout()
                
                # Convert plot to image
                img_buffer = io.BytesIO()
                plt.savefig(img_buffer, format='png', dpi=300, bbox_inches='tight')
                img_buffer.seek(0)
                
                # Add image to report
                fraud_time_img = Image(img_buffer, width=6*inch, height=5*inch)
                story.append(fraud_time_img)
                story.append(Spacer(1, 12))
                
                plt.close()
            
            story.append(PageBreak())
            
            # Add detailed transaction analysis
            story.append(Paragraph("4. Detailed Transaction Analysis", self.styles['SectionHeading']))
            
            # Add top fraudulent transactions
            story.append(Paragraph("4.1 Top Fraudulent Transactions", self.styles['SubsectionHeading']))
            
            # Create table for top fraud transactions
            if len(top_fraud) > 0:
                # Prepare data
                fraud_data = []
                fraud_data.append(['Transaction ID', 'Amount', 'Risk Score', 'Date', 'Risk Level'])
                
                for _, row in top_fraud.head(20).iterrows():
                    transaction_id = row.get('transaction_id', 'N/A')
                    amount = f"${row.get('amount', 0):,.2f}"
                    risk_score = f"{row.get('risk_score', 0):.3f}"
                    date = row.get('timestamp', 'N/A')
                    risk_level = row.get('risk_level', 'N/A')
                    
                    if isinstance(date, pd.Timestamp):
                        date = date.strftime('%Y-%m-%d')
                    
                    fraud_data.append([transaction_id, amount, risk_score, date, risk_level])
                
                fraud_table = Table(fraud_data, colWidths=[1.2*inch, 0.8*inch, 0.8*inch, 1*inch, 0.8*inch])
                fraud_table.setStyle(TableStyle([
                    ('BACKGROUND', (0, 0), (-1, 0), colors.darkblue),
                    ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),
                    ('ALIGN', (0, 0), (-1, -1), 'CENTER'),
                    ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),
                    ('FONTSIZE', (0, 0), (-1, 0), 8),
                    ('BOTTOMPADDING', (0, 0), (-1, 0), 12),
                    ('BACKGROUND', (0, 1), (-1, -1), colors.beige),
                    ('GRID', (0, 0), (-1, -1), 1, colors.black)
                ]))
                
                story.append(fraud_table)
                story.append(Spacer(1, 12))
            
            # Add detailed explanations if requested
            if include_explanations and explanations:
                story.append(Paragraph("4.2 Transaction Explanations", self.styles['SubsectionHeading']))
                
                # Add explanations for top 5 fraudulent transactions
                for i, (idx, explanation) in enumerate(list(explanations.items())[:5]):
                    story.append(Paragraph(f"Transaction {i+1}: {explanation.get('transaction_id', idx)}", 
                                          self.styles['SubsectionHeading']))
                    
                    # Add explanation text
                    explanation_text = explanation.get('text_explanation', 'No explanation available.')
                    story.append(Paragraph(explanation_text, self.styles['Body']))
                    
                    # Add top factors
                    if explanation.get('top_factors'):
                        story.append(Paragraph("Top Contributing Factors:", self.styles['Body']))
                        for factor, contribution in explanation['top_factors'][:3]:
                            story.append(Paragraph(f"- {factor}: {contribution:.3f}", self.styles['Body']))
                    
                    # Add rule violations
                    if explanation.get('rule_violations'):
                        story.append(Paragraph("Rule Violations:", self.styles['Body']))
                        for rule in explanation['rule_violations']:
                            story.append(Paragraph(f"- {rule}", self.styles['Body']))
                    
                    story.append(Spacer(1, 12))
            
            story.append(PageBreak())
            
            # Add risk factors
            story.append(Paragraph("5. Risk Factors", self.styles['SectionHeading']))
            
            # Add common risk factors
            risk_factors = [
                "1. High Transaction Amount: Transactions significantly above average for the sender or receiver.",
                "2. Unusual Geographic Patterns: Transactions from or to high-risk locations.",
                "3. Temporal Anomalies: Transactions at unusual times or in rapid succession.",
                "4. Behavioral Deviations: Transactions that deviate from established patterns.",
                "5. Network Anomalies: Transactions involving suspicious networks of entities."
            ]
            
            for factor in risk_factors:
                story.append(Paragraph(factor, self.styles['Body']))
            
            story.append(Spacer(1, 12))
            
            # Add recommendations
            story.append(Paragraph("6. Recommendations", self.styles['SectionHeading']))
            
            recommendations = [
                "1. Immediate Actions:",
                "   - Block all high-risk transactions (risk score > 0.9).",
                "   - Contact customers associated with high-risk transactions for verification.",
                "   - Flag accounts involved in multiple suspicious transactions for review.",
                "",
                "2. System Improvements:",
                "   - Update fraud detection rules based on recent fraud patterns.",
                "   - Implement additional verification steps for high-value transactions.",
                "   - Enhance monitoring of cross-border transactions.",
                "",
                "3. Process Enhancements:",
                "   - Conduct regular audits of the fraud detection system.",
                "   - Provide training to staff on identifying fraud indicators.",
                "   - Establish clear procedures for handling suspected fraud.",
                "",
                "4. Long-term Strategies:",
                "   - Develop machine learning models to adapt to evolving fraud patterns.",
                "   - Implement real-time fraud detection capabilities.",
                "   - Collaborate with industry partners to share fraud intelligence."
            ]
            
            for rec in recommendations:
                story.append(Paragraph(rec, self.styles['Body']))
            
            story.append(Spacer(1, 12))
            
            # Add appendix
            story.append(Paragraph("7. Appendix", self.styles['SectionHeading']))
            
            # Add methodology details
            story.append(Paragraph("7.1 Methodology Details", self.styles['SubsectionHeading']))
            
            methodology_details = """
            The fraud detection system utilizes a combination of the following techniques:
            
            - Unsupervised Learning: Isolation Forest, Local Outlier Factor, Autoencoders
            - Supervised Learning: Random Forest, XGBoost, Neural Networks
            - Rule-based Detection: Configurable rules for known fraud patterns
            - Feature Engineering: Statistical, graph-based, NLP, and time-series features
            
            Each transaction is assigned a risk score between 0 and 1, with higher scores indicating 
            greater likelihood of fraud. Transactions with scores above the threshold (typically 0.5) 
            are flagged for review.
            """
            
            story.append(Paragraph(methodology_details, self.styles['Body']))
            story.append(Spacer(1, 12))
            
            # Add footer
            footer_text = f"Generated by Fraud Detection System on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
            story.append(Paragraph(footer_text, self.styles['Footer']))
            
            # Build PDF
            doc.build(story)
            
            logger.info(f"Detailed fraud analysis report generated: {filepath}")
            return filepath
            
        except Exception as e:
            logger.error(f"Error generating detailed fraud analysis: {str(e)}")
            raise
    
    def generate_technical_report(self, df, risk_scores, model_results, include_charts=True):
        """
        Generate technical report
        
        Args:
            df (DataFrame): Transaction data
            risk_scores (DataFrame): Risk scores
            model_results (dict): Model results
            include_charts (bool): Whether to include charts
            
        Returns:
            str: Path to generated PDF
        """
        try:
            # Create filename
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"technical_report_{timestamp}.pdf"
            filepath = os.path.join(self.output_dir, filename)
            
            # Create document
            doc = SimpleDocTemplate(filepath, pagesize=A4)
            story = []
            
            # Add title
            story.append(Paragraph("Fraud Detection Technical Report", self.styles['CustomTitle']))
            story.append(Spacer(1, 12))
            
            # Add subtitle with date
            report_date = datetime.now().strftime("%B %d, %Y")
            story.append(Paragraph(f"Report Date: {report_date}", self.styles['CustomSubtitle']))
            story.append(PageBreak())
            
            # Add system overview
            story.append(Paragraph("1. System Overview", self.styles['SectionHeading']))
            
            overview_text = """
            The fraud detection system is designed to identify potentially fraudulent transactions 
            using a combination of machine learning techniques and rule-based methods. The system 
            processes transaction data in real-time, extracting various features and applying multiple 
            detection algorithms to generate risk scores for each transaction.
            """
            
            story.append(Paragraph(overview_text, self.styles['Body']))
            story.append(Spacer(1, 12))
            
            # Add data processing
            story.append(Paragraph("2. Data Processing", self.styles['SectionHeading']))
            
            data_processing_text = """
            The system processes transaction data through the following stages:
            
            1. Data Ingestion: Transactions are loaded from various sources (CSV, Excel, databases).
            2. Data Preprocessing: Missing values are handled, data types are converted, and basic 
               validation is performed.
            3. Feature Engineering: Statistical, graph-based, NLP, and time-series features are extracted.
            4. Model Application: Multiple models are applied to generate risk scores.
            5. Risk Aggregation: Scores from different models are combined to produce a final risk score.
            """
            
            story.append(Paragraph(data_processing_text, self.styles['Body']))
            story.append(Spacer(1, 12))
            
            # Add model performance
            story.append(Paragraph("3. Model Performance", self.styles['SectionHeading']))
            
            # Add unsupervised model performance
            if 'unsupervised' in model_results:
                story.append(Paragraph("3.1 Unsupervised Models", self.styles['SubsectionHeading']))
                
                unsupervised_text = f"""
                The system employs {len(model_results['unsupervised'])} unsupervised learning models:
                
                """
                
                for model_name in model_results['unsupervised']:
                    unsupervised_text += f"- {model_name}\n"
                
                story.append(Paragraph(unsupervised_text, self.styles['Body']))
                story.append(Spacer(1, 6))
            
            # Add supervised model performance
            if 'supervised' in model_results:
                story.append(Paragraph("3.2 Supervised Models", self.styles['SubsectionHeading']))
                
                supervised_text = f"""
                The system employs {len(model_results['supervised'])} supervised learning models:
                
                """
                
                for model_name in model_results['supervised']:
                    if 'performance' in model_results['supervised'][model_name]:
                        perf = model_results['supervised'][model_name]['performance']
                        auc = perf.get('roc_auc', 0)
                        supervised_text += f"- {model_name}: AUC = {auc:.3f}\n"
                    else:
                        supervised_text += f"- {model_name}\n"
                
                story.append(Paragraph(supervised_text, self.styles['Body']))
                story.append(Spacer(1, 6))
            
            # Add rule-based model performance
            if 'rule' in model_results:
                story.append(Paragraph("3.3 Rule-based Models", self.styles['SubsectionHeading']))
                
                rule_text = f"""
                The system employs {len(model_results['rule'].get('rules', {}))} rule-based detectors:
                
                """
                
                for rule_name in model_results['rule'].get('rules', {}):
                    rule_text += f"- {rule_name}\n"
                
                story.append(Paragraph(rule_text, self.styles['Body']))
                story.append(Spacer(1, 6))
            
            # Add feature importance if available
            if 'supervised' in model_results:
                story.append(Paragraph("3.4 Feature Importance", self.styles['SubsectionHeading']))
                
                # Get feature importance from Random Forest if available
                if 'random_forest' in model_results['supervised']:
                    rf_data = model_results['supervised']['random_forest']
                    if 'feature_importance' in rf_data:
                        importance_df = rf_data['feature_importance']
                        
                        # Create table for top features
                        feature_data = []
                        feature_data.append(['Feature', 'Importance'])
                        
                        for _, row in importance_df.head(10).iterrows():
                            feature_data.append([row['feature'], f"{row['importance']:.3f}"])
                        
                        feature_table = Table(feature_data, colWidths=[3*inch, 1.5*inch])
                        feature_table.setStyle(TableStyle([
                            ('BACKGROUND', (0, 0), (-1, 0), colors.darkblue),
                            ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),
                            ('ALIGN', (0, 0), (-1, -1), 'CENTER'),
                            ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),
                            ('FONTSIZE', (0, 0), (-1, 0), 10),
                            ('BOTTOMPADDING', (0, 0), (-1, 0), 12),
                            ('BACKGROUND', (0, 1), (-1, -1), colors.beige),
                            ('GRID', (0, 0), (-1, -1), 1, colors.black)
                        ]))
                        
                        story.append(feature_table)
                        story.append(Spacer(1, 6))
            
            # Add system architecture
            story.append(Paragraph("4. System Architecture", self.styles['SectionHeading']))
            
            architecture_text = """
            The fraud detection system is built with a modular architecture consisting of the following components:
            
            1. Data Ingestion Layer: Handles data loading from various sources.
            2. Feature Engineering Layer: Extracts features for fraud detection.
            3. Model Layer: Applies various detection algorithms.
            4. Risk Aggregation Layer: Combines model outputs.
            5. Reporting Layer: Generates reports and visualizations.
            6. API Layer: Provides interfaces for external systems.
            """
            
            story.append(Paragraph(architecture_text, self.styles['Body']))
            story.append(Spacer(1, 12))
            
            # Add performance metrics
            story.append(Paragraph("5. Performance Metrics", self.styles['SectionHeading']))
            
            # Calculate performance metrics
            total_transactions = len(df)
            fraud_transactions = risk_scores['is_fraud'].sum()
            fraud_percentage = (fraud_transactions / total_transactions) * 100
            
            metrics_data = [
                ['Metric', 'Value'],
                ['Total Transactions', f"{total_transactions:,}"],
                ['Fraudulent Transactions', f"{fraud_transactions:,}"],
                ['Fraud Percentage', f"{fraud_percentage:.2f}%"],
                ['Processing Time', f"{len(df) * 0.001:.2f} seconds (estimated)"]
            ]
            
            metrics_table = Table(metrics_data, colWidths=[3*inch, 2*inch])
            metrics_table.setStyle(TableStyle([
                ('BACKGROUND', (0, 0), (-1, 0), colors.darkblue),
                ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),
                ('ALIGN', (0, 0), (-1, -1), 'CENTER'),
                ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),
                ('FONTSIZE', (0, 0), (-1, 0), 12),
                ('BOTTOMPADDING', (0, 0), (-1, 0), 12),
                ('BACKGROUND', (0, 1), (-1, -1), colors.beige),
                ('GRID', (0, 0), (-1, -1), 1, colors.black)
            ]))
            
            story.append(metrics_table)
            story.append(Spacer(1, 12))
            
            # Add footer
            footer_text = f"Generated by Fraud Detection System on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
            story.append(Paragraph(footer_text, self.styles['Footer']))
            
            # Build PDF
            doc.build(story)
            
            logger.info(f"Technical report generated: {filepath}")
            return filepath
            
        except Exception as e:
            logger.error(f"Error generating technical report: {str(e)}")
            raise
    
    def generate_custom_report(self, df, risk_scores, top_fraud, explanations, model_results, 
                             include_charts=True, include_explanations=True, 
                             include_recommendations=True):
        """
        Generate custom report with user-specified sections
        
        Args:
            df (DataFrame): Transaction data
            risk_scores (DataFrame): Risk scores
            top_fraud (DataFrame): Top fraudulent transactions
            explanations (dict): Transaction explanations
            model_results (dict): Model results
            include_charts (bool): Whether to include charts
            include_explanations (bool): Whether to include explanations
            include_recommendations (bool): Whether to include recommendations
            
        Returns:
            str: Path to generated PDF
        """
        try:
            # Create filename
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"custom_report_{timestamp}.pdf"
            filepath = os.path.join(self.output_dir, filename)
            
            # Create document
            doc = SimpleDocTemplate(filepath, pagesize=A4)
            story = []
            
            # Add title
            story.append(Paragraph("Custom Fraud Detection Report", self.styles['CustomTitle']))
            story.append(Spacer(1, 12))
            
            # Add subtitle with date
            report_date = datetime.now().strftime("%B %d, %Y")
            story.append(Paragraph(f"Report Date: {report_date}", self.styles['CustomSubtitle']))
            story.append(PageBreak())
            
            # Add summary statistics
            story.append(Paragraph("Summary Statistics", self.styles['SectionHeading']))
            
            # Calculate statistics
            total_transactions = len(df)
            fraud_transactions = risk_scores['is_fraud'].sum()
            fraud_percentage = (fraud_transactions / total_transactions) * 100
            
            stats_text = f"""
            Total Transactions: {total_transactions:,}
            Fraudulent Transactions: {fraud_transactions:,}
            Fraud Percentage: {fraud_percentage:.2f}%
            """
            
            story.append(Paragraph(stats_text, self.styles['Body']))
            story.append(Spacer(1, 12))
            
            # Add risk distribution chart if requested
            if include_charts:
                story.append(Paragraph("Risk Distribution", self.styles['SectionHeading']))
                
                # Create risk distribution chart
                plt.figure(figsize=(8, 6))
                sns.histplot(risk_scores['risk_score'], bins=50, kde=True)
                plt.title('Distribution of Risk Scores')
                plt.xlabel('Risk Score')
                plt.ylabel('Frequency')
                
                # Convert plot to image
                img_buffer = io.BytesIO()
                plt.savefig(img_buffer, format='png', dpi=300, bbox_inches='tight')
                img_buffer.seek(0)
                
                # Add image to report
                risk_dist_img = Image(img_buffer, width=6*inch, height=4*inch)
                story.append(risk_dist_img)
                story.append(Spacer(1, 12))
                
                plt.close()
            
            # Add top fraudulent transactions
            story.append(Paragraph("Top Fraudulent Transactions", self.styles['SectionHeading']))
            
            # Create table for top fraud transactions
            if len(top_fraud) > 0:
                # Prepare data
                fraud_data = []
                fraud_data.append(['Transaction ID', 'Amount', 'Risk Score'])
                
                for _, row in top_fraud.head(10).iterrows():
                    transaction_id = row.get('transaction_id', 'N/A')
                    amount = f"${row.get('amount', 0):,.2f}"
                    risk_score = f"{row.get('risk_score', 0):.3f}"
                    
                    fraud_data.append([transaction_id, amount, risk_score])
                
                fraud_table = Table(fraud_data, colWidths=[2*inch, 1.5*inch, 1.5*inch])
                fraud_table.setStyle(TableStyle([
                    ('BACKGROUND', (0, 0), (-1, 0), colors.darkblue),
                    ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),
                    ('ALIGN', (0, 0), (-1, -1), 'CENTER'),
                    ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),
                    ('FONTSIZE', (0, 0), (-1, 0), 12),
                    ('BOTTOMPADDING', (0, 0), (-1, 0), 12),
                    ('BACKGROUND', (0, 1), (-1, -1), colors.beige),
                    ('GRID', (0, 0), (-1, -1), 1, colors.black)
                ]))
                
                story.append(fraud_table)
                story.append(Spacer(1, 12))
            
            # Add detailed explanations if requested
            if include_explanations and explanations:
                story.append(Paragraph("Transaction Explanations", self.styles['SectionHeading']))
                
                # Add explanations for top 3 fraudulent transactions
                for i, (idx, explanation) in enumerate(list(explanations.items())[:3]):
                    story.append(Paragraph(f"Transaction {i+1}", self.styles['SubsectionHeading']))
                    
                    # Add explanation text
                    explanation_text = explanation.get('text_explanation', 'No explanation available.')
                    story.append(Paragraph(explanation_text, self.styles['Body']))
                    story.append(Spacer(1, 6))
            
            # Add recommendations if requested
            if include_recommendations:
                story.append(Paragraph("Recommendations", self.styles['SectionHeading']))
                
                recommendations = [
                    "1. Review all high-risk transactions immediately.",
                    "2. Implement additional verification for large transactions.",
                    "3. Monitor patterns in fraudulent activity.",
                    "4. Update detection rules regularly.",
                    "5. Conduct periodic system audits."
                ]
                
                for rec in recommendations:
                    story.append(Paragraph(rec, self.styles['Body']))
                
                story.append(Spacer(1, 12))
            
            # Add footer
            footer_text = f"Generated by Fraud Detection System on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
            story.append(Paragraph(footer_text, self.styles['Footer']))
            
            # Build PDF
            doc.build(story)
            
            logger.info(f"Custom report generated: {filepath}")
            return filepath
            
        except Exception as e:
            logger.error(f"Error generating custom report: {str(e)}")
            raise

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\reporting\__init__.py ===

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\reporting\.ipynb_checkpoints\pdf_generator-checkpoint.py ===
"""
PDF Generator Module
Implements PDF report generation for fraud detection results
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import os
import io
import base64
from reportlab.lib.pagesizes import letter, A4
from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Table, TableStyle, Image, PageBreak
from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
from reportlab.lib.units import inch
from reportlab.lib import colors
from reportlab.lib.enums import TA_CENTER, TA_LEFT, TA_RIGHT
from reportlab.graphics.shapes import Drawing
from reportlab.graphics.charts.lineplots import LinePlot
from reportlab.graphics.charts.barcharts import VerticalBarChart
from reportlab.graphics.widgets import markers
import warnings
import logging
from typing import Dict, List, Tuple, Union

warnings.filterwarnings('ignore')
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class PDFGenerator:
    """
    Class for generating PDF reports for fraud detection results
    Implements professional audit report generation
    """
    
    def __init__(self, output_dir='../reports/generated'):
        """
        Initialize PDFGenerator
        
        Args:
            output_dir (str): Output directory for reports
        """
        self.output_dir = output_dir
        self.styles = getSampleStyleSheet()
        self.setup_custom_styles()
        
        # Create output directory if it doesn't exist
        os.makedirs(output_dir, exist_ok=True)
    
    def setup_custom_styles(self):
        """Setup custom styles for the report"""
        # Title style
        self.styles.add(ParagraphStyle(
            name='CustomTitle',
            parent=self.styles['Title'],
            fontSize=24,
            spaceAfter=30,
            alignment=TA_CENTER,
            textColor=colors.darkblue
        ))
        
        # Subtitle style
        self.styles.add(ParagraphStyle(
            name='CustomSubtitle',
            parent=self.styles['Heading1'],
            fontSize=16,
            spaceAfter=12,
            alignment=TA_CENTER,
            textColor=colors.darkblue
        ))
        
        # Section heading style
        self.styles.add(ParagraphStyle(
            name='SectionHeading',
            parent=self.styles['Heading2'],
            fontSize=14,
            spaceAfter=12,
            textColor=colors.darkblue
        ))
        
        # Subsection heading style
        self.styles.add(ParagraphStyle(
            name='SubsectionHeading',
            parent=self.styles['Heading3'],
            fontSize=12,
            spaceAfter=6,
            textColor=colors.darkblue
        ))
        
        # Body style
        self.styles.add(ParagraphStyle(
            name='Body',
            parent=self.styles['Normal'],
            fontSize=10,
            spaceAfter=6,
            alignment=TA_LEFT
        ))
        
        # Footer style
        self.styles.add(ParagraphStyle(
            name='Footer',
            parent=self.styles['Normal'],
            fontSize=8,
            alignment=TA_CENTER,
            textColor=colors.grey
        ))
    
    def generate_executive_summary(self, df, risk_scores, top_fraud, include_charts=True, include_recommendations=True):
        """
        Generate executive summary report
        
        Args:
            df (DataFrame): Transaction data
            risk_scores (DataFrame): Risk scores
            top_fraud (DataFrame): Top fraudulent transactions
            include_charts (bool): Whether to include charts
            include_recommendations (bool): Whether to include recommendations
            
        Returns:
            str: Path to generated PDF
        """
        try:
            # Create filename
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"executive_summary_{timestamp}.pdf"
            filepath = os.path.join(self.output_dir, filename)
            
            # Create document
            doc = SimpleDocTemplate(filepath, pagesize=A4)
            story = []
            
            # Add title
            story.append(Paragraph("Fraud Detection Executive Summary", self.styles['CustomTitle']))
            story.append(Spacer(1, 12))
            
            # Add subtitle with date
            report_date = datetime.now().strftime("%B %d, %Y")
            story.append(Paragraph(f"Report Date: {report_date}", self.styles['CustomSubtitle']))
            story.append(PageBreak())
            
            # Add summary statistics
            story.append(Paragraph("Summary Statistics", self.styles['SectionHeading']))
            
            # Calculate statistics
            total_transactions = len(df)
            fraud_transactions = risk_scores['is_fraud'].sum()
            fraud_percentage = (fraud_transactions / total_transactions) * 100
            total_amount = df['amount'].sum() if 'amount' in df.columns else 0
            fraud_amount = df.loc[risk_scores['is_fraud'], 'amount'].sum() if 'amount' in df.columns else 0
            
            # Create statistics table
            stats_data = [
                ['Metric', 'Value'],
                ['Total Transactions', f"{total_transactions:,}"],
                ['Fraudulent Transactions', f"{fraud_transactions:,}"],
                ['Fraud Percentage', f"{fraud_percentage:.2f}%"],
                ['Total Amount', f"${total_amount:,.2f}"],
                ['Fraud Amount', f"${fraud_amount:,.2f}"]
            ]
            
            stats_table = Table(stats_data, colWidths=[3*inch, 2*inch])
            stats_table.setStyle(TableStyle([
                ('BACKGROUND', (0, 0), (-1, 0), colors.darkblue),
                ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),
                ('ALIGN', (0, 0), (-1, -1), 'CENTER'),
                ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),
                ('FONTSIZE', (0, 0), (-1, 0), 12),
                ('BOTTOMPADDING', (0, 0), (-1, 0), 12),
                ('BACKGROUND', (0, 1), (-1, -1), colors.beige),
                ('GRID', (0, 0), (-1, -1), 1, colors.black)
            ]))
            
            story.append(stats_table)
            story.append(Spacer(1, 12))
            
            # Add risk distribution chart if requested
            if include_charts:
                story.append(Paragraph("Risk Distribution", self.styles['SectionHeading']))
                
                # Create risk distribution chart
                plt.figure(figsize=(8, 6))
                sns.histplot(risk_scores['risk_score'], bins=50, kde=True)
                plt.title('Distribution of Risk Scores')
                plt.xlabel('Risk Score')
                plt.ylabel('Frequency')
                
                # Convert plot to image
                img_buffer = io.BytesIO()
                plt.savefig(img_buffer, format='png', dpi=300, bbox_inches='tight')
                img_buffer.seek(0)
                
                # Add image to report
                risk_dist_img = Image(img_buffer, width=6*inch, height=4*inch)
                story.append(risk_dist_img)
                story.append(Spacer(1, 12))
                
                plt.close()
            
            # Add top fraudulent transactions
            story.append(Paragraph("Top Fraudulent Transactions", self.styles['SectionHeading']))
            
            # Create table for top fraud transactions
            if len(top_fraud) > 0:
                # Prepare data
                fraud_data = []
                fraud_data.append(['Transaction ID', 'Amount', 'Risk Score', 'Date'])
                
                for _, row in top_fraud.head(10).iterrows():
                    transaction_id = row.get('transaction_id', 'N/A')
                    amount = f"${row.get('amount', 0):,.2f}"
                    risk_score = f"{row.get('risk_score', 0):.3f}"
                    date = row.get('timestamp', 'N/A')
                    
                    if isinstance(date, pd.Timestamp):
                        date = date.strftime('%Y-%m-%d')
                    
                    fraud_data.append([transaction_id, amount, risk_score, date])
                
                fraud_table = Table(fraud_data, colWidths=[1.5*inch, 1*inch, 1*inch, 1.5*inch])
                fraud_table.setStyle(TableStyle([
                    ('BACKGROUND', (0, 0), (-1, 0), colors.darkblue),
                    ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),
                    ('ALIGN', (0, 0), (-1, -1), 'CENTER'),
                    ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),
                    ('FONTSIZE', (0, 0), (-1, 0), 10),
                    ('BOTTOMPADDING', (0, 0), (-1, 0), 12),
                    ('BACKGROUND', (0, 1), (-1, -1), colors.beige),
                    ('GRID', (0, 0), (-1, -1), 1, colors.black)
                ]))
                
                story.append(fraud_table)
                story.append(Spacer(1, 12))
            
            # Add recommendations if requested
            if include_recommendations:
                story.append(Paragraph("Recommendations", self.styles['SectionHeading']))
                
                recommendations = [
                    "1. Immediately review all high-risk transactions (risk score > 0.8).",
                    "2. Implement additional verification steps for transactions over $10,000.",
                    "3. Monitor patterns in fraudulent transactions to identify potential fraud rings.",
                    "4. Update fraud detection rules based on recent fraud patterns.",
                    "5. Conduct regular audits of the fraud detection system."
                ]
                
                for rec in recommendations:
                    story.append(Paragraph(rec, self.styles['Body']))
                
                story.append(Spacer(1, 12))
            
            # Add footer
            footer_text = f"Generated by Fraud Detection System on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
            story.append(Paragraph(footer_text, self.styles['Footer']))
            
            # Build PDF
            doc.build(story)
            
            logger.info(f"Executive summary report generated: {filepath}")
            return filepath
            
        except Exception as e:
            logger.error(f"Error generating executive summary: {str(e)}")
            raise
    
    def generate_detailed_fraud_analysis(self, df, risk_scores, top_fraud, explanations, 
                                        include_charts=True, include_explanations=True, 
                                        include_recommendations=True):
        """
        Generate detailed fraud analysis report
        
        Args:
            df (DataFrame): Transaction data
            risk_scores (DataFrame): Risk scores
            top_fraud (DataFrame): Top fraudulent transactions
            explanations (dict): Transaction explanations
            include_charts (bool): Whether to include charts
            include_explanations (bool): Whether to include explanations
            include_recommendations (bool): Whether to include recommendations
            
        Returns:
            str: Path to generated PDF
        """
        try:
            # Create filename
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"detailed_fraud_analysis_{timestamp}.pdf"
            filepath = os.path.join(self.output_dir, filename)
            
            # Create document
            doc = SimpleDocTemplate(filepath, pagesize=A4)
            story = []
            
            # Add title
            story.append(Paragraph("Detailed Fraud Analysis Report", self.styles['CustomTitle']))
            story.append(Spacer(1, 12))
            
            # Add subtitle with date
            report_date = datetime.now().strftime("%B %d, %Y")
            story.append(Paragraph(f"Report Date: {report_date}", self.styles['CustomSubtitle']))
            story.append(PageBreak())
            
            # Add table of contents
            story.append(Paragraph("Table of Contents", self.styles['SectionHeading']))
            
            toc_items = [
                "1. Executive Summary",
                "2. Methodology",
                "3. Analysis Results",
                "4. Detailed Transaction Analysis",
                "5. Risk Factors",
                "6. Recommendations",
                "7. Appendix"
            ]
            
            for item in toc_items:
                story.append(Paragraph(item, self.styles['Body']))
            
            story.append(PageBreak())
            
            # Add executive summary
            story.append(Paragraph("1. Executive Summary", self.styles['SectionHeading']))
            
            # Calculate statistics
            total_transactions = len(df)
            fraud_transactions = risk_scores['is_fraud'].sum()
            fraud_percentage = (fraud_transactions / total_transactions) * 100
            
            summary_text = f"""
            This report presents a detailed analysis of fraudulent transactions detected by the fraud detection system.
            The analysis identified {fraud_transactions:,} fraudulent transactions out of {total_transactions:,} total transactions,
            representing {fraud_percentage:.2f}% of all transactions. The total value of fraudulent transactions 
            amounts to ${df.loc[risk_scores['is_fraud'], 'amount'].sum():,.2f} if amount data is available.
            """
            
            story.append(Paragraph(summary_text, self.styles['Body']))
            story.append(Spacer(1, 12))
            
            # Add methodology
            story.append(Paragraph("2. Methodology", self.styles['SectionHeading']))
            
            methodology_text = """
            The fraud detection system employs a multi-layered approach combining unsupervised learning, 
            supervised learning, and rule-based methods to identify potentially fraudulent transactions. 
            The system analyzes various features including transaction amounts, frequency patterns, 
            geographic locations, temporal patterns, and behavioral characteristics to assign risk scores 
            to each transaction.
            """
            
            story.append(Paragraph(methodology_text, self.styles['Body']))
            story.append(Spacer(1, 12))
            
            # Add analysis results
            story.append(Paragraph("3. Analysis Results", self.styles['SectionHeading']))
            
            # Add risk distribution chart if requested
            if include_charts:
                story.append(Paragraph("3.1 Risk Distribution", self.styles['SubsectionHeading']))
                
                # Create risk distribution chart
                plt.figure(figsize=(8, 6))
                sns.histplot(risk_scores['risk_score'], bins=50, kde=True)
                plt.title('Distribution of Risk Scores')
                plt.xlabel('Risk Score')
                plt.ylabel('Frequency')
                
                # Convert plot to image
                img_buffer = io.BytesIO()
                plt.savefig(img_buffer, format='png', dpi=300, bbox_inches='tight')
                img_buffer.seek(0)
                
                # Add image to report
                risk_dist_img = Image(img_buffer, width=6*inch, height=4*inch)
                story.append(risk_dist_img)
                story.append(Spacer(1, 12))
                
                plt.close()
            
            # Add fraud by time chart if requested
            if include_charts and 'timestamp' in df.columns:
                story.append(Paragraph("3.2 Fraud by Time", self.styles['SubsectionHeading']))
                
                # Create fraud by time chart
                df_with_risk = df.copy()
                df_with_risk['is_fraud'] = risk_scores['is_fraud']
                df_with_risk['timestamp'] = pd.to_datetime(df_with_risk['timestamp'])
                df_with_risk['date'] = df_with_risk['timestamp'].dt.date
                
                fraud_by_date = df_with_risk.groupby('date')['is_fraud'].agg(['sum', 'count']).reset_index()
                fraud_by_date['fraud_rate'] = fraud_by_date['sum'] / fraud_by_date['count']
                
                plt.figure(figsize=(10, 6))
                plt.subplot(2, 1, 1)
                plt.plot(fraud_by_date['date'], fraud_by_date['sum'], marker='o')
                plt.title('Fraud Count by Date')
                plt.ylabel('Count')
                plt.xticks(rotation=45)
                
                plt.subplot(2, 1, 2)
                plt.plot(fraud_by_date['date'], fraud_by_date['fraud_rate'], marker='o', color='red')
                plt.title('Fraud Rate by Date')
                plt.ylabel('Rate')
                plt.xlabel('Date')
                plt.xticks(rotation=45)
                
                plt.tight_layout()
                
                # Convert plot to image
                img_buffer = io.BytesIO()
                plt.savefig(img_buffer, format='png', dpi=300, bbox_inches='tight')
                img_buffer.seek(0)
                
                # Add image to report
                fraud_time_img = Image(img_buffer, width=6*inch, height=5*inch)
                story.append(fraud_time_img)
                story.append(Spacer(1, 12))
                
                plt.close()
            
            story.append(PageBreak())
            
            # Add detailed transaction analysis
            story.append(Paragraph("4. Detailed Transaction Analysis", self.styles['SectionHeading']))
            
            # Add top fraudulent transactions
            story.append(Paragraph("4.1 Top Fraudulent Transactions", self.styles['SubsectionHeading']))
            
            # Create table for top fraud transactions
            if len(top_fraud) > 0:
                # Prepare data
                fraud_data = []
                fraud_data.append(['Transaction ID', 'Amount', 'Risk Score', 'Date', 'Risk Level'])
                
                for _, row in top_fraud.head(20).iterrows():
                    transaction_id = row.get('transaction_id', 'N/A')
                    amount = f"${row.get('amount', 0):,.2f}"
                    risk_score = f"{row.get('risk_score', 0):.3f}"
                    date = row.get('timestamp', 'N/A')
                    risk_level = row.get('risk_level', 'N/A')
                    
                    if isinstance(date, pd.Timestamp):
                        date = date.strftime('%Y-%m-%d')
                    
                    fraud_data.append([transaction_id, amount, risk_score, date, risk_level])
                
                fraud_table = Table(fraud_data, colWidths=[1.2*inch, 0.8*inch, 0.8*inch, 1*inch, 0.8*inch])
                fraud_table.setStyle(TableStyle([
                    ('BACKGROUND', (0, 0), (-1, 0), colors.darkblue),
                    ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),
                    ('ALIGN', (0, 0), (-1, -1), 'CENTER'),
                    ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),
                    ('FONTSIZE', (0, 0), (-1, 0), 8),
                    ('BOTTOMPADDING', (0, 0), (-1, 0), 12),
                    ('BACKGROUND', (0, 1), (-1, -1), colors.beige),
                    ('GRID', (0, 0), (-1, -1), 1, colors.black)
                ]))
                
                story.append(fraud_table)
                story.append(Spacer(1, 12))
            
            # Add detailed explanations if requested
            if include_explanations and explanations:
                story.append(Paragraph("4.2 Transaction Explanations", self.styles['SubsectionHeading']))
                
                # Add explanations for top 5 fraudulent transactions
                for i, (idx, explanation) in enumerate(list(explanations.items())[:5]):
                    story.append(Paragraph(f"Transaction {i+1}: {explanation.get('transaction_id', idx)}", 
                                          self.styles['SubsectionHeading']))
                    
                    # Add explanation text
                    explanation_text = explanation.get('text_explanation', 'No explanation available.')
                    story.append(Paragraph(explanation_text, self.styles['Body']))
                    
                    # Add top factors
                    if explanation.get('top_factors'):
                        story.append(Paragraph("Top Contributing Factors:", self.styles['Body']))
                        for factor, contribution in explanation['top_factors'][:3]:
                            story.append(Paragraph(f"- {factor}: {contribution:.3f}", self.styles['Body']))
                    
                    # Add rule violations
                    if explanation.get('rule_violations'):
                        story.append(Paragraph("Rule Violations:", self.styles['Body']))
                        for rule in explanation['rule_violations']:
                            story.append(Paragraph(f"- {rule}", self.styles['Body']))
                    
                    story.append(Spacer(1, 12))
            
            story.append(PageBreak())
            
            # Add risk factors
            story.append(Paragraph("5. Risk Factors", self.styles['SectionHeading']))
            
            # Add common risk factors
            risk_factors = [
                "1. High Transaction Amount: Transactions significantly above average for the sender or receiver.",
                "2. Unusual Geographic Patterns: Transactions from or to high-risk locations.",
                "3. Temporal Anomalies: Transactions at unusual times or in rapid succession.",
                "4. Behavioral Deviations: Transactions that deviate from established patterns.",
                "5. Network Anomalies: Transactions involving suspicious networks of entities."
            ]
            
            for factor in risk_factors:
                story.append(Paragraph(factor, self.styles['Body']))
            
            story.append(Spacer(1, 12))
            
            # Add recommendations
            story.append(Paragraph("6. Recommendations", self.styles['SectionHeading']))
            
            recommendations = [
                "1. Immediate Actions:",
                "   - Block all high-risk transactions (risk score > 0.9).",
                "   - Contact customers associated with high-risk transactions for verification.",
                "   - Flag accounts involved in multiple suspicious transactions for review.",
                "",
                "2. System Improvements:",
                "   - Update fraud detection rules based on recent fraud patterns.",
                "   - Implement additional verification steps for high-value transactions.",
                "   - Enhance monitoring of cross-border transactions.",
                "",
                "3. Process Enhancements:",
                "   - Conduct regular audits of the fraud detection system.",
                "   - Provide training to staff on identifying fraud indicators.",
                "   - Establish clear procedures for handling suspected fraud.",
                "",
                "4. Long-term Strategies:",
                "   - Develop machine learning models to adapt to evolving fraud patterns.",
                "   - Implement real-time fraud detection capabilities.",
                "   - Collaborate with industry partners to share fraud intelligence."
            ]
            
            for rec in recommendations:
                story.append(Paragraph(rec, self.styles['Body']))
            
            story.append(Spacer(1, 12))
            
            # Add appendix
            story.append(Paragraph("7. Appendix", self.styles['SectionHeading']))
            
            # Add methodology details
            story.append(Paragraph("7.1 Methodology Details", self.styles['SubsectionHeading']))
            
            methodology_details = """
            The fraud detection system utilizes a combination of the following techniques:
            
            - Unsupervised Learning: Isolation Forest, Local Outlier Factor, Autoencoders
            - Supervised Learning: Random Forest, XGBoost, Neural Networks
            - Rule-based Detection: Configurable rules for known fraud patterns
            - Feature Engineering: Statistical, graph-based, NLP, and time-series features
            
            Each transaction is assigned a risk score between 0 and 1, with higher scores indicating 
            greater likelihood of fraud. Transactions with scores above the threshold (typically 0.5) 
            are flagged for review.
            """
            
            story.append(Paragraph(methodology_details, self.styles['Body']))
            story.append(Spacer(1, 12))
            
            # Add footer
            footer_text = f"Generated by Fraud Detection System on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
            story.append(Paragraph(footer_text, self.styles['Footer']))
            
            # Build PDF
            doc.build(story)
            
            logger.info(f"Detailed fraud analysis report generated: {filepath}")
            return filepath
            
        except Exception as e:
            logger.error(f"Error generating detailed fraud analysis: {str(e)}")
            raise
    
    def generate_technical_report(self, df, risk_scores, model_results, include_charts=True):
        """
        Generate technical report
        
        Args:
            df (DataFrame): Transaction data
            risk_scores (DataFrame): Risk scores
            model_results (dict): Model results
            include_charts (bool): Whether to include charts
            
        Returns:
            str: Path to generated PDF
        """
        try:
            # Create filename
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"technical_report_{timestamp}.pdf"
            filepath = os.path.join(self.output_dir, filename)
            
            # Create document
            doc = SimpleDocTemplate(filepath, pagesize=A4)
            story = []
            
            # Add title
            story.append(Paragraph("Fraud Detection Technical Report", self.styles['CustomTitle']))
            story.append(Spacer(1, 12))
            
            # Add subtitle with date
            report_date = datetime.now().strftime("%B %d, %Y")
            story.append(Paragraph(f"Report Date: {report_date}", self.styles['CustomSubtitle']))
            story.append(PageBreak())
            
            # Add system overview
            story.append(Paragraph("1. System Overview", self.styles['SectionHeading']))
            
            overview_text = """
            The fraud detection system is designed to identify potentially fraudulent transactions 
            using a combination of machine learning techniques and rule-based methods. The system 
            processes transaction data in real-time, extracting various features and applying multiple 
            detection algorithms to generate risk scores for each transaction.
            """
            
            story.append(Paragraph(overview_text, self.styles['Body']))
            story.append(Spacer(1, 12))
            
            # Add data processing
            story.append(Paragraph("2. Data Processing", self.styles['SectionHeading']))
            
            data_processing_text = """
            The system processes transaction data through the following stages:
            
            1. Data Ingestion: Transactions are loaded from various sources (CSV, Excel, databases).
            2. Data Preprocessing: Missing values are handled, data types are converted, and basic 
               validation is performed.
            3. Feature Engineering: Statistical, graph-based, NLP, and time-series features are extracted.
            4. Model Application: Multiple models are applied to generate risk scores.
            5. Risk Aggregation: Scores from different models are combined to produce a final risk score.
            """
            
            story.append(Paragraph(data_processing_text, self.styles['Body']))
            story.append(Spacer(1, 12))
            
            # Add model performance
            story.append(Paragraph("3. Model Performance", self.styles['SectionHeading']))
            
            # Add unsupervised model performance
            if 'unsupervised' in model_results:
                story.append(Paragraph("3.1 Unsupervised Models", self.styles['SubsectionHeading']))
                
                unsupervised_text = f"""
                The system employs {len(model_results['unsupervised'])} unsupervised learning models:
                
                """
                
                for model_name in model_results['unsupervised']:
                    unsupervised_text += f"- {model_name}\n"
                
                story.append(Paragraph(unsupervised_text, self.styles['Body']))
                story.append(Spacer(1, 6))
            
            # Add supervised model performance
            if 'supervised' in model_results:
                story.append(Paragraph("3.2 Supervised Models", self.styles['SubsectionHeading']))
                
                supervised_text = f"""
                The system employs {len(model_results['supervised'])} supervised learning models:
                
                """
                
                for model_name in model_results['supervised']:
                    if 'performance' in model_results['supervised'][model_name]:
                        perf = model_results['supervised'][model_name]['performance']
                        auc = perf.get('roc_auc', 0)
                        supervised_text += f"- {model_name}: AUC = {auc:.3f}\n"
                    else:
                        supervised_text += f"- {model_name}\n"
                
                story.append(Paragraph(supervised_text, self.styles['Body']))
                story.append(Spacer(1, 6))
            
            # Add rule-based model performance
            if 'rule' in model_results:
                story.append(Paragraph("3.3 Rule-based Models", self.styles['SubsectionHeading']))
                
                rule_text = f"""
                The system employs {len(model_results['rule'].get('rules', {}))} rule-based detectors:
                
                """
                
                for rule_name in model_results['rule'].get('rules', {}):
                    rule_text += f"- {rule_name}\n"
                
                story.append(Paragraph(rule_text, self.styles['Body']))
                story.append(Spacer(1, 6))
            
            # Add feature importance if available
            if 'supervised' in model_results:
                story.append(Paragraph("3.4 Feature Importance", self.styles['SubsectionHeading']))
                
                # Get feature importance from Random Forest if available
                if 'random_forest' in model_results['supervised']:
                    rf_data = model_results['supervised']['random_forest']
                    if 'feature_importance' in rf_data:
                        importance_df = rf_data['feature_importance']
                        
                        # Create table for top features
                        feature_data = []
                        feature_data.append(['Feature', 'Importance'])
                        
                        for _, row in importance_df.head(10).iterrows():
                            feature_data.append([row['feature'], f"{row['importance']:.3f}"])
                        
                        feature_table = Table(feature_data, colWidths=[3*inch, 1.5*inch])
                        feature_table.setStyle(TableStyle([
                            ('BACKGROUND', (0, 0), (-1, 0), colors.darkblue),
                            ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),
                            ('ALIGN', (0, 0), (-1, -1), 'CENTER'),
                            ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),
                            ('FONTSIZE', (0, 0), (-1, 0), 10),
                            ('BOTTOMPADDING', (0, 0), (-1, 0), 12),
                            ('BACKGROUND', (0, 1), (-1, -1), colors.beige),
                            ('GRID', (0, 0), (-1, -1), 1, colors.black)
                        ]))
                        
                        story.append(feature_table)
                        story.append(Spacer(1, 6))
            
            # Add system architecture
            story.append(Paragraph("4. System Architecture", self.styles['SectionHeading']))
            
            architecture_text = """
            The fraud detection system is built with a modular architecture consisting of the following components:
            
            1. Data Ingestion Layer: Handles data loading from various sources.
            2. Feature Engineering Layer: Extracts features for fraud detection.
            3. Model Layer: Applies various detection algorithms.
            4. Risk Aggregation Layer: Combines model outputs.
            5. Reporting Layer: Generates reports and visualizations.
            6. API Layer: Provides interfaces for external systems.
            """
            
            story.append(Paragraph(architecture_text, self.styles['Body']))
            story.append(Spacer(1, 12))
            
            # Add performance metrics
            story.append(Paragraph("5. Performance Metrics", self.styles['SectionHeading']))
            
            # Calculate performance metrics
            total_transactions = len(df)
            fraud_transactions = risk_scores['is_fraud'].sum()
            fraud_percentage = (fraud_transactions / total_transactions) * 100
            
            metrics_data = [
                ['Metric', 'Value'],
                ['Total Transactions', f"{total_transactions:,}"],
                ['Fraudulent Transactions', f"{fraud_transactions:,}"],
                ['Fraud Percentage', f"{fraud_percentage:.2f}%"],
                ['Processing Time', f"{len(df) * 0.001:.2f} seconds (estimated)"]
            ]
            
            metrics_table = Table(metrics_data, colWidths=[3*inch, 2*inch])
            metrics_table.setStyle(TableStyle([
                ('BACKGROUND', (0, 0), (-1, 0), colors.darkblue),
                ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),
                ('ALIGN', (0, 0), (-1, -1), 'CENTER'),
                ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),
                ('FONTSIZE', (0, 0), (-1, 0), 12),
                ('BOTTOMPADDING', (0, 0), (-1, 0), 12),
                ('BACKGROUND', (0, 1), (-1, -1), colors.beige),
                ('GRID', (0, 0), (-1, -1), 1, colors.black)
            ]))
            
            story.append(metrics_table)
            story.append(Spacer(1, 12))
            
            # Add footer
            footer_text = f"Generated by Fraud Detection System on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
            story.append(Paragraph(footer_text, self.styles['Footer']))
            
            # Build PDF
            doc.build(story)
            
            logger.info(f"Technical report generated: {filepath}")
            return filepath
            
        except Exception as e:
            logger.error(f"Error generating technical report: {str(e)}")
            raise
    
    def generate_custom_report(self, df, risk_scores, top_fraud, explanations, model_results, 
                             include_charts=True, include_explanations=True, 
                             include_recommendations=True):
        """
        Generate custom report with user-specified sections
        
        Args:
            df (DataFrame): Transaction data
            risk_scores (DataFrame): Risk scores
            top_fraud (DataFrame): Top fraudulent transactions
            explanations (dict): Transaction explanations
            model_results (dict): Model results
            include_charts (bool): Whether to include charts
            include_explanations (bool): Whether to include explanations
            include_recommendations (bool): Whether to include recommendations
            
        Returns:
            str: Path to generated PDF
        """
        try:
            # Create filename
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"custom_report_{timestamp}.pdf"
            filepath = os.path.join(self.output_dir, filename)
            
            # Create document
            doc = SimpleDocTemplate(filepath, pagesize=A4)
            story = []
            
            # Add title
            story.append(Paragraph("Custom Fraud Detection Report", self.styles['CustomTitle']))
            story.append(Spacer(1, 12))
            
            # Add subtitle with date
            report_date = datetime.now().strftime("%B %d, %Y")
            story.append(Paragraph(f"Report Date: {report_date}", self.styles['CustomSubtitle']))
            story.append(PageBreak())
            
            # Add summary statistics
            story.append(Paragraph("Summary Statistics", self.styles['SectionHeading']))
            
            # Calculate statistics
            total_transactions = len(df)
            fraud_transactions = risk_scores['is_fraud'].sum()
            fraud_percentage = (fraud_transactions / total_transactions) * 100
            
            stats_text = f"""
            Total Transactions: {total_transactions:,}
            Fraudulent Transactions: {fraud_transactions:,}
            Fraud Percentage: {fraud_percentage:.2f}%
            """
            
            story.append(Paragraph(stats_text, self.styles['Body']))
            story.append(Spacer(1, 12))
            
            # Add risk distribution chart if requested
            if include_charts:
                story.append(Paragraph("Risk Distribution", self.styles['SectionHeading']))
                
                # Create risk distribution chart
                plt.figure(figsize=(8, 6))
                sns.histplot(risk_scores['risk_score'], bins=50, kde=True)
                plt.title('Distribution of Risk Scores')
                plt.xlabel('Risk Score')
                plt.ylabel('Frequency')
                
                # Convert plot to image
                img_buffer = io.BytesIO()
                plt.savefig(img_buffer, format='png', dpi=300, bbox_inches='tight')
                img_buffer.seek(0)
                
                # Add image to report
                risk_dist_img = Image(img_buffer, width=6*inch, height=4*inch)
                story.append(risk_dist_img)
                story.append(Spacer(1, 12))
                
                plt.close()
            
            # Add top fraudulent transactions
            story.append(Paragraph("Top Fraudulent Transactions", self.styles['SectionHeading']))
            
            # Create table for top fraud transactions
            if len(top_fraud) > 0:
                # Prepare data
                fraud_data = []
                fraud_data.append(['Transaction ID', 'Amount', 'Risk Score'])
                
                for _, row in top_fraud.head(10).iterrows():
                    transaction_id = row.get('transaction_id', 'N/A')
                    amount = f"${row.get('amount', 0):,.2f}"
                    risk_score = f"{row.get('risk_score', 0):.3f}"
                    
                    fraud_data.append([transaction_id, amount, risk_score])
                
                fraud_table = Table(fraud_data, colWidths=[2*inch, 1.5*inch, 1.5*inch])
                fraud_table.setStyle(TableStyle([
                    ('BACKGROUND', (0, 0), (-1, 0), colors.darkblue),
                    ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),
                    ('ALIGN', (0, 0), (-1, -1), 'CENTER'),
                    ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),
                    ('FONTSIZE', (0, 0), (-1, 0), 12),
                    ('BOTTOMPADDING', (0, 0), (-1, 0), 12),
                    ('BACKGROUND', (0, 1), (-1, -1), colors.beige),
                    ('GRID', (0, 0), (-1, -1), 1, colors.black)
                ]))
                
                story.append(fraud_table)
                story.append(Spacer(1, 12))
            
            # Add detailed explanations if requested
            if include_explanations and explanations:
                story.append(Paragraph("Transaction Explanations", self.styles['SectionHeading']))
                
                # Add explanations for top 3 fraudulent transactions
                for i, (idx, explanation) in enumerate(list(explanations.items())[:3]):
                    story.append(Paragraph(f"Transaction {i+1}", self.styles['SubsectionHeading']))
                    
                    # Add explanation text
                    explanation_text = explanation.get('text_explanation', 'No explanation available.')
                    story.append(Paragraph(explanation_text, self.styles['Body']))
                    story.append(Spacer(1, 6))
            
            # Add recommendations if requested
            if include_recommendations:
                story.append(Paragraph("Recommendations", self.styles['SectionHeading']))
                
                recommendations = [
                    "1. Review all high-risk transactions immediately.",
                    "2. Implement additional verification for large transactions.",
                    "3. Monitor patterns in fraudulent activity.",
                    "4. Update detection rules regularly.",
                    "5. Conduct periodic system audits."
                ]
                
                for rec in recommendations:
                    story.append(Paragraph(rec, self.styles['Body']))
                
                story.append(Spacer(1, 12))
            
            # Add footer
            footer_text = f"Generated by Fraud Detection System on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
            story.append(Paragraph(footer_text, self.styles['Footer']))
            
            # Build PDF
            doc.build(story)
            
            logger.info(f"Custom report generated: {filepath}")
            return filepath
            
        except Exception as e:
            logger.error(f"Error generating custom report: {str(e)}")
            raise

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\reporting\.ipynb_checkpoints\__init__-checkpoint.py ===

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\reporting\__pycache__\pdf_generator.cpython-39.pyc ===
a
    èöhñ´  „                   @   sH  d Z ddlZddlZddlmZ ddlZ	ddl
m
Z
 ddlZddlZddlZddlmZmZ ddlmZmZmZmZmZmZmZ ddlmZmZ ddlmZ ddlmZ dd	l m!Z!m"Z"m#Z# dd
l$m%Z% ddl&m'Z' ddl(m)Z) ddl*m+Z+ ddl,Z,ddl-Z-ddl.m/Z/m0Z0m1Z1m2Z2 e,†3d° e-j4e-j5dç e-†6e7°Z8G ddÑ dÉZ9dS )zS
PDF Generator Module
Implements PDF report generation for fraud detection results
È    N)⁄datetime)⁄letter⁄A4)⁄SimpleDocTemplate⁄	Paragraph⁄Spacer⁄Table⁄
TableStyle⁄Image⁄	PageBreak)⁄getSampleStyleSheet⁄ParagraphStyle)⁄inch)⁄colors)⁄	TA_CENTER⁄TA_LEFT⁄TA_RIGHT)⁄Drawing)⁄LinePlot)⁄VerticalBarChart)⁄markers)⁄Dict⁄List⁄Tuple⁄Union⁄ignore)⁄levelc                   @   sJ   e Zd ZdZdddÑZddÑ Zddd	ÑZdd
dÑZdddÑZdddÑZ	dS )⁄PDFGeneratorzz
    Class for generating PDF reports for fraud detection results
    Implements professional audit report generation
    ˙../reports/generatedc                 C   s(   || _ tÉ | _| †°  tj|ddç dS )z{
        Initialize PDFGenerator
        
        Args:
            output_dir (str): Output directory for reports
        T)⁄exist_okN)⁄
output_dirr   ⁄styles⁄setup_custom_styles⁄os⁄makedirs)⁄selfr    © r&   ˙mC:\Users\Tranquil Abyss\fraud-detection-platform\app\..\src\fraud_detection_engine\reporting\pdf_generator.py⁄__init__&   s    zPDFGenerator.__init__c              
   C   s“   | j †td| j d ddttjdç° | j †td| j d dd	ttjdç° | j †td
| j d dd	tjdç° | j †td| j d d	dtjdç° | j †td| j d ddtdç° | j †td| j d dttjdç° dS )z"Setup custom styles for the report⁄CustomTitle⁄TitleÈ   È   )⁄name⁄parent⁄fontSize⁄
spaceAfter⁄	alignment⁄	textColor⁄CustomSubtitleZHeading1È   È   ⁄SectionHeadingZHeading2È   )r-   r.   r/   r0   r2   ⁄SubsectionHeadingZHeading3È   ⁄Body⁄NormalÈ
   )r-   r.   r/   r0   r1   ⁄FooterÈ   )r-   r.   r/   r1   r2   N)r!   ⁄addr   r   r   ⁄darkbluer   ⁄grey)r%   r&   r&   r'   r"   4   sX    ˙
˙
˚	˚	˚	˚z PDFGenerator.setup_custom_stylesTc           !      C   s§  êz`t †° †d°}d|õ dù}tj†| j|°}t|tdç}	g }
|
†	t
d| jd É° |
†	tddÉ° t †° †d	°}|
†	t
d
|õ ù| jd É° |
†	tÉ ° |
†	t
d| jd É° t|É}|d †° }|| d }d|jv rÊ|d †° nd}d|jv êr|j|d df †° nd}ddgd|dõgd|dõgd|dõdùgdd|dõùgdd|dõùgg}t|dt dt gd ç}|†td!d"d#tjfd$d"d#tjfd%d&d'd(d!d)d*tjfd+d"d*dtjfgÉ° |
†	|° |
†	tddÉ° |êrà|
†	t
d,| jd É° tjd-d.ç tj|d/ d0d1d2ç t†d3° t† d4° t†!d5° t"†#° }tj$|d6d7d8d9ç |†%d° t&|d:t d;t d<ç}|
†	|° |
†	tddÉ° t†'°  |
†	t
d=| jd É° t|Édkêrƒg }|†	g d>¢° |†(d?°†)° D ]p\}}|†*d@dA°}d|†*dd°dõù}|†*d/d°dBõ}|†*dCdA°}t+|t,j-Éêr(|†dD°}|†	||||g° êqÃt|dEt dt dt dEt gd ç}|†td!d"d#tjfd$d"d#tjfd%d&dFd(d!d)d*tjfd+d"d*dtjfgÉ° |
†	|° |
†	tddÉ° |êr|
†	t
dG| jd É° g dH¢}|D ]}|
†	t
|| jdI É° êqÏ|
†	tddÉ° dJt †° †dK°õ ù}|
†	t
|| jdL É° |	†.|
° t/†0dM|õ ù° |W S  t1êyû }  z"t/†2dNt3| Éõ ù° Ç W Y dO} ~ n
dO} ~ 0 0 dOS )Pa∏  
        Generate executive summary report
        
        Args:
            df (DataFrame): Transaction data
            risk_scores (DataFrame): Risk scores
            top_fraud (DataFrame): Top fraudulent transactions
            include_charts (bool): Whether to include charts
            include_recommendations (bool): Whether to include recommendations
            
        Returns:
            str: Path to generated PDF
        ˙%Y%m%d_%H%M%SZexecutive_summary_˙.pdf©Zpagesizez!Fraud Detection Executive Summaryr)   È   r5   ˙	%B %d, %Y˙Report Date: r3   ˙Summary Statisticsr6   ⁄is_fraudÈd   ⁄amountr   ⁄Metric⁄Value˙Total Transactions˙,˙Fraudulent Transactions˙Fraud Percentage˙.2f˙%zTotal Amount˙$˙,.2fzFraud AmountÈ   È   ©⁄	colWidths⁄
BACKGROUND©r   r   ©Èˇˇˇˇr   ⁄	TEXTCOLOR©ZALIGNr[   ©r]   r]   ⁄CENTER©ZFONTNAMEr[   r\   zHelvetica-Bold©⁄FONTSIZEr[   r\   r5   ©ZBOTTOMPADDINGr[   r\   r5   ©r   rE   r`   ⁄GRID˙Risk Distribution©r>   r9   ©⁄figsize⁄
risk_scoreÈ2   T©⁄bins⁄kde˙Distribution of Risk Scores˙
Risk Score⁄	Frequency⁄pngÈ,  ⁄tight©⁄format⁄dpi⁄bbox_inchesr9   È   ©⁄width⁄height˙Top Fraudulent Transactions)˙Transaction ID⁄Amountrr   ⁄Dater<   ⁄transaction_id˙N/A˙.3f⁄	timestamp˙%Y-%m-%dÁ      ¯?©rd   r[   r\   r<   ⁄Recommendations)zD1. Immediately review all high-risk transactions (risk score > 0.8).zI2. Implement additional verification steps for transactions over $10,000.zQ3. Monitor patterns in fraudulent transactions to identify potential fraud rings.z?4. Update fraud detection rules based on recent fraud patterns.z85. Conduct regular audits of the fraud detection system.r:   ˙'Generated by Fraud Detection System on ˙%Y-%m-%d %H:%M:%Sr=   z$Executive summary report generated: z$Error generating executive summary: N)4r   ⁄now⁄strftimer#   ⁄path⁄joinr    r   r   ⁄appendr   r!   r   r   ⁄len⁄sum⁄columns⁄locr   r   ⁄setStyler	   r   r@   ⁄
whitesmoke⁄beige⁄black⁄plt⁄figure⁄sns⁄histplot⁄title⁄xlabel⁄ylabel⁄io⁄BytesIO⁄savefig⁄seekr
   ⁄close⁄head⁄iterrows⁄get⁄
isinstance⁄pd⁄	Timestamp⁄build⁄logger⁄info⁄	Exception⁄error⁄str)!r%   ⁄df⁄risk_scores⁄	top_fraud⁄include_charts⁄include_recommendationsrÜ   ⁄filename⁄filepath⁄doc⁄story⁄report_date⁄total_transactions⁄fraud_transactions⁄fraud_percentage⁄total_amountZfraud_amountZ
stats_dataZstats_table⁄
img_buffer⁄risk_dist_img⁄
fraud_data⁄_⁄rowrÉ   rK   rl   ⁄date⁄fraud_table⁄recommendations⁄rec⁄footer_text⁄er&   r&   r'   ⁄generate_executive_summaryn   s¥    &

˙	¯






$¯

z'PDFGenerator.generate_executive_summaryc           0      C   st  êz0t †° †d°}d|õ dù}	tj†| j|	°}
t|
tdç}g }|†	t
d| jd É° |†	tddÉ° t †° †d	°}|†	t
d
|õ ù| jd É° |†	tÉ ° |†	t
d| jd É° g d¢}|D ]}|†	t
|| jd É° qº|†	tÉ ° |†	t
d| jd É° t|É}|d †° }|| d }d|dõd|dõd|dõd|j|d df †° dõdù	}|†	t
|| jd É° |†	tddÉ° |†	t
d| jd É° d}|†	t
|| jd É° |†	tddÉ° |†	t
d| jd É° |êrÑ|†	t
d| jd  É° tjd!d"ç tj|d# d$d%d&ç t†d'° t†d(° t†d)° t†° }tj|d*d+d,d-ç |†d.° t|d/t d0t d1ç}|†	|° |†	tddÉ° t†°  |êrd2|jv êr|†	t
d3| jd  É° |† ° }|d |d< t!†"|d2 °|d2< |d2 j#j$|d4< |†%d4°d †&d5d6g°†'° }|d5 |d6  |d7< tjd8d"ç t†(d9dd° tj)|d4 |d5 d:d;ç t†d<° t†d=° tj*d>d?ç t†(d9dd9° tj)|d4 |d7 d:d@dAç t†dB° t†dC° t†dD° tj*d>d?ç t†+°  t†° }tj|d*d+d,d-ç |†d.° t|d/t dEt d1ç}|†	|° |†	tddÉ° t†°  |†	tÉ ° |†	t
dF| jd É° |†	t
dG| jd  É° t|Éd.kêråg }|†	g dH¢° |†,dI°†-° D ]~\}}|†.dJdK°}dL|†.dd.°dõù}|†.d#d.°dMõ}|†.d2dK°} |†.dNdK°}!t/| t!j0ÉêrË| †dO°} |†	|||| |!g° êqÄt1|dPt dQt dQt dt dQt gdRç}"|"†2t3dSdTdUt4j5fdVdTdUt4j6fdWdXdYdZdSd[d\t4j7fd]dTd\dt4j8fgÉ° |†	|"° |†	tddÉ° |êrË|êrË|†	t
d^| jd  É° t9t:|†;° Éd_dEÖ ÉD ê]\}#\}$}%|†	t
d`|#d õ da|%†.dJ|$°õ ù| jd  É° |%†.dbdc°}&|†	t
|&| jd É° |%†.dd°êrÜ|†	t
de| jd É° |%dd d_dfÖ D ].\}'}(|†	t
dg|'õ da|(dMõù| jd É° êqV|%†.dh°êr‘|†	t
di| jd É° |%dh D ]"})|†	t
dg|)õ ù| jd É° êq∞|†	tddÉ° êq∆|†	tÉ ° |†	t
dj| jd É° g dk¢}*|*D ]}'|†	t
|'| jd É° êq|†	tddÉ° |†	t
dl| jd É° g dm¢}+|+D ]},|†	t
|,| jd É° êqf|†	tddÉ° |†	t
dn| jd É° |†	t
do| jd  É° dp}-|†	t
|-| jd É° |†	tddÉ° dqt †° †dr°õ ù}.|†	t
|.| jds É° |†<|° t=†>dt|
õ ù° |
W S  t?êyn }/ z"t=†@dutA|/Éõ ù° Ç W Y d_}/~/n
d_}/~/0 0 d_S )vaA  
        Generate detailed fraud analysis report
        
        Args:
            df (DataFrame): Transaction data
            risk_scores (DataFrame): Risk scores
            top_fraud (DataFrame): Top fraudulent transactions
            explanations (dict): Transaction explanations
            include_charts (bool): Whether to include charts
            include_explanations (bool): Whether to include explanations
            include_recommendations (bool): Whether to include recommendations
            
        Returns:
            str: Path to generated PDF
        rB   Zdetailed_fraud_analysis_rC   rD   zDetailed Fraud Analysis Reportr)   rE   r5   rF   rG   r3   zTable of Contentsr6   )˙1. Executive Summary˙2. Methodology˙3. Analysis Results˙ 4. Detailed Transaction Analysis˙5. Risk Factors˙6. Recommendations˙7. Appendixr:   rÃ   rI   rJ   zù
            This report presents a detailed analysis of fraudulent transactions detected by the fraud detection system.
            The analysis identified rO   z  fraudulent transactions out of z. total transactions,
            representing rR   z[% of all transactions. The total value of fraudulent transactions 
            amounts to $rK   rU   z* if amount data is available.
            rÕ   a“  
            The fraud detection system employs a multi-layered approach combining unsupervised learning, 
            supervised learning, and rule-based methods to identify potentially fraudulent transactions. 
            The system analyzes various features including transaction amounts, frequency patterns, 
            geographic locations, temporal patterns, and behavioral characteristics to assign risk scores 
            to each transaction.
            rŒ   z3.1 Risk Distributionr8   ri   rj   rl   rm   Trn   rq   rr   rs   rt   ru   rv   rw   r   r9   r{   r|   rÜ   z3.2 Fraud by Timer≈   rì   ⁄count⁄
fraud_rate)r<   r9   rW   ⁄o)⁄markerzFraud Count by Date⁄CountÈ-   )⁄rotation⁄red)r÷   ⁄colorzFraud Rate by DateZRaterÇ   È   rœ   z4.1 Top Fraudulent Transactions)rÄ   rÅ   rr   rÇ   z
Risk LevelÈ   rÉ   rÑ   rT   rÖ   ⁄
risk_levelrá   g333333Û?göôôôôôÈ?rX   rZ   r[   r\   r^   r_   rb   )rd   r[   r\   r>   re   rf   r`   rg   z4.2 Transaction ExplanationsN˙Transaction z: ⁄text_explanation˙No explanation available.⁄top_factorszTop Contributing Factors:rV   ˙- ⁄rule_violationszRule Violations:r–   )z`1. High Transaction Amount: Transactions significantly above average for the sender or receiver.zL2. Unusual Geographic Patterns: Transactions from or to high-risk locations.zL3. Temporal Anomalies: Transactions at unusual times or in rapid succession.zN4. Behavioral Deviations: Transactions that deviate from established patterns.zM5. Network Anomalies: Transactions involving suspicious networks of entities.r—   )z1. Immediate Actions:z9   - Block all high-risk transactions (risk score > 0.9).zO   - Contact customers associated with high-risk transactions for verification.zK   - Flag accounts involved in multiple suspicious transactions for review.⁄ z2. System Improvements:zA   - Update fraud detection rules based on recent fraud patterns.zI   - Implement additional verification steps for high-value transactions.z5   - Enhance monitoring of cross-border transactions.rÂ   z3. Process Enhancements:z:   - Conduct regular audits of the fraud detection system.z?   - Provide training to staff on identifying fraud indicators.z=   - Establish clear procedures for handling suspected fraud.rÂ   z4. Long-term Strategies:zI   - Develop machine learning models to adapt to evolving fraud patterns.z6   - Implement real-time fraud detection capabilities.zD   - Collaborate with industry partners to share fraud intelligence.r“   z7.1 Methodology Detailsa√  
            The fraud detection system utilizes a combination of the following techniques:
            
            - Unsupervised Learning: Isolation Forest, Local Outlier Factor, Autoencoders
            - Supervised Learning: Random Forest, XGBoost, Neural Networks
            - Rule-based Detection: Configurable rules for known fraud patterns
            - Feature Engineering: Statistical, graph-based, NLP, and time-series features
            
            Each transaction is assigned a risk score between 0 and 1, with higher scores indicating 
            greater likelihood of fraud. Transactions with scores above the threshold (typically 0.5) 
            are flagged for review.
            rã   rå   r=   z*Detailed fraud analysis report generated: z*Error generating detailed fraud analysis: )Br   rç   ré   r#   rè   rê   r    r   r   rë   r   r!   r   r   rí   rì   rï   rö   rõ   rú   rù   rû   rü   r†   r°   r¢   r£   r§   r
   r   r•   rî   ⁄copyr™   ⁄to_datetime⁄dtr≈   ⁄groupby⁄agg⁄reset_index⁄subplot⁄plot⁄xticks⁄tight_layoutr¶   rß   r®   r©   r´   r   rñ   r	   r   r@   ró   rò   rô   ⁄	enumerate⁄list⁄itemsr¨   r≠   rÆ   rØ   r∞   r±   )0r%   r≤   r≥   r¥   ⁄explanationsrµ   ⁄include_explanationsr∂   rÜ   r∑   r∏   rπ   r∫   rª   Z	toc_items⁄itemrº   rΩ   ræ   Zsummary_textZmethodology_textr¿   r¡   Zdf_with_risk⁄fraud_by_dateZfraud_time_imgr¬   r√   rƒ   rÉ   rK   rl   r≈   rﬁ   r∆   ⁄i⁄idx⁄explanation⁄explanation_text⁄factor⁄contribution⁄ruleZrisk_factorsr«   r»   Zmethodology_detailsr…   r    r&   r&   r'   ⁄ generate_detailed_fraud_analysis  s&   
˛˛˝¸













*¯
& ˇ( 
z-PDFGenerator.generate_detailed_fraud_analysisc           "      C   sæ  êzzt †° †d°}d|õ dù}tj†| j|°}t|tdç}g }	|	†	t
d| jd É° |	†	tddÉ° t †° †d	°}
|	†	t
d
|
õ ù| jd É° |	†	tÉ ° |	†	t
d| jd É° d}|	†	t
|| jd É° |	†	tddÉ° |	†	t
d| jd É° d}|	†	t
|| jd É° |	†	tddÉ° |	†	t
d| jd É° d|v êr™|	†	t
d| jd É° dt|d Éõ dù}|d D ]}|d|õ dù7 }êql|	†	t
|| jd É° |	†	tddÉ° d|v êrl|	†	t
d| jd É° dt|d Éõ dù}|d D ]^}d|d | v êr2|d | d }|†dd °}|d|õ d!|d"õdù7 }n|d|õ dù7 }êqÊ|	†	t
|| jd É° |	†	tddÉ° d#|v êrˆ|	†	t
d$| jd É° dt|d# †d%i °Éõ d&ù}|d# †d%i °D ]}|d|õ dù7 }êq∏|	†	t
|| jd É° |	†	tddÉ° d|v êr|	†	t
d'| jd É° d(|d v êr|d d( }d)|v êr|d) }g }|†	d*d+g° |†d,°†° D ]$\}}|†	|d- |d. d"õg° êqbt|d/t d0t gd1ç}|†td2d3d4tjfd5d3d4tjfd6d7d8d9d2d:d;tjfd<d3d;dtjfgÉ° |	†	|° |	†	tddÉ° |	†	t
d=| jd É° d>}|	†	t
|| jd É° |	†	tddÉ° |	†	t
d?| jd É° t|É}|d@ †° }|| dA }dBdCgdD|dEõgdF|dEõgdG|dHõdIùgdJt|ÉdK dHõdLùgg}t|d/t dMt gd1ç}|†td2d3d4tjfd5d3d4tjfd6d7dNd9d2d:d;tjfd<d3d;dtjfgÉ° |	†	|° |	†	tddÉ° dOt †° †dP°õ ù} |	†	t
| | jdQ É° |†|	° t†dR|õ ù° |W S  têy∏ }! z"t† dSt!|!Éõ ù° Ç W Y dT}!~!n
dT}!~!0 0 dTS )UaR  
        Generate technical report
        
        Args:
            df (DataFrame): Transaction data
            risk_scores (DataFrame): Risk scores
            model_results (dict): Model results
            include_charts (bool): Whether to include charts
            
        Returns:
            str: Path to generated PDF
        rB   Ztechnical_report_rC   rD   z Fraud Detection Technical Reportr)   rE   r5   rF   rG   r3   z1. System Overviewr6   aã  
            The fraud detection system is designed to identify potentially fraudulent transactions 
            using a combination of machine learning techniques and rule-based methods. The system 
            processes transaction data in real-time, extracting various features and applying multiple 
            detection algorithms to generate risk scores for each transaction.
            r:   z2. Data Processingaá  
            The system processes transaction data through the following stages:
            
            1. Data Ingestion: Transactions are loaded from various sources (CSV, Excel, databases).
            2. Data Preprocessing: Missing values are handled, data types are converted, and basic 
               validation is performed.
            3. Feature Engineering: Statistical, graph-based, NLP, and time-series features are extracted.
            4. Model Application: Multiple models are applied to generate risk scores.
            5. Risk Aggregation: Scores from different models are combined to produce a final risk score.
            z3. Model Performance⁄unsupervisedz3.1 Unsupervised Modelsr8   z$
                The system employs z@ unsupervised learning models:
                
                r„   ⁄
r9   ⁄
supervisedz3.2 Supervised Modelsz> supervised learning models:
                
                ⁄performance⁄roc_aucr   z: AUC = rÖ   r˝   z3.3 Rule-based Models⁄rulesz8 rule-based detectors:
                
                z3.4 Feature Importance⁄random_forest⁄feature_importance⁄FeatureZ
Importancer<   ⁄feature⁄
importancerV   rà   rX   rZ   r[   r\   r^   r_   rb   râ   re   rf   r`   rg   z4. System Architecturea:  
            The fraud detection system is built with a modular architecture consisting of the following components:
            
            1. Data Ingestion Layer: Handles data loading from various sources.
            2. Feature Engineering Layer: Extracts features for fraud detection.
            3. Model Layer: Applies various detection algorithms.
            4. Risk Aggregation Layer: Combines model outputs.
            5. Reporting Layer: Generates reports and visualizations.
            6. API Layer: Provides interfaces for external systems.
            z5. Performance MetricsrI   rJ   rL   rM   rN   rO   rP   rQ   rR   rS   zProcessing Timeg¸©Ò“MbP?z seconds (estimated)rW   rc   rã   rå   r=   zTechnical report generated: z#Error generating technical report: N)"r   rç   ré   r#   rè   rê   r    r   r   rë   r   r!   r   r   rí   r®   r¶   rß   r   r   rñ   r	   r   r@   ró   rò   rô   rì   r¨   r≠   rÆ   rØ   r∞   r±   )"r%   r≤   r≥   ⁄model_resultsrµ   rÜ   r∑   r∏   rπ   r∫   rª   Zoverview_textZdata_processing_textZunsupervised_text⁄
model_nameZsupervised_textZperf⁄aucZ	rule_text⁄	rule_nameZrf_data⁄importance_dfZfeature_datar√   rƒ   Zfeature_tableZarchitecture_textrº   rΩ   ræ   Zmetrics_dataZmetrics_tabler…   r    r&   r&   r'   ⁄generate_technical_report>  s⁄    

ˇ

ˇ
ˇ

¯


˚¯

z&PDFGenerator.generate_technical_reportc	           $      C   sV  êzt †° †d°}	d|	õ dù}
tj†| j|
°}t|tdç}g }|†	t
d| jd É° |†	tddÉ° t †° †d	°}|†	t
d
|õ ù| jd É° |†	tÉ ° |†	t
d| jd É° t|É}|d †° }|| d }d|dõd|dõd|dõdù}|†	t
|| jd É° |†	tddÉ° |êr |†	t
d| jd É° tjddç tj|d dddç t†d° t†d° t†d ° t†° }tj|d!d"d#d$ç |†d%° t|d&t d't d(ç}|†	|° |†	tddÉ° t†°  |†	t
d)| jd É° t|Éd%kêr⁄g }|†	g d*¢° |†d+°†° D ]J\}}|† d,d-°}d.|† d/d%°d0õù}|† dd%°d1õ}|†	|||g° êqt!|d2t d3t d3t gd4ç}|†"t#d5d6d7t$j%fd8d6d7t$j&fd9d:d;d<d5d=d>t$j'fd?d6d>dt$j(fgÉ° |†	|° |†	tddÉ° |êrv|êrv|†	t
d@| jd É° t)t*|†+° ÉdAdBÖ ÉD ]`\}\}}|†	t
dC|d õ ù| jdD É° |† dEdF°}|†	t
|| jd É° |†	tdd&É° êq|êrÃ|†	t
dG| jd É° g dH¢} | D ]}!|†	t
|!| jd É° êqû|†	tddÉ° dIt †° †dJ°õ ù}"|†	t
|"| jdK É° |†,|° t-†.dL|õ ù° |W S  t/êyP }# z"t-†0dMt1|#Éõ ù° Ç W Y dA}#~#n
dA}#~#0 0 dAS )Na}  
        Generate custom report with user-specified sections
        
        Args:
            df (DataFrame): Transaction data
            risk_scores (DataFrame): Risk scores
            top_fraud (DataFrame): Top fraudulent transactions
            explanations (dict): Transaction explanations
            model_results (dict): Model results
            include_charts (bool): Whether to include charts
            include_explanations (bool): Whether to include explanations
            include_recommendations (bool): Whether to include recommendations
            
        Returns:
            str: Path to generated PDF
        rB   Zcustom_report_rC   rD   zCustom Fraud Detection Reportr)   rE   r5   rF   rG   r3   rH   r6   rI   rJ   z!
            Total Transactions: rO   z&
            Fraudulent Transactions: z
            Fraud Percentage: rR   z%
            r:   rh   ri   rj   rl   rm   Trn   rq   rr   rs   rt   ru   rv   rw   r   r9   r{   r|   r   )rÄ   rÅ   rr   r<   rÉ   rÑ   rT   rK   rU   rÖ   rW   rà   rX   rZ   r[   r\   r^   r_   rb   rc   re   rf   r`   rg   zTransaction ExplanationsNrV   rﬂ   r8   r‡   r·   rä   )z11. Review all high-risk transactions immediately.z<2. Implement additional verification for large transactions.z+3. Monitor patterns in fraudulent activity.z$4. Update detection rules regularly.z"5. Conduct periodic system audits.rã   rå   r=   zCustom report generated: z Error generating custom report: )2r   rç   ré   r#   rè   rê   r    r   r   rë   r   r!   r   r   rí   rì   rö   rõ   rú   rù   rû   rü   r†   r°   r¢   r£   r§   r
   r   r•   r¶   rß   r®   r   rñ   r	   r   r@   ró   rò   rô   r   rÒ   rÚ   r¨   r≠   rÆ   rØ   r∞   r±   )$r%   r≤   r≥   r¥   rÛ   r
  rµ   rÙ   r∂   rÜ   r∑   r∏   rπ   r∫   rª   rº   rΩ   ræ   Z
stats_textr¿   r¡   r¬   r√   rƒ   rÉ   rK   rl   r∆   r˜   r¯   r˘   r˙   r«   r»   r…   r    r&   r&   r'   ⁄generate_custom_report  s¢    ˇ˛˝





¯
$ 
z#PDFGenerator.generate_custom_reportN)r   )TT)TTT)T)TTT)
⁄__name__⁄
__module__⁄__qualname__⁄__doc__r(   r"   rÀ   r˛   r  r  r&   r&   r&   r'   r       s   
:
   ˛
  5
 S  ˛r   ):r  ⁄pandasr™   ⁄numpy⁄np⁄matplotlib.pyplot⁄pyplotrö   ⁄seabornrú   r   r#   r°   ⁄base64Zreportlab.lib.pagesizesr   r   Zreportlab.platypusr   r   r   r   r	   r
   r   Zreportlab.lib.stylesr   r   Zreportlab.lib.unitsr   Zreportlab.libr   Zreportlab.lib.enumsr   r   r   Zreportlab.graphics.shapesr   Z#reportlab.graphics.charts.lineplotsr   Z#reportlab.graphics.charts.barchartsr   Zreportlab.graphics.widgetsr   ⁄warnings⁄logging⁄typingr   r   r   r   ⁄filterwarnings⁄basicConfig⁄INFO⁄	getLoggerr  r≠   r   r&   r&   r&   r'   ⁄<module>   s2   $



=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\reporting\__pycache__\__init__.cpython-39.pyc ===
a
    èöh    „                   @   s   d S )N© r   r   r   ˙hC:\Users\Tranquil Abyss\fraud-detection-platform\app\..\src\fraud_detection_engine\reporting\__init__.py⁄<module>   Û    

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\utils\api_utils.py ===
"""
API Utilities Module
Handles checking API availability and providing demo data
"""
import yaml
import os
import logging
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
logger = logging.getLogger(__name__)

def is_api_available(service_name):
    """
    Check if an API service is available
    
    Args:
        service_name (str): Name of the service (e.g., 'gemini', 'openai', 'news_api')
        
    Returns:
        bool: True if API is available, False otherwise
    """
    try:
        # Get the directory where this script is located
        script_dir = os.path.dirname(os.path.abspath(__file__))
        config_path = os.path.join(script_dir, '..', '..', 'config', 'api_keys.yml')
        
        with open(config_path, 'r') as f:
            config = yaml.safe_load(f)
        
        api_key = config.get(service_name, {}).get('api_key', '')
        
        # Check if key is available and not a placeholder
        return api_key and api_key not in ["YOUR_" + service_name.upper() + "_API_KEY", "NOT_AVAILABLE"]
    except Exception as e:
        logger.warning(f"Error checking API availability for {service_name}: {str(e)}")
        return False

def get_demo_sanctions_data():
    """
    Get demo sanctions data for testing
    
    Returns:
        DataFrame: Demo sanctions data
    """
    try:
        # Create demo data
        demo_data = pd.DataFrame({
            'entity_id': ['ENT001', 'ENT002', 'ENT003'],
            'name': ['Demo Entity 1', 'Demo Entity 2', 'Demo Entity 3'],
            'country': ['North Korea', 'Iran', 'Syria'],
            'list_type': ['Sanctions List', 'Sanctions List', 'Sanctions List'],
            'added_date': ['2023-01-01', '2023-02-01', '2023-03-01']
        })
        
        logger.info("Using demo sanctions data")
        return demo_data
    except Exception as e:
        logger.error(f"Error creating demo sanctions data: {str(e)}")
        return pd.DataFrame()

def get_demo_tax_compliance_data():
    """
    Get demo tax compliance data for testing
    
    Returns:
        DataFrame: Demo tax compliance data
    """
    try:
        # Create demo data
        demo_data = pd.DataFrame({
            'tax_id': ['TAX001', 'TAX002', 'TAX003'],
            'entity_name': ['Demo Corp 1', 'Demo Corp 2', 'Demo Corp 3'],
            'compliance_status': ['Compliant', 'Non-compliant', 'Under Review'],
            'last_checked': ['2023-01-01', '2023-02-01', '2023-03-01']
        })
        
        logger.info("Using demo tax compliance data")
        return demo_data
    except Exception as e:
        logger.error(f"Error creating demo tax compliance data: {str(e)}")
        return pd.DataFrame()

def get_demo_bank_verification_data():
    """
    Get demo bank verification data for testing
    
    Returns:
        DataFrame: Demo bank verification data
    """
    try:
        # Create demo data
        demo_data = pd.DataFrame({
            'account_number': ['ACC001', 'ACC002', 'ACC003'],
            'bank_name': ['Demo Bank 1', 'Demo Bank 2', 'Demo Bank 3'],
            'verification_status': ['Verified', 'Not Verified', 'Pending'],
            'last_verified': ['2023-01-01', '2023-02-01', '2023-03-01']
        })
        
        logger.info("Using demo bank verification data")
        return demo_data
    except Exception as e:
        logger.error(f"Error creating demo bank verification data: {str(e)}")
        return pd.DataFrame()

def get_demo_identity_verification_data():
    """
    Get demo identity verification data for testing
    
    Returns:
        DataFrame: Demo identity verification data
    """
    try:
        # Create demo data
        demo_data = pd.DataFrame({
            'id_number': ['ID001', 'ID002', 'ID003'],
            'name': ['Demo Person 1', 'Demo Person 2', 'Demo Person 3'],
            'verification_status': ['Verified', 'Not Verified', 'Pending'],
            'last_verified': ['2023-01-01', '2023-02-01', '2023-03-01']
        })
        
        logger.info("Using demo identity verification data")
        return demo_data
    except Exception as e:
        logger.error(f"Error creating demo identity verification data: {str(e)}")
        return pd.DataFrame()

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\utils\__init__.py ===

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\utils\.ipynb_checkpoints\api_utils-checkpoint.py ===
"""
API Utilities Module
Handles checking API availability and providing demo data
"""
import yaml
import os
import logging
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
logger = logging.getLogger(__name__)

def is_api_available(service_name):
    """
    Check if an API service is available
    
    Args:
        service_name (str): Name of the service (e.g., 'gemini', 'openai', 'news_api')
        
    Returns:
        bool: True if API is available, False otherwise
    """
    try:
        # Get the directory where this script is located
        script_dir = os.path.dirname(os.path.abspath(__file__))
        config_path = os.path.join(script_dir, '..', '..', 'config', 'api_keys.yml')
        
        with open(config_path, 'r') as f:
            config = yaml.safe_load(f)
        
        api_key = config.get(service_name, {}).get('api_key', '')
        
        # Check if key is available and not a placeholder
        return api_key and api_key not in ["YOUR_" + service_name.upper() + "_API_KEY", "NOT_AVAILABLE"]
    except Exception as e:
        logger.warning(f"Error checking API availability for {service_name}: {str(e)}")
        return False

def get_demo_sanctions_data():
    """
    Get demo sanctions data for testing
    
    Returns:
        DataFrame: Demo sanctions data
    """
    try:
        # Create demo data
        demo_data = pd.DataFrame({
            'entity_id': ['ENT001', 'ENT002', 'ENT003'],
            'name': ['Demo Entity 1', 'Demo Entity 2', 'Demo Entity 3'],
            'country': ['North Korea', 'Iran', 'Syria'],
            'list_type': ['Sanctions List', 'Sanctions List', 'Sanctions List'],
            'added_date': ['2023-01-01', '2023-02-01', '2023-03-01']
        })
        
        logger.info("Using demo sanctions data")
        return demo_data
    except Exception as e:
        logger.error(f"Error creating demo sanctions data: {str(e)}")
        return pd.DataFrame()

def get_demo_tax_compliance_data():
    """
    Get demo tax compliance data for testing
    
    Returns:
        DataFrame: Demo tax compliance data
    """
    try:
        # Create demo data
        demo_data = pd.DataFrame({
            'tax_id': ['TAX001', 'TAX002', 'TAX003'],
            'entity_name': ['Demo Corp 1', 'Demo Corp 2', 'Demo Corp 3'],
            'compliance_status': ['Compliant', 'Non-compliant', 'Under Review'],
            'last_checked': ['2023-01-01', '2023-02-01', '2023-03-01']
        })
        
        logger.info("Using demo tax compliance data")
        return demo_data
    except Exception as e:
        logger.error(f"Error creating demo tax compliance data: {str(e)}")
        return pd.DataFrame()

def get_demo_bank_verification_data():
    """
    Get demo bank verification data for testing
    
    Returns:
        DataFrame: Demo bank verification data
    """
    try:
        # Create demo data
        demo_data = pd.DataFrame({
            'account_number': ['ACC001', 'ACC002', 'ACC003'],
            'bank_name': ['Demo Bank 1', 'Demo Bank 2', 'Demo Bank 3'],
            'verification_status': ['Verified', 'Not Verified', 'Pending'],
            'last_verified': ['2023-01-01', '2023-02-01', '2023-03-01']
        })
        
        logger.info("Using demo bank verification data")
        return demo_data
    except Exception as e:
        logger.error(f"Error creating demo bank verification data: {str(e)}")
        return pd.DataFrame()

def get_demo_identity_verification_data():
    """
    Get demo identity verification data for testing
    
    Returns:
        DataFrame: Demo identity verification data
    """
    try:
        # Create demo data
        demo_data = pd.DataFrame({
            'id_number': ['ID001', 'ID002', 'ID003'],
            'name': ['Demo Person 1', 'Demo Person 2', 'Demo Person 3'],
            'verification_status': ['Verified', 'Not Verified', 'Pending'],
            'last_verified': ['2023-01-01', '2023-02-01', '2023-03-01']
        })
        
        logger.info("Using demo identity verification data")
        return demo_data
    except Exception as e:
        logger.error(f"Error creating demo identity verification data: {str(e)}")
        return pd.DataFrame()

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\utils\.ipynb_checkpoints\__init__-checkpoint.py ===

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\utils\__pycache__\api_utils.cpython-39.pyc ===
a
    èöh  „                   @   sr   d Z ddlZddlZddlZddlZddlZddlmZm	Z	 e†
e°ZddÑ ZddÑ Zdd	Ñ Zd
dÑ ZddÑ ZdS )zP
API Utilities Module
Handles checking API availability and providing demo data
È    N)⁄datetime⁄	timedeltac              
   C   s‘   zét j†t j†t°°}t j†|dddd°}t|dÉè}t†|°}W d  É n1 sT0    Y  |†	| i °†	dd°}|oå|d| †
°  d	 d
fvW S  tyŒ } z(t†d| õ dt|Éõ ù° W Y d}~dS d}~0 0 dS )z‚
    Check if an API service is available
    
    Args:
        service_name (str): Name of the service (e.g., 'gemini', 'openai', 'news_api')
        
    Returns:
        bool: True if API is available, False otherwise
    z..⁄configzapi_keys.yml⁄rN⁄api_key⁄ ZYOUR_Z_API_KEYZNOT_AVAILABLEz$Error checking API availability for z: F)⁄os⁄path⁄dirname⁄abspath⁄__file__⁄join⁄open⁄yaml⁄	safe_load⁄get⁄upper⁄	Exception⁄logger⁄warning⁄str)⁄service_name⁄
script_dir⁄config_path⁄fr   r   ⁄e© r   ˙eC:\Users\Tranquil Abyss\fraud-detection-platform\app\..\src\fraud_detection_engine\utils\api_utils.py⁄is_api_available   s    
(r   c               
   C   sÄ   z:t †g d¢g d¢g d¢g d¢g d¢dú°} t†d° | W S  tyz } z(t†dt|Éõ ù° t †° W  Y d	}~S d	}~0 0 d	S )
zf
    Get demo sanctions data for testing
    
    Returns:
        DataFrame: Demo sanctions data
    )ZENT001ZENT002ZENT003)zDemo Entity 1zDemo Entity 2zDemo Entity 3)zNorth Korea⁄IranZSyria)˙Sanctions Listr    r    ©z
2023-01-01z
2023-02-01z
2023-03-01)Z	entity_id⁄name⁄country⁄	list_typeZ
added_datezUsing demo sanctions dataz$Error creating demo sanctions data: N©⁄pd⁄	DataFramer   ⁄infor   ⁄errorr   ©Z	demo_datar   r   r   r   ⁄get_demo_sanctions_data'   s    ˚
r+   c               
   C   sz   z4t †g d¢g d¢g d¢g d¢dú°} t†d° | W S  tyt } z(t†dt|Éõ ù° t †° W  Y d}~S d}~0 0 dS )	zp
    Get demo tax compliance data for testing
    
    Returns:
        DataFrame: Demo tax compliance data
    )ZTAX001ZTAX002ZTAX003)zDemo Corp 1zDemo Corp 2zDemo Corp 3)Z	CompliantzNon-compliantzUnder Reviewr!   )Ztax_idZentity_nameZcompliance_statusZlast_checkedzUsing demo tax compliance dataz)Error creating demo tax compliance data: Nr%   r*   r   r   r   ⁄get_demo_tax_compliance_data>   s    ¸
r,   c               
   C   sz   z4t †g d¢g d¢g d¢g d¢dú°} t†d° | W S  tyt } z(t†dt|Éõ ù° t †° W  Y d}~S d}~0 0 dS )	zv
    Get demo bank verification data for testing
    
    Returns:
        DataFrame: Demo bank verification data
    )ZACC001ZACC002ZACC003)zDemo Bank 1zDemo Bank 2zDemo Bank 3©ZVerifiedzNot Verified⁄Pendingr!   )Zaccount_numberZ	bank_name⁄verification_status⁄last_verifiedz!Using demo bank verification dataz,Error creating demo bank verification data: Nr%   r*   r   r   r   ⁄get_demo_bank_verification_dataT   s    ¸
r1   c               
   C   sz   z4t †g d¢g d¢g d¢g d¢dú°} t†d° | W S  tyt } z(t†dt|Éõ ù° t †° W  Y d}~S d}~0 0 dS )	z~
    Get demo identity verification data for testing
    
    Returns:
        DataFrame: Demo identity verification data
    )ZID001ZID002ZID003)zDemo Person 1zDemo Person 2zDemo Person 3r-   r!   )Z	id_numberr"   r/   r0   z%Using demo identity verification dataz0Error creating demo identity verification data: Nr%   r*   r   r   r   ⁄#get_demo_identity_verification_dataj   s    ¸
r2   )⁄__doc__r   r   ⁄logging⁄pandasr&   ⁄numpy⁄npr   r   ⁄	getLogger⁄__name__r   r   r+   r,   r1   r2   r   r   r   r   ⁄<module>   s   


=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\utils\__pycache__\__init__.cpython-39.pyc ===
a
    èöh    „                   @   s   d S )N© r   r   r   ˙dC:\Users\Tranquil Abyss\fraud-detection-platform\app\..\src\fraud_detection_engine\utils\__init__.py⁄<module>   Û    

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\__pycache__\__init__.cpython-313.pyc ===
Û
    cÄöh    „                   Û   ï g )N© r   Û    ⁄^C:\Users\Tranquil Abyss\fraud-detection-platform\app\..\src\fraud_detection_engine\__init__.py⁄<module>r      s   Òr   

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\__pycache__\__init__.cpython-39.pyc ===
a
    èöh    „                   @   s   d S )N© r   r   r   ˙^C:\Users\Tranquil Abyss\fraud-detection-platform\app\..\src\fraud_detection_engine\__init__.py⁄<module>   Û    

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\tests\test_features.py ===
"""
Test cases for feature engineering module
"""

import pytest
import pandas as pd
import numpy as np
import os
import sys

# Add src to path
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'src'))

from fraud_detection_engine.features.statistical_features import StatisticalFeatures
from fraud_detection_engine.features.graph_features import GraphFeatures
from fraud_detection_engine.features.nlp_features import NLPFeatures
from fraud_detection_engine.features.timeseries_features import TimeSeriesFeatures

class TestStatisticalFeatures:
    """Test cases for StatisticalFeatures class"""
    
    def setup_method(self):
        """Setup test data"""
        self.feature_extractor = StatisticalFeatures()
        
        # Create test data
        np.random.seed(42)
        self.test_data = pd.DataFrame({
            'transaction_id': range(1, 101),
            'timestamp': pd.date_range('2023-01-01', periods=100, freq='D'),
            'amount': np.random.lognormal(5, 1, 100),
            'sender_id': [f'sender_{i % 10}' for i in range(100)],
            'receiver_id': [f'receiver_{i % 10}' for i in range(100)]
        })
    
    def test_extract_features(self):
        """Test feature extraction"""
        result_df = self.feature_extractor.extract_features(self.test_data)
        
        # Check that features are added
        original_cols = set(self.test_data.columns)
        result_cols = set(result_df.columns)
        new_cols = result_cols - original_cols
        
        assert len(new_cols) > 0
        
        # Check specific features
        assert 'amount_zscore' in result_df.columns
        assert 'amount_mad_zscore' in result_df.columns
        assert 'amount_percentile_rank' in result_df.columns
        assert 'benford_chi_square' in result_df.columns
    
    def test_benford_features(self):
        """Test Benford's Law features"""
        result_df = self.feature_extractor._extract_benford_features(self.test_data)
        
        assert 'benford_chi_square' in result_df.columns
        assert 'benford_p_value' in result_df.columns
        
        # Check that deviation features are added
        for i in range(1, 6):
            assert f'benford_deviation_{i}' in result_df.columns
    
    def test_zscore_features(self):
        """Test Z-score features"""
        result_df = self.feature_extractor._extract_zscore_features(self.test_data)
        
        assert 'amount_zscore' in result_df.columns
        assert 'amount_zscore_outlier' in result_df.columns
        assert 'sender_amount_zscore' in result_df.columns
        assert 'receiver_amount_zscore' in result_df.columns
    
    def test_mad_features(self):
        """Test MAD features"""
        result_df = self.feature_extractor._extract_mad_features(self.test_data)
        
        assert 'amount_mad_zscore' in result_df.columns
        assert 'amount_mad_outlier' in result_df.columns
        assert 'sender_amount_mad_zscore' in result_df.columns
        assert 'receiver_amount_mad_zscore' in result_df.columns
    
    def test_fit_transform(self):
        """Test fit_transform method"""
        result_df = self.feature_extractor.fit_transform(self.test_data)
        
        # Check that features are added
        original_cols = set(self.test_data.columns)
        result_cols = set(result_df.columns)
        new_cols = result_cols - original_cols
        
        assert len(new_cols) > 0
        assert self.feature_extractor.fitted == True
    
    def test_transform(self):
        """Test transform method"""
        # First fit the extractor
        self.feature_extractor.fit_transform(self.test_data)
        
        # Create new data
        new_data = pd.DataFrame({
            'transaction_id': range(101, 111),
            'timestamp': pd.date_range('2023-04-11', periods=10, freq='D'),
            'amount': np.random.lognormal(5, 1, 10),
            'sender_id': [f'sender_{i}' for i in range(10)],
            'receiver_id': [f'receiver_{i}' for i in range(10)]
        })
        
        # Transform new data
        result_df = self.feature_extractor.transform(new_data)
        
        # Check that features are added
        original_cols = set(new_data.columns)
        result_cols = set(result_df.columns)
        new_cols = result_cols - original_cols
        
        assert len(new_cols) > 0

class TestGraphFeatures:
    """Test cases for GraphFeatures class"""
    
    def setup_method(self):
        """Setup test data"""
        self.feature_extractor = GraphFeatures()
        
        # Create test data
        self.test_data = pd.DataFrame({
            'transaction_id': range(1, 21),
            'timestamp': pd.date_range('2023-01-01', periods=20, freq='D'),
            'amount': np.random.uniform(100, 1000, 20),
            'sender_id': [f'sender_{i % 5}' for i in range(20)],
            'receiver_id': [f'receiver_{i % 5}' for i in range(20)]
        })
    
    def test_extract_features(self):
        """Test feature extraction"""
        result_df = self.feature_extractor.extract_features(self.test_data)
        
        # Check that features are added
        original_cols = set(self.test_data.columns)
        result_cols = set(result_df.columns)
        new_cols = result_cols - original_cols
        
        assert len(new_cols) > 0
        
        # Check specific features
        assert 'sender_degree_centrality' in result_df.columns
        assert 'receiver_degree_centrality' in result_df.columns
        assert 'sender_clustering_coefficient' in result_df.columns
        assert 'receiver_clustering_coefficient' in result_df.columns
    
    def test_build_graphs(self):
        """Test graph building"""
        self.feature_extractor._build_graphs(self.test_data)
        
        # Check that graphs are built
        assert self.feature_extractor.graph is not None
        assert self.feature_extractor.sender_graph is not None
        assert self.feature_extractor.receiver_graph is not None
        assert self.feature_extractor.bipartite_graph is not None
    
    def test_centrality_features(self):
        """Test centrality features"""
        result_df = self.feature_extractor._extract_centrality_features(self.test_data)
        
        assert 'sender_degree_centrality' in result_df.columns
        assert 'receiver_degree_centrality' in result_df.columns
        assert 'sender_betweenness_centrality' in result_df.columns
        assert 'receiver_betweenness_centrality' in result_df.columns
        assert 'sender_pagerank' in result_df.columns
        assert 'receiver_pagerank' in result_df.columns
    
    def test_clustering_features(self):
        """Test clustering features"""
        result_df = self.feature_extractor._extract_clustering_features(self.test_data)
        
        assert 'sender_clustering_coefficient' in result_df.columns
        assert 'receiver_clustering_coefficient' in result_df.columns
        assert 'sender_avg_neighbor_degree' in result_df.columns
        assert 'receiver_avg_neighbor_degree' in result_df.columns

class TestNLPFeatures:
    """Test cases for NLPFeatures class"""
    
    def setup_method(self):
        """Setup test data"""
        self.feature_extractor = NLPFeatures()
        
        # Create test data
        self.test_data = pd.DataFrame({
            'transaction_id': range(1, 11),
            'timestamp': pd.date_range('2023-01-01', periods=10, freq='D'),
            'amount': np.random.uniform(100, 1000, 10),
            'sender_id': [f'sender_{i}' for i in range(10)],
            'receiver_id': [f'receiver_{i}' for i in range(10)],
            'description': [
                'Payment for services',
                'URGENT: Send money immediately',
                'Transfer funds',
                'Suspicious transaction',
                'Regular payment',
                'HURRY: Quick transfer needed',
                'Business expense',
                'CONFIDENTIAL: Private transfer',
                'Normal transaction',
                'Send $1000 ASAP'
            ],
            'notes': [
                'This is a normal transaction',
                'Please send money urgently',
                'Business related transfer',
                'This looks suspicious',
                'Regular monthly payment',
                'Need this done quickly',
                'Office supplies',
                'Keep this confidential',
                'Standard transaction',
                'Emergency funds needed'
            ]
        })
    
    def test_extract_features(self):
        """Test feature extraction"""
        result_df = self.feature_extractor.extract_features(self.test_data)
        
        # Check that features are added
        original_cols = set(self.test_data.columns)
        result_cols = set(result_df.columns)
        new_cols = result_cols - original_cols
        
        assert len(new_cols) > 0
        
        # Check specific features
        assert 'description_char_count' in result_df.columns
        assert 'description_sentiment_compound' in result_df.columns
        assert 'description_fraud_keyword_count' in result_df.columns
        assert 'description_money_pattern_count' in result_df.columns
    
    def test_basic_text_features(self):
        """Test basic text features"""
        result_df = self.feature_extractor._extract_basic_text_features(self.test_data)
        
        assert 'description_char_count' in result_df.columns
        assert 'description_word_count' in result_df.columns
        assert 'description_sentence_count' in result_df.columns
        assert 'description_avg_word_length' in result_df.columns
        assert 'description_punctuation_count' in result_df.columns
    
    def test_sentiment_features(self):
        """Test sentiment features"""
        result_df = self.feature_extractor._extract_sentiment_features(self.test_data)
        
        assert 'description_sentiment_neg' in result_df.columns
        assert 'description_sentiment_neu' in result_df.columns
        assert 'description_sentiment_pos' in result_df.columns
        assert 'description_sentiment_compound' in result_df.columns
        assert 'description_textblob_polarity' in result_df.columns
        assert 'description_textblob_subjectivity' in result_df.columns
    
    def test_keyword_features(self):
        """Test keyword features"""
        result_df = self.feature_extractor._extract_keyword_features(self.test_data)
        
        assert 'description_fraud_keyword_count' in result_df.columns
        assert 'description_has_fraud_keywords' in result_df.columns
        assert 'description_urgency_keyword_count' in result_df.columns
        assert 'description_secrecy_keyword_count' in result_df.columns
        assert 'description_money_keyword_count' in result_df.columns

class TestTimeSeriesFeatures:
    """Test cases for TimeSeriesFeatures class"""
    
    def setup_method(self):
        """Setup test data"""
        self.feature_extractor = TimeSeriesFeatures()
        
        # Create test data with timestamps
        np.random.seed(42)
        timestamps = pd.date_range('2023-01-01', periods=100, freq='H')
        
        self.test_data = pd.DataFrame({
            'transaction_id': range(1, 101),
            'timestamp': timestamps,
            'amount': np.random.lognormal(5, 1, 100),
            'sender_id': [f'sender_{i % 10}' for i in range(100)],
            'receiver_id': [f'receiver_{i % 10}' for i in range(100)]
        })
    
    def test_extract_features(self):
        """Test feature extraction"""
        result_df = self.feature_extractor.extract_features(self.test_data)
        
        # Check that features are added
        original_cols = set(self.test_data.columns)
        result_cols = set(result_df.columns)
        new_cols = result_cols - original_cols
        
        assert len(new_cols) > 0
        
        # Check specific features
        assert 'hour' in result_df.columns
        assert 'dayofweek' in result_df.columns
        assert 'is_weekend' in result_df.columns
        assert 'is_business_hours' in result_df.columns
        assert 'transaction_frequency_1H' in result_df.columns
    
    def test_temporal_features(self):
        """Test temporal features"""
        result_df = self.feature_extractor._extract_temporal_features(self.test_data)
        
        assert 'hour' in result_df.columns
        assert 'day' in result_df.columns
        assert 'month' in result_df.columns
        assert 'year' in result_df.columns
        assert 'dayofweek' in result_df.columns
        assert 'is_weekend' in result_df.columns
        assert 'is_business_hours' in result_df.columns
    
    def test_frequency_features(self):
        """Test frequency features"""
        result_df = self.feature_extractor._extract_frequency_features(self.test_data)
        
        assert 'transaction_frequency_1H' in result_df.columns
        assert 'transaction_frequency_6H' in result_df.columns
        assert 'transaction_frequency_24H' in result_df.columns
        assert 'sender_frequency_1H' in result_df.columns
        assert 'receiver_frequency_1H' in result_df.columns
    
    def test_burstiness_features(self):
        """Test burstiness features"""
        result_df = self.feature_extractor._extract_burstiness_features(self.test_data)
        
        assert 'burstiness_coefficient' in result_df.columns
        assert 'local_burstiness_10' in result_df.columns
        assert 'is_in_burst' in result_df.columns
        assert 'burst_duration' in result_df.columns

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\tests\test_ingestion.py ===
"""
Test cases for data ingestion module
"""

import pytest
import pandas as pd
import numpy as np
import os
import tempfile
import sys

# Add src to path
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'src'))

from fraud_detection_engine.ingestion.data_loader import DataLoader
from fraud_detection_engine.ingestion.column_mapper import ColumnMapper

class TestDataLoader:
    """Test cases for DataLoader class"""
    
    def setup_method(self):
        """Setup test data"""
        self.loader = DataLoader()
        
        # Create test data
        self.test_data = pd.DataFrame({
            'transaction_id': range(1, 101),
            'timestamp': pd.date_range('2023-01-01', periods=100, freq='D'),
            'amount': np.random.uniform(10, 1000, 100),
            'sender_id': [f'sender_{i}' for i in range(1, 101)],
            'receiver_id': [f'receiver_{i}' for i in range(1, 101)],
            'currency': ['USD'] * 100,
            'description': [f'Transaction {i}' for i in range(1, 101)]
        })
        
        # Create temporary CSV file
        self.temp_csv = tempfile.NamedTemporaryFile(suffix='.csv', delete=False)
        self.test_data.to_csv(self.temp_csv.name, index=False)
        self.temp_csv.close()
    
    def teardown_method(self):
        """Clean up test data"""
        os.unlink(self.temp_csv.name)
    
    def test_load_csv(self):
        """Test loading CSV file"""
        df = self.loader.load_data(self.temp_csv.name)
        
        assert len(df) == 100
        assert 'transaction_id' in df.columns
        assert 'timestamp' in df.columns
        assert 'amount' in df.columns
        assert 'sender_id' in df.columns
        assert 'receiver_id' in df.columns
    
    def test_preprocess_data(self):
        """Test data preprocessing"""
        # Create data with missing values
        test_data_with_missing = self.test_data.copy()
        test_data_with_missing.loc[0, 'amount'] = np.nan
        test_data_with_missing.loc[1, 'sender_id'] = np.nan
        
        # Preprocess data
        processed_data = self.loader._preprocess_data(test_data_with_missing)
        
        # Check that missing values are handled
        assert not processed_data['amount'].isna().any()
        assert not processed_data['sender_id'].isna().any()
        
        # Check that timestamp is converted to datetime
        assert pd.api.types.is_datetime64_any_dtype(processed_data['timestamp'])
    
    def test_validate_data(self):
        """Test data validation"""
        validation_result = self.loader.validate_data()
        
        assert validation_result['valid'] == True
        assert len(validation_result['errors']) == 0
    
    def test_get_data_info(self):
        """Test getting data information"""
        self.loader.load_data(self.temp_csv.name)
        info = self.loader.get_data_info()
        
        assert 'shape' in info
        assert 'columns' in info
        assert 'dtypes' in info
        assert 'missing_values' in info
        assert 'memory_usage' in info
        assert 'numeric_stats' in info
        assert 'categorical_stats' in info
    
    def test_get_sample(self):
        """Test getting data sample"""
        self.loader.load_data(self.temp_csv.name)
        sample = self.loader.get_sample(n=5)
        
        assert len(sample) == 5
    
    def test_save_data(self):
        """Test saving data"""
        self.loader.load_data(self.temp_csv.name)
        
        # Create temporary file for saving
        temp_save = tempfile.NamedTemporaryFile(suffix='.csv', delete=False)
        temp_save.close()
        
        try:
            self.loader.save_data(temp_save.name)
            
            # Load saved data and verify
            loaded_data = pd.read_csv(temp_save.name)
            assert len(loaded_data) == 100
        finally:
            os.unlink(temp_save.name)

class TestColumnMapper:
    """Test cases for ColumnMapper class"""
    
    def setup_method(self):
        """Setup test data"""
        self.mapper = ColumnMapper()
        
        # Test user columns
        self.user_columns = [
            'tx_id', 'trans_date', 'amt', 'from_id', 'to_id',
            'sender_location', 'receiver_location', 'tx_type'
        ]
    
    def test_get_expected_columns(self):
        """Test getting expected columns"""
        expected_columns = self.mapper.get_expected_columns()
        
        assert 'transaction_id' in expected_columns
        assert 'timestamp' in expected_columns
        assert 'amount' in expected_columns
        assert 'sender_id' in expected_columns
        assert 'receiver_id' in expected_columns
    
    def test_auto_map_columns(self):
        """Test automatic column mapping"""
        mapping = self.mapper.auto_map_columns(self.user_columns)
        
        assert 'tx_id' in mapping
        assert mapping['tx_id'] == 'transaction_id'
        
        assert 'trans_date' in mapping
        assert mapping['trans_date'] == 'timestamp'
        
        assert 'amt' in mapping
        assert mapping['amt'] == 'amount'
        
        assert 'from_id' in mapping
        assert mapping['from_id'] == 'sender_id'
        
        assert 'to_id' in mapping
        assert mapping['to_id'] == 'receiver_id'
    
    def test_apply_mapping(self):
        """Test applying column mapping"""
        # Create test dataframe
        test_df = pd.DataFrame({
            'tx_id': range(1, 6),
            'trans_date': pd.date_range('2023-01-01', periods=5),
            'amt': [100, 200, 300, 400, 500],
            'from_id': ['s1', 's2', 's3', 's4', 's5'],
            'to_id': ['r1', 'r2', 'r3', 'r4', 'r5']
        })
        
        # Get mapping
        mapping = self.mapper.auto_map_columns(test_df.columns.tolist())
        
        # Apply mapping
        mapped_df = self.mapper.apply_mapping(test_df, mapping)
        
        # Check that mapped columns exist
        assert 'transaction_id' in mapped_df.columns
        assert 'timestamp' in mapped_df.columns
        assert 'amount' in mapped_df.columns
        assert 'sender_id' in mapped_df.columns
        assert 'receiver_id' in mapped_df.columns
    
    def test_validate_mapping(self):
        """Test validating mapping"""
        mapping = self.mapper.auto_map_columns(self.user_columns)
        validation = self.mapper.validate_mapping(mapping)
        
        assert validation['valid'] == True
        assert len(validation['errors']) == 0
    
    def test_get_mapping_suggestions(self):
        """Test getting mapping suggestions"""
        suggestions = self.mapper.get_mapping_suggestions(self.user_columns)
        
        assert 'tx_id' in suggestions
        assert len(suggestions['tx_id']) > 0
        
        # Check that top suggestion is correct
        top_suggestion = suggestions['tx_id'][0]
        assert top_suggestion['column'] == 'transaction_id'
        assert top_suggestion['confidence'] > 0.8

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\tests\test_models.py ===
"""
Test cases for models module
"""

import pytest
import pandas as pd
import numpy as np
import os
import sys

# Add src to path
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'src'))

from fraud_detection_engine.models.unsupervised import UnsupervisedModels
from fraud_detection_engine.models.supervised import SupervisedModels
from fraud_detection_engine.models.rule_based import RuleEngine

class TestUnsupervisedModels:
    """Test cases for UnsupervisedModels class"""
    
    def setup_method(self):
        """Setup test data"""
        self.models = UnsupervisedModels(contamination=0.1)
        
        # Create test data
        np.random.seed(42)
        self.test_data = pd.DataFrame({
            'feature1': np.random.normal(0, 1, 100),
            'feature2': np.random.normal(0, 1, 100),
            'feature3': np.random.normal(0, 1, 100),
            'feature4': np.random.normal(0, 1, 100),
            'feature5': np.random.normal(0, 1, 100)
        })
        
        # Add some outliers
        self.test_data.loc[0:4, 'feature1'] = 10  # Outliers
        self.test_data.loc[5:9, 'feature2'] = -10  # Outliers
    
    def test_run_models(self):
        """Test running models"""
        results = self.models.run_models(self.test_data)
        
        # Check that results are returned
        assert isinstance(results, dict)
        assert len(results) > 0
        
        # Check that each model has required keys
        for model_name, model_results in results.items():
            assert 'predictions' in model_results
            assert 'scores' in model_results
            assert 'model' in model_results
            assert 'feature_names' in model_results
    
    def test_isolation_forest(self):
        """Test Isolation Forest model"""
        results = self.models.run_models(self.test_data)
        
        if 'isolation_forest' in results:
            model_results = results['isolation_forest']
            
            # Check predictions
            predictions = model_results['predictions']
            assert len(predictions) == len(self.test_data)
            assert set(predictions) == {-1, 1}  # -1 for outliers, 1 for inliers
            
            # Check scores
            scores = model_results['scores']
            assert len(scores) == len(self.test_data)
            assert all(0 <= score <= 1 for score in scores)  # Normalized scores
    
    def test_local_outlier_factor(self):
        """Test Local Outlier Factor model"""
        results = self.models.run_models(self.test_data)
        
        if 'local_outlier_factor' in results:
            model_results = results['local_outlier_factor']
            
            # Check predictions
            predictions = model_results['predictions']
            assert len(predictions) == len(self.test_data)
            assert set(predictions) == {-1, 1}  # -1 for outliers, 1 for inliers
            
            # Check scores
            scores = model_results['scores']
            assert len(scores) == len(self.test_data)
            assert all(0 <= score <= 1 for score in scores)  # Normalized scores
    
    def test_autoencoder(self):
        """Test Autoencoder model"""
        results = self.models.run_models(self.test_data)
        
        if 'autoencoder' in results:
            model_results = results['autoencoder']
            
            # Check predictions
            predictions = model_results['predictions']
            assert len(predictions) == len(self.test_data)
            assert set(predictions) == {-1, 1}  # -1 for outliers, 1 for inliers
            
            # Check scores
            scores = model_results['scores']
            assert len(scores) == len(self.test_data)
            assert all(0 <= score <= 1 for score in scores)  # Normalized scores
    
    def test_predict(self):
        """Test making predictions"""
        # First fit models
        self.models.run_models(self.test_data)
        
        # Create new data
        new_data = pd.DataFrame({
            'feature1': np.random.normal(0, 1, 10),
            'feature2': np.random.normal(0, 1, 10),
            'feature3': np.random.normal(0, 1, 10),
            'feature4': np.random.normal(0, 1, 10),
            'feature5': np.random.normal(0, 1, 10)
        })
        
        # Test prediction with Isolation Forest
        if 'isolation_forest' in self.models.models:
            result = self.models.predict(new_data, 'isolation_forest')
            
            assert 'predictions' in result
            assert 'scores' in result
            assert len(result['predictions']) == len(new_data)
            assert len(result['scores']) == len(new_data)
    
    def test_get_feature_importance(self):
        """Test getting feature importance"""
        # First fit models
        self.models.run_models(self.test_data)
        
        # Test with Isolation Forest
        if 'isolation_forest' in self.models.models:
            importance = self.models.get_feature_importance('isolation_forest')
            
            assert isinstance(importance, pd.DataFrame)
            assert 'feature' in importance.columns
            assert 'importance' in importance.columns
            assert len(importance) == len(self.test_data.columns)

class TestSupervisedModels:
    """Test cases for SupervisedModels class"""
    
    def setup_method(self):
        """Setup test data"""
        self.models = SupervisedModels(test_size=0.3, random_state=42)
        
        # Create test data
        np.random.seed(42)
        self.test_data = pd.DataFrame({
            'feature1': np.random.normal(0, 1, 100),
            'feature2': np.random.normal(0, 1, 100),
            'feature3': np.random.normal(0, 1, 100),
            'feature4': np.random.normal(0, 1, 100),
            'feature5': np.random.normal(0, 1, 100),
            'fraud_flag': np.random.choice([0, 1], 100, p=[0.9, 0.1])  # Imbalanced classes
        })
    
    def test_run_models(self):
        """Test running models"""
        results = self.models.run_models(self.test_data)
        
        # Check that results are returned
        assert isinstance(results, dict)
        assert len(results) > 0
        
        # Check that each model has required keys
        for model_name, model_results in results.items():
            assert 'model' in model_results
            assert 'performance' in model_results
            assert 'feature_importance' in model_results
            assert 'feature_names' in model_results
    
    def test_random_forest(self):
        """Test Random Forest model"""
        results = self.models.run_models(self.test_data)
        
        if 'random_forest' in results:
            model_results = results['random_forest']
            
            # Check model
            model = model_results['model']
            assert model is not None
            
            # Check performance
            performance = model_results['performance']
            assert 'accuracy' in performance
            assert 'precision' in performance
            assert 'recall' in performance
            assert 'f1' in performance
            assert 'roc_auc' in performance
            
            # Check feature importance
            importance = model_results['feature_importance']
            assert isinstance(importance, pd.DataFrame)
            assert 'feature' in importance.columns
            assert 'importance' in importance.columns
    
    def test_xgboost(self):
        """Test XGBoost model"""
        results = self.models.run_models(self.test_data)
        
        if 'xgboost' in results:
            model_results = results['xgboost']
            
            # Check model
            model = model_results['model']
            assert model is not None
            
            # Check performance
            performance = model_results['performance']
            assert 'accuracy' in performance
            assert 'precision' in performance
            assert 'recall' in performance
            assert 'f1' in performance
            assert 'roc_auc' in performance
            
            # Check feature importance
            importance = model_results['feature_importance']
            assert isinstance(importance, pd.DataFrame)
            assert 'feature' in importance.columns
            assert 'importance' in importance.columns
    
    def test_predict(self):
        """Test making predictions"""
        # First fit models
        self.models.run_models(self.test_data)
        
        # Create new data
        new_data = pd.DataFrame({
            'feature1': np.random.normal(0, 1, 10),
            'feature2': np.random.normal(0, 1, 10),
            'feature3': np.random.normal(0, 1, 10),
            'feature4': np.random.normal(0, 1, 10),
            'feature5': np.random.normal(0, 1, 10)
        })
        
        # Test prediction with Random Forest
        if 'random_forest' in self.models.models:
            result = self.models.predict(new_data, 'random_forest')
            
            assert 'predictions' in result
            assert 'probabilities' in result
            assert len(result['predictions']) == len(new_data)
            assert len(result['probabilities']) == len(new_data)
    
    def test_get_feature_importance(self):
        """Test getting feature importance"""
        # First fit models
        self.models.run_models(self.test_data)
        
        # Test with Random Forest
        if 'random_forest' in self.models.models:
            importance = self.models.get_feature_importance('random_forest')
            
            assert isinstance(importance, pd.DataFrame)
            assert 'feature' in importance.columns
            assert 'importance' in importance.columns
    
    def test_get_performance(self):
        """Test getting performance metrics"""
        # First fit models
        self.models.run_models(self.test_data)
        
        # Test with Random Forest
        if 'random_forest' in self.models.models:
            performance = self.models.get_performance('random_forest')
            
            assert isinstance(performance, dict)
            assert 'accuracy' in performance
            assert 'precision' in performance
            assert 'recall' in performance
            assert 'f1' in performance
            assert 'roc_auc' in performance

class TestRuleEngine:
    """Test cases for RuleEngine class"""
    
    def setup_method(self):
        """Setup test data"""
        self.rule_engine = RuleEngine(threshold=0.5)
        
        # Create test data
        self.test_data = pd.DataFrame({
            'transaction_id': range(1, 11),
            'timestamp': pd.date_range('2023-01-01', periods=10, freq='D'),
            'amount': [100, 200, 10000, 5000, 15000, 300, 400, 12000, 8000, 20000],
            'sender_id': [f'sender_{i}' for i in range(10)],
            'receiver_id': [f'receiver_{i}' for i in range(10)],
            'sender_location': ['USA'] * 5 + ['Canada'] * 5,
            'receiver_location': ['USA'] * 8 + ['North Korea'] * 2,
            'description': [
                'Normal payment',
                'Regular transfer',
                'Large amount',
                'Big transaction',
                'Huge payment',
                'Small amount',
                'Normal transfer',
                'Large transaction',
                'Big payment',
                'Massive amount'
            ]
        })
    
    def test_apply_rules(self):
        """Test applying rules"""
        results = self.rule_engine.apply_rules(self.test_data)
        
        # Check that results are returned
        assert isinstance(results, dict)
        
        # Check required keys
        assert 'rule_results' in results
        assert 'rule_scores' in results
        assert 'total_scores' in results
        assert 'normalized_scores' in results
        assert 'rule_violations' in results
        assert 'violated_rule_names' in results
        assert 'rules' in results
        assert 'rule_weights' in results
        assert 'rule_descriptions' in results
    
    def test_high_amount_rule(self):
        """Test high amount rule"""
        # Apply rules
        results = self.rule_engine.apply_rules(self.test_data)
        
        # Check rule results
        if 'high_amount' in results['rule_results']:
            rule_results = results['rule_results']['high_amount']
            
            # Should flag transactions with amount > 10000
            expected_flags = [amount > 10000 for amount in self.test_data['amount']]
            assert rule_results == expected_flags
    
    def test_cross_border_rule(self):
        """Test cross border rule"""
        # Apply rules
        results = self.rule_engine.apply_rules(self.test_data)
        
        # Check rule results
        if 'cross_border' in results['rule_results']:
            rule_results = results['rule_results']['cross_border']
            
            # Should flag transactions with different sender and receiver countries
            expected_flags = []
            for i in range(len(self.test_data)):
                sender_country = self.test_data.loc[i, 'sender_location'].split(',')[0]
                receiver_country = self.test_data.loc[i, 'receiver_location'].split(',')[0]
                expected_flags.append(sender_country != receiver_country)
            
            assert rule_results == expected_flags
    
    def test_high_risk_country_rule(self):
        """Test high risk country rule"""
        # Apply rules
        results = self.rule_engine.apply_rules(self.test_data)
        
        # Check rule results
        if 'high_risk_country' in results['rule_results']:
            rule_results = results['rule_results']['high_risk_country']
            
            # Should flag transactions involving North Korea
            expected_flags = ['North Korea' in location for location in self.test_data['receiver_location']]
            assert rule_results == expected_flags
    
    def test_rule_violations(self):
        """Test rule violations"""
        # Apply rules
        results = self.rule_engine.apply_rules(self.test_data)
        
        # Check that violations are detected
        violations = results['rule_violations']
        assert len(violations) == len(self.test_data)
        
        # Check that some transactions are flagged
        assert any(violations)  # At least one transaction should be flagged
    
    def test_add_rule(self):
        """Test adding custom rule"""
        # Add custom rule
        def custom_rule(row):
            return row.get('amount', 0) > 5000
        
        self.rule_engine.add_rule('custom_rule', custom_rule, weight=0.5, description='Custom rule')
        
        # Check that rule is added
        assert 'custom_rule' in self.rule_engine.rules
        assert self.rule_engine.rule_weights['custom_rule'] == 0.5
        assert self.rule_engine.rule_descriptions['custom_rule'] == 'Custom rule'
    
    def test_remove_rule(self):
        """Test removing rule"""
        # Remove a rule
        if 'high_amount' in self.rule_engine.rules:
            self.rule_engine.remove_rule('high_amount')
            
            # Check that rule is removed
            assert 'high_amount' not in self.rule_engine.rules
            assert 'high_amount' not in self.rule_engine.rule_weights
            assert 'high_amount' not in self.rule_engine.rule_descriptions
    
    def test_update_rule_weight(self):
        """Test updating rule weight"""
        # Update rule weight
        self.rule_engine.update_rule_weight('high_amount', 0.5)
        
        # Check that weight is updated
        assert self.rule_engine.rule_weights['high_amount'] == 0.5
    
    def test_get_rules(self):
        """Test getting rules"""
        rules_info = self.rule_engine.get_rules()
        
        # Check structure
        assert 'rules' in rules_info
        assert 'weights' in rules_info
        assert 'descriptions' in rules_info
        
        # Check that rules are returned
        assert len(rules_info['rules']) > 0
        assert len(rules_info['weights']) > 0
        assert len(rules_info['descriptions']) > 0

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\tests\.ipynb_checkpoints\test_features-checkpoint.py ===
"""
Test cases for feature engineering module
"""

import pytest
import pandas as pd
import numpy as np
import os
import sys

# Add src to path
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'src'))

from fraud_detection_engine.features.statistical_features import StatisticalFeatures
from fraud_detection_engine.features.graph_features import GraphFeatures
from fraud_detection_engine.features.nlp_features import NLPFeatures
from fraud_detection_engine.features.timeseries_features import TimeSeriesFeatures

class TestStatisticalFeatures:
    """Test cases for StatisticalFeatures class"""
    
    def setup_method(self):
        """Setup test data"""
        self.feature_extractor = StatisticalFeatures()
        
        # Create test data
        np.random.seed(42)
        self.test_data = pd.DataFrame({
            'transaction_id': range(1, 101),
            'timestamp': pd.date_range('2023-01-01', periods=100, freq='D'),
            'amount': np.random.lognormal(5, 1, 100),
            'sender_id': [f'sender_{i % 10}' for i in range(100)],
            'receiver_id': [f'receiver_{i % 10}' for i in range(100)]
        })
    
    def test_extract_features(self):
        """Test feature extraction"""
        result_df = self.feature_extractor.extract_features(self.test_data)
        
        # Check that features are added
        original_cols = set(self.test_data.columns)
        result_cols = set(result_df.columns)
        new_cols = result_cols - original_cols
        
        assert len(new_cols) > 0
        
        # Check specific features
        assert 'amount_zscore' in result_df.columns
        assert 'amount_mad_zscore' in result_df.columns
        assert 'amount_percentile_rank' in result_df.columns
        assert 'benford_chi_square' in result_df.columns
    
    def test_benford_features(self):
        """Test Benford's Law features"""
        result_df = self.feature_extractor._extract_benford_features(self.test_data)
        
        assert 'benford_chi_square' in result_df.columns
        assert 'benford_p_value' in result_df.columns
        
        # Check that deviation features are added
        for i in range(1, 6):
            assert f'benford_deviation_{i}' in result_df.columns
    
    def test_zscore_features(self):
        """Test Z-score features"""
        result_df = self.feature_extractor._extract_zscore_features(self.test_data)
        
        assert 'amount_zscore' in result_df.columns
        assert 'amount_zscore_outlier' in result_df.columns
        assert 'sender_amount_zscore' in result_df.columns
        assert 'receiver_amount_zscore' in result_df.columns
    
    def test_mad_features(self):
        """Test MAD features"""
        result_df = self.feature_extractor._extract_mad_features(self.test_data)
        
        assert 'amount_mad_zscore' in result_df.columns
        assert 'amount_mad_outlier' in result_df.columns
        assert 'sender_amount_mad_zscore' in result_df.columns
        assert 'receiver_amount_mad_zscore' in result_df.columns
    
    def test_fit_transform(self):
        """Test fit_transform method"""
        result_df = self.feature_extractor.fit_transform(self.test_data)
        
        # Check that features are added
        original_cols = set(self.test_data.columns)
        result_cols = set(result_df.columns)
        new_cols = result_cols - original_cols
        
        assert len(new_cols) > 0
        assert self.feature_extractor.fitted == True
    
    def test_transform(self):
        """Test transform method"""
        # First fit the extractor
        self.feature_extractor.fit_transform(self.test_data)
        
        # Create new data
        new_data = pd.DataFrame({
            'transaction_id': range(101, 111),
            'timestamp': pd.date_range('2023-04-11', periods=10, freq='D'),
            'amount': np.random.lognormal(5, 1, 10),
            'sender_id': [f'sender_{i}' for i in range(10)],
            'receiver_id': [f'receiver_{i}' for i in range(10)]
        })
        
        # Transform new data
        result_df = self.feature_extractor.transform(new_data)
        
        # Check that features are added
        original_cols = set(new_data.columns)
        result_cols = set(result_df.columns)
        new_cols = result_cols - original_cols
        
        assert len(new_cols) > 0

class TestGraphFeatures:
    """Test cases for GraphFeatures class"""
    
    def setup_method(self):
        """Setup test data"""
        self.feature_extractor = GraphFeatures()
        
        # Create test data
        self.test_data = pd.DataFrame({
            'transaction_id': range(1, 21),
            'timestamp': pd.date_range('2023-01-01', periods=20, freq='D'),
            'amount': np.random.uniform(100, 1000, 20),
            'sender_id': [f'sender_{i % 5}' for i in range(20)],
            'receiver_id': [f'receiver_{i % 5}' for i in range(20)]
        })
    
    def test_extract_features(self):
        """Test feature extraction"""
        result_df = self.feature_extractor.extract_features(self.test_data)
        
        # Check that features are added
        original_cols = set(self.test_data.columns)
        result_cols = set(result_df.columns)
        new_cols = result_cols - original_cols
        
        assert len(new_cols) > 0
        
        # Check specific features
        assert 'sender_degree_centrality' in result_df.columns
        assert 'receiver_degree_centrality' in result_df.columns
        assert 'sender_clustering_coefficient' in result_df.columns
        assert 'receiver_clustering_coefficient' in result_df.columns
    
    def test_build_graphs(self):
        """Test graph building"""
        self.feature_extractor._build_graphs(self.test_data)
        
        # Check that graphs are built
        assert self.feature_extractor.graph is not None
        assert self.feature_extractor.sender_graph is not None
        assert self.feature_extractor.receiver_graph is not None
        assert self.feature_extractor.bipartite_graph is not None
    
    def test_centrality_features(self):
        """Test centrality features"""
        result_df = self.feature_extractor._extract_centrality_features(self.test_data)
        
        assert 'sender_degree_centrality' in result_df.columns
        assert 'receiver_degree_centrality' in result_df.columns
        assert 'sender_betweenness_centrality' in result_df.columns
        assert 'receiver_betweenness_centrality' in result_df.columns
        assert 'sender_pagerank' in result_df.columns
        assert 'receiver_pagerank' in result_df.columns
    
    def test_clustering_features(self):
        """Test clustering features"""
        result_df = self.feature_extractor._extract_clustering_features(self.test_data)
        
        assert 'sender_clustering_coefficient' in result_df.columns
        assert 'receiver_clustering_coefficient' in result_df.columns
        assert 'sender_avg_neighbor_degree' in result_df.columns
        assert 'receiver_avg_neighbor_degree' in result_df.columns

class TestNLPFeatures:
    """Test cases for NLPFeatures class"""
    
    def setup_method(self):
        """Setup test data"""
        self.feature_extractor = NLPFeatures()
        
        # Create test data
        self.test_data = pd.DataFrame({
            'transaction_id': range(1, 11),
            'timestamp': pd.date_range('2023-01-01', periods=10, freq='D'),
            'amount': np.random.uniform(100, 1000, 10),
            'sender_id': [f'sender_{i}' for i in range(10)],
            'receiver_id': [f'receiver_{i}' for i in range(10)],
            'description': [
                'Payment for services',
                'URGENT: Send money immediately',
                'Transfer funds',
                'Suspicious transaction',
                'Regular payment',
                'HURRY: Quick transfer needed',
                'Business expense',
                'CONFIDENTIAL: Private transfer',
                'Normal transaction',
                'Send $1000 ASAP'
            ],
            'notes': [
                'This is a normal transaction',
                'Please send money urgently',
                'Business related transfer',
                'This looks suspicious',
                'Regular monthly payment',
                'Need this done quickly',
                'Office supplies',
                'Keep this confidential',
                'Standard transaction',
                'Emergency funds needed'
            ]
        })
    
    def test_extract_features(self):
        """Test feature extraction"""
        result_df = self.feature_extractor.extract_features(self.test_data)
        
        # Check that features are added
        original_cols = set(self.test_data.columns)
        result_cols = set(result_df.columns)
        new_cols = result_cols - original_cols
        
        assert len(new_cols) > 0
        
        # Check specific features
        assert 'description_char_count' in result_df.columns
        assert 'description_sentiment_compound' in result_df.columns
        assert 'description_fraud_keyword_count' in result_df.columns
        assert 'description_money_pattern_count' in result_df.columns
    
    def test_basic_text_features(self):
        """Test basic text features"""
        result_df = self.feature_extractor._extract_basic_text_features(self.test_data)
        
        assert 'description_char_count' in result_df.columns
        assert 'description_word_count' in result_df.columns
        assert 'description_sentence_count' in result_df.columns
        assert 'description_avg_word_length' in result_df.columns
        assert 'description_punctuation_count' in result_df.columns
    
    def test_sentiment_features(self):
        """Test sentiment features"""
        result_df = self.feature_extractor._extract_sentiment_features(self.test_data)
        
        assert 'description_sentiment_neg' in result_df.columns
        assert 'description_sentiment_neu' in result_df.columns
        assert 'description_sentiment_pos' in result_df.columns
        assert 'description_sentiment_compound' in result_df.columns
        assert 'description_textblob_polarity' in result_df.columns
        assert 'description_textblob_subjectivity' in result_df.columns
    
    def test_keyword_features(self):
        """Test keyword features"""
        result_df = self.feature_extractor._extract_keyword_features(self.test_data)
        
        assert 'description_fraud_keyword_count' in result_df.columns
        assert 'description_has_fraud_keywords' in result_df.columns
        assert 'description_urgency_keyword_count' in result_df.columns
        assert 'description_secrecy_keyword_count' in result_df.columns
        assert 'description_money_keyword_count' in result_df.columns

class TestTimeSeriesFeatures:
    """Test cases for TimeSeriesFeatures class"""
    
    def setup_method(self):
        """Setup test data"""
        self.feature_extractor = TimeSeriesFeatures()
        
        # Create test data with timestamps
        np.random.seed(42)
        timestamps = pd.date_range('2023-01-01', periods=100, freq='H')
        
        self.test_data = pd.DataFrame({
            'transaction_id': range(1, 101),
            'timestamp': timestamps,
            'amount': np.random.lognormal(5, 1, 100),
            'sender_id': [f'sender_{i % 10}' for i in range(100)],
            'receiver_id': [f'receiver_{i % 10}' for i in range(100)]
        })
    
    def test_extract_features(self):
        """Test feature extraction"""
        result_df = self.feature_extractor.extract_features(self.test_data)
        
        # Check that features are added
        original_cols = set(self.test_data.columns)
        result_cols = set(result_df.columns)
        new_cols = result_cols - original_cols
        
        assert len(new_cols) > 0
        
        # Check specific features
        assert 'hour' in result_df.columns
        assert 'dayofweek' in result_df.columns
        assert 'is_weekend' in result_df.columns
        assert 'is_business_hours' in result_df.columns
        assert 'transaction_frequency_1H' in result_df.columns
    
    def test_temporal_features(self):
        """Test temporal features"""
        result_df = self.feature_extractor._extract_temporal_features(self.test_data)
        
        assert 'hour' in result_df.columns
        assert 'day' in result_df.columns
        assert 'month' in result_df.columns
        assert 'year' in result_df.columns
        assert 'dayofweek' in result_df.columns
        assert 'is_weekend' in result_df.columns
        assert 'is_business_hours' in result_df.columns
    
    def test_frequency_features(self):
        """Test frequency features"""
        result_df = self.feature_extractor._extract_frequency_features(self.test_data)
        
        assert 'transaction_frequency_1H' in result_df.columns
        assert 'transaction_frequency_6H' in result_df.columns
        assert 'transaction_frequency_24H' in result_df.columns
        assert 'sender_frequency_1H' in result_df.columns
        assert 'receiver_frequency_1H' in result_df.columns
    
    def test_burstiness_features(self):
        """Test burstiness features"""
        result_df = self.feature_extractor._extract_burstiness_features(self.test_data)
        
        assert 'burstiness_coefficient' in result_df.columns
        assert 'local_burstiness_10' in result_df.columns
        assert 'is_in_burst' in result_df.columns
        assert 'burst_duration' in result_df.columns

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\tests\.ipynb_checkpoints\test_ingestion-checkpoint.py ===
"""
Test cases for data ingestion module
"""

import pytest
import pandas as pd
import numpy as np
import os
import tempfile
import sys

# Add src to path
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'src'))

from fraud_detection_engine.ingestion.data_loader import DataLoader
from fraud_detection_engine.ingestion.column_mapper import ColumnMapper

class TestDataLoader:
    """Test cases for DataLoader class"""
    
    def setup_method(self):
        """Setup test data"""
        self.loader = DataLoader()
        
        # Create test data
        self.test_data = pd.DataFrame({
            'transaction_id': range(1, 101),
            'timestamp': pd.date_range('2023-01-01', periods=100, freq='D'),
            'amount': np.random.uniform(10, 1000, 100),
            'sender_id': [f'sender_{i}' for i in range(1, 101)],
            'receiver_id': [f'receiver_{i}' for i in range(1, 101)],
            'currency': ['USD'] * 100,
            'description': [f'Transaction {i}' for i in range(1, 101)]
        })
        
        # Create temporary CSV file
        self.temp_csv = tempfile.NamedTemporaryFile(suffix='.csv', delete=False)
        self.test_data.to_csv(self.temp_csv.name, index=False)
        self.temp_csv.close()
    
    def teardown_method(self):
        """Clean up test data"""
        os.unlink(self.temp_csv.name)
    
    def test_load_csv(self):
        """Test loading CSV file"""
        df = self.loader.load_data(self.temp_csv.name)
        
        assert len(df) == 100
        assert 'transaction_id' in df.columns
        assert 'timestamp' in df.columns
        assert 'amount' in df.columns
        assert 'sender_id' in df.columns
        assert 'receiver_id' in df.columns
    
    def test_preprocess_data(self):
        """Test data preprocessing"""
        # Create data with missing values
        test_data_with_missing = self.test_data.copy()
        test_data_with_missing.loc[0, 'amount'] = np.nan
        test_data_with_missing.loc[1, 'sender_id'] = np.nan
        
        # Preprocess data
        processed_data = self.loader._preprocess_data(test_data_with_missing)
        
        # Check that missing values are handled
        assert not processed_data['amount'].isna().any()
        assert not processed_data['sender_id'].isna().any()
        
        # Check that timestamp is converted to datetime
        assert pd.api.types.is_datetime64_any_dtype(processed_data['timestamp'])
    
    def test_validate_data(self):
        """Test data validation"""
        validation_result = self.loader.validate_data()
        
        assert validation_result['valid'] == True
        assert len(validation_result['errors']) == 0
    
    def test_get_data_info(self):
        """Test getting data information"""
        self.loader.load_data(self.temp_csv.name)
        info = self.loader.get_data_info()
        
        assert 'shape' in info
        assert 'columns' in info
        assert 'dtypes' in info
        assert 'missing_values' in info
        assert 'memory_usage' in info
        assert 'numeric_stats' in info
        assert 'categorical_stats' in info
    
    def test_get_sample(self):
        """Test getting data sample"""
        self.loader.load_data(self.temp_csv.name)
        sample = self.loader.get_sample(n=5)
        
        assert len(sample) == 5
    
    def test_save_data(self):
        """Test saving data"""
        self.loader.load_data(self.temp_csv.name)
        
        # Create temporary file for saving
        temp_save = tempfile.NamedTemporaryFile(suffix='.csv', delete=False)
        temp_save.close()
        
        try:
            self.loader.save_data(temp_save.name)
            
            # Load saved data and verify
            loaded_data = pd.read_csv(temp_save.name)
            assert len(loaded_data) == 100
        finally:
            os.unlink(temp_save.name)

class TestColumnMapper:
    """Test cases for ColumnMapper class"""
    
    def setup_method(self):
        """Setup test data"""
        self.mapper = ColumnMapper()
        
        # Test user columns
        self.user_columns = [
            'tx_id', 'trans_date', 'amt', 'from_id', 'to_id',
            'sender_location', 'receiver_location', 'tx_type'
        ]
    
    def test_get_expected_columns(self):
        """Test getting expected columns"""
        expected_columns = self.mapper.get_expected_columns()
        
        assert 'transaction_id' in expected_columns
        assert 'timestamp' in expected_columns
        assert 'amount' in expected_columns
        assert 'sender_id' in expected_columns
        assert 'receiver_id' in expected_columns
    
    def test_auto_map_columns(self):
        """Test automatic column mapping"""
        mapping = self.mapper.auto_map_columns(self.user_columns)
        
        assert 'tx_id' in mapping
        assert mapping['tx_id'] == 'transaction_id'
        
        assert 'trans_date' in mapping
        assert mapping['trans_date'] == 'timestamp'
        
        assert 'amt' in mapping
        assert mapping['amt'] == 'amount'
        
        assert 'from_id' in mapping
        assert mapping['from_id'] == 'sender_id'
        
        assert 'to_id' in mapping
        assert mapping['to_id'] == 'receiver_id'
    
    def test_apply_mapping(self):
        """Test applying column mapping"""
        # Create test dataframe
        test_df = pd.DataFrame({
            'tx_id': range(1, 6),
            'trans_date': pd.date_range('2023-01-01', periods=5),
            'amt': [100, 200, 300, 400, 500],
            'from_id': ['s1', 's2', 's3', 's4', 's5'],
            'to_id': ['r1', 'r2', 'r3', 'r4', 'r5']
        })
        
        # Get mapping
        mapping = self.mapper.auto_map_columns(test_df.columns.tolist())
        
        # Apply mapping
        mapped_df = self.mapper.apply_mapping(test_df, mapping)
        
        # Check that mapped columns exist
        assert 'transaction_id' in mapped_df.columns
        assert 'timestamp' in mapped_df.columns
        assert 'amount' in mapped_df.columns
        assert 'sender_id' in mapped_df.columns
        assert 'receiver_id' in mapped_df.columns
    
    def test_validate_mapping(self):
        """Test validating mapping"""
        mapping = self.mapper.auto_map_columns(self.user_columns)
        validation = self.mapper.validate_mapping(mapping)
        
        assert validation['valid'] == True
        assert len(validation['errors']) == 0
    
    def test_get_mapping_suggestions(self):
        """Test getting mapping suggestions"""
        suggestions = self.mapper.get_mapping_suggestions(self.user_columns)
        
        assert 'tx_id' in suggestions
        assert len(suggestions['tx_id']) > 0
        
        # Check that top suggestion is correct
        top_suggestion = suggestions['tx_id'][0]
        assert top_suggestion['column'] == 'transaction_id'
        assert top_suggestion['confidence'] > 0.8

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\tests\.ipynb_checkpoints\test_models-checkpoint.py ===
"""
Test cases for models module
"""

import pytest
import pandas as pd
import numpy as np
import os
import sys

# Add src to path
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'src'))

from fraud_detection_engine.models.unsupervised import UnsupervisedModels
from fraud_detection_engine.models.supervised import SupervisedModels
from fraud_detection_engine.models.rule_based import RuleEngine

class TestUnsupervisedModels:
    """Test cases for UnsupervisedModels class"""
    
    def setup_method(self):
        """Setup test data"""
        self.models = UnsupervisedModels(contamination=0.1)
        
        # Create test data
        np.random.seed(42)
        self.test_data = pd.DataFrame({
            'feature1': np.random.normal(0, 1, 100),
            'feature2': np.random.normal(0, 1, 100),
            'feature3': np.random.normal(0, 1, 100),
            'feature4': np.random.normal(0, 1, 100),
            'feature5': np.random.normal(0, 1, 100)
        })
        
        # Add some outliers
        self.test_data.loc[0:4, 'feature1'] = 10  # Outliers
        self.test_data.loc[5:9, 'feature2'] = -10  # Outliers
    
    def test_run_models(self):
        """Test running models"""
        results = self.models.run_models(self.test_data)
        
        # Check that results are returned
        assert isinstance(results, dict)
        assert len(results) > 0
        
        # Check that each model has required keys
        for model_name, model_results in results.items():
            assert 'predictions' in model_results
            assert 'scores' in model_results
            assert 'model' in model_results
            assert 'feature_names' in model_results
    
    def test_isolation_forest(self):
        """Test Isolation Forest model"""
        results = self.models.run_models(self.test_data)
        
        if 'isolation_forest' in results:
            model_results = results['isolation_forest']
            
            # Check predictions
            predictions = model_results['predictions']
            assert len(predictions) == len(self.test_data)
            assert set(predictions) == {-1, 1}  # -1 for outliers, 1 for inliers
            
            # Check scores
            scores = model_results['scores']
            assert len(scores) == len(self.test_data)
            assert all(0 <= score <= 1 for score in scores)  # Normalized scores
    
    def test_local_outlier_factor(self):
        """Test Local Outlier Factor model"""
        results = self.models.run_models(self.test_data)
        
        if 'local_outlier_factor' in results:
            model_results = results['local_outlier_factor']
            
            # Check predictions
            predictions = model_results['predictions']
            assert len(predictions) == len(self.test_data)
            assert set(predictions) == {-1, 1}  # -1 for outliers, 1 for inliers
            
            # Check scores
            scores = model_results['scores']
            assert len(scores) == len(self.test_data)
            assert all(0 <= score <= 1 for score in scores)  # Normalized scores
    
    def test_autoencoder(self):
        """Test Autoencoder model"""
        results = self.models.run_models(self.test_data)
        
        if 'autoencoder' in results:
            model_results = results['autoencoder']
            
            # Check predictions
            predictions = model_results['predictions']
            assert len(predictions) == len(self.test_data)
            assert set(predictions) == {-1, 1}  # -1 for outliers, 1 for inliers
            
            # Check scores
            scores = model_results['scores']
            assert len(scores) == len(self.test_data)
            assert all(0 <= score <= 1 for score in scores)  # Normalized scores
    
    def test_predict(self):
        """Test making predictions"""
        # First fit models
        self.models.run_models(self.test_data)
        
        # Create new data
        new_data = pd.DataFrame({
            'feature1': np.random.normal(0, 1, 10),
            'feature2': np.random.normal(0, 1, 10),
            'feature3': np.random.normal(0, 1, 10),
            'feature4': np.random.normal(0, 1, 10),
            'feature5': np.random.normal(0, 1, 10)
        })
        
        # Test prediction with Isolation Forest
        if 'isolation_forest' in self.models.models:
            result = self.models.predict(new_data, 'isolation_forest')
            
            assert 'predictions' in result
            assert 'scores' in result
            assert len(result['predictions']) == len(new_data)
            assert len(result['scores']) == len(new_data)
    
    def test_get_feature_importance(self):
        """Test getting feature importance"""
        # First fit models
        self.models.run_models(self.test_data)
        
        # Test with Isolation Forest
        if 'isolation_forest' in self.models.models:
            importance = self.models.get_feature_importance('isolation_forest')
            
            assert isinstance(importance, pd.DataFrame)
            assert 'feature' in importance.columns
            assert 'importance' in importance.columns
            assert len(importance) == len(self.test_data.columns)

class TestSupervisedModels:
    """Test cases for SupervisedModels class"""
    
    def setup_method(self):
        """Setup test data"""
        self.models = SupervisedModels(test_size=0.3, random_state=42)
        
        # Create test data
        np.random.seed(42)
        self.test_data = pd.DataFrame({
            'feature1': np.random.normal(0, 1, 100),
            'feature2': np.random.normal(0, 1, 100),
            'feature3': np.random.normal(0, 1, 100),
            'feature4': np.random.normal(0, 1, 100),
            'feature5': np.random.normal(0, 1, 100),
            'fraud_flag': np.random.choice([0, 1], 100, p=[0.9, 0.1])  # Imbalanced classes
        })
    
    def test_run_models(self):
        """Test running models"""
        results = self.models.run_models(self.test_data)
        
        # Check that results are returned
        assert isinstance(results, dict)
        assert len(results) > 0
        
        # Check that each model has required keys
        for model_name, model_results in results.items():
            assert 'model' in model_results
            assert 'performance' in model_results
            assert 'feature_importance' in model_results
            assert 'feature_names' in model_results
    
    def test_random_forest(self):
        """Test Random Forest model"""
        results = self.models.run_models(self.test_data)
        
        if 'random_forest' in results:
            model_results = results['random_forest']
            
            # Check model
            model = model_results['model']
            assert model is not None
            
            # Check performance
            performance = model_results['performance']
            assert 'accuracy' in performance
            assert 'precision' in performance
            assert 'recall' in performance
            assert 'f1' in performance
            assert 'roc_auc' in performance
            
            # Check feature importance
            importance = model_results['feature_importance']
            assert isinstance(importance, pd.DataFrame)
            assert 'feature' in importance.columns
            assert 'importance' in importance.columns
    
    def test_xgboost(self):
        """Test XGBoost model"""
        results = self.models.run_models(self.test_data)
        
        if 'xgboost' in results:
            model_results = results['xgboost']
            
            # Check model
            model = model_results['model']
            assert model is not None
            
            # Check performance
            performance = model_results['performance']
            assert 'accuracy' in performance
            assert 'precision' in performance
            assert 'recall' in performance
            assert 'f1' in performance
            assert 'roc_auc' in performance
            
            # Check feature importance
            importance = model_results['feature_importance']
            assert isinstance(importance, pd.DataFrame)
            assert 'feature' in importance.columns
            assert 'importance' in importance.columns
    
    def test_predict(self):
        """Test making predictions"""
        # First fit models
        self.models.run_models(self.test_data)
        
        # Create new data
        new_data = pd.DataFrame({
            'feature1': np.random.normal(0, 1, 10),
            'feature2': np.random.normal(0, 1, 10),
            'feature3': np.random.normal(0, 1, 10),
            'feature4': np.random.normal(0, 1, 10),
            'feature5': np.random.normal(0, 1, 10)
        })
        
        # Test prediction with Random Forest
        if 'random_forest' in self.models.models:
            result = self.models.predict(new_data, 'random_forest')
            
            assert 'predictions' in result
            assert 'probabilities' in result
            assert len(result['predictions']) == len(new_data)
            assert len(result['probabilities']) == len(new_data)
    
    def test_get_feature_importance(self):
        """Test getting feature importance"""
        # First fit models
        self.models.run_models(self.test_data)
        
        # Test with Random Forest
        if 'random_forest' in self.models.models:
            importance = self.models.get_feature_importance('random_forest')
            
            assert isinstance(importance, pd.DataFrame)
            assert 'feature' in importance.columns
            assert 'importance' in importance.columns
    
    def test_get_performance(self):
        """Test getting performance metrics"""
        # First fit models
        self.models.run_models(self.test_data)
        
        # Test with Random Forest
        if 'random_forest' in self.models.models:
            performance = self.models.get_performance('random_forest')
            
            assert isinstance(performance, dict)
            assert 'accuracy' in performance
            assert 'precision' in performance
            assert 'recall' in performance
            assert 'f1' in performance
            assert 'roc_auc' in performance

class TestRuleEngine:
    """Test cases for RuleEngine class"""
    
    def setup_method(self):
        """Setup test data"""
        self.rule_engine = RuleEngine(threshold=0.5)
        
        # Create test data
        self.test_data = pd.DataFrame({
            'transaction_id': range(1, 11),
            'timestamp': pd.date_range('2023-01-01', periods=10, freq='D'),
            'amount': [100, 200, 10000, 5000, 15000, 300, 400, 12000, 8000, 20000],
            'sender_id': [f'sender_{i}' for i in range(10)],
            'receiver_id': [f'receiver_{i}' for i in range(10)],
            'sender_location': ['USA'] * 5 + ['Canada'] * 5,
            'receiver_location': ['USA'] * 8 + ['North Korea'] * 2,
            'description': [
                'Normal payment',
                'Regular transfer',
                'Large amount',
                'Big transaction',
                'Huge payment',
                'Small amount',
                'Normal transfer',
                'Large transaction',
                'Big payment',
                'Massive amount'
            ]
        })
    
    def test_apply_rules(self):
        """Test applying rules"""
        results = self.rule_engine.apply_rules(self.test_data)
        
        # Check that results are returned
        assert isinstance(results, dict)
        
        # Check required keys
        assert 'rule_results' in results
        assert 'rule_scores' in results
        assert 'total_scores' in results
        assert 'normalized_scores' in results
        assert 'rule_violations' in results
        assert 'violated_rule_names' in results
        assert 'rules' in results
        assert 'rule_weights' in results
        assert 'rule_descriptions' in results
    
    def test_high_amount_rule(self):
        """Test high amount rule"""
        # Apply rules
        results = self.rule_engine.apply_rules(self.test_data)
        
        # Check rule results
        if 'high_amount' in results['rule_results']:
            rule_results = results['rule_results']['high_amount']
            
            # Should flag transactions with amount > 10000
            expected_flags = [amount > 10000 for amount in self.test_data['amount']]
            assert rule_results == expected_flags
    
    def test_cross_border_rule(self):
        """Test cross border rule"""
        # Apply rules
        results = self.rule_engine.apply_rules(self.test_data)
        
        # Check rule results
        if 'cross_border' in results['rule_results']:
            rule_results = results['rule_results']['cross_border']
            
            # Should flag transactions with different sender and receiver countries
            expected_flags = []
            for i in range(len(self.test_data)):
                sender_country = self.test_data.loc[i, 'sender_location'].split(',')[0]
                receiver_country = self.test_data.loc[i, 'receiver_location'].split(',')[0]
                expected_flags.append(sender_country != receiver_country)
            
            assert rule_results == expected_flags
    
    def test_high_risk_country_rule(self):
        """Test high risk country rule"""
        # Apply rules
        results = self.rule_engine.apply_rules(self.test_data)
        
        # Check rule results
        if 'high_risk_country' in results['rule_results']:
            rule_results = results['rule_results']['high_risk_country']
            
            # Should flag transactions involving North Korea
            expected_flags = ['North Korea' in location for location in self.test_data['receiver_location']]
            assert rule_results == expected_flags
    
    def test_rule_violations(self):
        """Test rule violations"""
        # Apply rules
        results = self.rule_engine.apply_rules(self.test_data)
        
        # Check that violations are detected
        violations = results['rule_violations']
        assert len(violations) == len(self.test_data)
        
        # Check that some transactions are flagged
        assert any(violations)  # At least one transaction should be flagged
    
    def test_add_rule(self):
        """Test adding custom rule"""
        # Add custom rule
        def custom_rule(row):
            return row.get('amount', 0) > 5000
        
        self.rule_engine.add_rule('custom_rule', custom_rule, weight=0.5, description='Custom rule')
        
        # Check that rule is added
        assert 'custom_rule' in self.rule_engine.rules
        assert self.rule_engine.rule_weights['custom_rule'] == 0.5
        assert self.rule_engine.rule_descriptions['custom_rule'] == 'Custom rule'
    
    def test_remove_rule(self):
        """Test removing rule"""
        # Remove a rule
        if 'high_amount' in self.rule_engine.rules:
            self.rule_engine.remove_rule('high_amount')
            
            # Check that rule is removed
            assert 'high_amount' not in self.rule_engine.rules
            assert 'high_amount' not in self.rule_engine.rule_weights
            assert 'high_amount' not in self.rule_engine.rule_descriptions
    
    def test_update_rule_weight(self):
        """Test updating rule weight"""
        # Update rule weight
        self.rule_engine.update_rule_weight('high_amount', 0.5)
        
        # Check that weight is updated
        assert self.rule_engine.rule_weights['high_amount'] == 0.5
    
    def test_get_rules(self):
        """Test getting rules"""
        rules_info = self.rule_engine.get_rules()
        
        # Check structure
        assert 'rules' in rules_info
        assert 'weights' in rules_info
        assert 'descriptions' in rules_info
        
        # Check that rules are returned
        assert len(rules_info['rules']) > 0
        assert len(rules_info['weights']) > 0
        assert len(rules_info['descriptions']) > 0


=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\environment.yml ===

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\README.md ===
# Financial Fraud Detection System

A comprehensive, world-class fraud detection system that uses advanced machine learning, statistical analysis, and AI techniques to identify potentially fraudulent transactions.

## Features

### üõ°Ô∏è Multi-Layer Detection
- **Unsupervised Learning**: Isolation Forest, Local Outlier Factor, Autoencoders, One-Class SVM
- **Supervised Learning**: Random Forest, XGBoost, LightGBM, Neural Networks
- **Rule-Based Detection**: Configurable rules with weights for known fraud patterns

### üîç Advanced Feature Engineering
- **Statistical Features**: Benford's Law, Z-score, MAD, percentile analysis
- **Graph Features**: Centrality measures, clustering coefficients, community detection
- **NLP Features**: Sentiment analysis, keyword detection, pattern matching
- **Time Series Features**: Burstiness analysis, gap analysis, temporal patterns

### üìä Intelligent Analysis
- **Risk Scoring**: Weighted combination of algorithm outputs
- **Explainable AI**: SHAP values and natural language explanations
- **Dynamic Thresholds**: Percentile-based and adaptive thresholds
- **Real-time Monitoring**: Continuous learning from new data

### üìà Professional Reporting
- **Executive Summary**: High-level overview for stakeholders
- **Detailed Analysis**: Comprehensive forensic audit reports
- **Technical Reports**: Model performance and system metrics
- **Custom Reports**: User-configurable report generation

### üåê User-Friendly Interface
- **Streamlit Dashboard**: Interactive web-based interface
- **Data Upload**: Support for CSV, Excel files up to 10GB
- **Column Mapping**: AI-powered column matching with confirmation
- **Visualization**: Interactive charts and graphs

## Installation

### Option 1: Using Conda (Recommended)

1. Clone the repository:
```bash
git clone https://github.com/your-username/fraud-detection-system.git
cd fraud-detection-system

2. Create and activate the conda environment:
conda env create -f environment.yml
conda activate fraud-detection

3.Install additional packages:
pip install -r requirements.txt



Usage
Running the Application

1. Start the Streamlit application:
streamlit run app/main_dashboard.py

2. Open your web browser and navigate to the provided URL (usually http://localhost:8501)


Expected Data Format
The system expects transaction data with the following columns:

transaction_id,timestamp,amount,currency,sender_id,receiver_id,sender_account_type,receiver_account_type,sender_bank,receiver_bank,sender_location,receiver_location,transaction_type,transaction_category,merchant_id,merchant_category,ip_address,device_id,description,notes,authorization_status,chargeback_flag,fraud_flag



Data Types:
transaction_id: string/integer (unique identifier)
timestamp: datetime (ISO format: YYYY-MM-DD HH:MM:SS)
amount: float (transaction value)
currency: string (3-letter ISO code)
sender_id: string (entity initiating transaction)
receiver_id: string (entity receiving transaction)
sender_account_type: string (personal, business, corporate, etc.)
receiver_account_type: string (personal, business, corporate, etc.)
sender_bank: string (name of sender's bank)
receiver_bank: string (name of receiver's bank)
sender_location: string (country, state, city)
receiver_location: string (country, state, city)
transaction_type: string (transfer, payment, withdrawal, deposit)
transaction_category: string (retail, services, gambling, etc.)
merchant_id: string (if applicable)
merchant_category: string (if applicable)
ip_address: string (IPv4 or IPv6)
device_id: string (unique device identifier)
description: string (transaction description)
notes: string (additional notes)
authorization_status: string (approved, declined, pending)
chargeback_flag: boolean (True/False)
fraud_flag: boolean (True/False, for supervised learning)

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\requirements.txt ===
# Core Data Processing
pandas>=1.5.0
numpy>=1.21.0
scipy>=1.9.0

# Machine Learning
scikit-learn>=1.1.0
xgboost>=1.6.0
lightgbm>=3.3.0
imbalanced-learn>=0.9.0

plotly==5.22.0

# Deep Learning
tensorflow>=2.10.0
keras>=2.10.0

# Graph Analysis
networkx>=2.8.0
python-louvain>=0.16

# Natural Language Processing
nltk>=3.7
textblob>=0.17.1
gensim>=4.2.0

# Visualization
matplotlib>=3.5.0
seaborn>=0.11.0
plotly>=5.10.0
graphviz>=0.20.0

# Explainability
shap>=0.41.0

# Large Data Processing
dask>=2022.8.0

# Web Interface
streamlit>=1.12.0

# Report Generation
reportlab>=3.6.0
fpdf2>=2.6.0

# Configuration
pyyaml>=6.0

# API and Web
requests>=2.28.0

# Utilities
tqdm>=4.64.0
joblib>=1.1.0
python-dateutil>=2.8.0
pytz>=2022.1

# Testing
pytest>=7.1.0
pytest-cov>=3.0.0

# Development
jupyter>=1.0.0
ipykernel>=6.15.0

xlrd

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\.devcontainer\devcontainer.json ===
{
  "name": "Python 3",
  // Or use a Dockerfile or Docker Compose file. More info: https://containers.dev/guide/dockerfile
  "image": "mcr.microsoft.com/devcontainers/python:1-3.11-bullseye",
  "customizations": {
    "codespaces": {
      "openFiles": [
        "README.md",
        "app/main_dashboard.py"
      ]
    },
    "vscode": {
      "settings": {},
      "extensions": [
        "ms-python.python",
        "ms-python.vscode-pylance"
      ]
    }
  },
  "updateContentCommand": "[ -f packages.txt ] && sudo apt update && sudo apt upgrade -y && sudo xargs apt install -y <packages.txt; [ -f requirements.txt ] && pip3 install --user -r requirements.txt; pip3 install --user streamlit; echo '‚úÖ Packages installed and Requirements met'",
  "postAttachCommand": {
    "server": "streamlit run app/main_dashboard.py --server.enableCORS false --server.enableXsrfProtection false"
  },
  "portsAttributes": {
    "8501": {
      "label": "Application",
      "onAutoForward": "openPreview"
    }
  },
  "forwardPorts": [
    8501
  ]
}

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\.ipynb_checkpoints\all_content-checkpoint.txt ===
 
=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\tests\.ipynb_checkpoints\test_models-checkpoint.py === 
"""
Test cases for models module
"""

import pytest
import pandas as pd
import numpy as np
import os
import sys

# Add src to path
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'src'))

from fraud_detection_engine.models.unsupervised import UnsupervisedModels
from fraud_detection_engine.models.supervised import SupervisedModels
from fraud_detection_engine.models.rule_based import RuleEngine

class TestUnsupervisedModels:
    """Test cases for UnsupervisedModels class"""
    
    def setup_method(self):
        """Setup test data"""
        self.models = UnsupervisedModels(contamination=0.1)
        
        # Create test data
        np.random.seed(42)
        self.test_data = pd.DataFrame({
            'feature1': np.random.normal(0, 1, 100),
            'feature2': np.random.normal(0, 1, 100),
            'feature3': np.random.normal(0, 1, 100),
            'feature4': np.random.normal(0, 1, 100),
            'feature5': np.random.normal(0, 1, 100)
        })
        
        # Add some outliers
        self.test_data.loc[0:4, 'feature1'] = 10  # Outliers
        self.test_data.loc[5:9, 'feature2'] = -10  # Outliers
    
    def test_run_models(self):
        """Test running models"""
        results = self.models.run_models(self.test_data)
        
        # Check that results are returned
        assert isinstance(results, dict)
        assert len(results) > 0
        
        # Check that each model has required keys
        for model_name, model_results in results.items():
            assert 'predictions' in model_results
            assert 'scores' in model_results
            assert 'model' in model_results
            assert 'feature_names' in model_results
    
    def test_isolation_forest(self):
        """Test Isolation Forest model"""
        results = self.models.run_models(self.test_data)
        
        if 'isolation_forest' in results:
            model_results = results['isolation_forest']
            
            # Check predictions
            predictions = model_results['predictions']
            assert len(predictions) == len(self.test_data)
            assert set(predictions) == {-1, 1}  # -1 for outliers, 1 for inliers
            
            # Check scores
            scores = model_results['scores']
            assert len(scores) == len(self.test_data)
            assert all(0 <= score <= 1 for score in scores)  # Normalized scores
    
    def test_local_outlier_factor(self):
        """Test Local Outlier Factor model"""
        results = self.models.run_models(self.test_data)
        
        if 'local_outlier_factor' in results:
            model_results = results['local_outlier_factor']
            
            # Check predictions
            predictions = model_results['predictions']
            assert len(predictions) == len(self.test_data)
            assert set(predictions) == {-1, 1}  # -1 for outliers, 1 for inliers
            
            # Check scores
            scores = model_results['scores']
            assert len(scores) == len(self.test_data)
            assert all(0 <= score <= 1 for score in scores)  # Normalized scores
    
    def test_autoencoder(self):
        """Test Autoencoder model"""
        results = self.models.run_models(self.test_data)
        
        if 'autoencoder' in results:
            model_results = results['autoencoder']
            
            # Check predictions
            predictions = model_results['predictions']
            assert len(predictions) == len(self.test_data)
            assert set(predictions) == {-1, 1}  # -1 for outliers, 1 for inliers
            
            # Check scores
            scores = model_results['scores']
            assert len(scores) == len(self.test_data)
            assert all(0 <= score <= 1 for score in scores)  # Normalized scores
    
    def test_predict(self):
        """Test making predictions"""
        # First fit models
        self.models.run_models(self.test_data)
        
        # Create new data
        new_data = pd.DataFrame({
            'feature1': np.random.normal(0, 1, 10),
            'feature2': np.random.normal(0, 1, 10),
            'feature3': np.random.normal(0, 1, 10),
            'feature4': np.random.normal(0, 1, 10),
            'feature5': np.random.normal(0, 1, 10)
        })
        
        # Test prediction with Isolation Forest
        if 'isolation_forest' in self.models.models:
            result = self.models.predict(new_data, 'isolation_forest')
            
            assert 'predictions' in result
            assert 'scores' in result
            assert len(result['predictions']) == len(new_data)
            assert len(result['scores']) == len(new_data)
    
    def test_get_feature_importance(self):
        """Test getting feature importance"""
        # First fit models
        self.models.run_models(self.test_data)
        
        # Test with Isolation Forest
        if 'isolation_forest' in self.models.models:
            importance = self.models.get_feature_importance('isolation_forest')
            
            assert isinstance(importance, pd.DataFrame)
            assert 'feature' in importance.columns
            assert 'importance' in importance.columns
            assert len(importance) == len(self.test_data.columns)

class TestSupervisedModels:
    """Test cases for SupervisedModels class"""
    
    def setup_method(self):
        """Setup test data"""
        self.models = SupervisedModels(test_size=0.3, random_state=42)
        
        # Create test data
        np.random.seed(42)
        self.test_data = pd.DataFrame({
            'feature1': np.random.normal(0, 1, 100),
            'feature2': np.random.normal(0, 1, 100),
            'feature3': np.random.normal(0, 1, 100),
            'feature4': np.random.normal(0, 1, 100),
            'feature5': np.random.normal(0, 1, 100),
            'fraud_flag': np.random.choice([0, 1], 100, p=[0.9, 0.1])  # Imbalanced classes
        })
    
    def test_run_models(self):
        """Test running models"""
        results = self.models.run_models(self.test_data)
        
        # Check that results are returned
        assert isinstance(results, dict)
        assert len(results) > 0
        
        # Check that each model has required keys
        for model_name, model_results in results.items():
            assert 'model' in model_results
            assert 'performance' in model_results
            assert 'feature_importance' in model_results
            assert 'feature_names' in model_results
    
    def test_random_forest(self):
        """Test Random Forest model"""
        results = self.models.run_models(self.test_data)
        
        if 'random_forest' in results:
            model_results = results['random_forest']
            
            # Check model
            model = model_results['model']
            assert model is not None
            
            # Check performance
            performance = model_results['performance']
            assert 'accuracy' in performance
            assert 'precision' in performance
            assert 'recall' in performance
            assert 'f1' in performance
            assert 'roc_auc' in performance
            
            # Check feature importance
            importance = model_results['feature_importance']
            assert isinstance(importance, pd.DataFrame)
            assert 'feature' in importance.columns
            assert 'importance' in importance.columns
    
    def test_xgboost(self):
        """Test XGBoost model"""
        results = self.models.run_models(self.test_data)
        
        if 'xgboost' in results:
            model_results = results['xgboost']
            
            # Check model
            model = model_results['model']
            assert model is not None
            
            # Check performance
            performance = model_results['performance']
            assert 'accuracy' in performance
            assert 'precision' in performance
            assert 'recall' in performance
            assert 'f1' in performance
            assert 'roc_auc' in performance
            
            # Check feature importance
            importance = model_results['feature_importance']
            assert isinstance(importance, pd.DataFrame)
            assert 'feature' in importance.columns
            assert 'importance' in importance.columns
    
    def test_predict(self):
        """Test making predictions"""
        # First fit models
        self.models.run_models(self.test_data)
        
        # Create new data
        new_data = pd.DataFrame({
            'feature1': np.random.normal(0, 1, 10),
            'feature2': np.random.normal(0, 1, 10),
            'feature3': np.random.normal(0, 1, 10),
            'feature4': np.random.normal(0, 1, 10),
            'feature5': np.random.normal(0, 1, 10)
        })
        
        # Test prediction with Random Forest
        if 'random_forest' in self.models.models:
            result = self.models.predict(new_data, 'random_forest')
            
            assert 'predictions' in result
            assert 'probabilities' in result
            assert len(result['predictions']) == len(new_data)
            assert len(result['probabilities']) == len(new_data)
    
    def test_get_feature_importance(self):
        """Test getting feature importance"""
        # First fit models
        self.models.run_models(self.test_data)
        
        # Test with Random Forest
        if 'random_forest' in self.models.models:
            importance = self.models.get_feature_importance('random_forest')
            
            assert isinstance(importance, pd.DataFrame)
            assert 'feature' in importance.columns
            assert 'importance' in importance.columns
    
    def test_get_performance(self):
        """Test getting performance metrics"""
        # First fit models
        self.models.run_models(self.test_data)
        
        # Test with Random Forest
        if 'random_forest' in self.models.models:
            performance = self.models.get_performance('random_forest')
            
            assert isinstance(performance, dict)
            assert 'accuracy' in performance
            assert 'precision' in performance
            assert 'recall' in performance
            assert 'f1' in performance
            assert 'roc_auc' in performance

class TestRuleEngine:
    """Test cases for RuleEngine class"""
    
    def setup_method(self):
        """Setup test data"""
        self.rule_engine = RuleEngine(threshold=0.5)
        
        # Create test data
        self.test_data = pd.DataFrame({
            'transaction_id': range(1, 11),
            'timestamp': pd.date_range('2023-01-01', periods=10, freq='D'),
            'amount': [100, 200, 10000, 5000, 15000, 300, 400, 12000, 8000, 20000],
            'sender_id': [f'sender_{i}' for i in range(10)],
            'receiver_id': [f'receiver_{i}' for i in range(10)],
            'sender_location': ['USA'] * 5 + ['Canada'] * 5,
            'receiver_location': ['USA'] * 8 + ['North Korea'] * 2,
            'description': [
                'Normal payment',
                'Regular transfer',
                'Large amount',
                'Big transaction',
                'Huge payment',
                'Small amount',
                'Normal transfer',
                'Large transaction',
                'Big payment',
                'Massive amount'
            ]
        })
    
    def test_apply_rules(self):
        """Test applying rules"""
        results = self.rule_engine.apply_rules(self.test_data)
        
        # Check that results are returned
        assert isinstance(results, dict)
        
        # Check required keys
        assert 'rule_results' in results
        assert 'rule_scores' in results
        assert 'total_scores' in results
        assert 'normalized_scores' in results
        assert 'rule_violations' in results
        assert 'violated_rule_names' in results
        assert 'rules' in results
        assert 'rule_weights' in results
        assert 'rule_descriptions' in results
    
    def test_high_amount_rule(self):
        """Test high amount rule"""
        # Apply rules
        results = self.rule_engine.apply_rules(self.test_data)
        
        # Check rule results
        if 'high_amount' in results['rule_results']:
            rule_results = results['rule_results']['high_amount']
            
            # Should flag transactions with amount > 10000
            expected_flags = [amount > 10000 for amount in self.test_data['amount']]
            assert rule_results == expected_flags
    
    def test_cross_border_rule(self):
        """Test cross border rule"""
        # Apply rules
        results = self.rule_engine.apply_rules(self.test_data)
        
        # Check rule results
        if 'cross_border' in results['rule_results']:
            rule_results = results['rule_results']['cross_border']
            
            # Should flag transactions with different sender and receiver countries
            expected_flags = []
            for i in range(len(self.test_data)):
                sender_country = self.test_data.loc[i, 'sender_location'].split(',')[0]
                receiver_country = self.test_data.loc[i, 'receiver_location'].split(',')[0]
                expected_flags.append(sender_country != receiver_country)
            
            assert rule_results == expected_flags
    
    def test_high_risk_country_rule(self):
        """Test high risk country rule"""
        # Apply rules
        results = self.rule_engine.apply_rules(self.test_data)
        
        # Check rule results
        if 'high_risk_country' in results['rule_results']:
            rule_results = results['rule_results']['high_risk_country']
            
            # Should flag transactions involving North Korea
            expected_flags = ['North Korea' in location for location in self.test_data['receiver_location']]
            assert rule_results == expected_flags
    
    def test_rule_violations(self):
        """Test rule violations"""
        # Apply rules
        results = self.rule_engine.apply_rules(self.test_data)
        
        # Check that violations are detected
        violations = results['rule_violations']
        assert len(violations) == len(self.test_data)
        
        # Check that some transactions are flagged
        assert any(violations)  # At least one transaction should be flagged
    
    def test_add_rule(self):
        """Test adding custom rule"""
        # Add custom rule
        def custom_rule(row):
            return row.get('amount', 0) > 5000
        
        self.rule_engine.add_rule('custom_rule', custom_rule, weight=0.5, description='Custom rule')
        
        # Check that rule is added
        assert 'custom_rule' in self.rule_engine.rules
        assert self.rule_engine.rule_weights['custom_rule'] == 0.5
        assert self.rule_engine.rule_descriptions['custom_rule'] == 'Custom rule'
    
    def test_remove_rule(self):
        """Test removing rule"""
        # Remove a rule
        if 'high_amount' in self.rule_engine.rules:
            self.rule_engine.remove_rule('high_amount')
            
            # Check that rule is removed
            assert 'high_amount' not in self.rule_engine.rules
            assert 'high_amount' not in self.rule_engine.rule_weights
            assert 'high_amount' not in self.rule_engine.rule_descriptions
    
    def test_update_rule_weight(self):
        """Test updating rule weight"""
        # Update rule weight
        self.rule_engine.update_rule_weight('high_amount', 0.5)
        
        # Check that weight is updated
        assert self.rule_engine.rule_weights['high_amount'] == 0.5
    
    def test_get_rules(self):
        """Test getting rules"""
        rules_info = self.rule_engine.get_rules()
        
        # Check structure
        assert 'rules' in rules_info
        assert 'weights' in rules_info
        assert 'descriptions' in rules_info
        
        # Check that rules are returned
        assert len(rules_info['rules']) > 0
        assert len(rules_info['weights']) > 0
        assert len(rules_info['descriptions']) > 0

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\.ipynb_checkpoints\environment-checkpoint.yml ===
name: fraud-detection
channels:
  - conda-forge
  - defaults
dependencies:
  - python=3.9
  - pip>=22.0.0
  - voila
  
  # Core Data Processing
  - pandas>=1.5.0
  - numpy>=1.21.0
  - scipy>=1.9.0
  
  # Machine Learning
  - scikit-learn>=1.1.0
  - xgboost>=1.6.0
  - lightgbm>=3.3.0
  - imbalanced-learn>=0.9.0
  
  # Deep Learning
  - tensorflow>=2.10.0
  - keras>=2.10.0
  
  # Graph Analysis
  - networkx>=2.8.0
  - python-louvain>=0.16
  
  # Natural Language Processing
  - nltk>=3.7
  - textblob>=0.17.1
  - gensim>=4.2.0
  
  # Visualization
  - matplotlib>=3.5.0
  - seaborn>=0.11.0
  - plotly>=5.10.0
  - python-graphviz>=0.20.0
  
  # Explainability
  - shap>=0.41.0
  
  # Large Data Processing
  - dask>=2022.8.0
  
  # Web Interface
  - streamlit>=1.12.0
  
  # Report Generation
  - reportlab>=3.6.0
  
  # Configuration
  - pyyaml>=6.0
  
  # API and Web
  - requests>=2.28.0
  
  # Utilities
  - tqdm>=4.64.0
  - joblib>=1.1.0
  - python-dateutil>=2.8.0
  - pytz>=2022.1
  
  # Testing
  - pytest>=7.1.0
  - pytest-cov>=3.0.0
  
  # Development
  - jupyter>=1.0.0
  - ipykernel>=6.15.0
  
  # Additional packages via pip
  - pip:
    - fpdf2>=2.6.0

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\.ipynb_checkpoints\README-checkpoint.md ===

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\.ipynb_checkpoints\requirements-checkpoint.txt ===
# Core Data Processing
pandas>=1.5.0
numpy>=1.21.0
scipy>=1.9.0

# Machine Learning
scikit-learn>=1.1.0
xgboost>=1.6.0
lightgbm>=3.3.0
imbalanced-learn>=0.9.0

plotly==5.22.0

# Deep Learning
tensorflow>=2.10.0
keras>=2.10.0

# Graph Analysis
networkx>=2.8.0
python-louvain>=0.16

# Natural Language Processing
nltk>=3.7
textblob>=0.17.1
gensim>=4.2.0

# Visualization
matplotlib>=3.5.0
seaborn>=0.11.0
plotly>=5.10.0
graphviz>=0.20.0

# Explainability
shap>=0.41.0

# Large Data Processing
dask>=2022.8.0

# Web Interface
streamlit>=1.12.0

# Report Generation
reportlab>=3.6.0
fpdf2>=2.6.0

# Configuration
pyyaml>=6.0

# API and Web
requests>=2.28.0

# Utilities
tqdm>=4.64.0
joblib>=1.1.0
python-dateutil>=2.8.0
pytz>=2022.1

# Testing
pytest>=7.1.0
pytest-cov>=3.0.0

# Development
jupyter>=1.0.0
ipykernel>=6.15.0

xlrd

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\.streamlit\config.toml ===
[runtime]
python = "3.9"


=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\app\main_dashboard.py ===
"""
Main Dashboard for Financial Fraud Detection System
Streamlit-based user interface for the fraud detection platform
"""
import streamlit as st
import pandas as pd
import numpy as np
import os
import sys
import time
import json
import plotly.express as px
import plotly.graph_objects as go
from datetime import datetime, timedelta
import yaml
from io import BytesIO, StringIO
import logging
import tempfile
import chardet
import traceback
from typing import Optional
# Add src to path to import our modules
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'src'))
from fraud_detection_engine.ingestion.data_loader import DataLoader
from fraud_detection_engine.ingestion.column_mapper import ColumnMapper
from fraud_detection_engine.features.statistical_features import StatisticalFeatures
from fraud_detection_engine.features.graph_features import GraphFeatures
from fraud_detection_engine.features.nlp_features import NLPFeatures
from fraud_detection_engine.features.timeseries_features import TimeSeriesFeatures
from fraud_detection_engine.models.unsupervised import UnsupervisedModels
from fraud_detection_engine.models.supervised import SupervisedModels
from fraud_detection_engine.models.rule_based import RuleEngine
from fraud_detection_engine.analysis.risk_scorer import RiskScorer
from fraud_detection_engine.analysis.explainability import Explainability
from fraud_detection_engine.reporting.pdf_generator import PDFGenerator
from fraud_detection_engine.utils.api_utils import is_api_available
# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)
# Set page configuration
st.set_page_config(
    page_title="Financial Fraud Detection System",
    page_icon="üõ°Ô∏è",
    layout="wide",
    initial_sidebar_state="expanded"
)
# Load configuration
@st.cache_resource
def load_config():
    """Load configuration from YAML files"""
    config = {}
    try:
        # Get the directory where this script is located
        script_dir = os.path.dirname(os.path.abspath(__file__))
        
        # Construct paths to config files relative to script location
        model_params_path = os.path.join(script_dir, '..', 'config', 'model_params.yml')
        rule_engine_path = os.path.join(script_dir, '..', 'config', 'rule_engine_config.yml')
        api_keys_path = os.path.join(script_dir, '..', 'config', 'api_keys.yml')
        
        with open(model_params_path, 'r') as f:
            config['model_params'] = yaml.safe_load(f)
        with open(rule_engine_path, 'r') as f:
            config['rule_engine'] = yaml.safe_load(f)
        with open(api_keys_path, 'r') as f:
            config['api_keys'] = yaml.safe_load(f)
    except Exception as e:
        st.error(f"Error loading configuration: {str(e)}")
        config = {
            'model_params': {},
            'rule_engine': {},
            'api_keys': {}
        }
    return config
config = load_config()
# Initialize session state variables
if 'data' not in st.session_state:
    st.session_state.data = None
if 'processed_data' not in st.session_state:
    st.session_state.processed_data = None
if 'risk_scores' not in st.session_state:
    st.session_state.risk_scores = None
if 'column_mapping' not in st.session_state:
    st.session_state.column_mapping = None
if 'model_results' not in st.session_state:
    st.session_state.model_results = {}
if 'explanations' not in st.session_state:
    st.session_state.explanations = None
if 'processing_complete' not in st.session_state:
    st.session_state.processing_complete = False
if 'original_data' not in st.session_state:
    st.session_state.original_data = None
if 'features_df' not in st.session_state:
    st.session_state.features_df = None
# Sidebar
def render_sidebar():
    """Render the sidebar with navigation and controls"""
    st.sidebar.title("üõ°Ô∏è Fraud Detection System")
    st.sidebar.image("https://www.streamlit.io/images/brand/streamlit-logo-secondary-colormark-darktext.png", width=200)
    
    # Navigation
    st.sidebar.subheader("Navigation")
    page = st.sidebar.radio("Go to", [
        "Data Upload", 
        "Column Mapping", 
        "Analysis Settings", 
        "Run Detection", 
        "Results Dashboard", 
        "Explainability", 
        "Reports"
    ])
    
    # System status
    st.sidebar.subheader("System Status")
    if st.session_state.data is not None:
        st.sidebar.success(f"Data loaded: {len(st.session_state.data)} transactions")
    else:
        st.sidebar.warning("No data loaded")
    
    if st.session_state.processing_complete:
        st.sidebar.success("Analysis complete")
    else:
        st.sidebar.info("Analysis pending")
    
    # API Status
    st.sidebar.subheader("API Status")
    api_status = {
        'Gemini': is_api_available('gemini'),
        'OpenAI': is_api_available('openai'),
        'News API': is_api_available('news_api'),
        'Geolocation': is_api_available('geolocation'),
        'Sanctions': is_api_available('sanctions'),
        'Tax Compliance': is_api_available('tax_compliance'),
        'Bank Verification': is_api_available('bank_verification'),
        'Identity Verification': is_api_available('identity_verification')
    }
    
    for api, status in api_status.items():
        if status:
            st.sidebar.success(f"{api}: ‚úÖ Available")
        else:
            st.sidebar.warning(f"{api}: ‚ùå Not Available")
    
    # Configuration
    st.sidebar.subheader("Configuration")
    if st.sidebar.button("Reload Config"):
        config = load_config()
        st.sidebar.success("Configuration reloaded")
    
    # About section
    st.sidebar.subheader("About")
    st.sidebar.info("""
    This is a comprehensive financial fraud detection system that uses advanced machine learning, statistical analysis, and AI techniques to identify potentially fraudulent transactions.
    """)
    
    return page
# Helper function to detect file encoding
def detect_file_encoding(file_content):
    """Detect the encoding of file content"""
    try:
        result = chardet.detect(file_content)
        return result['encoding']
    except Exception as e:
        logger.error(f"Error detecting file encoding: {str(e)}")
        return None
# Helper function to read CSV with multiple encoding attempts
def read_csv_with_encodings(file_content, filename):
    """Read CSV file trying multiple encodings"""
    encodings = ['utf-8', 'latin-1', 'cp1252', 'iso-8859-1', 'utf-16']
    
    # First try with detected encoding
    detected_encoding = detect_file_encoding(file_content)
    if detected_encoding:
        try:
            st.write(f"Trying detected encoding: {detected_encoding}")
            return pd.read_csv(StringIO(file_content.decode(detected_encoding)))
        except Exception as e:
            st.warning(f"Failed with detected encoding {detected_encoding}: {str(e)}")
    
    # Try common encodings
    for encoding in encodings:
        try:
            st.write(f"Trying encoding: {encoding}")
            return pd.read_csv(StringIO(file_content.decode(encoding)))
        except UnicodeDecodeError:
            st.warning(f"Failed with {encoding} encoding: Unicode decode error")
        except Exception as e:
            st.warning(f"Failed with {encoding} encoding: {str(e)}")
    
    return None
# Helper function to read Excel with multiple engines
def read_excel_with_engines(file_content, filename):
    """Read Excel file trying multiple engines"""
    # Try default engine first
    try:
        st.write("Trying default Excel engine")
        return pd.read_excel(BytesIO(file_content))
    except Exception as e:
        st.warning(f"Failed with default engine: {str(e)}")
    
    # Try specific engines based on file extension
    if filename.endswith('.xlsx'):
        try:
            st.write("Trying openpyxl engine")
            return pd.read_excel(BytesIO(file_content), engine='openpyxl')
        except Exception as e:
            st.warning(f"Failed with openpyxl: {str(e)}")
    elif filename.endswith('.xls'):
        try:
            st.write("Trying xlrd engine")
            return pd.read_excel(BytesIO(file_content), engine='xlrd')
        except Exception as e:
            st.warning(f"Failed with xlrd: {str(e)}")
    
    return None
# Data Upload Page
def render_data_upload():
    """Render the data upload page"""
    st.title("üìÅ Data Upload")
    st.markdown("""
    Upload your transaction data for analysis. The system supports CSV and Excel files.
    """)
    
    # Debug information
    st.write("### Debug Information")
    st.write(f"Current working directory: {os.getcwd()}")
    st.write(f"Script directory: {os.path.dirname(os.path.abspath(__file__))}")
    
    # File upload with enhanced error handling
    uploaded_file = st.file_uploader(
        "Choose a CSV or Excel file",
        type=['csv', 'xlsx', 'xls'],
        help="Upload a file containing transaction data",
        key="file_uploader"
    )
    
    # Debug: Show if file was uploaded
    if uploaded_file is not None:
        st.write("### File Upload Details")
        st.write(f"File name: {uploaded_file.name}")
        st.write(f"File type: {uploaded_file.type}")
        st.write(f"File size: {uploaded_file.size} bytes")
    
    if uploaded_file is not None:
        try:
            # Read file content directly
            file_content = uploaded_file.read()
            
            # Read the file based on its type
            df = None
            
            if uploaded_file.name.endswith('.csv'):
                st.write("### Processing CSV file")
                df = read_csv_with_encodings(file_content, uploaded_file.name)
                
                if df is None:
                    st.error("Could not read CSV file with any encoding")
                    
                    # Show file content for debugging
                    st.write("### File Content (first 1000 bytes)")
                    try:
                        st.write(file_content[:1000])
                    except Exception as e:
                        st.error(f"Could not display file content: {str(e)}")
                    
                    # Try to create a simple CSV from the content
                    st.write("### Attempting to create CSV from content")
                    try:
                        # Try different encodings directly
                        for encoding in ['utf-8', 'latin-1', 'cp1252']:
                            try:
                                content_str = file_content.decode(encoding, errors='ignore')
                                df = pd.read_csv(StringIO(content_str))
                                st.success(f"Successfully read CSV with {encoding} encoding")
                                break
                            except:
                                continue
                    except Exception as e:
                        st.error(f"Failed to create CSV from content: {str(e)}")
                
            elif uploaded_file.name.endswith(('.xlsx', '.xls')):
                st.write("### Processing Excel file")
                df = read_excel_with_engines(file_content, uploaded_file.name)
                
                if df is None:
                    st.error("Could not read Excel file with any engine")
            else:
                st.error(f"Unsupported file type: {uploaded_file.name}")
            
            # Validate the dataframe
            if df is None:
                st.error("Failed to read the uploaded file")
                return
            
            if df.empty:
                st.error("The uploaded file appears to be empty")
                return
            
            # Display data info
            st.success(f"File uploaded successfully! Shape: {df.shape}")
            st.write("### Data Preview")
            st.dataframe(df.head(10))
            
            # Store in session state
            st.session_state.data = df
            st.session_state.original_data = df.copy()
            st.write("Data stored in session state")
            
            # Show column information
            st.write("### Column Information")
            col_info = pd.DataFrame({
                'Column': df.columns,
                'Data Type': df.dtypes,
                'Non-Null Count': df.count(),
                'Null Count': df.isnull().sum(),
                'Unique Values': df.nunique()
            })
            st.dataframe(col_info)
            
            # Basic statistics
            st.write("### Basic Statistics")
            numeric_cols = df.select_dtypes(include=[np.number]).columns
            if len(numeric_cols) > 0:
                st.dataframe(df[numeric_cols].describe())
            else:
                st.info("No numeric columns found for statistics")
                
            # Check file structure
            st.write("### File Structure Check")
            if len(df.columns) > 0:
                st.write(f"Found {len(df.columns)} columns")
                st.write("Columns:", list(df.columns))
            else:
                st.error("No columns found in the file")
                
        except Exception as e:
            st.error(f"Error processing file: {str(e)}")
            st.write("### Error Details")
            st.text(traceback.format_exc())
    
    # Sample data option
    st.markdown("---")
    st.subheader("Or Use Sample Data")
    
    if st.button("Load Sample Data"):
        try:
            # Look for sample data in multiple locations
            possible_paths = [
                os.path.join('..', 'data', 'sample_transactions.csv'),
                os.path.join('data', 'sample_transactions.csv'),
                'sample_transactions.csv'
            ]
            
            sample_path = None
            for path in possible_paths:
                st.write(f"Checking path: {path}")
                if os.path.exists(path):
                    sample_path = path
                    st.write(f"Found sample data at: {path}")
                    break
            
            if sample_path:
                df = pd.read_csv(sample_path)
                st.session_state.data = df
                st.session_state.original_data = df.copy()
                st.success(f"Sample data loaded! Shape: {df.shape}")
                st.dataframe(df.head(10))
            else:
                st.warning("Sample data file not found, creating new sample...")
                
                # Create sample data if it doesn't exist
                np.random.seed(42)
                
                # Create more realistic sample data
                n_samples = 1000
                start_date = datetime(2023, 1, 1)
                
                sample_data = pd.DataFrame({
                    'transaction_id': [f'TXN{i:06d}' for i in range(1, n_samples + 1)],
                    'timestamp': [start_date + timedelta(days=np.random.randint(0, 365)) for _ in range(n_samples)],
                    'amount': np.random.lognormal(8, 1.5, n_samples),  # More realistic amounts
                    'sender_id': [f'CUST{np.random.randint(1, 201):03d}' for _ in range(n_samples)],
                    'receiver_id': [f'CUST{np.random.randint(1, 201):03d}' for _ in range(n_samples)],
                    'currency': ['USD'] * n_samples,
                    'description': [f'Transaction type {np.random.choice(["PAYMENT", "TRANSFER", "PURCHASE", "WITHDRAWAL"])}' for _ in range(n_samples)],
                    'transaction_type': np.random.choice(['debit', 'credit'], n_samples),
                    'merchant_category': np.random.choice(['retail', 'services', 'utilities', 'finance'], n_samples)
                })
                
                # Ensure data directory exists
                data_dir = os.path.join('..', 'data')
                if not os.path.exists(data_dir):
                    os.makedirs(data_dir)
                
                sample_path = os.path.join(data_dir, 'sample_transactions.csv')
                sample_data.to_csv(sample_path, index=False)
                st.write(f"Created sample data at: {sample_path}")
                
                # Load the newly created sample data
                st.session_state.data = sample_data
                st.session_state.original_data = sample_data.copy()
                st.success(f"Sample data loaded! Shape: {sample_data.shape}")
                st.dataframe(sample_data.head(10))
                
        except Exception as e:
            st.error(f"Error loading sample data: {str(e)}")
            st.write("### Error Details")
            st.text(traceback.format_exc())
    
    # Additional troubleshooting section
    st.markdown("---")
    st.subheader("Troubleshooting File Upload Issues")
    
    with st.expander("Click here if you're having trouble uploading files"):
        st.write("""
        ### Common Issues and Solutions:
        
        1. **File not uploading at all**:
           - Try using a different browser (Chrome, Firefox, or Edge work best)
           - Clear your browser cache and cookies
           - Check if your file size is too large (try with a smaller file first)
           - Make sure the file is not open in another program
           - Try refreshing the page and uploading again
        
        2. **CSV file encoding issues**:
           - Try saving your CSV file with UTF-8 encoding
           - Use Excel to "Save As" and select "CSV UTF-8" format
           - Try converting your file to Excel format (.xlsx)
           - Check if your CSV has special characters that might cause encoding issues
        
        3. **Permission issues**:
           - Make sure the file is not read-only
           - Try moving the file to your desktop first
           - Check if your antivirus software is blocking the upload
           - Try running the application with administrator privileges
        
        4. **Browser compatibility**:
           - Update your browser to the latest version
           - Try incognito/private browsing mode
           - Disable browser extensions temporarily
           - Try a different browser
        
        5. **File format issues**:
           - Make sure your CSV file has proper headers
           - Check if your file has consistent formatting
           - Try opening the file in Excel and re-saving it
           - Remove any special characters from column names
        
        ### Alternative Methods:
        
        If the file uploader still doesn't work, you can:
        1. Use the "Load Sample Data" button above
        2. Place your CSV file in the `data` folder with the name `sample_transactions.csv`
        3. Try converting your file to Excel format (.xlsx)
        4. Use the text area below to paste your CSV data directly:
        """)
        
        # Add text area for direct CSV input
        st.write("### Direct CSV Input")
        csv_text = st.text_area("Paste your CSV data here:", height=200)
        
        if st.button("Load from Text"):
            try:
                if csv_text.strip():
                    # Use StringIO to read the CSV data
                    df = pd.read_csv(StringIO(csv_text))
                    st.session_state.data = df
                    st.session_state.original_data = df.copy()
                    st.success(f"Data loaded from text! Shape: {df.shape}")
                    st.dataframe(df.head(10))
                else:
                    st.warning("Please paste some CSV data")
            except Exception as e:
                st.error(f"Error loading data from text: {str(e)}")
                st.write("### Error Details")
                st.text(traceback.format_exc())
# Column Mapping Page
def render_column_mapping():
    """Render the column mapping page"""
    st.title("üîÑ Column Mapping")
    
    if st.session_state.data is None:
        st.warning("Please upload data first")
        return
    
    st.markdown("""
    Map your column headers to the expected format. The system will attempt to auto-map columns,
    but you can review and adjust as needed.
    """)
    
    # Initialize column mapper
    mapper = ColumnMapper()
    
    # Get expected columns
    expected_columns = mapper.get_expected_columns()
    
    # Get actual columns
    actual_columns = list(st.session_state.data.columns)
    
    # Auto-map columns
    if st.session_state.column_mapping is None:
        with st.spinner("Auto-mapping columns..."):
            st.session_state.column_mapping = mapper.auto_map_columns(actual_columns, expected_columns)
    
    # Display mapping
    st.write("### Column Mapping")
    
    # Create a DataFrame for the mapping
    mapping_data = []
    for actual_col in actual_columns:
        expected_col = st.session_state.column_mapping.get(actual_col, "Not mapped")
        mapping_data.append({
            "Your Column": actual_col,
            "Maps To": expected_col
        })
    
    mapping_df = pd.DataFrame(mapping_data)
    st.dataframe(mapping_df)
    
    # Allow manual adjustment
    st.write("### Adjust Mapping")
    adjusted_mapping = {}
    
    for actual_col in actual_columns:
        current_mapping = st.session_state.column_mapping.get(actual_col, "Not mapped")
        options = ["Not mapped"] + expected_columns
        selected = st.selectbox(
            f"Map '{actual_col}' to:",
            options=options,
            index=options.index(current_mapping) if current_mapping in options else 0,
            key=f"map_{actual_col}"
        )
        
        if selected != "Not mapped":
            adjusted_mapping[actual_col] = selected
    
    # Update button
    if st.button("Update Mapping"):
        st.session_state.column_mapping = adjusted_mapping
        st.success("Column mapping updated!")
    
    # Show mapped columns preview
    if st.button("Preview Mapped Data"):
        try:
            mapped_data = mapper.apply_mapping(st.session_state.data, st.session_state.column_mapping)
            st.write("### Mapped Data Preview")
            st.dataframe(mapped_data.head(10))
        except Exception as e:
            st.error(f"Error applying mapping: {str(e)}")
    
    # Continue to analysis
    if st.button("Continue to Analysis"):
        try:
            mapped_data = mapper.apply_mapping(st.session_state.data, st.session_state.column_mapping)
            st.session_state.processed_data = mapped_data
            st.success("Data processed successfully! You can now configure analysis settings.")
        except Exception as e:
            st.error(f"Error processing data: {str(e)}")
# Analysis Settings Page
def render_analysis_settings():
    """Render the analysis settings page"""
    st.title("‚öôÔ∏è Analysis Settings")
    
    if st.session_state.processed_data is None:
        st.warning("Please process data first")
        return
    
    st.markdown("""
    Configure the fraud detection analysis settings. You can select which algorithms to run,
    adjust parameters, and set thresholds.
    """)
    
    # Feature engineering settings
    st.subheader("Feature Engineering")
    
    feature_options = {
        "Statistical Features": True,
        "Graph Features": True,
        "NLP Features": True,
        "Time Series Features": True
    }
    
    selected_features = {}
    for feature, default in feature_options.items():
        selected_features[feature] = st.checkbox(feature, value=default)
    
    # Model settings
    st.subheader("Model Selection")
    
    model_options = {
        "Unsupervised Models": True,
        "Supervised Models": True,
        "Rule-Based Models": True
    }
    
    selected_models = {}
    for model, default in model_options.items():
        selected_models[model] = st.checkbox(model, value=default)
    
    # Advanced settings
    with st.expander("Advanced Settings"):
        st.write("### Unsupervised Model Parameters")
        
        contamination = st.slider(
            "Contamination (expected fraud rate)",
            min_value=0.001,
            max_value=0.5,
            value=0.01,
            step=0.001,
            help="Expected proportion of outliers in the data"
        )
        
        st.write("### Supervised Model Parameters")
        
        if 'fraud_flag' in st.session_state.processed_data.columns:
            test_size = st.slider(
                "Test Size",
                min_value=0.1,
                max_value=0.5,
                value=0.2,
                step=0.05,
                help="Proportion of data to use for testing"
            )
        else:
            st.info("No fraud_flag column found. Supervised learning will use synthetic labels.")
            test_size = 0.2
        
        st.write("### Rule Engine Parameters")
        
        rule_threshold = st.slider(
            "Rule Threshold",
            min_value=0.0,
            max_value=1.0,
            value=0.7,
            step=0.05,
            help="Threshold for rule-based detection"
        )
    
    # Risk scoring settings
    st.subheader("Risk Scoring")
    
    # Map the internal values to display names
    method_display_names = {
        "weighted_average": "Weighted Average",
        "maximum": "Maximum Score", 
        "custom": "Custom Weights"
    }
    
    scoring_method = st.selectbox(
        "Scoring Method",
        list(method_display_names.keys()),
        format_func=lambda x: method_display_names[x],
        help="Method to combine scores from different models"
    )
    
    if scoring_method == "custom":
        st.write("### Custom Weights")
        unsupervised_weight = st.slider("Unsupervised Weight", 0.0, 1.0, 0.4, 0.05)
        supervised_weight = st.slider("Supervised Weight", 0.0, 1.0, 0.4, 0.05)
        rule_weight = st.slider("Rule Weight", 0.0, 1.0, 0.2, 0.05)
        
        # Normalize weights
        total = unsupervised_weight + supervised_weight + rule_weight
        if total > 0:
            unsupervised_weight /= total
            supervised_weight /= total
            rule_weight /= total
        
        custom_weights = {
            "unsupervised": unsupervised_weight,
            "supervised": supervised_weight,
            "rule": rule_weight
        }
    else:
        custom_weights = None
    
    # Threshold settings
    st.subheader("Detection Thresholds")
    
    threshold_method = st.selectbox(
        "Threshold Method",
        ["Fixed", "Percentile", "Dynamic"],
        help="Method to determine fraud threshold"
    )
    
    if threshold_method == "Fixed":
        threshold = st.slider(
            "Risk Threshold",
            min_value=0.0,
            max_value=1.0,
            value=0.5,
            step=0.05,
            help="Transactions above this threshold will be flagged as potential fraud"
        )
    elif threshold_method == "Percentile":
        percentile = st.slider(
            "Percentile",
            min_value=90,
            max_value=100,
            value=95,
            step=1,
            help="Transactions above this percentile of risk scores will be flagged"
        )
        threshold = None
    else:  # Dynamic
        st.info("Dynamic thresholds will be calculated based on data distribution")
        threshold = None
    
    # Save settings
    if st.button("Save Settings"):
        settings = {
            "features": selected_features,
            "models": selected_models,
            "contamination": contamination,
            "test_size": test_size,
            "rule_threshold": rule_threshold,
            "scoring_method": scoring_method,
            "custom_weights": custom_weights,
            "threshold_method": threshold_method,
            "threshold": threshold,
            "percentile": percentile if threshold_method == "Percentile" else None
        }
        
        st.session_state.settings = settings
        st.success("Settings saved! You can now run the detection analysis.")
# Clean dataframe function
def clean_dataframe(df):
    """
    Clean a DataFrame by handling infinity, NaN, and extreme values
    
    Args:
        df (DataFrame): Input DataFrame
        
    Returns:
        DataFrame: Cleaned DataFrame
    """
    try:
        # Replace infinity with NaN
        df = df.replace([np.inf, -np.inf], np.nan)
        
        # Handle extreme values for numeric columns
        for col in df.select_dtypes(include=[np.number]).columns:
            # Calculate percentiles
            p1 = np.nanpercentile(df[col], 1)
            p99 = np.nanpercentile(df[col], 99)
            
            # Cap extreme values
            if not np.isnan(p1) and not np.isnan(p99):
                # Use a more conservative capping approach
                iqr = p99 - p1
                lower_bound = p1 - 1.5 * iqr
                upper_bound = p99 + 1.5 * iqr
                
                df[col] = np.where(df[col] < lower_bound, lower_bound, df[col])
                df[col] = np.where(df[col] > upper_bound, upper_bound, df[col])
            
            # Replace any remaining NaN with median
            median_val = df[col].median()
            if not np.isnan(median_val):
                df[col] = df[col].fillna(median_val)
        
        # Handle categorical columns
        for col in df.select_dtypes(include=['object', 'category']).columns:
            # Fill NaN with mode or 'Unknown'
            mode_val = df[col].mode()
            if len(mode_val) > 0:
                df[col] = df[col].fillna(mode_val[0])
            else:
                df[col] = df[col].fillna('Unknown')
        
        return df
    except Exception as e:
        logger.error(f"Error cleaning DataFrame: {str(e)}")
        return df
# Run Detection Page
def render_run_detection():
    """Render the run detection page"""
    st.title("üöÄ Run Fraud Detection")
    
    if st.session_state.processed_data is None:
        st.warning("Please process data first")
        return
    
    if 'settings' not in st.session_state:
        st.warning("Please configure analysis settings first")
        return
    
    st.markdown("""
    Run the fraud detection analysis using the configured settings. This may take some time
    depending on the size of your dataset and selected algorithms.
    """)
    
    # Show settings summary
    st.subheader("Analysis Settings Summary")
    
    settings = st.session_state.settings
    
    st.write("#### Feature Engineering")
    for feature, enabled in settings["features"].items():
        status = "‚úÖ Enabled" if enabled else "‚ùå Disabled"
        st.write(f"- {feature}: {status}")
    
    st.write("#### Models")
    for model, enabled in settings["models"].items():
        status = "‚úÖ Enabled" if enabled else "‚ùå Disabled"
        st.write(f"- {model}: {status}")
    
    st.write("#### Risk Scoring")
    method_display_names = {
        "weighted_average": "Weighted Average",
        "maximum": "Maximum Score", 
        "custom": "Custom Weights"
    }
    st.write(f"- Method: {method_display_names.get(settings['scoring_method'], settings['scoring_method'])}")
    if settings['custom_weights']:
        st.write(f"- Unsupervised Weight: {settings['custom_weights']['unsupervised']:.2f}")
        st.write(f"- Supervised Weight: {settings['custom_weights']['supervised']:.2f}")
        st.write(f"- Rule Weight: {settings['custom_weights']['rule']:.2f}")
    
    st.write("#### Threshold")
    st.write(f"- Method: {settings['threshold_method']}")
    if settings['threshold_method'] == 'Fixed':
        st.write(f"- Value: {settings['threshold']}")
    elif settings['threshold_method'] == 'Percentile':
        st.write(f"- Percentile: {settings['percentile']}%")
    
    # Run button
    if st.button("Run Detection Analysis"):
        # Create progress bar
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        try:
            # Step 1: Feature Engineering
            status_text.text("Step 1/5: Feature Engineering...")
            progress_bar.progress(10)
            
            features_df = st.session_state.processed_data.copy()
            
            # Clean data before feature engineering
            features_df = clean_dataframe(features_df)
            
            # Statistical features
            if settings["features"]["Statistical Features"]:
                stat_features = StatisticalFeatures()
                features_df = stat_features.extract_features(features_df)
                # Clean after feature extraction
                features_df = clean_dataframe(features_df)
                progress_bar.progress(20)
            
            # Graph features
            if settings["features"]["Graph Features"]:
                graph_features = GraphFeatures()
                features_df = graph_features.extract_features(features_df)
                # Clean after feature extraction
                features_df = clean_dataframe(features_df)
                progress_bar.progress(30)
            
            # NLP features
            if settings["features"]["NLP Features"]:
                nlp_features = NLPFeatures()
                features_df = nlp_features.extract_features(features_df)
                # Clean after feature extraction
                features_df = clean_dataframe(features_df)
                progress_bar.progress(40)
            
            # Time series features
            if settings["features"]["Time Series Features"]:
                ts_features = TimeSeriesFeatures()
                features_df = ts_features.extract_features(features_df)
                # Clean after feature extraction
                features_df = clean_dataframe(features_df)
                progress_bar.progress(50)
            
            # Step 2: Model Training/Prediction
            status_text.text("Step 2/5: Running Models...")
            model_results = {}
            
            # Unsupervised models
            if settings["models"]["Unsupervised Models"]:
                unsupervised = UnsupervisedModels(contamination=settings["contamination"])
                unsupervised_results = unsupervised.run_models(features_df)
                model_results["unsupervised"] = unsupervised_results
                progress_bar.progress(60)
            
            # Supervised models
            if settings["models"]["Supervised Models"]:
                supervised = SupervisedModels(test_size=settings["test_size"])
                supervised_results = supervised.run_models(features_df)
                model_results["supervised"] = supervised_results
                progress_bar.progress(70)
            
            # Rule-based models
            if settings["models"]["Rule-Based Models"]:
                rule_engine = RuleEngine(threshold=settings["rule_threshold"])
                rule_results = rule_engine.apply_rules(features_df)
                model_results["rule"] = rule_results
                progress_bar.progress(80)
            
            # Step 3: Risk Scoring
            status_text.text("Step 3/5: Calculating Risk Scores...")
            
            # Initialize risk scorer with properly formatted method
            scoring_method = settings["scoring_method"]
            risk_scorer = RiskScorer(
                method=scoring_method,
                custom_weights=settings["custom_weights"]
            )
            
            risk_scores = risk_scorer.calculate_scores(model_results, features_df)
            progress_bar.progress(90)
            
            # Step 4: Apply Thresholds
            status_text.text("Step 4/5: Applying Thresholds...")
            if settings['threshold_method'] == "Fixed":
                threshold = settings['threshold']
                risk_scores["is_fraud"] = risk_scores["risk_score"] > threshold
            elif settings['threshold_method'] == "Percentile":
                percentile = settings['percentile']
                threshold = np.percentile(risk_scores["risk_score"], percentile)
                risk_scores["is_fraud"] = risk_scores["risk_score"] > threshold
            else:  # Dynamic
                # Calculate dynamic threshold based on distribution
                mean_score = risk_scores["risk_score"].mean()
                std_score = risk_scores["risk_score"].std()
                threshold = mean_score + 2 * std_score
                risk_scores["is_fraud"] = risk_scores["risk_score"] > threshold
            
            progress_bar.progress(95)
            
            # Step 5: Generate Explanations
            status_text.text("Step 5/5: Generating Explanations...")
            explainability = Explainability()
            explanations = explainability.generate_explanations(
                features_df, 
                risk_scores, 
                model_results
            )
            
            progress_bar.progress(100)
            
            # Store results in session state
            st.session_state.features_df = features_df
            st.session_state.model_results = model_results
            st.session_state.risk_scores = risk_scores
            st.session_state.explanations = explanations
            st.session_state.threshold = threshold
            st.session_state.processing_complete = True
            
            status_text.text("Analysis complete!")
            st.success(f"Fraud detection analysis complete! Found {risk_scores['is_fraud'].sum()} potentially fraudulent transactions out of {len(risk_scores)} total.")
            
        except Exception as e:
            st.error(f"Error during analysis: {str(e)}")
            st.exception(e)
# Results Dashboard Page
def render_results_dashboard():
    """Render the results dashboard page"""
    st.title("üìä Results Dashboard")
    
    if not st.session_state.processing_complete:
        st.warning("Please run the detection analysis first")
        return
    
    risk_scores = st.session_state.risk_scores
    
    # Summary statistics
    st.subheader("Summary Statistics")
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("Total Transactions", len(risk_scores))
    
    with col2:
        fraud_count = risk_scores['is_fraud'].sum()
        st.metric("Fraudulent Transactions", fraud_count)
    
    with col3:
        fraud_percentage = (fraud_count / len(risk_scores)) * 100
        st.metric("Fraud Percentage", f"{fraud_percentage:.2f}%")
    
    with col4:
        threshold = st.session_state.threshold
        st.metric("Detection Threshold", f"{threshold:.4f}")
    
    # Risk score distribution
    st.subheader("Risk Score Distribution")
    
    fig = px.histogram(
        risk_scores,
        x="risk_score",
        color="is_fraud",
        nbins=50,
        title="Distribution of Risk Scores",
        color_discrete_map={False: "blue", True: "red"},
        opacity=0.7
    )
    
    fig.add_vline(
        x=threshold,
        line_dash="dash",
        line_color="green",
        annotation_text=f"Threshold: {threshold:.4f}"
    )
    
    st.plotly_chart(fig, use_container_width=True)
    
    # Top risky transactions
    st.subheader("Top Risky Transactions")
    
    top_risky = risk_scores[risk_scores['is_fraud']].nlargest(10, 'risk_score')
    
    if len(top_risky) > 0:
        # Create a display dataframe with risk scores and available details
        display_data = top_risky.copy()
        
        # Try to add transaction details from original data if available
        if hasattr(st.session_state, 'original_data') and st.session_state.original_data is not None:
            original_data = st.session_state.original_data
            
            # Get columns that exist in both dataframes
            common_cols = set(original_data.columns) & set(display_data.columns)
            
            # If there are common columns, merge them
            if common_cols:
                display_data = display_data.drop(columns=list(common_cols), errors='ignore')
                display_data = display_data.merge(
                    original_data[list(common_cols)],
                    left_index=True,
                    right_index=True,
                    how='left'
                )
        
        # If we still don't have transaction_id, try to get it from features_df
        if 'transaction_id' not in display_data.columns and hasattr(st.session_state, 'features_df'):
            features_df = st.session_state.features_df
            if 'transaction_id' in features_df.columns:
                display_data = display_data.merge(
                    features_df[['transaction_id']],
                    left_index=True,
                    right_index=True,
                    how='left'
                )
        
        # If we still don't have transaction_id, use the index as a fallback
        if 'transaction_id' not in display_data.columns:
            display_data['transaction_id'] = display_data.index
        
        # Ensure we have the required columns for display
        display_cols = []
        
        # Always include transaction_id and risk_score
        if 'transaction_id' in display_data.columns:
            display_cols.append('transaction_id')
        display_cols.append('risk_score')
        
        # Add other available columns if they exist
        for col in ['amount', 'timestamp', 'sender_id', 'receiver_id']:
            if col in display_data.columns:
                display_cols.append(col)
        
        # Display the dataframe
        st.dataframe(
            display_data[display_cols].sort_values('risk_score', ascending=False)
        )
    else:
        st.info("No fraudulent transactions detected")
    
    # Feature importance
    st.subheader("Feature Importance")
    
    if 'supervised' in st.session_state.model_results and 'feature_importance' in st.session_state.model_results['supervised']:
        feature_importance = st.session_state.model_results['supervised']['feature_importance']
        
        # Get top 20 features
        top_features = feature_importance.head(20)
        
        fig = px.bar(
            top_features,
            x='importance',
            y='feature',
            orientation='h',
            title="Top 20 Feature Importance",
            color='importance',
            color_continuous_scale='Viridis'
        )
        
        fig.update_layout(yaxis={'categoryorder': 'total ascending'})
        st.plotly_chart(fig, use_container_width=True)
    else:
        st.info("Feature importance not available. Run supervised models to see feature importance.")
    
    # Model performance
    st.subheader("Model Performance")
    
    if 'supervised' in st.session_state.model_results and 'performance' in st.session_state.model_results['supervised']:
        performance = st.session_state.model_results['supervised']['performance']
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.write("##### Classification Report")
            st.dataframe(performance['classification_report'])
        
        with col2:
            st.write("##### Confusion Matrix")
            cm = performance['confusion_matrix']
            
            fig = px.imshow(
                cm,
                text_auto=True,
                aspect="auto",
                labels=dict(x="Predicted", y="Actual", color="Count"),
                x=["Not Fraud", "Fraud"],
                y=["Not Fraud", "Fraud"],
                color_continuous_scale='Blues'
            )
            
            st.plotly_chart(fig, use_container_width=True)
    else:
        st.info("Model performance metrics not available. Run supervised models with labeled data to see performance metrics.")
    
    # Time series analysis
    if hasattr(st.session_state, 'original_data') and st.session_state.original_data is not None and 'timestamp' in st.session_state.original_data.columns:
        st.subheader("Fraud Over Time")
        
        # Create a copy for time series analysis
        ts_data = st.session_state.original_data.copy()
        ts_data['risk_score'] = risk_scores['risk_score']
        ts_data['is_fraud'] = risk_scores['is_fraud']
        
        # Convert timestamp to datetime if needed
        if not pd.api.types.is_datetime64_any_dtype(ts_data['timestamp']):
            ts_data['timestamp'] = pd.to_datetime(ts_data['timestamp'])
        
        # Extract date components
        ts_data['date'] = ts_data['timestamp'].dt.date
        ts_data['hour'] = ts_data['timestamp'].dt.hour
        
        # Fraud by date
        fraud_by_date = ts_data.groupby('date').agg({
            'is_fraud': ['sum', 'count'],
            'risk_score': 'mean'
        }).reset_index()
        
        fraud_by_date.columns = ['date', 'fraud_count', 'total_count', 'avg_risk_score']
        fraud_by_date['fraud_rate'] = fraud_by_date['fraud_count'] / fraud_by_date['total_count']
        
        fig = px.line(
            fraud_by_date,
            x='date',
            y=['fraud_count', 'fraud_rate'],
            title="Fraud Count and Rate Over Time",
            labels={'value': 'Count / Rate', 'date': 'Date'}
        )
        
        st.plotly_chart(fig, use_container_width=True)
        
        # Fraud by hour of day
        fraud_by_hour = ts_data.groupby('hour').agg({
            'is_fraud': ['sum', 'count'],
            'risk_score': 'mean'
        }).reset_index()
        
        fraud_by_hour.columns = ['hour', 'fraud_count', 'total_count', 'avg_risk_score']
        fraud_by_hour['fraud_rate'] = fraud_by_hour['fraud_count'] / fraud_by_hour['total_count']
        
        fig = px.bar(
            fraud_by_hour,
            x='hour',
            y='fraud_count',
            title="Fraud Count by Hour of Day",
            color='avg_risk_score',
            color_continuous_scale='Viridis'
        )
        
        st.plotly_chart(fig, use_container_width=True)
    elif hasattr(st.session_state, 'features_df') and st.session_state.features_df is not None and 'timestamp' in st.session_state.features_df.columns:
        st.subheader("Fraud Over Time")
        
        # Create a copy for time series analysis
        ts_data = st.session_state.features_df.copy()
        ts_data['risk_score'] = risk_scores['risk_score']
        ts_data['is_fraud'] = risk_scores['is_fraud']
        
        # Convert timestamp to datetime if needed
        if not pd.api.types.is_datetime64_any_dtype(ts_data['timestamp']):
            ts_data['timestamp'] = pd.to_datetime(ts_data['timestamp'])
        
        # Extract date components
        ts_data['date'] = ts_data['timestamp'].dt.date
        ts_data['hour'] = ts_data['timestamp'].dt.hour
        
        # Fraud by date
        fraud_by_date = ts_data.groupby('date').agg({
            'is_fraud': ['sum', 'count'],
            'risk_score': 'mean'
        }).reset_index()
        
        fraud_by_date.columns = ['date', 'fraud_count', 'total_count', 'avg_risk_score']
        fraud_by_date['fraud_rate'] = fraud_by_date['fraud_count'] / fraud_by_date['total_count']
        
        fig = px.line(
            fraud_by_date,
            x='date',
            y=['fraud_count', 'fraud_rate'],
            title="Fraud Count and Rate Over Time",
            labels={'value': 'Count / Rate', 'date': 'Date'}
        )
        
        st.plotly_chart(fig, use_container_width=True)
        
        # Fraud by hour of day
        fraud_by_hour = ts_data.groupby('hour').agg({
            'is_fraud': ['sum', 'count'],
            'risk_score': 'mean'
        }).reset_index()
        
        fraud_by_hour.columns = ['hour', 'fraud_count', 'total_count', 'avg_risk_score']
        fraud_by_hour['fraud_rate'] = fraud_by_hour['fraud_count'] / fraud_by_hour['total_count']
        
        fig = px.bar(
            fraud_by_hour,
            x='hour',
            y='fraud_count',
            title="Fraud Count by Hour of Day",
            color='avg_risk_score',
            color_continuous_scale='Viridis'
        )
        
        st.plotly_chart(fig, use_container_width=True)
    else:
        st.info("Timestamp data not available for time series analysis")
# Explainability Page
def render_explainability():
    """Render the explainability page"""
    st.title("üîç Explainability")
    
    if not st.session_state.processing_complete:
        st.warning("Please run the detection analysis first")
        return
    
    risk_scores = st.session_state.risk_scores
    explanations = st.session_state.explanations
    
    # Transaction selector
    st.subheader("Transaction Analysis")
    
    # Get fraudulent transactions
    fraud_transactions = risk_scores[risk_scores['is_fraud']]
    
    if len(fraud_transactions) == 0:
        st.info("No fraudulent transactions detected")
        return
    
    # Create a selector for transaction
    # Check if we have transaction_id in features_df
    if hasattr(st.session_state, 'features_df') and st.session_state.features_df is not None and 'transaction_id' in st.session_state.features_df.columns:
        # Merge with features to get transaction IDs
        fraud_with_ids = fraud_transactions.copy()
        fraud_with_ids = fraud_with_ids.merge(
            st.session_state.features_df[['transaction_id']],
            left_index=True,
            right_index=True,
            how='left'
        )
        
        # Create transaction options
        transaction_options = []
        for idx in fraud_with_ids.index:
            # Check if transaction_id exists and is not NaN
            if 'transaction_id' in fraud_with_ids.columns and pd.notna(fraud_with_ids.loc[idx, 'transaction_id']):
                transaction_id = fraud_with_ids.loc[idx, 'transaction_id']
                transaction_options.append(f"{transaction_id} (Index: {idx})")
            else:
                transaction_options.append(f"Index: {idx}")
        
        selected_transaction = st.selectbox("Select a transaction to analyze", transaction_options)
        
        # Parse the selected option to get the index
        if "Index:" in selected_transaction:
            selected_index = int(selected_transaction.split("Index: ")[1])
        else:
            # Extract transaction_id from the selected option
            transaction_id = selected_transaction.split(" (Index: ")[0]
            # Find the index for this transaction_id
            selected_index = fraud_with_ids[fraud_with_ids['transaction_id'] == transaction_id].index[0]
    else:
        # Use index as identifier
        transaction_options = [f"Index: {idx}" for idx in fraud_transactions.index]
        selected_transaction = st.selectbox("Select a transaction to analyze", transaction_options)
        selected_index = int(selected_transaction.split("Index: ")[1])
    
    # Display transaction details
    st.write("### Transaction Details")
    
    # Try to get transaction details from features_df
    if hasattr(st.session_state, 'features_df') and st.session_state.features_df is not None:
        try:
            transaction_data = st.session_state.features_df.loc[selected_index]
            
            # Display key fields
            key_fields = ['transaction_id', 'amount', 'timestamp', 'sender_id', 'receiver_id', 
                         'transaction_type', 'transaction_category', 'description']
            
            for field in key_fields:
                if field in transaction_data:
                    st.write(f"**{field.replace('_', ' ').title()}:** {transaction_data[field]}")
        except Exception as e:
            st.warning(f"Could not retrieve transaction details: {str(e)}")
    
    # Risk score
    risk_score = risk_scores.loc[selected_index, 'risk_score']
    st.metric("Risk Score", f"{risk_score:.4f}")
    
    # Explanation
    st.write("### Fraud Explanation")
    
    if selected_index in explanations:
        explanation = explanations[selected_index]
        
        # Show top contributing factors
        if 'top_factors' in explanation:
            st.write("#### Top Contributing Factors")
            
            factors_df = pd.DataFrame(explanation['top_factors'])
            factors_df.columns = ['Feature', 'Contribution']
            
            # Create a bar chart
            fig = px.bar(
                factors_df,
                x='Contribution',
                y='Feature',
                orientation='h',
                title="Feature Contributions to Risk Score",
                color='Contribution',
                color_continuous_scale='RdBu'
            )
            
            fig.update_layout(yaxis={'categoryorder': 'total ascending'})
            st.plotly_chart(fig, use_container_width=True)
        
        # Show rule violations
        if 'rule_violations' in explanation and len(explanation['rule_violations']) > 0:
            st.write("#### Rule Violations")
            
            for rule in explanation['rule_violations']:
                st.write(f"- {rule}")
        
        # Show model predictions
        if 'model_predictions' in explanation:
            st.write("#### Model Predictions")
            
            model_df = pd.DataFrame([
                {'Model': model, 'Score': score} 
                for model, score in explanation['model_predictions'].items()
            ])
            
            st.dataframe(model_df)
        
        # Show natural language explanation
        if 'text_explanation' in explanation:
            st.write("#### Explanation Summary")
            st.write(explanation['text_explanation'])
    else:
        st.info("No explanation available for this transaction")
    
    # What-if analysis
    st.write("### What-If Analysis")
    
    st.write("Adjust feature values to see how they affect the risk score:")
    
    # Get top features for this transaction
    if selected_index in explanations and 'top_factors' in explanations[selected_index]:
        top_features = [factor[0] for factor in explanations[selected_index]['top_factors'][:5]]
        
        # Create sliders for top features
        adjustments = {}
        for feature in top_features:
            if feature in st.session_state.features_df.columns:
                current_value = st.session_state.features_df.loc[selected_index, feature]
                
                # Determine appropriate range
                min_val = st.session_state.features_df[feature].min()
                max_val = st.session_state.features_df[feature].max()
                
                # Handle categorical features
                if st.session_state.features_df[feature].dtype == 'object':
                    options = st.session_state.features_df[feature].unique().tolist()
                    adjusted_value = st.selectbox(
                        f"Adjust {feature}",
                        options=options,
                        index=options.index(current_value) if current_value in options else 0
                    )
                else:
                    adjusted_value = st.slider(
                        f"Adjust {feature}",
                        min_value=float(min_val),
                        max_value=float(max_val),
                        value=float(current_value)
                    )
                
                adjustments[feature] = adjusted_value
        
        # Recalculate risk score button
        if st.button("Recalculate Risk Score"):
            # Create a copy of the original data
            modified_data = st.session_state.features_df.copy().loc[[selected_index]]
            
            # Apply adjustments
            for feature, value in adjustments.items():
                modified_data.loc[selected_index, feature] = value
            
            # Recalculate features
            modified_features = modified_data.copy()
            
            # Statistical features
            if st.session_state.settings["features"]["Statistical Features"]:
                stat_features = StatisticalFeatures()
                modified_features = stat_features.extract_features(modified_features)
            
            # Graph features
            if st.session_state.settings["features"]["Graph Features"]:
                graph_features = GraphFeatures()
                modified_features = graph_features.extract_features(modified_features)
            
            # NLP features
            if st.session_state.settings["features"]["NLP Features"]:
                nlp_features = NLPFeatures()
                modified_features = nlp_features.extract_features(modified_features)
            
            # Time series features
            if st.session_state.settings["features"]["Time Series Features"]:
                ts_features = TimeSeriesFeatures()
                modified_features = ts_features.extract_features(modified_features)
            
            # Get model predictions
            model_predictions = {}
            
            # Unsupervised models
            if 'unsupervised' in st.session_state.model_results:
                for model_name, model in st.session_state.model_results['unsupervised']['models'].items():
                    try:
                        # Get features used by this model
                        if 'feature_names' in st.session_state.model_results['unsupervised']:
                            feature_names = st.session_state.model_results['unsupervised']['feature_names']
                            model_features = modified_features[feature_names[model_name]]
                            prediction = model.decision_function(model_features)[0]
                            model_predictions[f"unsupervised_{model_name}"] = prediction
                    except Exception as e:
                        st.warning(f"Error predicting with {model_name}: {str(e)}")
            
            # Supervised models
            if 'supervised' in st.session_state.model_results:
                for model_name, model in st.session_state.model_results['supervised']['models'].items():
                    try:
                        # Get features used by this model
                        if 'feature_names' in st.session_state.model_results['supervised']:
                            feature_names = st.session_state.model_results['supervised']['feature_names']
                            model_features = modified_features[feature_names[model_name]]
                            prediction = model.predict_proba(model_features)[0, 1]
                            model_predictions[f"supervised_{model_name}"] = prediction
                    except Exception as e:
                        st.warning(f"Error predicting with {model_name}: {str(e)}")
            
            # Rule-based models
            if 'rule' in st.session_state.model_results:
                rule_score = 0.0
                violated_rules = []
                
                for rule_name, rule_func in st.session_state.model_results['rule']['rules'].items():
                    try:
                        if rule_func(modified_data.iloc[0]):
                            rule_score += 1.0 / len(st.session_state.model_results['rule']['rules'])
                            violated_rules.append(rule_name)
                    except Exception as e:
                        st.warning(f"Error applying rule {rule_name}: {str(e)}")
                
                model_predictions["rule"] = rule_score
            
            # Calculate new risk score
            risk_scorer = RiskScorer(
                method=st.session_state.settings["scoring_method"],
                custom_weights=st.session_state.settings["custom_weights"]
            )
            
            new_risk_score = risk_scorer.calculate_scores(
                {"unsupervised": model_predictions, "supervised": model_predictions, "rule": model_predictions},
                modified_features
            )
            
            new_risk_score = new_risk_score.loc[selected_index, 'risk_score']
            
            # Display comparison
            st.write("#### Risk Score Comparison")
            
            col1, col2 = st.columns(2)
            
            with col1:
                st.metric("Original Risk Score", f"{risk_score:.4f}")
            
            with col2:
                st.metric("New Risk Score", f"{new_risk_score:.4f}", 
                         delta=f"{new_risk_score - risk_score:.4f}")
            
            # Show if it would still be flagged as fraud
            threshold = st.session_state.threshold
            is_fraud_original = risk_score > threshold
            is_fraud_new = new_risk_score > threshold
            
            st.write(f"Original classification: {'Fraud' if is_fraud_original else 'Not Fraud'}")
            st.write(f"New classification: {'Fraud' if is_fraud_new else 'Not Fraud'}")
            
            if is_fraud_original != is_fraud_new:
                st.warning("Classification changed with the adjustments!")
            else:
                st.info("Classification remains the same")
# Reports Page
def render_reports():
    """Render the reports page"""
    st.title("üìÑ Reports")
    
    if not st.session_state.processing_complete:
        st.warning("Please run the detection analysis first")
        return
    
    risk_scores = st.session_state.risk_scores
    features_df = st.session_state.features_df
    
    st.markdown("""
    Generate comprehensive reports for auditors and stakeholders. Reports can be downloaded
    in PDF format and include detailed analysis of fraudulent transactions.
    """)
    
    # Report options
    st.subheader("Report Options")
    
    report_type = st.selectbox(
        "Select Report Type",
        ["Executive Summary", "Detailed Fraud Analysis", "Technical Report", "Custom Report"]
    )
    
    include_charts = st.checkbox("Include Charts and Visualizations", value=True)
    include_explanations = st.checkbox("Include Detailed Explanations", value=True)
    include_recommendations = st.checkbox("Include Recommendations", value=True)
    
    # Transaction selection
    st.subheader("Transaction Selection")
    
    fraud_transactions = risk_scores[risk_scores['is_fraud']]
    
    if len(fraud_transactions) > 0:
        max_transactions = min(len(fraud_transactions), 100)  # Limit to 100 for performance
        num_transactions = st.slider(
            "Number of Fraudulent Transactions to Include",
            min_value=1,
            max_value=max_transactions,
            value=min(10, max_transactions),
            step=1
        )
        
        # Sort by risk score and select top N
        top_fraud = fraud_transactions.nlargest(num_transactions, 'risk_score')
    else:
        st.info("No fraudulent transactions detected")
        return
    
    # Report generation options
    st.subheader("Report Generation")
    
    if st.button("Generate Report"):
        with st.spinner("Generating report..."):
            try:
                # Initialize PDF generator
                pdf_generator = PDFGenerator()
                
                # Generate report based on type
                if report_type == "Executive Summary":
                    report_path = pdf_generator.generate_executive_summary(
                        features_df,
                        risk_scores,
                        top_fraud,
                        include_charts=include_charts,
                        include_recommendations=include_recommendations
                    )
                elif report_type == "Detailed Fraud Analysis":
                    report_path = pdf_generator.generate_detailed_fraud_analysis(
                        features_df,
                        risk_scores,
                        top_fraud,
                        st.session_state.explanations,
                        include_charts=include_charts,
                        include_explanations=include_explanations,
                        include_recommendations=include_recommendations
                    )
                elif report_type == "Technical Report":
                    report_path = pdf_generator.generate_technical_report(
                        features_df,
                        risk_scores,
                        st.session_state.model_results,
                        include_charts=include_charts
                    )
                else:  # Custom Report
                    report_path = pdf_generator.generate_custom_report(
                        features_df,
                        risk_scores,
                        top_fraud,
                        st.session_state.explanations,
                        st.session_state.model_results,
                        include_charts=include_charts,
                        include_explanations=include_explanations,
                        include_recommendations=include_recommendations
                    )
                
                # Display success message
                st.success(f"Report generated successfully!")
                
                # Provide download button
                with open(report_path, "rb") as file:
                    st.download_button(
                        label="Download Report",
                        data=file,
                        file_name=os.path.basename(report_path),
                        mime="application/pdf"
                    )
                
                # Show report preview
                st.subheader("Report Preview")
                st.info("Report preview is not available in this interface. Please download the PDF to view the full report.")
                
            except Exception as e:
                st.error(f"Error generating report: {str(e)}")
                st.exception(e)
# Main function
def main():
    """Main function to run the Streamlit app"""
    page = render_sidebar()
    
    if page == "Data Upload":
        render_data_upload()
    elif page == "Column Mapping":
        render_column_mapping()
    elif page == "Analysis Settings":
        render_analysis_settings()
    elif page == "Run Detection":
        render_run_detection()
    elif page == "Results Dashboard":
        render_results_dashboard()
    elif page == "Explainability":
        render_explainability()
    elif page == "Reports":
        render_reports()
if __name__ == "__main__":
    main()

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\app\.ipynb_checkpoints\main_dashboard-checkpoint.py ===
"""
Main Dashboard for Financial Fraud Detection System
Streamlit-based user interface for the fraud detection platform
"""
import streamlit as st
import pandas as pd
import numpy as np
import os
import sys
import time
import json
import plotly.express as px
import plotly.graph_objects as go
from datetime import datetime, timedelta
import yaml
from io import BytesIO, StringIO
import logging
import tempfile
import chardet
import traceback
from typing import Optional
# Add src to path to import our modules
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'src'))
from fraud_detection_engine.ingestion.data_loader import DataLoader
from fraud_detection_engine.ingestion.column_mapper import ColumnMapper
from fraud_detection_engine.features.statistical_features import StatisticalFeatures
from fraud_detection_engine.features.graph_features import GraphFeatures
from fraud_detection_engine.features.nlp_features import NLPFeatures
from fraud_detection_engine.features.timeseries_features import TimeSeriesFeatures
from fraud_detection_engine.models.unsupervised import UnsupervisedModels
from fraud_detection_engine.models.supervised import SupervisedModels
from fraud_detection_engine.models.rule_based import RuleEngine
from fraud_detection_engine.analysis.risk_scorer import RiskScorer
from fraud_detection_engine.analysis.explainability import Explainability
from fraud_detection_engine.reporting.pdf_generator import PDFGenerator
from fraud_detection_engine.utils.api_utils import is_api_available
# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)
# Set page configuration
st.set_page_config(
    page_title="Financial Fraud Detection System",
    page_icon="üõ°Ô∏è",
    layout="wide",
    initial_sidebar_state="expanded"
)
# Load configuration
@st.cache_resource
def load_config():
    """Load configuration from YAML files"""
    config = {}
    try:
        # Get the directory where this script is located
        script_dir = os.path.dirname(os.path.abspath(__file__))
        
        # Construct paths to config files relative to script location
        model_params_path = os.path.join(script_dir, '..', 'config', 'model_params.yml')
        rule_engine_path = os.path.join(script_dir, '..', 'config', 'rule_engine_config.yml')
        api_keys_path = os.path.join(script_dir, '..', 'config', 'api_keys.yml')
        
        with open(model_params_path, 'r') as f:
            config['model_params'] = yaml.safe_load(f)
        with open(rule_engine_path, 'r') as f:
            config['rule_engine'] = yaml.safe_load(f)
        with open(api_keys_path, 'r') as f:
            config['api_keys'] = yaml.safe_load(f)
    except Exception as e:
        st.error(f"Error loading configuration: {str(e)}")
        config = {
            'model_params': {},
            'rule_engine': {},
            'api_keys': {}
        }
    return config
config = load_config()
# Initialize session state variables
if 'data' not in st.session_state:
    st.session_state.data = None
if 'processed_data' not in st.session_state:
    st.session_state.processed_data = None
if 'risk_scores' not in st.session_state:
    st.session_state.risk_scores = None
if 'column_mapping' not in st.session_state:
    st.session_state.column_mapping = None
if 'model_results' not in st.session_state:
    st.session_state.model_results = {}
if 'explanations' not in st.session_state:
    st.session_state.explanations = None
if 'processing_complete' not in st.session_state:
    st.session_state.processing_complete = False
if 'original_data' not in st.session_state:
    st.session_state.original_data = None
if 'features_df' not in st.session_state:
    st.session_state.features_df = None
# Sidebar
def render_sidebar():
    """Render the sidebar with navigation and controls"""
    st.sidebar.title("üõ°Ô∏è Fraud Detection System")
    st.sidebar.image("https://www.streamlit.io/images/brand/streamlit-logo-secondary-colormark-darktext.png", width=200)
    
    # Navigation
    st.sidebar.subheader("Navigation")
    page = st.sidebar.radio("Go to", [
        "Data Upload", 
        "Column Mapping", 
        "Analysis Settings", 
        "Run Detection", 
        "Results Dashboard", 
        "Explainability", 
        "Reports"
    ])
    
    # System status
    st.sidebar.subheader("System Status")
    if st.session_state.data is not None:
        st.sidebar.success(f"Data loaded: {len(st.session_state.data)} transactions")
    else:
        st.sidebar.warning("No data loaded")
    
    if st.session_state.processing_complete:
        st.sidebar.success("Analysis complete")
    else:
        st.sidebar.info("Analysis pending")
    
    # API Status
    st.sidebar.subheader("API Status")
    api_status = {
        'Gemini': is_api_available('gemini'),
        'OpenAI': is_api_available('openai'),
        'News API': is_api_available('news_api'),
        'Geolocation': is_api_available('geolocation'),
        'Sanctions': is_api_available('sanctions'),
        'Tax Compliance': is_api_available('tax_compliance'),
        'Bank Verification': is_api_available('bank_verification'),
        'Identity Verification': is_api_available('identity_verification')
    }
    
    for api, status in api_status.items():
        if status:
            st.sidebar.success(f"{api}: ‚úÖ Available")
        else:
            st.sidebar.warning(f"{api}: ‚ùå Not Available")
    
    # Configuration
    st.sidebar.subheader("Configuration")
    if st.sidebar.button("Reload Config"):
        config = load_config()
        st.sidebar.success("Configuration reloaded")
    
    # About section
    st.sidebar.subheader("About")
    st.sidebar.info("""
    This is a comprehensive financial fraud detection system that uses advanced machine learning, statistical analysis, and AI techniques to identify potentially fraudulent transactions.
    """)
    
    return page
# Helper function to detect file encoding
def detect_file_encoding(file_content):
    """Detect the encoding of file content"""
    try:
        result = chardet.detect(file_content)
        return result['encoding']
    except Exception as e:
        logger.error(f"Error detecting file encoding: {str(e)}")
        return None
# Helper function to read CSV with multiple encoding attempts
def read_csv_with_encodings(file_content, filename):
    """Read CSV file trying multiple encodings"""
    encodings = ['utf-8', 'latin-1', 'cp1252', 'iso-8859-1', 'utf-16']
    
    # First try with detected encoding
    detected_encoding = detect_file_encoding(file_content)
    if detected_encoding:
        try:
            st.write(f"Trying detected encoding: {detected_encoding}")
            return pd.read_csv(StringIO(file_content.decode(detected_encoding)))
        except Exception as e:
            st.warning(f"Failed with detected encoding {detected_encoding}: {str(e)}")
    
    # Try common encodings
    for encoding in encodings:
        try:
            st.write(f"Trying encoding: {encoding}")
            return pd.read_csv(StringIO(file_content.decode(encoding)))
        except UnicodeDecodeError:
            st.warning(f"Failed with {encoding} encoding: Unicode decode error")
        except Exception as e:
            st.warning(f"Failed with {encoding} encoding: {str(e)}")
    
    return None
# Helper function to read Excel with multiple engines
def read_excel_with_engines(file_content, filename):
    """Read Excel file trying multiple engines"""
    # Try default engine first
    try:
        st.write("Trying default Excel engine")
        return pd.read_excel(BytesIO(file_content))
    except Exception as e:
        st.warning(f"Failed with default engine: {str(e)}")
    
    # Try specific engines based on file extension
    if filename.endswith('.xlsx'):
        try:
            st.write("Trying openpyxl engine")
            return pd.read_excel(BytesIO(file_content), engine='openpyxl')
        except Exception as e:
            st.warning(f"Failed with openpyxl: {str(e)}")
    elif filename.endswith('.xls'):
        try:
            st.write("Trying xlrd engine")
            return pd.read_excel(BytesIO(file_content), engine='xlrd')
        except Exception as e:
            st.warning(f"Failed with xlrd: {str(e)}")
    
    return None
# Data Upload Page
def render_data_upload():
    """Render the data upload page"""
    st.title("üìÅ Data Upload")
    st.markdown("""
    Upload your transaction data for analysis. The system supports CSV and Excel files.
    """)
    
    # Debug information
    st.write("### Debug Information")
    st.write(f"Current working directory: {os.getcwd()}")
    st.write(f"Script directory: {os.path.dirname(os.path.abspath(__file__))}")
    
    # File upload with enhanced error handling
    uploaded_file = st.file_uploader(
        "Choose a CSV or Excel file",
        type=['csv', 'xlsx', 'xls'],
        help="Upload a file containing transaction data",
        key="file_uploader"
    )
    
    # Debug: Show if file was uploaded
    if uploaded_file is not None:
        st.write("### File Upload Details")
        st.write(f"File name: {uploaded_file.name}")
        st.write(f"File type: {uploaded_file.type}")
        st.write(f"File size: {uploaded_file.size} bytes")
    
    if uploaded_file is not None:
        try:
            # Read file content directly
            file_content = uploaded_file.read()
            
            # Read the file based on its type
            df = None
            
            if uploaded_file.name.endswith('.csv'):
                st.write("### Processing CSV file")
                df = read_csv_with_encodings(file_content, uploaded_file.name)
                
                if df is None:
                    st.error("Could not read CSV file with any encoding")
                    
                    # Show file content for debugging
                    st.write("### File Content (first 1000 bytes)")
                    try:
                        st.write(file_content[:1000])
                    except Exception as e:
                        st.error(f"Could not display file content: {str(e)}")
                    
                    # Try to create a simple CSV from the content
                    st.write("### Attempting to create CSV from content")
                    try:
                        # Try different encodings directly
                        for encoding in ['utf-8', 'latin-1', 'cp1252']:
                            try:
                                content_str = file_content.decode(encoding, errors='ignore')
                                df = pd.read_csv(StringIO(content_str))
                                st.success(f"Successfully read CSV with {encoding} encoding")
                                break
                            except:
                                continue
                    except Exception as e:
                        st.error(f"Failed to create CSV from content: {str(e)}")
                
            elif uploaded_file.name.endswith(('.xlsx', '.xls')):
                st.write("### Processing Excel file")
                df = read_excel_with_engines(file_content, uploaded_file.name)
                
                if df is None:
                    st.error("Could not read Excel file with any engine")
            else:
                st.error(f"Unsupported file type: {uploaded_file.name}")
            
            # Validate the dataframe
            if df is None:
                st.error("Failed to read the uploaded file")
                return
            
            if df.empty:
                st.error("The uploaded file appears to be empty")
                return
            
            # Display data info
            st.success(f"File uploaded successfully! Shape: {df.shape}")
            st.write("### Data Preview")
            st.dataframe(df.head(10))
            
            # Store in session state
            st.session_state.data = df
            st.session_state.original_data = df.copy()
            st.write("Data stored in session state")
            
            # Show column information
            st.write("### Column Information")
            col_info = pd.DataFrame({
                'Column': df.columns,
                'Data Type': df.dtypes,
                'Non-Null Count': df.count(),
                'Null Count': df.isnull().sum(),
                'Unique Values': df.nunique()
            })
            st.dataframe(col_info)
            
            # Basic statistics
            st.write("### Basic Statistics")
            numeric_cols = df.select_dtypes(include=[np.number]).columns
            if len(numeric_cols) > 0:
                st.dataframe(df[numeric_cols].describe())
            else:
                st.info("No numeric columns found for statistics")
                
            # Check file structure
            st.write("### File Structure Check")
            if len(df.columns) > 0:
                st.write(f"Found {len(df.columns)} columns")
                st.write("Columns:", list(df.columns))
            else:
                st.error("No columns found in the file")
                
        except Exception as e:
            st.error(f"Error processing file: {str(e)}")
            st.write("### Error Details")
            st.text(traceback.format_exc())
    
    # Sample data option
    st.markdown("---")
    st.subheader("Or Use Sample Data")
    
    if st.button("Load Sample Data"):
        try:
            # Look for sample data in multiple locations
            possible_paths = [
                os.path.join('..', 'data', 'sample_transactions.csv'),
                os.path.join('data', 'sample_transactions.csv'),
                'sample_transactions.csv'
            ]
            
            sample_path = None
            for path in possible_paths:
                st.write(f"Checking path: {path}")
                if os.path.exists(path):
                    sample_path = path
                    st.write(f"Found sample data at: {path}")
                    break
            
            if sample_path:
                df = pd.read_csv(sample_path)
                st.session_state.data = df
                st.session_state.original_data = df.copy()
                st.success(f"Sample data loaded! Shape: {df.shape}")
                st.dataframe(df.head(10))
            else:
                st.warning("Sample data file not found, creating new sample...")
                
                # Create sample data if it doesn't exist
                np.random.seed(42)
                
                # Create more realistic sample data
                n_samples = 1000
                start_date = datetime(2023, 1, 1)
                
                sample_data = pd.DataFrame({
                    'transaction_id': [f'TXN{i:06d}' for i in range(1, n_samples + 1)],
                    'timestamp': [start_date + timedelta(days=np.random.randint(0, 365)) for _ in range(n_samples)],
                    'amount': np.random.lognormal(8, 1.5, n_samples),  # More realistic amounts
                    'sender_id': [f'CUST{np.random.randint(1, 201):03d}' for _ in range(n_samples)],
                    'receiver_id': [f'CUST{np.random.randint(1, 201):03d}' for _ in range(n_samples)],
                    'currency': ['USD'] * n_samples,
                    'description': [f'Transaction type {np.random.choice(["PAYMENT", "TRANSFER", "PURCHASE", "WITHDRAWAL"])}' for _ in range(n_samples)],
                    'transaction_type': np.random.choice(['debit', 'credit'], n_samples),
                    'merchant_category': np.random.choice(['retail', 'services', 'utilities', 'finance'], n_samples)
                })
                
                # Ensure data directory exists
                data_dir = os.path.join('..', 'data')
                if not os.path.exists(data_dir):
                    os.makedirs(data_dir)
                
                sample_path = os.path.join(data_dir, 'sample_transactions.csv')
                sample_data.to_csv(sample_path, index=False)
                st.write(f"Created sample data at: {sample_path}")
                
                # Load the newly created sample data
                st.session_state.data = sample_data
                st.session_state.original_data = sample_data.copy()
                st.success(f"Sample data loaded! Shape: {sample_data.shape}")
                st.dataframe(sample_data.head(10))
                
        except Exception as e:
            st.error(f"Error loading sample data: {str(e)}")
            st.write("### Error Details")
            st.text(traceback.format_exc())
    
    # Additional troubleshooting section
    st.markdown("---")
    st.subheader("Troubleshooting File Upload Issues")
    
    with st.expander("Click here if you're having trouble uploading files"):
        st.write("""
        ### Common Issues and Solutions:
        
        1. **File not uploading at all**:
           - Try using a different browser (Chrome, Firefox, or Edge work best)
           - Clear your browser cache and cookies
           - Check if your file size is too large (try with a smaller file first)
           - Make sure the file is not open in another program
           - Try refreshing the page and uploading again
        
        2. **CSV file encoding issues**:
           - Try saving your CSV file with UTF-8 encoding
           - Use Excel to "Save As" and select "CSV UTF-8" format
           - Try converting your file to Excel format (.xlsx)
           - Check if your CSV has special characters that might cause encoding issues
        
        3. **Permission issues**:
           - Make sure the file is not read-only
           - Try moving the file to your desktop first
           - Check if your antivirus software is blocking the upload
           - Try running the application with administrator privileges
        
        4. **Browser compatibility**:
           - Update your browser to the latest version
           - Try incognito/private browsing mode
           - Disable browser extensions temporarily
           - Try a different browser
        
        5. **File format issues**:
           - Make sure your CSV file has proper headers
           - Check if your file has consistent formatting
           - Try opening the file in Excel and re-saving it
           - Remove any special characters from column names
        
        ### Alternative Methods:
        
        If the file uploader still doesn't work, you can:
        1. Use the "Load Sample Data" button above
        2. Place your CSV file in the `data` folder with the name `sample_transactions.csv`
        3. Try converting your file to Excel format (.xlsx)
        4. Use the text area below to paste your CSV data directly:
        """)
        
        # Add text area for direct CSV input
        st.write("### Direct CSV Input")
        csv_text = st.text_area("Paste your CSV data here:", height=200)
        
        if st.button("Load from Text"):
            try:
                if csv_text.strip():
                    # Use StringIO to read the CSV data
                    df = pd.read_csv(StringIO(csv_text))
                    st.session_state.data = df
                    st.session_state.original_data = df.copy()
                    st.success(f"Data loaded from text! Shape: {df.shape}")
                    st.dataframe(df.head(10))
                else:
                    st.warning("Please paste some CSV data")
            except Exception as e:
                st.error(f"Error loading data from text: {str(e)}")
                st.write("### Error Details")
                st.text(traceback.format_exc())
# Column Mapping Page
def render_column_mapping():
    """Render the column mapping page"""
    st.title("üîÑ Column Mapping")
    
    if st.session_state.data is None:
        st.warning("Please upload data first")
        return
    
    st.markdown("""
    Map your column headers to the expected format. The system will attempt to auto-map columns,
    but you can review and adjust as needed.
    """)
    
    # Initialize column mapper
    mapper = ColumnMapper()
    
    # Get expected columns
    expected_columns = mapper.get_expected_columns()
    
    # Get actual columns
    actual_columns = list(st.session_state.data.columns)
    
    # Auto-map columns
    if st.session_state.column_mapping is None:
        with st.spinner("Auto-mapping columns..."):
            st.session_state.column_mapping = mapper.auto_map_columns(actual_columns, expected_columns)
    
    # Display mapping
    st.write("### Column Mapping")
    
    # Create a DataFrame for the mapping
    mapping_data = []
    for actual_col in actual_columns:
        expected_col = st.session_state.column_mapping.get(actual_col, "Not mapped")
        mapping_data.append({
            "Your Column": actual_col,
            "Maps To": expected_col
        })
    
    mapping_df = pd.DataFrame(mapping_data)
    st.dataframe(mapping_df)
    
    # Allow manual adjustment
    st.write("### Adjust Mapping")
    adjusted_mapping = {}
    
    for actual_col in actual_columns:
        current_mapping = st.session_state.column_mapping.get(actual_col, "Not mapped")
        options = ["Not mapped"] + expected_columns
        selected = st.selectbox(
            f"Map '{actual_col}' to:",
            options=options,
            index=options.index(current_mapping) if current_mapping in options else 0,
            key=f"map_{actual_col}"
        )
        
        if selected != "Not mapped":
            adjusted_mapping[actual_col] = selected
    
    # Update button
    if st.button("Update Mapping"):
        st.session_state.column_mapping = adjusted_mapping
        st.success("Column mapping updated!")
    
    # Show mapped columns preview
    if st.button("Preview Mapped Data"):
        try:
            mapped_data = mapper.apply_mapping(st.session_state.data, st.session_state.column_mapping)
            st.write("### Mapped Data Preview")
            st.dataframe(mapped_data.head(10))
        except Exception as e:
            st.error(f"Error applying mapping: {str(e)}")
    
    # Continue to analysis
    if st.button("Continue to Analysis"):
        try:
            mapped_data = mapper.apply_mapping(st.session_state.data, st.session_state.column_mapping)
            st.session_state.processed_data = mapped_data
            st.success("Data processed successfully! You can now configure analysis settings.")
        except Exception as e:
            st.error(f"Error processing data: {str(e)}")
# Analysis Settings Page
def render_analysis_settings():
    """Render the analysis settings page"""
    st.title("‚öôÔ∏è Analysis Settings")
    
    if st.session_state.processed_data is None:
        st.warning("Please process data first")
        return
    
    st.markdown("""
    Configure the fraud detection analysis settings. You can select which algorithms to run,
    adjust parameters, and set thresholds.
    """)
    
    # Feature engineering settings
    st.subheader("Feature Engineering")
    
    feature_options = {
        "Statistical Features": True,
        "Graph Features": True,
        "NLP Features": True,
        "Time Series Features": True
    }
    
    selected_features = {}
    for feature, default in feature_options.items():
        selected_features[feature] = st.checkbox(feature, value=default)
    
    # Model settings
    st.subheader("Model Selection")
    
    model_options = {
        "Unsupervised Models": True,
        "Supervised Models": True,
        "Rule-Based Models": True
    }
    
    selected_models = {}
    for model, default in model_options.items():
        selected_models[model] = st.checkbox(model, value=default)
    
    # Advanced settings
    with st.expander("Advanced Settings"):
        st.write("### Unsupervised Model Parameters")
        
        contamination = st.slider(
            "Contamination (expected fraud rate)",
            min_value=0.001,
            max_value=0.5,
            value=0.01,
            step=0.001,
            help="Expected proportion of outliers in the data"
        )
        
        st.write("### Supervised Model Parameters")
        
        if 'fraud_flag' in st.session_state.processed_data.columns:
            test_size = st.slider(
                "Test Size",
                min_value=0.1,
                max_value=0.5,
                value=0.2,
                step=0.05,
                help="Proportion of data to use for testing"
            )
        else:
            st.info("No fraud_flag column found. Supervised learning will use synthetic labels.")
            test_size = 0.2
        
        st.write("### Rule Engine Parameters")
        
        rule_threshold = st.slider(
            "Rule Threshold",
            min_value=0.0,
            max_value=1.0,
            value=0.7,
            step=0.05,
            help="Threshold for rule-based detection"
        )
    
    # Risk scoring settings
    st.subheader("Risk Scoring")
    
    # Map the internal values to display names
    method_display_names = {
        "weighted_average": "Weighted Average",
        "maximum": "Maximum Score", 
        "custom": "Custom Weights"
    }
    
    scoring_method = st.selectbox(
        "Scoring Method",
        list(method_display_names.keys()),
        format_func=lambda x: method_display_names[x],
        help="Method to combine scores from different models"
    )
    
    if scoring_method == "custom":
        st.write("### Custom Weights")
        unsupervised_weight = st.slider("Unsupervised Weight", 0.0, 1.0, 0.4, 0.05)
        supervised_weight = st.slider("Supervised Weight", 0.0, 1.0, 0.4, 0.05)
        rule_weight = st.slider("Rule Weight", 0.0, 1.0, 0.2, 0.05)
        
        # Normalize weights
        total = unsupervised_weight + supervised_weight + rule_weight
        if total > 0:
            unsupervised_weight /= total
            supervised_weight /= total
            rule_weight /= total
        
        custom_weights = {
            "unsupervised": unsupervised_weight,
            "supervised": supervised_weight,
            "rule": rule_weight
        }
    else:
        custom_weights = None
    
    # Threshold settings
    st.subheader("Detection Thresholds")
    
    threshold_method = st.selectbox(
        "Threshold Method",
        ["Fixed", "Percentile", "Dynamic"],
        help="Method to determine fraud threshold"
    )
    
    if threshold_method == "Fixed":
        threshold = st.slider(
            "Risk Threshold",
            min_value=0.0,
            max_value=1.0,
            value=0.5,
            step=0.05,
            help="Transactions above this threshold will be flagged as potential fraud"
        )
    elif threshold_method == "Percentile":
        percentile = st.slider(
            "Percentile",
            min_value=90,
            max_value=100,
            value=95,
            step=1,
            help="Transactions above this percentile of risk scores will be flagged"
        )
        threshold = None
    else:  # Dynamic
        st.info("Dynamic thresholds will be calculated based on data distribution")
        threshold = None
    
    # Save settings
    if st.button("Save Settings"):
        settings = {
            "features": selected_features,
            "models": selected_models,
            "contamination": contamination,
            "test_size": test_size,
            "rule_threshold": rule_threshold,
            "scoring_method": scoring_method,
            "custom_weights": custom_weights,
            "threshold_method": threshold_method,
            "threshold": threshold,
            "percentile": percentile if threshold_method == "Percentile" else None
        }
        
        st.session_state.settings = settings
        st.success("Settings saved! You can now run the detection analysis.")
# Clean dataframe function
def clean_dataframe(df):
    """
    Clean a DataFrame by handling infinity, NaN, and extreme values
    
    Args:
        df (DataFrame): Input DataFrame
        
    Returns:
        DataFrame: Cleaned DataFrame
    """
    try:
        # Replace infinity with NaN
        df = df.replace([np.inf, -np.inf], np.nan)
        
        # Handle extreme values for numeric columns
        for col in df.select_dtypes(include=[np.number]).columns:
            # Calculate percentiles
            p1 = np.nanpercentile(df[col], 1)
            p99 = np.nanpercentile(df[col], 99)
            
            # Cap extreme values
            if not np.isnan(p1) and not np.isnan(p99):
                # Use a more conservative capping approach
                iqr = p99 - p1
                lower_bound = p1 - 1.5 * iqr
                upper_bound = p99 + 1.5 * iqr
                
                df[col] = np.where(df[col] < lower_bound, lower_bound, df[col])
                df[col] = np.where(df[col] > upper_bound, upper_bound, df[col])
            
            # Replace any remaining NaN with median
            median_val = df[col].median()
            if not np.isnan(median_val):
                df[col] = df[col].fillna(median_val)
        
        # Handle categorical columns
        for col in df.select_dtypes(include=['object', 'category']).columns:
            # Fill NaN with mode or 'Unknown'
            mode_val = df[col].mode()
            if len(mode_val) > 0:
                df[col] = df[col].fillna(mode_val[0])
            else:
                df[col] = df[col].fillna('Unknown')
        
        return df
    except Exception as e:
        logger.error(f"Error cleaning DataFrame: {str(e)}")
        return df
# Run Detection Page
def render_run_detection():
    """Render the run detection page"""
    st.title("üöÄ Run Fraud Detection")
    
    if st.session_state.processed_data is None:
        st.warning("Please process data first")
        return
    
    if 'settings' not in st.session_state:
        st.warning("Please configure analysis settings first")
        return
    
    st.markdown("""
    Run the fraud detection analysis using the configured settings. This may take some time
    depending on the size of your dataset and selected algorithms.
    """)
    
    # Show settings summary
    st.subheader("Analysis Settings Summary")
    
    settings = st.session_state.settings
    
    st.write("#### Feature Engineering")
    for feature, enabled in settings["features"].items():
        status = "‚úÖ Enabled" if enabled else "‚ùå Disabled"
        st.write(f"- {feature}: {status}")
    
    st.write("#### Models")
    for model, enabled in settings["models"].items():
        status = "‚úÖ Enabled" if enabled else "‚ùå Disabled"
        st.write(f"- {model}: {status}")
    
    st.write("#### Risk Scoring")
    method_display_names = {
        "weighted_average": "Weighted Average",
        "maximum": "Maximum Score", 
        "custom": "Custom Weights"
    }
    st.write(f"- Method: {method_display_names.get(settings['scoring_method'], settings['scoring_method'])}")
    if settings['custom_weights']:
        st.write(f"- Unsupervised Weight: {settings['custom_weights']['unsupervised']:.2f}")
        st.write(f"- Supervised Weight: {settings['custom_weights']['supervised']:.2f}")
        st.write(f"- Rule Weight: {settings['custom_weights']['rule']:.2f}")
    
    st.write("#### Threshold")
    st.write(f"- Method: {settings['threshold_method']}")
    if settings['threshold_method'] == 'Fixed':
        st.write(f"- Value: {settings['threshold']}")
    elif settings['threshold_method'] == 'Percentile':
        st.write(f"- Percentile: {settings['percentile']}%")
    
    # Run button
    if st.button("Run Detection Analysis"):
        # Create progress bar
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        try:
            # Step 1: Feature Engineering
            status_text.text("Step 1/5: Feature Engineering...")
            progress_bar.progress(10)
            
            features_df = st.session_state.processed_data.copy()
            
            # Clean data before feature engineering
            features_df = clean_dataframe(features_df)
            
            # Statistical features
            if settings["features"]["Statistical Features"]:
                stat_features = StatisticalFeatures()
                features_df = stat_features.extract_features(features_df)
                # Clean after feature extraction
                features_df = clean_dataframe(features_df)
                progress_bar.progress(20)
            
            # Graph features
            if settings["features"]["Graph Features"]:
                graph_features = GraphFeatures()
                features_df = graph_features.extract_features(features_df)
                # Clean after feature extraction
                features_df = clean_dataframe(features_df)
                progress_bar.progress(30)
            
            # NLP features
            if settings["features"]["NLP Features"]:
                nlp_features = NLPFeatures()
                features_df = nlp_features.extract_features(features_df)
                # Clean after feature extraction
                features_df = clean_dataframe(features_df)
                progress_bar.progress(40)
            
            # Time series features
            if settings["features"]["Time Series Features"]:
                ts_features = TimeSeriesFeatures()
                features_df = ts_features.extract_features(features_df)
                # Clean after feature extraction
                features_df = clean_dataframe(features_df)
                progress_bar.progress(50)
            
            # Step 2: Model Training/Prediction
            status_text.text("Step 2/5: Running Models...")
            model_results = {}
            
            # Unsupervised models
            if settings["models"]["Unsupervised Models"]:
                unsupervised = UnsupervisedModels(contamination=settings["contamination"])
                unsupervised_results = unsupervised.run_models(features_df)
                model_results["unsupervised"] = unsupervised_results
                progress_bar.progress(60)
            
            # Supervised models
            if settings["models"]["Supervised Models"]:
                supervised = SupervisedModels(test_size=settings["test_size"])
                supervised_results = supervised.run_models(features_df)
                model_results["supervised"] = supervised_results
                progress_bar.progress(70)
            
            # Rule-based models
            if settings["models"]["Rule-Based Models"]:
                rule_engine = RuleEngine(threshold=settings["rule_threshold"])
                rule_results = rule_engine.apply_rules(features_df)
                model_results["rule"] = rule_results
                progress_bar.progress(80)
            
            # Step 3: Risk Scoring
            status_text.text("Step 3/5: Calculating Risk Scores...")
            
            # Initialize risk scorer with properly formatted method
            scoring_method = settings["scoring_method"]
            risk_scorer = RiskScorer(
                method=scoring_method,
                custom_weights=settings["custom_weights"]
            )
            
            risk_scores = risk_scorer.calculate_scores(model_results, features_df)
            progress_bar.progress(90)
            
            # Step 4: Apply Thresholds
            status_text.text("Step 4/5: Applying Thresholds...")
            if settings['threshold_method'] == "Fixed":
                threshold = settings['threshold']
                risk_scores["is_fraud"] = risk_scores["risk_score"] > threshold
            elif settings['threshold_method'] == "Percentile":
                percentile = settings['percentile']
                threshold = np.percentile(risk_scores["risk_score"], percentile)
                risk_scores["is_fraud"] = risk_scores["risk_score"] > threshold
            else:  # Dynamic
                # Calculate dynamic threshold based on distribution
                mean_score = risk_scores["risk_score"].mean()
                std_score = risk_scores["risk_score"].std()
                threshold = mean_score + 2 * std_score
                risk_scores["is_fraud"] = risk_scores["risk_score"] > threshold
            
            progress_bar.progress(95)
            
            # Step 5: Generate Explanations
            status_text.text("Step 5/5: Generating Explanations...")
            explainability = Explainability()
            explanations = explainability.generate_explanations(
                features_df, 
                risk_scores, 
                model_results
            )
            
            progress_bar.progress(100)
            
            # Store results in session state
            st.session_state.features_df = features_df
            st.session_state.model_results = model_results
            st.session_state.risk_scores = risk_scores
            st.session_state.explanations = explanations
            st.session_state.threshold = threshold
            st.session_state.processing_complete = True
            
            status_text.text("Analysis complete!")
            st.success(f"Fraud detection analysis complete! Found {risk_scores['is_fraud'].sum()} potentially fraudulent transactions out of {len(risk_scores)} total.")
            
        except Exception as e:
            st.error(f"Error during analysis: {str(e)}")
            st.exception(e)
# Results Dashboard Page
def render_results_dashboard():
    """Render the results dashboard page"""
    st.title("üìä Results Dashboard")
    
    if not st.session_state.processing_complete:
        st.warning("Please run the detection analysis first")
        return
    
    risk_scores = st.session_state.risk_scores
    
    # Summary statistics
    st.subheader("Summary Statistics")
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("Total Transactions", len(risk_scores))
    
    with col2:
        fraud_count = risk_scores['is_fraud'].sum()
        st.metric("Fraudulent Transactions", fraud_count)
    
    with col3:
        fraud_percentage = (fraud_count / len(risk_scores)) * 100
        st.metric("Fraud Percentage", f"{fraud_percentage:.2f}%")
    
    with col4:
        threshold = st.session_state.threshold
        st.metric("Detection Threshold", f"{threshold:.4f}")
    
    # Risk score distribution
    st.subheader("Risk Score Distribution")
    
    fig = px.histogram(
        risk_scores,
        x="risk_score",
        color="is_fraud",
        nbins=50,
        title="Distribution of Risk Scores",
        color_discrete_map={False: "blue", True: "red"},
        opacity=0.7
    )
    
    fig.add_vline(
        x=threshold,
        line_dash="dash",
        line_color="green",
        annotation_text=f"Threshold: {threshold:.4f}"
    )
    
    st.plotly_chart(fig, use_container_width=True)
    
    # Top risky transactions
    st.subheader("Top Risky Transactions")
    
    top_risky = risk_scores[risk_scores['is_fraud']].nlargest(10, 'risk_score')
    
    if len(top_risky) > 0:
        # Create a display dataframe with risk scores and available details
        display_data = top_risky.copy()
        
        # Try to add transaction details from original data if available
        if hasattr(st.session_state, 'original_data') and st.session_state.original_data is not None:
            original_data = st.session_state.original_data
            
            # Get columns that exist in both dataframes
            common_cols = set(original_data.columns) & set(display_data.columns)
            
            # If there are common columns, merge them
            if common_cols:
                display_data = display_data.drop(columns=list(common_cols), errors='ignore')
                display_data = display_data.merge(
                    original_data[list(common_cols)],
                    left_index=True,
                    right_index=True,
                    how='left'
                )
        
        # If we still don't have transaction_id, try to get it from features_df
        if 'transaction_id' not in display_data.columns and hasattr(st.session_state, 'features_df'):
            features_df = st.session_state.features_df
            if 'transaction_id' in features_df.columns:
                display_data = display_data.merge(
                    features_df[['transaction_id']],
                    left_index=True,
                    right_index=True,
                    how='left'
                )
        
        # If we still don't have transaction_id, use the index as a fallback
        if 'transaction_id' not in display_data.columns:
            display_data['transaction_id'] = display_data.index
        
        # Ensure we have the required columns for display
        display_cols = []
        
        # Always include transaction_id and risk_score
        if 'transaction_id' in display_data.columns:
            display_cols.append('transaction_id')
        display_cols.append('risk_score')
        
        # Add other available columns if they exist
        for col in ['amount', 'timestamp', 'sender_id', 'receiver_id']:
            if col in display_data.columns:
                display_cols.append(col)
        
        # Display the dataframe
        st.dataframe(
            display_data[display_cols].sort_values('risk_score', ascending=False)
        )
    else:
        st.info("No fraudulent transactions detected")
    
    # Feature importance
    st.subheader("Feature Importance")
    
    if 'supervised' in st.session_state.model_results and 'feature_importance' in st.session_state.model_results['supervised']:
        feature_importance = st.session_state.model_results['supervised']['feature_importance']
        
        # Get top 20 features
        top_features = feature_importance.head(20)
        
        fig = px.bar(
            top_features,
            x='importance',
            y='feature',
            orientation='h',
            title="Top 20 Feature Importance",
            color='importance',
            color_continuous_scale='Viridis'
        )
        
        fig.update_layout(yaxis={'categoryorder': 'total ascending'})
        st.plotly_chart(fig, use_container_width=True)
    else:
        st.info("Feature importance not available. Run supervised models to see feature importance.")
    
    # Model performance
    st.subheader("Model Performance")
    
    if 'supervised' in st.session_state.model_results and 'performance' in st.session_state.model_results['supervised']:
        performance = st.session_state.model_results['supervised']['performance']
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.write("##### Classification Report")
            st.dataframe(performance['classification_report'])
        
        with col2:
            st.write("##### Confusion Matrix")
            cm = performance['confusion_matrix']
            
            fig = px.imshow(
                cm,
                text_auto=True,
                aspect="auto",
                labels=dict(x="Predicted", y="Actual", color="Count"),
                x=["Not Fraud", "Fraud"],
                y=["Not Fraud", "Fraud"],
                color_continuous_scale='Blues'
            )
            
            st.plotly_chart(fig, use_container_width=True)
    else:
        st.info("Model performance metrics not available. Run supervised models with labeled data to see performance metrics.")
    
    # Time series analysis
    if hasattr(st.session_state, 'original_data') and st.session_state.original_data is not None and 'timestamp' in st.session_state.original_data.columns:
        st.subheader("Fraud Over Time")
        
        # Create a copy for time series analysis
        ts_data = st.session_state.original_data.copy()
        ts_data['risk_score'] = risk_scores['risk_score']
        ts_data['is_fraud'] = risk_scores['is_fraud']
        
        # Convert timestamp to datetime if needed
        if not pd.api.types.is_datetime64_any_dtype(ts_data['timestamp']):
            ts_data['timestamp'] = pd.to_datetime(ts_data['timestamp'])
        
        # Extract date components
        ts_data['date'] = ts_data['timestamp'].dt.date
        ts_data['hour'] = ts_data['timestamp'].dt.hour
        
        # Fraud by date
        fraud_by_date = ts_data.groupby('date').agg({
            'is_fraud': ['sum', 'count'],
            'risk_score': 'mean'
        }).reset_index()
        
        fraud_by_date.columns = ['date', 'fraud_count', 'total_count', 'avg_risk_score']
        fraud_by_date['fraud_rate'] = fraud_by_date['fraud_count'] / fraud_by_date['total_count']
        
        fig = px.line(
            fraud_by_date,
            x='date',
            y=['fraud_count', 'fraud_rate'],
            title="Fraud Count and Rate Over Time",
            labels={'value': 'Count / Rate', 'date': 'Date'}
        )
        
        st.plotly_chart(fig, use_container_width=True)
        
        # Fraud by hour of day
        fraud_by_hour = ts_data.groupby('hour').agg({
            'is_fraud': ['sum', 'count'],
            'risk_score': 'mean'
        }).reset_index()
        
        fraud_by_hour.columns = ['hour', 'fraud_count', 'total_count', 'avg_risk_score']
        fraud_by_hour['fraud_rate'] = fraud_by_hour['fraud_count'] / fraud_by_hour['total_count']
        
        fig = px.bar(
            fraud_by_hour,
            x='hour',
            y='fraud_count',
            title="Fraud Count by Hour of Day",
            color='avg_risk_score',
            color_continuous_scale='Viridis'
        )
        
        st.plotly_chart(fig, use_container_width=True)
    elif hasattr(st.session_state, 'features_df') and st.session_state.features_df is not None and 'timestamp' in st.session_state.features_df.columns:
        st.subheader("Fraud Over Time")
        
        # Create a copy for time series analysis
        ts_data = st.session_state.features_df.copy()
        ts_data['risk_score'] = risk_scores['risk_score']
        ts_data['is_fraud'] = risk_scores['is_fraud']
        
        # Convert timestamp to datetime if needed
        if not pd.api.types.is_datetime64_any_dtype(ts_data['timestamp']):
            ts_data['timestamp'] = pd.to_datetime(ts_data['timestamp'])
        
        # Extract date components
        ts_data['date'] = ts_data['timestamp'].dt.date
        ts_data['hour'] = ts_data['timestamp'].dt.hour
        
        # Fraud by date
        fraud_by_date = ts_data.groupby('date').agg({
            'is_fraud': ['sum', 'count'],
            'risk_score': 'mean'
        }).reset_index()
        
        fraud_by_date.columns = ['date', 'fraud_count', 'total_count', 'avg_risk_score']
        fraud_by_date['fraud_rate'] = fraud_by_date['fraud_count'] / fraud_by_date['total_count']
        
        fig = px.line(
            fraud_by_date,
            x='date',
            y=['fraud_count', 'fraud_rate'],
            title="Fraud Count and Rate Over Time",
            labels={'value': 'Count / Rate', 'date': 'Date'}
        )
        
        st.plotly_chart(fig, use_container_width=True)
        
        # Fraud by hour of day
        fraud_by_hour = ts_data.groupby('hour').agg({
            'is_fraud': ['sum', 'count'],
            'risk_score': 'mean'
        }).reset_index()
        
        fraud_by_hour.columns = ['hour', 'fraud_count', 'total_count', 'avg_risk_score']
        fraud_by_hour['fraud_rate'] = fraud_by_hour['fraud_count'] / fraud_by_hour['total_count']
        
        fig = px.bar(
            fraud_by_hour,
            x='hour',
            y='fraud_count',
            title="Fraud Count by Hour of Day",
            color='avg_risk_score',
            color_continuous_scale='Viridis'
        )
        
        st.plotly_chart(fig, use_container_width=True)
    else:
        st.info("Timestamp data not available for time series analysis")
# Explainability Page
def render_explainability():
    """Render the explainability page"""
    st.title("üîç Explainability")
    
    if not st.session_state.processing_complete:
        st.warning("Please run the detection analysis first")
        return
    
    risk_scores = st.session_state.risk_scores
    explanations = st.session_state.explanations
    
    # Transaction selector
    st.subheader("Transaction Analysis")
    
    # Get fraudulent transactions
    fraud_transactions = risk_scores[risk_scores['is_fraud']]
    
    if len(fraud_transactions) == 0:
        st.info("No fraudulent transactions detected")
        return
    
    # Create a selector for transaction
    # Check if we have transaction_id in features_df
    if hasattr(st.session_state, 'features_df') and st.session_state.features_df is not None and 'transaction_id' in st.session_state.features_df.columns:
        # Merge with features to get transaction IDs
        fraud_with_ids = fraud_transactions.copy()
        fraud_with_ids = fraud_with_ids.merge(
            st.session_state.features_df[['transaction_id']],
            left_index=True,
            right_index=True,
            how='left'
        )
        
        # Create transaction options
        transaction_options = []
        for idx in fraud_with_ids.index:
            # Check if transaction_id exists and is not NaN
            if 'transaction_id' in fraud_with_ids.columns and pd.notna(fraud_with_ids.loc[idx, 'transaction_id']):
                transaction_id = fraud_with_ids.loc[idx, 'transaction_id']
                transaction_options.append(f"{transaction_id} (Index: {idx})")
            else:
                transaction_options.append(f"Index: {idx}")
        
        selected_transaction = st.selectbox("Select a transaction to analyze", transaction_options)
        
        # Parse the selected option to get the index
        if "Index:" in selected_transaction:
            selected_index = int(selected_transaction.split("Index: ")[1])
        else:
            # Extract transaction_id from the selected option
            transaction_id = selected_transaction.split(" (Index: ")[0]
            # Find the index for this transaction_id
            selected_index = fraud_with_ids[fraud_with_ids['transaction_id'] == transaction_id].index[0]
    else:
        # Use index as identifier
        transaction_options = [f"Index: {idx}" for idx in fraud_transactions.index]
        selected_transaction = st.selectbox("Select a transaction to analyze", transaction_options)
        selected_index = int(selected_transaction.split("Index: ")[1])
    
    # Display transaction details
    st.write("### Transaction Details")
    
    # Try to get transaction details from features_df
    if hasattr(st.session_state, 'features_df') and st.session_state.features_df is not None:
        try:
            transaction_data = st.session_state.features_df.loc[selected_index]
            
            # Display key fields
            key_fields = ['transaction_id', 'amount', 'timestamp', 'sender_id', 'receiver_id', 
                         'transaction_type', 'transaction_category', 'description']
            
            for field in key_fields:
                if field in transaction_data:
                    st.write(f"**{field.replace('_', ' ').title()}:** {transaction_data[field]}")
        except Exception as e:
            st.warning(f"Could not retrieve transaction details: {str(e)}")
    
    # Risk score
    risk_score = risk_scores.loc[selected_index, 'risk_score']
    st.metric("Risk Score", f"{risk_score:.4f}")
    
    # Explanation
    st.write("### Fraud Explanation")
    
    if selected_index in explanations:
        explanation = explanations[selected_index]
        
        # Show top contributing factors
        if 'top_factors' in explanation:
            st.write("#### Top Contributing Factors")
            
            factors_df = pd.DataFrame(explanation['top_factors'])
            factors_df.columns = ['Feature', 'Contribution']
            
            # Create a bar chart
            fig = px.bar(
                factors_df,
                x='Contribution',
                y='Feature',
                orientation='h',
                title="Feature Contributions to Risk Score",
                color='Contribution',
                color_continuous_scale='RdBu'
            )
            
            fig.update_layout(yaxis={'categoryorder': 'total ascending'})
            st.plotly_chart(fig, use_container_width=True)
        
        # Show rule violations
        if 'rule_violations' in explanation and len(explanation['rule_violations']) > 0:
            st.write("#### Rule Violations")
            
            for rule in explanation['rule_violations']:
                st.write(f"- {rule}")
        
        # Show model predictions
        if 'model_predictions' in explanation:
            st.write("#### Model Predictions")
            
            model_df = pd.DataFrame([
                {'Model': model, 'Score': score} 
                for model, score in explanation['model_predictions'].items()
            ])
            
            st.dataframe(model_df)
        
        # Show natural language explanation
        if 'text_explanation' in explanation:
            st.write("#### Explanation Summary")
            st.write(explanation['text_explanation'])
    else:
        st.info("No explanation available for this transaction")
    
    # What-if analysis
    st.write("### What-If Analysis")
    
    st.write("Adjust feature values to see how they affect the risk score:")
    
    # Get top features for this transaction
    if selected_index in explanations and 'top_factors' in explanations[selected_index]:
        top_features = [factor[0] for factor in explanations[selected_index]['top_factors'][:5]]
        
        # Create sliders for top features
        adjustments = {}
        for feature in top_features:
            if feature in st.session_state.features_df.columns:
                current_value = st.session_state.features_df.loc[selected_index, feature]
                
                # Determine appropriate range
                min_val = st.session_state.features_df[feature].min()
                max_val = st.session_state.features_df[feature].max()
                
                # Handle categorical features
                if st.session_state.features_df[feature].dtype == 'object':
                    options = st.session_state.features_df[feature].unique().tolist()
                    adjusted_value = st.selectbox(
                        f"Adjust {feature}",
                        options=options,
                        index=options.index(current_value) if current_value in options else 0
                    )
                else:
                    adjusted_value = st.slider(
                        f"Adjust {feature}",
                        min_value=float(min_val),
                        max_value=float(max_val),
                        value=float(current_value)
                    )
                
                adjustments[feature] = adjusted_value
        
        # Recalculate risk score button
        if st.button("Recalculate Risk Score"):
            # Create a copy of the original data
            modified_data = st.session_state.features_df.copy().loc[[selected_index]]
            
            # Apply adjustments
            for feature, value in adjustments.items():
                modified_data.loc[selected_index, feature] = value
            
            # Recalculate features
            modified_features = modified_data.copy()
            
            # Statistical features
            if st.session_state.settings["features"]["Statistical Features"]:
                stat_features = StatisticalFeatures()
                modified_features = stat_features.extract_features(modified_features)
            
            # Graph features
            if st.session_state.settings["features"]["Graph Features"]:
                graph_features = GraphFeatures()
                modified_features = graph_features.extract_features(modified_features)
            
            # NLP features
            if st.session_state.settings["features"]["NLP Features"]:
                nlp_features = NLPFeatures()
                modified_features = nlp_features.extract_features(modified_features)
            
            # Time series features
            if st.session_state.settings["features"]["Time Series Features"]:
                ts_features = TimeSeriesFeatures()
                modified_features = ts_features.extract_features(modified_features)
            
            # Get model predictions
            model_predictions = {}
            
            # Unsupervised models
            if 'unsupervised' in st.session_state.model_results:
                for model_name, model in st.session_state.model_results['unsupervised']['models'].items():
                    try:
                        # Get features used by this model
                        if 'feature_names' in st.session_state.model_results['unsupervised']:
                            feature_names = st.session_state.model_results['unsupervised']['feature_names']
                            model_features = modified_features[feature_names[model_name]]
                            prediction = model.decision_function(model_features)[0]
                            model_predictions[f"unsupervised_{model_name}"] = prediction
                    except Exception as e:
                        st.warning(f"Error predicting with {model_name}: {str(e)}")
            
            # Supervised models
            if 'supervised' in st.session_state.model_results:
                for model_name, model in st.session_state.model_results['supervised']['models'].items():
                    try:
                        # Get features used by this model
                        if 'feature_names' in st.session_state.model_results['supervised']:
                            feature_names = st.session_state.model_results['supervised']['feature_names']
                            model_features = modified_features[feature_names[model_name]]
                            prediction = model.predict_proba(model_features)[0, 1]
                            model_predictions[f"supervised_{model_name}"] = prediction
                    except Exception as e:
                        st.warning(f"Error predicting with {model_name}: {str(e)}")
            
            # Rule-based models
            if 'rule' in st.session_state.model_results:
                rule_score = 0.0
                violated_rules = []
                
                for rule_name, rule_func in st.session_state.model_results['rule']['rules'].items():
                    try:
                        if rule_func(modified_data.iloc[0]):
                            rule_score += 1.0 / len(st.session_state.model_results['rule']['rules'])
                            violated_rules.append(rule_name)
                    except Exception as e:
                        st.warning(f"Error applying rule {rule_name}: {str(e)}")
                
                model_predictions["rule"] = rule_score
            
            # Calculate new risk score
            risk_scorer = RiskScorer(
                method=st.session_state.settings["scoring_method"],
                custom_weights=st.session_state.settings["custom_weights"]
            )
            
            new_risk_score = risk_scorer.calculate_scores(
                {"unsupervised": model_predictions, "supervised": model_predictions, "rule": model_predictions},
                modified_features
            )
            
            new_risk_score = new_risk_score.loc[selected_index, 'risk_score']
            
            # Display comparison
            st.write("#### Risk Score Comparison")
            
            col1, col2 = st.columns(2)
            
            with col1:
                st.metric("Original Risk Score", f"{risk_score:.4f}")
            
            with col2:
                st.metric("New Risk Score", f"{new_risk_score:.4f}", 
                         delta=f"{new_risk_score - risk_score:.4f}")
            
            # Show if it would still be flagged as fraud
            threshold = st.session_state.threshold
            is_fraud_original = risk_score > threshold
            is_fraud_new = new_risk_score > threshold
            
            st.write(f"Original classification: {'Fraud' if is_fraud_original else 'Not Fraud'}")
            st.write(f"New classification: {'Fraud' if is_fraud_new else 'Not Fraud'}")
            
            if is_fraud_original != is_fraud_new:
                st.warning("Classification changed with the adjustments!")
            else:
                st.info("Classification remains the same")
# Reports Page
def render_reports():
    """Render the reports page"""
    st.title("üìÑ Reports")
    
    if not st.session_state.processing_complete:
        st.warning("Please run the detection analysis first")
        return
    
    risk_scores = st.session_state.risk_scores
    features_df = st.session_state.features_df
    
    st.markdown("""
    Generate comprehensive reports for auditors and stakeholders. Reports can be downloaded
    in PDF format and include detailed analysis of fraudulent transactions.
    """)
    
    # Report options
    st.subheader("Report Options")
    
    report_type = st.selectbox(
        "Select Report Type",
        ["Executive Summary", "Detailed Fraud Analysis", "Technical Report", "Custom Report"]
    )
    
    include_charts = st.checkbox("Include Charts and Visualizations", value=True)
    include_explanations = st.checkbox("Include Detailed Explanations", value=True)
    include_recommendations = st.checkbox("Include Recommendations", value=True)
    
    # Transaction selection
    st.subheader("Transaction Selection")
    
    fraud_transactions = risk_scores[risk_scores['is_fraud']]
    
    if len(fraud_transactions) > 0:
        max_transactions = min(len(fraud_transactions), 100)  # Limit to 100 for performance
        num_transactions = st.slider(
            "Number of Fraudulent Transactions to Include",
            min_value=1,
            max_value=max_transactions,
            value=min(10, max_transactions),
            step=1
        )
        
        # Sort by risk score and select top N
        top_fraud = fraud_transactions.nlargest(num_transactions, 'risk_score')
    else:
        st.info("No fraudulent transactions detected")
        return
    
    # Report generation options
    st.subheader("Report Generation")
    
    if st.button("Generate Report"):
        with st.spinner("Generating report..."):
            try:
                # Initialize PDF generator
                pdf_generator = PDFGenerator()
                
                # Generate report based on type
                if report_type == "Executive Summary":
                    report_path = pdf_generator.generate_executive_summary(
                        features_df,
                        risk_scores,
                        top_fraud,
                        include_charts=include_charts,
                        include_recommendations=include_recommendations
                    )
                elif report_type == "Detailed Fraud Analysis":
                    report_path = pdf_generator.generate_detailed_fraud_analysis(
                        features_df,
                        risk_scores,
                        top_fraud,
                        st.session_state.explanations,
                        include_charts=include_charts,
                        include_explanations=include_explanations,
                        include_recommendations=include_recommendations
                    )
                elif report_type == "Technical Report":
                    report_path = pdf_generator.generate_technical_report(
                        features_df,
                        risk_scores,
                        st.session_state.model_results,
                        include_charts=include_charts
                    )
                else:  # Custom Report
                    report_path = pdf_generator.generate_custom_report(
                        features_df,
                        risk_scores,
                        top_fraud,
                        st.session_state.explanations,
                        st.session_state.model_results,
                        include_charts=include_charts,
                        include_explanations=include_explanations,
                        include_recommendations=include_recommendations
                    )
                
                # Display success message
                st.success(f"Report generated successfully!")
                
                # Provide download button
                with open(report_path, "rb") as file:
                    st.download_button(
                        label="Download Report",
                        data=file,
                        file_name=os.path.basename(report_path),
                        mime="application/pdf"
                    )
                
                # Show report preview
                st.subheader("Report Preview")
                st.info("Report preview is not available in this interface. Please download the PDF to view the full report.")
                
            except Exception as e:
                st.error(f"Error generating report: {str(e)}")
                st.exception(e)
# Main function
def main():
    """Main function to run the Streamlit app"""
    page = render_sidebar()
    
    if page == "Data Upload":
        render_data_upload()
    elif page == "Column Mapping":
        render_column_mapping()
    elif page == "Analysis Settings":
        render_analysis_settings()
    elif page == "Run Detection":
        render_run_detection()
    elif page == "Results Dashboard":
        render_results_dashboard()
    elif page == "Explainability":
        render_explainability()
    elif page == "Reports":
        render_reports()
if __name__ == "__main__":
    main()

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\config\api_keys.yml ===
# API keys for external services

# Gemini API for AI integration
gemini:
  api_key: "AIzaSyB_lLIZ_ZIE__qxMS-XwrqA6G5X6qY05W4"
  model: "gemini-pro"
  temperature: 0.2
  max_tokens: 1000
  top_p: 0.8
  top_k: 40

# OpenAI API (alternative)
openai:
  api_key: "sk-ijklmnop5678efghijklmnop5678efghijklmnop"
  model: "gpt-4"
  temperature: 0.2
  max_tokens: 1000

# News API for fraud pattern updates
news_api:
  api_key: "e42e914093885afa580f2084f00d06e7"
  sources: ["reuters", "bloomberg", "financial-times"]
  keywords: ["fraud", "money laundering", "financial crime", "cybersecurity"]
  lookback_days: 7

# Geolocation API
geolocation:
  api_key: "9520e3680de446a2b4dab44072e937d4"
  endpoint: "https://api.geolocation.example.com/v1"

# Sanctions list APIs - NOT AVAILABLE
sanctions:
  api_key: "NOT_AVAILABLE"
  update_frequency: "daily"

# Tax compliance database - NOT AVAILABLE
tax_compliance:
  api_key: "NOT_AVAILABLE"
  endpoint: "https://api.taxcompliance.example.com/v1"

# Bank verification API - NOT AVAILABLE
bank_verification:
  api_key: "NOT_AVAILABLE"
  endpoint: "https://api.bankverification.example.com/v1"

# Identity verification API - NOT AVAILABLE
identity_verification:
  api_key: "NOT_AVAILABLE"
  endpoint: "https://api.identityverification.example.com/v1"

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\config\model_params.yml ===
# Model parameters for fraud detection system

# Unsupervised model parameters
unsupervised:
  isolation_forest:
    n_estimators: 100
    max_samples: 'auto'
    contamination: 0.01
    max_features: 1.0
    bootstrap: False
    n_jobs: -1
    random_state: 42
    
  local_outlier_factor:
    n_neighbors: 20
    algorithm: 'auto'
    leaf_size: 30
    metric: 'minkowski'
    p: 2
    metric_params: null
    contamination: 0.01
    n_jobs: -1
    
  autoencoder:
    encoding_dim: 10
    hidden_layers: [50, 25]
    activation: 'relu'
    optimizer: 'adam'
    loss: 'mean_squared_error'
    epochs: 100
    batch_size: 32
    validation_split: 0.1
    verbose: 0

# Supervised model parameters
supervised:
  random_forest:
    n_estimators: 100
    criterion: 'gini'
    max_depth: null
    min_samples_split: 2
    min_samples_leaf: 1
    min_weight_fraction_leaf: 0.0
    max_features: 'auto'
    max_leaf_nodes: null
    min_impurity_decrease: 0.0
    bootstrap: True
    oob_score: False
    n_jobs: -1
    random_state: 42
    class_weight: 'balanced'
    
  xgboost:
    n_estimators: 100
    max_depth: 6
    learning_rate: 0.1
    subsample: 1.0
    colsample_bytree: 1.0
    gamma: 0
    reg_alpha: 0
    reg_lambda: 1
    random_state: 42
    scale_pos_weight: 1
    
  logistic_regression:
    penalty: 'l2'
    C: 1.0
    fit_intercept: True
    intercept_scaling: 1
    class_weight: 'balanced'
    random_state: 42
    solver: 'lbfgs'
    max_iter: 1000
    multi_class: 'auto'
    
  svm:
    C: 1.0
    kernel: 'rbf'
    degree: 3
    gamma: 'scale'
    coef0: 0.0
    shrinking: True
    probability: True
    tol: 0.001
    cache_size: 200
    class_weight: 'balanced'
    random_state: 42

# Neural network parameters
neural_network:
  hidden_layers: [128, 64, 32]
  activation: 'relu'
  optimizer: 'adam'
  loss: 'binary_crossentropy'
  metrics: ['accuracy', 'precision', 'recall']
  epochs: 100
  batch_size: 32
  validation_split: 0.2
  verbose: 1
  dropout_rate: 0.2
  early_stopping_patience: 10

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\config\rule_engine_config.yml ===
# Rule engine configuration for fraud detection

# Rule categories
categories:
  amount_rules:
    enabled: true
    weight: 0.3
    
  frequency_rules:
    enabled: true
    weight: 0.2
    
  location_rules:
    enabled: true
    weight: 0.2
    
  time_rules:
    enabled: true
    weight: 0.15
    
  identity_rules:
    enabled: true
    weight: 0.15

# Amount-based rules
amount_rules:
  high_amount:
    enabled: true
    threshold: 10000
    description: "Transaction amount exceeds threshold"
    
  unusual_amount_for_sender:
    enabled: true
    std_multiplier: 3
    min_transactions: 5
    description: "Amount is unusual for the sender"
    
  unusual_amount_for_receiver:
    enabled: true
    std_multiplier: 3
    min_transactions: 5
    description: "Amount is unusual for the receiver"
    
  round_amount:
    enabled: true
    threshold: 1000
    description: "Transaction amount is suspiciously round"
    
  amount_multiple:
    enabled: true
    multiple: 1000
    description: "Transaction amount is a multiple of a suspicious value"

# Frequency-based rules
frequency_rules:
  high_frequency_sender:
    enabled: true
    time_window: "1H"
    max_transactions: 10
    description: "High transaction frequency from sender"
    
  high_frequency_receiver:
    enabled: true
    time_window: "1H"
    max_transactions: 10
    description: "High transaction frequency to receiver"
    
  rapid_succession:
    enabled: true
    time_window: "5M"
    min_transactions: 3
    description: "Multiple transactions in rapid succession"
    
  burst_pattern:
    enabled: true
    time_window: "1H"
    threshold_multiplier: 3
    description: "Burst of transactions compared to normal pattern"

# Location-based rules
location_rules:
  cross_border:
    enabled: true
    description: "Transaction crosses international borders"
    
  high_risk_country:
    enabled: true
    countries: ["North Korea", "Iran", "Syria", "Cuba"]
    description: "Transaction involves high-risk country"
    
  unusual_location_for_sender:
    enabled: true
    description: "Transaction from unusual location for sender"
    
  unusual_location_for_receiver:
    enabled: true
    description: "Transaction to unusual location for receiver"
    
  ip_mismatch:
    enabled: true
    description: "IP location does not match sender location"

# Time-based rules
time_rules:
  unusual_hour:
    enabled: true
    start_hour: 23
    end_hour: 5
    description: "Transaction during unusual hours"
    
  weekend:
    enabled: true
    description: "Transaction on weekend"
    
  holiday:
    enabled: true
    description: "Transaction on public holiday"
    
  rapid_time_pattern:
    enabled: true
    time_window: "1D"
    min_transactions: 5
    description: "Unusual pattern of transaction timing"

# Identity-based rules
identity_rules:
  new_sender:
    enabled: true
    time_window: "7D"
    description: "First transaction from new sender"
    
  new_receiver:
    enabled: true
    time_window: "7D"
    description: "First transaction to new receiver"
    
  high_risk_entity:
    enabled: true
    description: "Transaction involves high-risk entity"
    
  sanctioned_entity:
    enabled: true
    description: "Transaction involves sanctioned entity"
    
  peer_to_peer_pattern:
    enabled: true
    description: "Pattern suggesting peer-to-peer transfer scheme"

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\config\.ipynb_checkpoints\api_keys-checkpoint.yml ===
# API keys for external services

# Gemini API for AI integration
gemini:
  api_key: "AIzaSyB_lLIZ_ZIE__qxMS-XwrqA6G5X6qY05W4"
  model: "gemini-pro"
  temperature: 0.2
  max_tokens: 1000
  top_p: 0.8
  top_k: 40

# OpenAI API (alternative)
openai:
  api_key: "sk-ijklmnop5678efghijklmnop5678efghijklmnop"
  model: "gpt-4"
  temperature: 0.2
  max_tokens: 1000

# News API for fraud pattern updates
news_api:
  api_key: "e42e914093885afa580f2084f00d06e7"
  sources: ["reuters", "bloomberg", "financial-times"]
  keywords: ["fraud", "money laundering", "financial crime", "cybersecurity"]
  lookback_days: 7

# Geolocation API
geolocation:
  api_key: "9520e3680de446a2b4dab44072e937d4"
  endpoint: "https://api.geolocation.example.com/v1"

# Sanctions list APIs - NOT AVAILABLE
sanctions:
  api_key: "NOT_AVAILABLE"
  update_frequency: "daily"

# Tax compliance database - NOT AVAILABLE
tax_compliance:
  api_key: "NOT_AVAILABLE"
  endpoint: "https://api.taxcompliance.example.com/v1"

# Bank verification API - NOT AVAILABLE
bank_verification:
  api_key: "NOT_AVAILABLE"
  endpoint: "https://api.bankverification.example.com/v1"

# Identity verification API - NOT AVAILABLE
identity_verification:
  api_key: "NOT_AVAILABLE"
  endpoint: "https://api.identityverification.example.com/v1"

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\config\.ipynb_checkpoints\model_params-checkpoint.yml ===
# Model parameters for fraud detection system

# Unsupervised model parameters
unsupervised:
  isolation_forest:
    n_estimators: 100
    max_samples: 'auto'
    contamination: 0.01
    max_features: 1.0
    bootstrap: False
    n_jobs: -1
    random_state: 42
    
  local_outlier_factor:
    n_neighbors: 20
    algorithm: 'auto'
    leaf_size: 30
    metric: 'minkowski'
    p: 2
    metric_params: null
    contamination: 0.01
    n_jobs: -1
    
  autoencoder:
    encoding_dim: 10
    hidden_layers: [50, 25]
    activation: 'relu'
    optimizer: 'adam'
    loss: 'mean_squared_error'
    epochs: 100
    batch_size: 32
    validation_split: 0.1
    verbose: 0

# Supervised model parameters
supervised:
  random_forest:
    n_estimators: 100
    criterion: 'gini'
    max_depth: null
    min_samples_split: 2
    min_samples_leaf: 1
    min_weight_fraction_leaf: 0.0
    max_features: 'auto'
    max_leaf_nodes: null
    min_impurity_decrease: 0.0
    bootstrap: True
    oob_score: False
    n_jobs: -1
    random_state: 42
    class_weight: 'balanced'
    
  xgboost:
    n_estimators: 100
    max_depth: 6
    learning_rate: 0.1
    subsample: 1.0
    colsample_bytree: 1.0
    gamma: 0
    reg_alpha: 0
    reg_lambda: 1
    random_state: 42
    scale_pos_weight: 1
    
  logistic_regression:
    penalty: 'l2'
    C: 1.0
    fit_intercept: True
    intercept_scaling: 1
    class_weight: 'balanced'
    random_state: 42
    solver: 'lbfgs'
    max_iter: 1000
    multi_class: 'auto'
    
  svm:
    C: 1.0
    kernel: 'rbf'
    degree: 3
    gamma: 'scale'
    coef0: 0.0
    shrinking: True
    probability: True
    tol: 0.001
    cache_size: 200
    class_weight: 'balanced'
    random_state: 42

# Neural network parameters
neural_network:
  hidden_layers: [128, 64, 32]
  activation: 'relu'
  optimizer: 'adam'
  loss: 'binary_crossentropy'
  metrics: ['accuracy', 'precision', 'recall']
  epochs: 100
  batch_size: 32
  validation_split: 0.2
  verbose: 1
  dropout_rate: 0.2
  early_stopping_patience: 10

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\config\.ipynb_checkpoints\rule_engine_config-checkpoint.yml ===
# Rule engine configuration for fraud detection

# Rule categories
categories:
  amount_rules:
    enabled: true
    weight: 0.3
    
  frequency_rules:
    enabled: true
    weight: 0.2
    
  location_rules:
    enabled: true
    weight: 0.2
    
  time_rules:
    enabled: true
    weight: 0.15
    
  identity_rules:
    enabled: true
    weight: 0.15

# Amount-based rules
amount_rules:
  high_amount:
    enabled: true
    threshold: 10000
    description: "Transaction amount exceeds threshold"
    
  unusual_amount_for_sender:
    enabled: true
    std_multiplier: 3
    min_transactions: 5
    description: "Amount is unusual for the sender"
    
  unusual_amount_for_receiver:
    enabled: true
    std_multiplier: 3
    min_transactions: 5
    description: "Amount is unusual for the receiver"
    
  round_amount:
    enabled: true
    threshold: 1000
    description: "Transaction amount is suspiciously round"
    
  amount_multiple:
    enabled: true
    multiple: 1000
    description: "Transaction amount is a multiple of a suspicious value"

# Frequency-based rules
frequency_rules:
  high_frequency_sender:
    enabled: true
    time_window: "1H"
    max_transactions: 10
    description: "High transaction frequency from sender"
    
  high_frequency_receiver:
    enabled: true
    time_window: "1H"
    max_transactions: 10
    description: "High transaction frequency to receiver"
    
  rapid_succession:
    enabled: true
    time_window: "5M"
    min_transactions: 3
    description: "Multiple transactions in rapid succession"
    
  burst_pattern:
    enabled: true
    time_window: "1H"
    threshold_multiplier: 3
    description: "Burst of transactions compared to normal pattern"

# Location-based rules
location_rules:
  cross_border:
    enabled: true
    description: "Transaction crosses international borders"
    
  high_risk_country:
    enabled: true
    countries: ["North Korea", "Iran", "Syria", "Cuba"]
    description: "Transaction involves high-risk country"
    
  unusual_location_for_sender:
    enabled: true
    description: "Transaction from unusual location for sender"
    
  unusual_location_for_receiver:
    enabled: true
    description: "Transaction to unusual location for receiver"
    
  ip_mismatch:
    enabled: true
    description: "IP location does not match sender location"

# Time-based rules
time_rules:
  unusual_hour:
    enabled: true
    start_hour: 23
    end_hour: 5
    description: "Transaction during unusual hours"
    
  weekend:
    enabled: true
    description: "Transaction on weekend"
    
  holiday:
    enabled: true
    description: "Transaction on public holiday"
    
  rapid_time_pattern:
    enabled: true
    time_window: "1D"
    min_transactions: 5
    description: "Unusual pattern of transaction timing"

# Identity-based rules
identity_rules:
  new_sender:
    enabled: true
    time_window: "7D"
    description: "First transaction from new sender"
    
  new_receiver:
    enabled: true
    time_window: "7D"
    description: "First transaction to new receiver"
    
  high_risk_entity:
    enabled: true
    description: "Transaction involves high-risk entity"
    
  sanctioned_entity:
    enabled: true
    description: "Transaction involves sanctioned entity"
    
  peer_to_peer_pattern:
    enabled: true
    description: "Pattern suggesting peer-to-peer transfer scheme"

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\data\sample_transactions.csv ===
transaction_id,timestamp,amount,currency,sender_id,receiver_id,sender_account_type,receiver_account_type,sender_bank,receiver_bank,sender_location,receiver_location,transaction_type,transaction_category,merchant_id,merchant_category,ip_address,device_id,description,notes,authorization_status,chargeback_flag,fraud_flag
1,2023-01-01 09:00:00,150.50,USD,sender_001,receiver_001,personal,business,Chase,Bank of America,New York,USA,transfer,retail,merch_001,electronics,192.168.1.1,device_001,Payment for services,Regular payment,approved,False,0
2,2023-01-01 10:30:00,2500.00,USD,sender_002,receiver_002,business,personal,Wells Fargo,Citi,Los Angeles,USA,transfer,services,merch_002,professional,192.168.1.2,device_002,Business transfer,Urgent payment,approved,False,0
3,2023-01-01 11:45:00,10000.00,USD,sender_003,receiver_003,personal,personal,Bank of America,Chase,Chicago,USA,transfer,retail,merch_003,electronics,192.168.1.3,device_003,High value transfer,Suspicious,approved,False,0
4,2023-01-01 14:20:00,75.25,USD,sender_004,receiver_004,personal,personal,Citi,Wells Fargo,Houston,USA,payment,retail,merch_004,groceries,192.168.1.4,device_004,Grocery shopping,Regular,approved,False,0
5,2023-01-01 16:55:00,5000.00,USD,sender_005,receiver_005,business,business,JPMorgan Chase,Goldman Sachs,New York,USA,transfer,investment,merch_005,financial,192.168.1.5,device_005,Investment transfer,Regular,approved,False,0
6,2023-01-02 08:15:00,200.00,USD,sender_006,receiver_006,personal,personal,Bank of America,Citi,Miami,USA,payment,retail,merch_006,dining,192.168.1.6,device_006,Restaurant payment,Regular,approved,False,0
7,2023-01-02 09:45:00,15000.00,USD,sender_007,receiver_007,business,personal,Goldman Sachs,Wells Fargo,New York,USA,transfer,investment,merch_007,financial,192.168.1.7,device_007,Large investment,Suspicious,approved,False,1
8,2023-01-02 13:30:00,125.75,USD,sender_008,receiver_008,personal,personal,Chase,Bank of America,Boston,USA,payment,retail,merch_008,electronics,192.168.1.8,device_008,Online purchase,Regular,approved,False,0
9,2023-01-02 15:20:00,7500.00,USD,sender_009,receiver_009,business,business,Wells Fargo,JPMorgan Chase,Chicago,USA,transfer,business,merch_009,professional,192.168.1.9,device_009,Business expense,Regular,approved,False,0
10,2023-01-02 17:45:00,300.00,USD,sender_010,receiver_010,personal,personal,Citi,Goldman Sachs,Los Angeles,USA,payment,retail,merch_010,groceries,192.168.1.10,device_010,Grocery shopping,Regular,approved,False,0

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\data\.ipynb_checkpoints\sample_transactions-checkpoint.csv ===
transaction_id,timestamp,amount,currency,sender_id,receiver_id,sender_account_type,receiver_account_type,sender_bank,receiver_bank,sender_location,receiver_location,transaction_type,transaction_category,merchant_id,merchant_category,ip_address,device_id,description,notes,authorization_status,chargeback_flag,fraud_flag
1,2023-01-01 09:00:00,150.50,USD,sender_001,receiver_001,personal,business,Chase,Bank of America,New York,USA,transfer,retail,merch_001,electronics,192.168.1.1,device_001,Payment for services,Regular payment,approved,False,0
2,2023-01-01 10:30:00,2500.00,USD,sender_002,receiver_002,business,personal,Wells Fargo,Citi,Los Angeles,USA,transfer,services,merch_002,professional,192.168.1.2,device_002,Business transfer,Urgent payment,approved,False,0
3,2023-01-01 11:45:00,10000.00,USD,sender_003,receiver_003,personal,personal,Bank of America,Chase,Chicago,USA,transfer,retail,merch_003,electronics,192.168.1.3,device_003,High value transfer,Suspicious,approved,False,0
4,2023-01-01 14:20:00,75.25,USD,sender_004,receiver_004,personal,personal,Citi,Wells Fargo,Houston,USA,payment,retail,merch_004,groceries,192.168.1.4,device_004,Grocery shopping,Regular,approved,False,0
5,2023-01-01 16:55:00,5000.00,USD,sender_005,receiver_005,business,business,JPMorgan Chase,Goldman Sachs,New York,USA,transfer,investment,merch_005,financial,192.168.1.5,device_005,Investment transfer,Regular,approved,False,0
6,2023-01-02 08:15:00,200.00,USD,sender_006,receiver_006,personal,personal,Bank of America,Citi,Miami,USA,payment,retail,merch_006,dining,192.168.1.6,device_006,Restaurant payment,Regular,approved,False,0
7,2023-01-02 09:45:00,15000.00,USD,sender_007,receiver_007,business,personal,Goldman Sachs,Wells Fargo,New York,USA,transfer,investment,merch_007,financial,192.168.1.7,device_007,Large investment,Suspicious,approved,False,1
8,2023-01-02 13:30:00,125.75,USD,sender_008,receiver_008,personal,personal,Chase,Bank of America,Boston,USA,payment,retail,merch_008,electronics,192.168.1.8,device_008,Online purchase,Regular,approved,False,0
9,2023-01-02 15:20:00,7500.00,USD,sender_009,receiver_009,business,business,Wells Fargo,JPMorgan Chase,Chicago,USA,transfer,business,merch_009,professional,192.168.1.9,device_009,Business expense,Regular,approved,False,0
10,2023-01-02 17:45:00,300.00,USD,sender_010,receiver_010,personal,personal,Citi,Goldman Sachs,Los Angeles,USA,payment,retail,merch_010,groceries,192.168.1.10,device_010,Grocery shopping,Regular,approved,False,0

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\data\external\sanctions_list.csv ===

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\data\external\.ipynb_checkpoints\sanctions_list-checkpoint.csv ===

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\__init__.py ===

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\.ipynb_checkpoints\__init__-checkpoint.py ===

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\analysis\explainability.py ===
"""
Explainability Module
Implements explainable AI techniques for fraud detection
"""

import pandas as pd
import numpy as np
import shap
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.inspection import permutation_importance, partial_dependence
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier, export_text, export_graphviz
import graphviz
import warnings
import logging
from typing import Dict, List, Tuple, Union

from fraud_detection_engine.utils.api_utils import is_api_available

warnings.filterwarnings('ignore')
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class Explainability:
    """
    Class for generating explanations for fraud detection predictions
    Implements SHAP, LIME, and other explainability techniques
    """
    
    def __init__(self, config=None):
        """
        Initialize Explainability
        
        Args:
            config (dict, optional): Configuration parameters
        """
        self.config = config or {}
        self.explainers = {}
        self.explanations = {}
        self.feature_names = {}
        self.fitted = False
    
    def generate_explanations(self, df, risk_scores, model_results):
        """
        Generate explanations for all transactions
        
        Args:
            df (DataFrame): Input data
            risk_scores (DataFrame): Risk scores
            model_results (dict): Results from different models
            
        Returns:
            dict: Explanations for each transaction
        """
        try:
            # Get feature columns
            feature_cols = [col for col in df.columns if col not in 
                           ['transaction_id', 'sender_id', 'receiver_id', 'fraud_flag']]
            
            # Prepare data
            X = df[feature_cols].fillna(0)
            
            # Store feature names
            self.feature_names = feature_cols
            
            # Initialize explanations dictionary
            explanations = {}
            
            # Generate explanations for each transaction
            for idx, row in df.iterrows():
                try:
                    # Get risk score
                    risk_score = risk_scores.loc[idx, 'risk_score']
                    
                    # Generate explanation only for high-risk transactions
                    if risk_score > 0.5:  # Threshold for explanation
                        explanation = self._generate_transaction_explanation(
                            idx, row, X, risk_score, model_results
                        )
                        explanations[idx] = explanation
                except Exception as e:
                    logger.warning(f"Error generating explanation for transaction {idx}: {str(e)}")
            
            self.explanations = explanations
            self.fitted = True
            
            logger.info(f"Generated explanations for {len(explanations)} transactions")
            return explanations
            
        except Exception as e:
            logger.error(f"Error generating explanations: {str(e)}")
            raise
    
    def _generate_transaction_explanation(self, idx, row, X, risk_score, model_results):
        """
        Generate explanation for a single transaction
        
        Args:
            idx (int): Transaction index
            row (Series): Transaction data
            X (DataFrame): Feature data
            risk_score (float): Risk score
            model_results (dict): Model results
            
        Returns:
            dict: Explanation for the transaction
        """
        try:
            explanation = {
                'transaction_id': row.get('transaction_id', idx),
                'risk_score': risk_score,
                'top_factors': [],
                'rule_violations': [],
                'model_predictions': {},
                'feature_contributions': {},
                'text_explanation': ''
            }
            
            # Get feature contributions from different models
            feature_contributions = {}
            
            # Process unsupervised models
            if 'unsupervised' in model_results:
                unsupervised_contributions = self._get_unsupervised_contributions(
                    idx, X, model_results['unsupervised']
                )
                feature_contributions.update(unsupervised_contributions)
                
                # Add model predictions
                for model_name, model_data in model_results['unsupervised'].items():
                    if 'scores' in model_data and idx < len(model_data['scores']):
                        explanation['model_predictions'][f'unsupervised_{model_name}'] = model_data['scores'][idx]
            
            # Process supervised models
            if 'supervised' in model_results:
                supervised_contributions = self._get_supervised_contributions(
                    idx, X, model_results['supervised']
                )
                feature_contributions.update(supervised_contributions)
                
                # Add model predictions
                for model_name, model_data in model_results['supervised'].items():
                    if 'probabilities' in model_data and idx < len(model_data['probabilities']):
                        explanation['model_predictions'][f'supervised_{model_name}'] = model_data['probabilities'][idx]
            
            # Process rule-based models
            if 'rule' in model_results:
                rule_contributions = self._get_rule_contributions(
                    idx, row, model_results['rule']
                )
                feature_contributions.update(rule_contributions)
                
                # Add rule violations
                if 'violated_rule_names' in model_results['rule'] and idx < len(model_results['rule']['violated_rule_names']):
                    explanation['rule_violations'] = model_results['rule']['violated_rule_names'][idx]
            
            # Aggregate feature contributions
            explanation['feature_contributions'] = feature_contributions
            
            # Get top contributing factors
            top_factors = sorted(
                feature_contributions.items(),
                key=lambda x: abs(x[1]),
                reverse=True
            )[:10]  # Top 10 factors
            
            explanation['top_factors'] = top_factors
            
            # Generate text explanation
            explanation['text_explanation'] = self._generate_text_explanation(
                explanation, row
            )
            
            return explanation
            
        except Exception as e:
            logger.error(f"Error generating explanation for transaction {idx}: {str(e)}")
            return {
                'transaction_id': row.get('transaction_id', idx),
                'risk_score': risk_score,
                'error': str(e)
            }
    
    def _get_unsupervised_contributions(self, idx, X, unsupervised_results):
        """
        Get feature contributions from unsupervised models
        
        Args:
            idx (int): Transaction index
            X (DataFrame): Feature data
            unsupervised_results (dict): Unsupervised model results
            
        Returns:
            dict: Feature contributions
        """
        try:
            contributions = {}
            
            # Process each unsupervised model
            for model_name, model_data in unsupervised_results.items():
                if 'model' in model_data and 'feature_names' in model_data:
                    model = model_data['model']
                    feature_names = model_data['feature_names']
                    
                    # Get feature values for this transaction
                    x = X.loc[idx:idx+1, feature_names]
                    
                    # Calculate feature contributions based on model type
                    if model_name == 'isolation_forest':
                        # For Isolation Forest, use feature importance
                        if hasattr(model, 'feature_importances_'):
                            feature_importance = model.feature_importances_
                            for i, feature in enumerate(feature_names):
                                if i < len(feature_importance):
                                    contributions[f'{feature}_isolation_forest'] = feature_importance[i]
                    
                    elif model_name == 'local_outlier_factor':
                        # For LOF, use distance to neighbors
                        if hasattr(model, '_fit_X') and hasattr(model, '_distances'):
                            # Get distances to neighbors
                            distances = model._distances[idx]
                            # Calculate contribution based on feature differences
                            for i, feature in enumerate(feature_names):
                                if i < x.shape[1]:
                                    feature_value = x.iloc[0, i]
                                    # Calculate how much this feature contributes to the distance
                                    feature_diff = np.abs(feature_value - model._fit_X[:, i].mean())
                                    contributions[f'{feature}_lof'] = feature_diff
                    
                    elif model_name == 'autoencoder':
                        # For Autoencoder, use reconstruction error per feature
                        if hasattr(model, 'predict'):
                            reconstruction = model.predict(x)
                            error_per_feature = np.power(x.values - reconstruction, 2)
                            for i, feature in enumerate(feature_names):
                                if i < error_per_feature.shape[1]:
                                    contributions[f'{feature}_autoencoder'] = error_per_feature[0, i]
            
            return contributions
            
        except Exception as e:
            logger.error(f"Error getting unsupervised contributions: {str(e)}")
            return {}
    
    def _get_supervised_contributions(self, idx, X, supervised_results):
        """
        Get feature contributions from supervised models
        
        Args:
            idx (int): Transaction index
            X (DataFrame): Feature data
            supervised_results (dict): Supervised model results
            
        Returns:
            dict: Feature contributions
        """
        try:
            contributions = {}
            
            # Process each supervised model
            for model_name, model_data in supervised_results.items():
                if 'model' in model_data and 'feature_names' in model_data:
                    model = model_data['model']
                    feature_names = model_data['feature_names']
                    
                    # Get feature values for this transaction
                    x = X.loc[idx:idx+1, feature_names]
                    
                    # Calculate SHAP values if available
                    if 'shap_values' in model_data:
                        shap_values = model_data['shap_values']
                        if isinstance(shap_values, list):
                            # For multi-class SHAP values
                            if len(shap_values) > 1:
                                shap_vals = shap_values[1][idx]  # Use positive class
                            else:
                                shap_vals = shap_values[0][idx]
                        else:
                            shap_vals = shap_values[idx]
                        
                        for i, feature in enumerate(feature_names):
                            if i < len(shap_vals):
                                contributions[f'{feature}_{model_name}'] = shap_vals[i]
                    
                    # Use feature importance as fallback
                    elif 'feature_importance' in model_data:
                        importance_df = model_data['feature_importance']
                        for _, row in importance_df.iterrows():
                            feature = row['feature']
                            importance = row['importance']
                            contributions[f'{feature}_{model_name}'] = importance
            
            return contributions
            
        except Exception as e:
            logger.error(f"Error getting supervised contributions: {str(e)}")
            return {}
    
    def _get_rule_contributions(self, idx, row, rule_results):
        """
        Get feature contributions from rule-based model
        
        Args:
            idx (int): Transaction index
            row (Series): Transaction data
            rule_results (dict): Rule model results
            
        Returns:
            dict: Feature contributions
        """
        try:
            contributions = {}
            
            # Get violated rules for this transaction
            if 'violated_rule_names' in rule_results and idx < len(rule_results['violated_rule_names']):
                violated_rules = rule_results['violated_rule_names'][idx]
                
                # Get rule weights
                rule_weights = rule_results.get('rule_weights', {})
                
                # Map rules to features
                rule_to_features = {
                    'high_amount': ['amount'],
                    'unusual_amount_for_sender': ['amount', 'sender_id'],
                    'unusual_amount_for_receiver': ['amount', 'receiver_id'],
                    'round_amount': ['amount'],
                    'high_frequency_sender': ['sender_id', 'timestamp'],
                    'high_frequency_receiver': ['receiver_id', 'timestamp'],
                    'rapid_succession': ['timestamp'],
                    'cross_border': ['sender_location', 'receiver_location'],
                    'high_risk_country': ['sender_location', 'receiver_location'],
                    'unusual_location_for_sender': ['sender_location'],
                    'unusual_hour': ['timestamp'],
                    'weekend': ['timestamp'],
                    'new_sender': ['sender_id'],
                    'new_receiver': ['receiver_id'],
                    'sanctions_check': ['sender_id', 'receiver_id'],
                    'tax_compliance': ['sender_id', 'receiver_id'],
                    'bank_verification': ['sender_id', 'receiver_id'],
                    'identity_verification': ['sender_id', 'receiver_id']
                }
                
                # Calculate contributions based on violated rules
                for rule in violated_rules:
                    weight = rule_weights.get(rule, 1.0)
                    features = rule_to_features.get(rule, [])
                    
                    for feature in features:
                        if feature in row:
                            contributions[f'{feature}_rule'] = contributions.get(f'{feature}_rule', 0) + weight
            
            return contributions
            
        except Exception as e:
            logger.error(f"Error getting rule contributions: {str(e)}")
            return {}
    
    def _generate_text_explanation(self, explanation, row):
        """
        Generate natural language explanation
        
        Args:
            explanation (dict): Explanation data
            row (Series): Transaction data
            
        Returns:
            str: Text explanation
        """
        try:
            # Start with risk level
            risk_score = explanation['risk_score']
            
            if risk_score >= 0.9:
                risk_level = "critical"
            elif risk_score >= 0.7:
                risk_level = "high"
            elif risk_score >= 0.5:
                risk_level = "medium"
            else:
                risk_level = "low"
            
            text = f"This transaction has a {risk_level} risk score of {risk_score:.2f}. "
            
            # Add top contributing factors
            if explanation['top_factors']:
                top_factors = explanation['top_factors'][:3]  # Top 3 factors
                factor_names = [factor[0].split('_')[0] for factor in top_factors]
                
                if len(factor_names) == 1:
                    text += f"The primary factor is {factor_names[0]}. "
                elif len(factor_names) == 2:
                    text += f"The main factors are {factor_names[0]} and {factor_names[1]}. "
                else:
                    text += f"The main factors are {', '.join(factor_names[:-1])}, and {factor_names[-1]}. "
            
            # Add rule violations
            if explanation['rule_violations']:
                if len(explanation['rule_violations']) == 1:
                    text += f"It violates the rule: {explanation['rule_violations'][0]}. "
                else:
                    text += f"It violates {len(explanation['rule_violations'])} rules: {', '.join(explanation['rule_violations'][:3])}. "
            
            # Add transaction details
            if 'amount' in row:
                text += f"The transaction amount is {row['amount']}. "
            
            if 'sender_id' in row and 'receiver_id' in row:
                text += f"It involves sender {row['sender_id']} and receiver {row['receiver_id']}. "
            
            if 'timestamp' in row:
                text += f"The transaction occurred at {row['timestamp']}. "
            
            # Add AI-generated explanation if APIs are available
            ai_explanation = ""
            if is_api_available('gemini'):
                # Here you would call Gemini API to generate explanation
                # For now, add a placeholder
                ai_explanation = "AI analysis pending implementation. "
            elif is_api_available('openai'):
                # Here you would call OpenAI API to generate explanation
                # For now, add a placeholder
                ai_explanation = "AI analysis pending implementation. "
            
            if ai_explanation:
                text += f"AI analysis: {ai_explanation}"
            
            # Add recommendation
            if risk_score >= 0.7:
                text += "This transaction should be reviewed immediately."
            elif risk_score >= 0.5:
                text += "This transaction should be reviewed."
            else:
                text += "This transaction appears to be low risk."
            
            return text
            
        except Exception as e:
            logger.error(f"Error generating text explanation: {str(e)}")
            return "Unable to generate explanation."
    
    def get_explanation(self, transaction_id):
        """
        Get explanation for a specific transaction
        
        Args:
            transaction_id (str): Transaction ID
            
        Returns:
            dict: Explanation for the transaction
        """
        if not self.fitted:
            raise ValueError("Explanations not generated. Call generate_explanations first.")
        
        # Find explanation by transaction ID
        for idx, explanation in self.explanations.items():
            if explanation.get('transaction_id') == transaction_id:
                return explanation
        
        return None
    
    def plot_feature_importance(self, model_name=None, top_n=20):
        """
        Plot feature importance for a model
        
        Args:
            model_name (str, optional): Name of the model
            top_n (int): Number of top features to show
        """
        try:
            # Aggregate feature contributions across all explanations
            feature_contributions = {}
            
            for explanation in self.explanations.values():
                if 'feature_contributions' in explanation:
                    for feature, contribution in explanation['feature_contributions'].items():
                        if model_name is None or model_name in feature:
                            feature_contributions[feature] = feature_contributions.get(feature, 0) + abs(contribution)
            
            if not feature_contributions:
                logger.warning("No feature contributions found")
                return None
            
            # Create DataFrame
            importance_df = pd.DataFrame({
                'feature': list(feature_contributions.keys()),
                'importance': list(feature_contributions.values())
            }).sort_values('importance', ascending=False)
            
            # Get top features
            top_features = importance_df.head(top_n)
            
            # Create plot
            plt.figure(figsize=(10, 8))
            sns.barplot(x='importance', y='feature', data=top_features)
            plt.title(f'Top {top_n} Feature Importance')
            plt.tight_layout()
            
            return plt.gcf()
            
        except Exception as e:
            logger.error(f"Error plotting feature importance: {str(e)}")
            raise
    
    def plot_shap_summary(self, model_name, X):
        """
        Plot SHAP summary for a model
        
        Args:
            model_name (str): Name of the model
            X (DataFrame): Feature data
        """
        try:
            # This would require access to the model and SHAP values
            # For demonstration, we'll create a placeholder plot
            
            plt.figure(figsize=(10, 8))
            plt.title(f'SHAP Summary for {model_name}')
            plt.text(0.5, 0.5, 'SHAP summary plot would be displayed here', 
                    ha='center', va='center', fontsize=12)
            plt.tight_layout()
            
            return plt.gcf()
            
        except Exception as e:
            logger.error(f"Error plotting SHAP summary: {str(e)}")
            raise
    
    def plot_decision_path(self, model_name, X, idx):
        """
        Plot decision path for a tree-based model
        
        Args:
            model_name (str): Name of the model
            X (DataFrame): Feature data
            idx (int): Transaction index
        """
        try:
            # This would require access to the model
            # For demonstration, we'll create a placeholder plot
            
            plt.figure(figsize=(10, 8))
            plt.title(f'Decision Path for {model_name}')
            plt.text(0.5, 0.5, 'Decision path would be displayed here', 
                    ha='center', va='center', fontsize=12)
            plt.tight_layout()
            
            return plt.gcf()
            
        except Exception as e:
            logger.error(f"Error plotting decision path: {str(e)}")
            raise
    
    def get_counterfactual_explanation(self, idx, X, model_name, target_class=0):
        """
        Generate counterfactual explanation
        
        Args:
            idx (int): Transaction index
            X (DataFrame): Feature data
            model_name (str): Name of the model
            target_class (int): Target class (0 for non-fraud, 1 for fraud)
            
        Returns:
            dict: Counterfactual explanation
        """
        try:
            # This is a simplified implementation
            # In practice, you would use more sophisticated methods
            
            # Get original prediction
            original_row = X.loc[idx:idx+1]
            
            # Generate counterfactual by modifying features
            counterfactual = original_row.copy()
            
            # Modify top features to change prediction
            # This is a placeholder implementation
            for col in counterfactual.columns:
                if col in ['amount']:
                    # Reduce amount by 50%
                    counterfactual[col] = counterfactual[col] * 0.5
            
            return {
                'original': original_row.to_dict('records')[0],
                'counterfactual': counterfactual.to_dict('records')[0],
                'changes': {
                    'amount': 'Reduced by 50%'
                }
            }
            
        except Exception as e:
            logger.error(f"Error generating counterfactual explanation: {str(e)}")
            return {}

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\analysis\risk_scorer.py ===
"""
Risk Scorer Module
Implements risk scoring for fraud detection
"""
import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
from sklearn.calibration import CalibratedClassifierCV
import warnings
import logging
from typing import Dict, List, Tuple, Union
warnings.filterwarnings('ignore')
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class RiskScorer:
    """
    Class for calculating risk scores from multiple models
    Implements weighted combination of algorithm outputs
    """
    
    def __init__(self, method='weighted_average', custom_weights=None):
        """
        Initialize RiskScorer
        
        Args:
            method (str): Scoring method ('weighted_average', 'maximum', 'custom')
            custom_weights (dict, optional): Custom weights for models
        """
        # Convert method to lowercase and replace spaces with underscores
        self.method = method.lower().replace(" ", "_")
        self.custom_weights = custom_weights or {
            'unsupervised': 0.4,
            'supervised': 0.4,
            'rule': 0.2
        }
        self.scalers = {}
        self.calibrators = {}
        self.fitted = False
    
    def calculate_scores(self, model_results, df):
        """
        Calculate risk scores from model results
        
        Args:
            model_results (dict): Results from different models
            df (DataFrame): Original data
            
        Returns:
            DataFrame: Risk scores
        """
        try:
            # Initialize result DataFrame
            result_df = df.copy()
            
            # Extract scores from each model type
            unsupervised_scores = None
            supervised_scores = None
            rule_scores = None
            
            # Process unsupervised model results
            if 'unsupervised' in model_results:
                unsupervised_scores = self._process_unsupervised_results(model_results['unsupervised'])
            
            # Process supervised model results
            if 'supervised' in model_results:
                supervised_scores = self._process_supervised_results(model_results['supervised'])
            
            # Process rule-based results
            if 'rule' in model_results:
                rule_scores = self._process_rule_results(model_results['rule'])
            
            # Combine scores based on method
            if self.method == 'weighted_average':
                combined_scores = self._weighted_average_combination(
                    unsupervised_scores, supervised_scores, rule_scores
                )
            elif self.method == 'maximum':
                combined_scores = self._maximum_combination(
                    unsupervised_scores, supervised_scores, rule_scores
                )
            elif self.method == 'custom':
                combined_scores = self._custom_combination(
                    unsupervised_scores, supervised_scores, rule_scores
                )
            else:
                raise ValueError(f"Unknown scoring method: {self.method}")
            
            # Add combined scores to result
            result_df['risk_score'] = combined_scores
            
            # Add individual model scores
            if unsupervised_scores is not None:
                result_df['unsupervised_score'] = unsupervised_scores
            
            if supervised_scores is not None:
                result_df['supervised_score'] = supervised_scores
            
            if rule_scores is not None:
                result_df['rule_score'] = rule_scores
            
            # Calculate percentile rank
            result_df['risk_percentile'] = result_df['risk_score'].rank(pct=True)
            
            # Calculate risk level
            result_df['risk_level'] = pd.cut(
                result_df['risk_percentile'],
                bins=[0, 0.7, 0.9, 0.95, 1.0],
                labels=['Low', 'Medium', 'High', 'Critical']
            )
            
            self.fitted = True
            return result_df
            
        except Exception as e:
            logger.error(f"Error calculating risk scores: {str(e)}")
            raise
    
    def _process_unsupervised_results(self, unsupervised_results):
        """
        Process unsupervised model results
        
        Args:
            unsupervised_results (dict): Results from unsupervised models
            
        Returns:
            array: Combined unsupervised scores
        """
        try:
            # Collect scores from all unsupervised models
            all_scores = []
            model_names = []
            
            for model_name, model_data in unsupervised_results.items():
                if 'scores' in model_data:
                    scores = model_data['scores']
                    all_scores.append(scores)
                    model_names.append(model_name)
            
            if not all_scores:
                return None
            
            # Convert to numpy array
            scores_array = np.array(all_scores).T
            
            # Calculate average score
            avg_scores = np.mean(scores_array, axis=1)
            
            # Normalize to 0-1 range
            scaler = MinMaxScaler()
            normalized_scores = scaler.fit_transform(avg_scores.reshape(-1, 1)).flatten()
            
            # Store scaler
            self.scalers['unsupervised'] = scaler
            
            return normalized_scores
            
        except Exception as e:
            logger.error(f"Error processing unsupervised results: {str(e)}")
            raise
    
    def _process_supervised_results(self, supervised_results):
        """
        Process supervised model results
        
        Args:
            supervised_results (dict): Results from supervised models
            
        Returns:
            array: Combined supervised scores
        """
        try:
            # Collect probabilities from all supervised models
            all_probabilities = []
            model_names = []
            
            for model_name, model_data in supervised_results.items():
                if 'probabilities' in model_data:
                    probabilities = model_data['probabilities']
                    all_probabilities.append(probabilities)
                    model_names.append(model_name)
            
            if not all_probabilities:
                return None
            
            # Convert to numpy array
            probabilities_array = np.array(all_probabilities).T
            
            # Calculate average probability
            avg_probabilities = np.mean(probabilities_array, axis=1)
            
            # Apply calibration if available
            if 'supervised' in self.calibrators:
                calibrated_scores = self.calibrators['supervised'].predict_proba(avg_probabilities.reshape(-1, 1))[:, 1]
            else:
                calibrated_scores = avg_probabilities
            
            # Store calibrator
            self.calibrators['supervised'] = CalibratedClassifierCV(
                method='isotonic', cv='prefit'
            )
            
            return calibrated_scores
            
        except Exception as e:
            logger.error(f"Error processing supervised results: {str(e)}")
            raise
    
    def _process_rule_results(self, rule_results):
        """
        Process rule-based results
        
        Args:
            rule_results (dict): Results from rule engine
            
        Returns:
            array: Combined rule scores
        """
        try:
            if 'normalized_scores' in rule_results:
                rule_scores = rule_results['normalized_scores']
                # Ensure rule_scores is a numpy array
                if not isinstance(rule_scores, np.ndarray):
                    rule_scores = np.array(rule_scores)
            else:
                return None
            
            # No calibration for now to avoid the reshape issue
            calibrated_scores = rule_scores
            
            # Store calibrator
            self.calibrators['rule'] = CalibratedClassifierCV(
                method='isotonic', cv='prefit'
            )
            
            return calibrated_scores
            
        except Exception as e:
            logger.error(f"Error processing rule results: {str(e)}")
            raise
    
    def _weighted_average_combination(self, unsupervised_scores, supervised_scores, rule_scores):
        """
        Combine scores using weighted average
        
        Args:
            unsupervised_scores (array): Unsupervised model scores
            supervised_scores (array): Supervised model scores
            rule_scores (array): Rule-based scores
            
        Returns:
            array: Combined scores
        """
        try:
            # Get weights
            unsupervised_weight = self.custom_weights.get('unsupervised', 0.4)
            supervised_weight = self.custom_weights.get('supervised', 0.4)
            rule_weight = self.custom_weights.get('rule', 0.2)
            
            # Normalize weights based on available scores
            available_weights = []
            if unsupervised_scores is not None:
                available_weights.append(unsupervised_weight)
            if supervised_scores is not None:
                available_weights.append(supervised_weight)
            if rule_scores is not None:
                available_weights.append(rule_weight)
            
            total_weight = sum(available_weights)
            if total_weight > 0:
                if unsupervised_scores is not None:
                    unsupervised_weight /= total_weight
                if supervised_scores is not None:
                    supervised_weight /= total_weight
                if rule_scores is not None:
                    rule_weight /= total_weight
            
            # Determine the length of the combined scores array
            if unsupervised_scores is not None:
                length = len(unsupervised_scores)
            elif supervised_scores is not None:
                length = len(supervised_scores)
            elif rule_scores is not None:
                length = len(rule_scores)
            else:
                # If no scores available, return empty array
                return np.array([])
            
            # Initialize combined scores
            combined_scores = np.zeros(length)
            
            # Add weighted scores
            if unsupervised_scores is not None:
                # Ensure unsupervised_scores is a numpy array
                if not isinstance(unsupervised_scores, np.ndarray):
                    unsupervised_scores = np.array(unsupervised_scores)
                combined_scores += unsupervised_weight * unsupervised_scores
            
            if supervised_scores is not None:
                # Ensure supervised_scores is a numpy array
                if not isinstance(supervised_scores, np.ndarray):
                    supervised_scores = np.array(supervised_scores)
                combined_scores += supervised_weight * supervised_scores
            
            if rule_scores is not None:
                # Ensure rule_scores is a numpy array
                if not isinstance(rule_scores, np.ndarray):
                    rule_scores = np.array(rule_scores)
                combined_scores += rule_weight * rule_scores
            
            return combined_scores
            
        except Exception as e:
            logger.error(f"Error in weighted average combination: {str(e)}")
            raise
    
    def _maximum_combination(self, unsupervised_scores, supervised_scores, rule_scores):
        """
        Combine scores using maximum
        
        Args:
            unsupervised_scores (array): Unsupervised model scores
            supervised_scores (array): Supervised model scores
            rule_scores (array): Rule-based scores
            
        Returns:
            array: Combined scores
        """
        try:
            # Collect all available scores
            all_scores = []
            
            if unsupervised_scores is not None:
                all_scores.append(unsupervised_scores)
            
            if supervised_scores is not None:
                all_scores.append(supervised_scores)
            
            if rule_scores is not None:
                all_scores.append(rule_scores)
            
            if not all_scores:
                return np.array([])
            
            # Take maximum score for each transaction
            combined_scores = np.max(all_scores, axis=0)
            
            return combined_scores
            
        except Exception as e:
            logger.error(f"Error in maximum combination: {str(e)}")
            raise
    
    def _custom_combination(self, unsupervised_scores, supervised_scores, rule_scores):
        """
        Combine scores using custom method
        
        Args:
            unsupervised_scores (array): Unsupervised model scores
            supervised_scores (array): Supervised model scores
            rule_scores (array): Rule-based scores
            
        Returns:
            array: Combined scores
        """
        try:
            # Get custom weights
            unsupervised_weight = self.custom_weights.get('unsupervised', 0.4)
            supervised_weight = self.custom_weights.get('supervised', 0.4)
            rule_weight = self.custom_weights.get('rule', 0.2)
            
            # Apply non-linear transformation to supervised scores
            if supervised_scores is not None:
                # Emphasize high scores
                supervised_transformed = np.power(supervised_scores, 1.5)
            else:
                supervised_transformed = None
            
            # Apply non-linear transformation to rule scores
            if rule_scores is not None:
                # Emphasize high scores even more
                rule_transformed = np.power(rule_scores, 2.0)
            else:
                rule_transformed = None
            
            # Combine using weighted average with transformed scores
            return self._weighted_average_combination(
                unsupervised_scores, supervised_transformed, rule_transformed
            )
            
        except Exception as e:
            logger.error(f"Error in custom combination: {str(e)}")
            raise
    
    def update_weights(self, new_weights):
        """
        Update the weights for combining scores
        
        Args:
            new_weights (dict): New weights for models
        """
        self.custom_weights.update(new_weights)
        logger.info(f"Updated weights: {self.custom_weights}")
    
    def get_weights(self):
        """
        Get current weights
        
        Returns:
            dict: Current weights
        """
        return self.custom_weights.copy()
    
    def calibrate_scores(self, method='isotonic'):
        """
        Calibrate scores to better reflect true probabilities
        
        Args:
            method (str): Calibration method ('isotonic', 'sigmoid')
        """
        try:
            # Update calibrators
            for model_type in ['unsupervised', 'supervised', 'rule']:
                if model_type in self.calibrators:
                    self.calibrators[model_type] = CalibratedClassifierCV(
                        method=method, cv='prefit'
                    )
            
            logger.info(f"Updated calibrators with method: {method}")
            
        except Exception as e:
            logger.error(f"Error calibrating scores: {str(e)}")
            raise

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\analysis\__init__.py ===

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\analysis\.ipynb_checkpoints\explainability-checkpoint.py ===
"""
Explainability Module
Implements explainable AI techniques for fraud detection
"""

import pandas as pd
import numpy as np
import shap
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.inspection import permutation_importance, partial_dependence
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier, export_text, export_graphviz
import graphviz
import warnings
import logging
from typing import Dict, List, Tuple, Union

from fraud_detection_engine.utils.api_utils import is_api_available

warnings.filterwarnings('ignore')
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class Explainability:
    """
    Class for generating explanations for fraud detection predictions
    Implements SHAP, LIME, and other explainability techniques
    """
    
    def __init__(self, config=None):
        """
        Initialize Explainability
        
        Args:
            config (dict, optional): Configuration parameters
        """
        self.config = config or {}
        self.explainers = {}
        self.explanations = {}
        self.feature_names = {}
        self.fitted = False
    
    def generate_explanations(self, df, risk_scores, model_results):
        """
        Generate explanations for all transactions
        
        Args:
            df (DataFrame): Input data
            risk_scores (DataFrame): Risk scores
            model_results (dict): Results from different models
            
        Returns:
            dict: Explanations for each transaction
        """
        try:
            # Get feature columns
            feature_cols = [col for col in df.columns if col not in 
                           ['transaction_id', 'sender_id', 'receiver_id', 'fraud_flag']]
            
            # Prepare data
            X = df[feature_cols].fillna(0)
            
            # Store feature names
            self.feature_names = feature_cols
            
            # Initialize explanations dictionary
            explanations = {}
            
            # Generate explanations for each transaction
            for idx, row in df.iterrows():
                try:
                    # Get risk score
                    risk_score = risk_scores.loc[idx, 'risk_score']
                    
                    # Generate explanation only for high-risk transactions
                    if risk_score > 0.5:  # Threshold for explanation
                        explanation = self._generate_transaction_explanation(
                            idx, row, X, risk_score, model_results
                        )
                        explanations[idx] = explanation
                except Exception as e:
                    logger.warning(f"Error generating explanation for transaction {idx}: {str(e)}")
            
            self.explanations = explanations
            self.fitted = True
            
            logger.info(f"Generated explanations for {len(explanations)} transactions")
            return explanations
            
        except Exception as e:
            logger.error(f"Error generating explanations: {str(e)}")
            raise
    
    def _generate_transaction_explanation(self, idx, row, X, risk_score, model_results):
        """
        Generate explanation for a single transaction
        
        Args:
            idx (int): Transaction index
            row (Series): Transaction data
            X (DataFrame): Feature data
            risk_score (float): Risk score
            model_results (dict): Model results
            
        Returns:
            dict: Explanation for the transaction
        """
        try:
            explanation = {
                'transaction_id': row.get('transaction_id', idx),
                'risk_score': risk_score,
                'top_factors': [],
                'rule_violations': [],
                'model_predictions': {},
                'feature_contributions': {},
                'text_explanation': ''
            }
            
            # Get feature contributions from different models
            feature_contributions = {}
            
            # Process unsupervised models
            if 'unsupervised' in model_results:
                unsupervised_contributions = self._get_unsupervised_contributions(
                    idx, X, model_results['unsupervised']
                )
                feature_contributions.update(unsupervised_contributions)
                
                # Add model predictions
                for model_name, model_data in model_results['unsupervised'].items():
                    if 'scores' in model_data and idx < len(model_data['scores']):
                        explanation['model_predictions'][f'unsupervised_{model_name}'] = model_data['scores'][idx]
            
            # Process supervised models
            if 'supervised' in model_results:
                supervised_contributions = self._get_supervised_contributions(
                    idx, X, model_results['supervised']
                )
                feature_contributions.update(supervised_contributions)
                
                # Add model predictions
                for model_name, model_data in model_results['supervised'].items():
                    if 'probabilities' in model_data and idx < len(model_data['probabilities']):
                        explanation['model_predictions'][f'supervised_{model_name}'] = model_data['probabilities'][idx]
            
            # Process rule-based models
            if 'rule' in model_results:
                rule_contributions = self._get_rule_contributions(
                    idx, row, model_results['rule']
                )
                feature_contributions.update(rule_contributions)
                
                # Add rule violations
                if 'violated_rule_names' in model_results['rule'] and idx < len(model_results['rule']['violated_rule_names']):
                    explanation['rule_violations'] = model_results['rule']['violated_rule_names'][idx]
            
            # Aggregate feature contributions
            explanation['feature_contributions'] = feature_contributions
            
            # Get top contributing factors
            top_factors = sorted(
                feature_contributions.items(),
                key=lambda x: abs(x[1]),
                reverse=True
            )[:10]  # Top 10 factors
            
            explanation['top_factors'] = top_factors
            
            # Generate text explanation
            explanation['text_explanation'] = self._generate_text_explanation(
                explanation, row
            )
            
            return explanation
            
        except Exception as e:
            logger.error(f"Error generating explanation for transaction {idx}: {str(e)}")
            return {
                'transaction_id': row.get('transaction_id', idx),
                'risk_score': risk_score,
                'error': str(e)
            }
    
    def _get_unsupervised_contributions(self, idx, X, unsupervised_results):
        """
        Get feature contributions from unsupervised models
        
        Args:
            idx (int): Transaction index
            X (DataFrame): Feature data
            unsupervised_results (dict): Unsupervised model results
            
        Returns:
            dict: Feature contributions
        """
        try:
            contributions = {}
            
            # Process each unsupervised model
            for model_name, model_data in unsupervised_results.items():
                if 'model' in model_data and 'feature_names' in model_data:
                    model = model_data['model']
                    feature_names = model_data['feature_names']
                    
                    # Get feature values for this transaction
                    x = X.loc[idx:idx+1, feature_names]
                    
                    # Calculate feature contributions based on model type
                    if model_name == 'isolation_forest':
                        # For Isolation Forest, use feature importance
                        if hasattr(model, 'feature_importances_'):
                            feature_importance = model.feature_importances_
                            for i, feature in enumerate(feature_names):
                                if i < len(feature_importance):
                                    contributions[f'{feature}_isolation_forest'] = feature_importance[i]
                    
                    elif model_name == 'local_outlier_factor':
                        # For LOF, use distance to neighbors
                        if hasattr(model, '_fit_X') and hasattr(model, '_distances'):
                            # Get distances to neighbors
                            distances = model._distances[idx]
                            # Calculate contribution based on feature differences
                            for i, feature in enumerate(feature_names):
                                if i < x.shape[1]:
                                    feature_value = x.iloc[0, i]
                                    # Calculate how much this feature contributes to the distance
                                    feature_diff = np.abs(feature_value - model._fit_X[:, i].mean())
                                    contributions[f'{feature}_lof'] = feature_diff
                    
                    elif model_name == 'autoencoder':
                        # For Autoencoder, use reconstruction error per feature
                        if hasattr(model, 'predict'):
                            reconstruction = model.predict(x)
                            error_per_feature = np.power(x.values - reconstruction, 2)
                            for i, feature in enumerate(feature_names):
                                if i < error_per_feature.shape[1]:
                                    contributions[f'{feature}_autoencoder'] = error_per_feature[0, i]
            
            return contributions
            
        except Exception as e:
            logger.error(f"Error getting unsupervised contributions: {str(e)}")
            return {}
    
    def _get_supervised_contributions(self, idx, X, supervised_results):
        """
        Get feature contributions from supervised models
        
        Args:
            idx (int): Transaction index
            X (DataFrame): Feature data
            supervised_results (dict): Supervised model results
            
        Returns:
            dict: Feature contributions
        """
        try:
            contributions = {}
            
            # Process each supervised model
            for model_name, model_data in supervised_results.items():
                if 'model' in model_data and 'feature_names' in model_data:
                    model = model_data['model']
                    feature_names = model_data['feature_names']
                    
                    # Get feature values for this transaction
                    x = X.loc[idx:idx+1, feature_names]
                    
                    # Calculate SHAP values if available
                    if 'shap_values' in model_data:
                        shap_values = model_data['shap_values']
                        if isinstance(shap_values, list):
                            # For multi-class SHAP values
                            if len(shap_values) > 1:
                                shap_vals = shap_values[1][idx]  # Use positive class
                            else:
                                shap_vals = shap_values[0][idx]
                        else:
                            shap_vals = shap_values[idx]
                        
                        for i, feature in enumerate(feature_names):
                            if i < len(shap_vals):
                                contributions[f'{feature}_{model_name}'] = shap_vals[i]
                    
                    # Use feature importance as fallback
                    elif 'feature_importance' in model_data:
                        importance_df = model_data['feature_importance']
                        for _, row in importance_df.iterrows():
                            feature = row['feature']
                            importance = row['importance']
                            contributions[f'{feature}_{model_name}'] = importance
            
            return contributions
            
        except Exception as e:
            logger.error(f"Error getting supervised contributions: {str(e)}")
            return {}
    
    def _get_rule_contributions(self, idx, row, rule_results):
        """
        Get feature contributions from rule-based model
        
        Args:
            idx (int): Transaction index
            row (Series): Transaction data
            rule_results (dict): Rule model results
            
        Returns:
            dict: Feature contributions
        """
        try:
            contributions = {}
            
            # Get violated rules for this transaction
            if 'violated_rule_names' in rule_results and idx < len(rule_results['violated_rule_names']):
                violated_rules = rule_results['violated_rule_names'][idx]
                
                # Get rule weights
                rule_weights = rule_results.get('rule_weights', {})
                
                # Map rules to features
                rule_to_features = {
                    'high_amount': ['amount'],
                    'unusual_amount_for_sender': ['amount', 'sender_id'],
                    'unusual_amount_for_receiver': ['amount', 'receiver_id'],
                    'round_amount': ['amount'],
                    'high_frequency_sender': ['sender_id', 'timestamp'],
                    'high_frequency_receiver': ['receiver_id', 'timestamp'],
                    'rapid_succession': ['timestamp'],
                    'cross_border': ['sender_location', 'receiver_location'],
                    'high_risk_country': ['sender_location', 'receiver_location'],
                    'unusual_location_for_sender': ['sender_location'],
                    'unusual_hour': ['timestamp'],
                    'weekend': ['timestamp'],
                    'new_sender': ['sender_id'],
                    'new_receiver': ['receiver_id'],
                    'sanctions_check': ['sender_id', 'receiver_id'],
                    'tax_compliance': ['sender_id', 'receiver_id'],
                    'bank_verification': ['sender_id', 'receiver_id'],
                    'identity_verification': ['sender_id', 'receiver_id']
                }
                
                # Calculate contributions based on violated rules
                for rule in violated_rules:
                    weight = rule_weights.get(rule, 1.0)
                    features = rule_to_features.get(rule, [])
                    
                    for feature in features:
                        if feature in row:
                            contributions[f'{feature}_rule'] = contributions.get(f'{feature}_rule', 0) + weight
            
            return contributions
            
        except Exception as e:
            logger.error(f"Error getting rule contributions: {str(e)}")
            return {}
    
    def _generate_text_explanation(self, explanation, row):
        """
        Generate natural language explanation
        
        Args:
            explanation (dict): Explanation data
            row (Series): Transaction data
            
        Returns:
            str: Text explanation
        """
        try:
            # Start with risk level
            risk_score = explanation['risk_score']
            
            if risk_score >= 0.9:
                risk_level = "critical"
            elif risk_score >= 0.7:
                risk_level = "high"
            elif risk_score >= 0.5:
                risk_level = "medium"
            else:
                risk_level = "low"
            
            text = f"This transaction has a {risk_level} risk score of {risk_score:.2f}. "
            
            # Add top contributing factors
            if explanation['top_factors']:
                top_factors = explanation['top_factors'][:3]  # Top 3 factors
                factor_names = [factor[0].split('_')[0] for factor in top_factors]
                
                if len(factor_names) == 1:
                    text += f"The primary factor is {factor_names[0]}. "
                elif len(factor_names) == 2:
                    text += f"The main factors are {factor_names[0]} and {factor_names[1]}. "
                else:
                    text += f"The main factors are {', '.join(factor_names[:-1])}, and {factor_names[-1]}. "
            
            # Add rule violations
            if explanation['rule_violations']:
                if len(explanation['rule_violations']) == 1:
                    text += f"It violates the rule: {explanation['rule_violations'][0]}. "
                else:
                    text += f"It violates {len(explanation['rule_violations'])} rules: {', '.join(explanation['rule_violations'][:3])}. "
            
            # Add transaction details
            if 'amount' in row:
                text += f"The transaction amount is {row['amount']}. "
            
            if 'sender_id' in row and 'receiver_id' in row:
                text += f"It involves sender {row['sender_id']} and receiver {row['receiver_id']}. "
            
            if 'timestamp' in row:
                text += f"The transaction occurred at {row['timestamp']}. "
            
            # Add AI-generated explanation if APIs are available
            ai_explanation = ""
            if is_api_available('gemini'):
                # Here you would call Gemini API to generate explanation
                # For now, add a placeholder
                ai_explanation = "AI analysis pending implementation. "
            elif is_api_available('openai'):
                # Here you would call OpenAI API to generate explanation
                # For now, add a placeholder
                ai_explanation = "AI analysis pending implementation. "
            
            if ai_explanation:
                text += f"AI analysis: {ai_explanation}"
            
            # Add recommendation
            if risk_score >= 0.7:
                text += "This transaction should be reviewed immediately."
            elif risk_score >= 0.5:
                text += "This transaction should be reviewed."
            else:
                text += "This transaction appears to be low risk."
            
            return text
            
        except Exception as e:
            logger.error(f"Error generating text explanation: {str(e)}")
            return "Unable to generate explanation."
    
    def get_explanation(self, transaction_id):
        """
        Get explanation for a specific transaction
        
        Args:
            transaction_id (str): Transaction ID
            
        Returns:
            dict: Explanation for the transaction
        """
        if not self.fitted:
            raise ValueError("Explanations not generated. Call generate_explanations first.")
        
        # Find explanation by transaction ID
        for idx, explanation in self.explanations.items():
            if explanation.get('transaction_id') == transaction_id:
                return explanation
        
        return None
    
    def plot_feature_importance(self, model_name=None, top_n=20):
        """
        Plot feature importance for a model
        
        Args:
            model_name (str, optional): Name of the model
            top_n (int): Number of top features to show
        """
        try:
            # Aggregate feature contributions across all explanations
            feature_contributions = {}
            
            for explanation in self.explanations.values():
                if 'feature_contributions' in explanation:
                    for feature, contribution in explanation['feature_contributions'].items():
                        if model_name is None or model_name in feature:
                            feature_contributions[feature] = feature_contributions.get(feature, 0) + abs(contribution)
            
            if not feature_contributions:
                logger.warning("No feature contributions found")
                return None
            
            # Create DataFrame
            importance_df = pd.DataFrame({
                'feature': list(feature_contributions.keys()),
                'importance': list(feature_contributions.values())
            }).sort_values('importance', ascending=False)
            
            # Get top features
            top_features = importance_df.head(top_n)
            
            # Create plot
            plt.figure(figsize=(10, 8))
            sns.barplot(x='importance', y='feature', data=top_features)
            plt.title(f'Top {top_n} Feature Importance')
            plt.tight_layout()
            
            return plt.gcf()
            
        except Exception as e:
            logger.error(f"Error plotting feature importance: {str(e)}")
            raise
    
    def plot_shap_summary(self, model_name, X):
        """
        Plot SHAP summary for a model
        
        Args:
            model_name (str): Name of the model
            X (DataFrame): Feature data
        """
        try:
            # This would require access to the model and SHAP values
            # For demonstration, we'll create a placeholder plot
            
            plt.figure(figsize=(10, 8))
            plt.title(f'SHAP Summary for {model_name}')
            plt.text(0.5, 0.5, 'SHAP summary plot would be displayed here', 
                    ha='center', va='center', fontsize=12)
            plt.tight_layout()
            
            return plt.gcf()
            
        except Exception as e:
            logger.error(f"Error plotting SHAP summary: {str(e)}")
            raise
    
    def plot_decision_path(self, model_name, X, idx):
        """
        Plot decision path for a tree-based model
        
        Args:
            model_name (str): Name of the model
            X (DataFrame): Feature data
            idx (int): Transaction index
        """
        try:
            # This would require access to the model
            # For demonstration, we'll create a placeholder plot
            
            plt.figure(figsize=(10, 8))
            plt.title(f'Decision Path for {model_name}')
            plt.text(0.5, 0.5, 'Decision path would be displayed here', 
                    ha='center', va='center', fontsize=12)
            plt.tight_layout()
            
            return plt.gcf()
            
        except Exception as e:
            logger.error(f"Error plotting decision path: {str(e)}")
            raise
    
    def get_counterfactual_explanation(self, idx, X, model_name, target_class=0):
        """
        Generate counterfactual explanation
        
        Args:
            idx (int): Transaction index
            X (DataFrame): Feature data
            model_name (str): Name of the model
            target_class (int): Target class (0 for non-fraud, 1 for fraud)
            
        Returns:
            dict: Counterfactual explanation
        """
        try:
            # This is a simplified implementation
            # In practice, you would use more sophisticated methods
            
            # Get original prediction
            original_row = X.loc[idx:idx+1]
            
            # Generate counterfactual by modifying features
            counterfactual = original_row.copy()
            
            # Modify top features to change prediction
            # This is a placeholder implementation
            for col in counterfactual.columns:
                if col in ['amount']:
                    # Reduce amount by 50%
                    counterfactual[col] = counterfactual[col] * 0.5
            
            return {
                'original': original_row.to_dict('records')[0],
                'counterfactual': counterfactual.to_dict('records')[0],
                'changes': {
                    'amount': 'Reduced by 50%'
                }
            }
            
        except Exception as e:
            logger.error(f"Error generating counterfactual explanation: {str(e)}")
            return {}

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\analysis\.ipynb_checkpoints\risk_scorer-checkpoint.py ===
"""
Risk Scorer Module
Implements risk scoring for fraud detection
"""
import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
from sklearn.calibration import CalibratedClassifierCV
import warnings
import logging
from typing import Dict, List, Tuple, Union
warnings.filterwarnings('ignore')
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class RiskScorer:
    """
    Class for calculating risk scores from multiple models
    Implements weighted combination of algorithm outputs
    """
    
    def __init__(self, method='weighted_average', custom_weights=None):
        """
        Initialize RiskScorer
        
        Args:
            method (str): Scoring method ('weighted_average', 'maximum', 'custom')
            custom_weights (dict, optional): Custom weights for models
        """
        # Convert method to lowercase and replace spaces with underscores
        self.method = method.lower().replace(" ", "_")
        self.custom_weights = custom_weights or {
            'unsupervised': 0.4,
            'supervised': 0.4,
            'rule': 0.2
        }
        self.scalers = {}
        self.calibrators = {}
        self.fitted = False
    
    def calculate_scores(self, model_results, df):
        """
        Calculate risk scores from model results
        
        Args:
            model_results (dict): Results from different models
            df (DataFrame): Original data
            
        Returns:
            DataFrame: Risk scores
        """
        try:
            # Initialize result DataFrame
            result_df = df.copy()
            
            # Extract scores from each model type
            unsupervised_scores = None
            supervised_scores = None
            rule_scores = None
            
            # Process unsupervised model results
            if 'unsupervised' in model_results:
                unsupervised_scores = self._process_unsupervised_results(model_results['unsupervised'])
            
            # Process supervised model results
            if 'supervised' in model_results:
                supervised_scores = self._process_supervised_results(model_results['supervised'])
            
            # Process rule-based results
            if 'rule' in model_results:
                rule_scores = self._process_rule_results(model_results['rule'])
            
            # Combine scores based on method
            if self.method == 'weighted_average':
                combined_scores = self._weighted_average_combination(
                    unsupervised_scores, supervised_scores, rule_scores
                )
            elif self.method == 'maximum':
                combined_scores = self._maximum_combination(
                    unsupervised_scores, supervised_scores, rule_scores
                )
            elif self.method == 'custom':
                combined_scores = self._custom_combination(
                    unsupervised_scores, supervised_scores, rule_scores
                )
            else:
                raise ValueError(f"Unknown scoring method: {self.method}")
            
            # Add combined scores to result
            result_df['risk_score'] = combined_scores
            
            # Add individual model scores
            if unsupervised_scores is not None:
                result_df['unsupervised_score'] = unsupervised_scores
            
            if supervised_scores is not None:
                result_df['supervised_score'] = supervised_scores
            
            if rule_scores is not None:
                result_df['rule_score'] = rule_scores
            
            # Calculate percentile rank
            result_df['risk_percentile'] = result_df['risk_score'].rank(pct=True)
            
            # Calculate risk level
            result_df['risk_level'] = pd.cut(
                result_df['risk_percentile'],
                bins=[0, 0.7, 0.9, 0.95, 1.0],
                labels=['Low', 'Medium', 'High', 'Critical']
            )
            
            self.fitted = True
            return result_df
            
        except Exception as e:
            logger.error(f"Error calculating risk scores: {str(e)}")
            raise
    
    def _process_unsupervised_results(self, unsupervised_results):
        """
        Process unsupervised model results
        
        Args:
            unsupervised_results (dict): Results from unsupervised models
            
        Returns:
            array: Combined unsupervised scores
        """
        try:
            # Collect scores from all unsupervised models
            all_scores = []
            model_names = []
            
            for model_name, model_data in unsupervised_results.items():
                if 'scores' in model_data:
                    scores = model_data['scores']
                    all_scores.append(scores)
                    model_names.append(model_name)
            
            if not all_scores:
                return None
            
            # Convert to numpy array
            scores_array = np.array(all_scores).T
            
            # Calculate average score
            avg_scores = np.mean(scores_array, axis=1)
            
            # Normalize to 0-1 range
            scaler = MinMaxScaler()
            normalized_scores = scaler.fit_transform(avg_scores.reshape(-1, 1)).flatten()
            
            # Store scaler
            self.scalers['unsupervised'] = scaler
            
            return normalized_scores
            
        except Exception as e:
            logger.error(f"Error processing unsupervised results: {str(e)}")
            raise
    
    def _process_supervised_results(self, supervised_results):
        """
        Process supervised model results
        
        Args:
            supervised_results (dict): Results from supervised models
            
        Returns:
            array: Combined supervised scores
        """
        try:
            # Collect probabilities from all supervised models
            all_probabilities = []
            model_names = []
            
            for model_name, model_data in supervised_results.items():
                if 'probabilities' in model_data:
                    probabilities = model_data['probabilities']
                    all_probabilities.append(probabilities)
                    model_names.append(model_name)
            
            if not all_probabilities:
                return None
            
            # Convert to numpy array
            probabilities_array = np.array(all_probabilities).T
            
            # Calculate average probability
            avg_probabilities = np.mean(probabilities_array, axis=1)
            
            # Apply calibration if available
            if 'supervised' in self.calibrators:
                calibrated_scores = self.calibrators['supervised'].predict_proba(avg_probabilities.reshape(-1, 1))[:, 1]
            else:
                calibrated_scores = avg_probabilities
            
            # Store calibrator
            self.calibrators['supervised'] = CalibratedClassifierCV(
                method='isotonic', cv='prefit'
            )
            
            return calibrated_scores
            
        except Exception as e:
            logger.error(f"Error processing supervised results: {str(e)}")
            raise
    
    def _process_rule_results(self, rule_results):
        """
        Process rule-based results
        
        Args:
            rule_results (dict): Results from rule engine
            
        Returns:
            array: Combined rule scores
        """
        try:
            if 'normalized_scores' in rule_results:
                rule_scores = rule_results['normalized_scores']
                # Ensure rule_scores is a numpy array
                if not isinstance(rule_scores, np.ndarray):
                    rule_scores = np.array(rule_scores)
            else:
                return None
            
            # No calibration for now to avoid the reshape issue
            calibrated_scores = rule_scores
            
            # Store calibrator
            self.calibrators['rule'] = CalibratedClassifierCV(
                method='isotonic', cv='prefit'
            )
            
            return calibrated_scores
            
        except Exception as e:
            logger.error(f"Error processing rule results: {str(e)}")
            raise
    
    def _weighted_average_combination(self, unsupervised_scores, supervised_scores, rule_scores):
        """
        Combine scores using weighted average
        
        Args:
            unsupervised_scores (array): Unsupervised model scores
            supervised_scores (array): Supervised model scores
            rule_scores (array): Rule-based scores
            
        Returns:
            array: Combined scores
        """
        try:
            # Get weights
            unsupervised_weight = self.custom_weights.get('unsupervised', 0.4)
            supervised_weight = self.custom_weights.get('supervised', 0.4)
            rule_weight = self.custom_weights.get('rule', 0.2)
            
            # Normalize weights based on available scores
            available_weights = []
            if unsupervised_scores is not None:
                available_weights.append(unsupervised_weight)
            if supervised_scores is not None:
                available_weights.append(supervised_weight)
            if rule_scores is not None:
                available_weights.append(rule_weight)
            
            total_weight = sum(available_weights)
            if total_weight > 0:
                if unsupervised_scores is not None:
                    unsupervised_weight /= total_weight
                if supervised_scores is not None:
                    supervised_weight /= total_weight
                if rule_scores is not None:
                    rule_weight /= total_weight
            
            # Determine the length of the combined scores array
            if unsupervised_scores is not None:
                length = len(unsupervised_scores)
            elif supervised_scores is not None:
                length = len(supervised_scores)
            elif rule_scores is not None:
                length = len(rule_scores)
            else:
                # If no scores available, return empty array
                return np.array([])
            
            # Initialize combined scores
            combined_scores = np.zeros(length)
            
            # Add weighted scores
            if unsupervised_scores is not None:
                # Ensure unsupervised_scores is a numpy array
                if not isinstance(unsupervised_scores, np.ndarray):
                    unsupervised_scores = np.array(unsupervised_scores)
                combined_scores += unsupervised_weight * unsupervised_scores
            
            if supervised_scores is not None:
                # Ensure supervised_scores is a numpy array
                if not isinstance(supervised_scores, np.ndarray):
                    supervised_scores = np.array(supervised_scores)
                combined_scores += supervised_weight * supervised_scores
            
            if rule_scores is not None:
                # Ensure rule_scores is a numpy array
                if not isinstance(rule_scores, np.ndarray):
                    rule_scores = np.array(rule_scores)
                combined_scores += rule_weight * rule_scores
            
            return combined_scores
            
        except Exception as e:
            logger.error(f"Error in weighted average combination: {str(e)}")
            raise
    
    def _maximum_combination(self, unsupervised_scores, supervised_scores, rule_scores):
        """
        Combine scores using maximum
        
        Args:
            unsupervised_scores (array): Unsupervised model scores
            supervised_scores (array): Supervised model scores
            rule_scores (array): Rule-based scores
            
        Returns:
            array: Combined scores
        """
        try:
            # Collect all available scores
            all_scores = []
            
            if unsupervised_scores is not None:
                all_scores.append(unsupervised_scores)
            
            if supervised_scores is not None:
                all_scores.append(supervised_scores)
            
            if rule_scores is not None:
                all_scores.append(rule_scores)
            
            if not all_scores:
                return np.array([])
            
            # Take maximum score for each transaction
            combined_scores = np.max(all_scores, axis=0)
            
            return combined_scores
            
        except Exception as e:
            logger.error(f"Error in maximum combination: {str(e)}")
            raise
    
    def _custom_combination(self, unsupervised_scores, supervised_scores, rule_scores):
        """
        Combine scores using custom method
        
        Args:
            unsupervised_scores (array): Unsupervised model scores
            supervised_scores (array): Supervised model scores
            rule_scores (array): Rule-based scores
            
        Returns:
            array: Combined scores
        """
        try:
            # Get custom weights
            unsupervised_weight = self.custom_weights.get('unsupervised', 0.4)
            supervised_weight = self.custom_weights.get('supervised', 0.4)
            rule_weight = self.custom_weights.get('rule', 0.2)
            
            # Apply non-linear transformation to supervised scores
            if supervised_scores is not None:
                # Emphasize high scores
                supervised_transformed = np.power(supervised_scores, 1.5)
            else:
                supervised_transformed = None
            
            # Apply non-linear transformation to rule scores
            if rule_scores is not None:
                # Emphasize high scores even more
                rule_transformed = np.power(rule_scores, 2.0)
            else:
                rule_transformed = None
            
            # Combine using weighted average with transformed scores
            return self._weighted_average_combination(
                unsupervised_scores, supervised_transformed, rule_transformed
            )
            
        except Exception as e:
            logger.error(f"Error in custom combination: {str(e)}")
            raise
    
    def update_weights(self, new_weights):
        """
        Update the weights for combining scores
        
        Args:
            new_weights (dict): New weights for models
        """
        self.custom_weights.update(new_weights)
        logger.info(f"Updated weights: {self.custom_weights}")
    
    def get_weights(self):
        """
        Get current weights
        
        Returns:
            dict: Current weights
        """
        return self.custom_weights.copy()
    
    def calibrate_scores(self, method='isotonic'):
        """
        Calibrate scores to better reflect true probabilities
        
        Args:
            method (str): Calibration method ('isotonic', 'sigmoid')
        """
        try:
            # Update calibrators
            for model_type in ['unsupervised', 'supervised', 'rule']:
                if model_type in self.calibrators:
                    self.calibrators[model_type] = CalibratedClassifierCV(
                        method=method, cv='prefit'
                    )
            
            logger.info(f"Updated calibrators with method: {method}")
            
        except Exception as e:
            logger.error(f"Error calibrating scores: {str(e)}")
            raise

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\analysis\.ipynb_checkpoints\__init__-checkpoint.py ===

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\analysis\__pycache__\explainability.cpython-39.pyc ===
a
    èöhUb  „                   @   s–   d Z ddlZddlZddlZddlmZ ddl	Z
ddlmZmZ ddlmZ ddlmZmZmZ ddlZddlZddlZddlmZmZmZmZ ddlmZ e†d° ejej d	ç e†!e"°Z#G d
dÑ dÉZ$dS )zP
Explainability Module
Implements explainable AI techniques for fraud detection
È    N)⁄permutation_importance⁄partial_dependence)⁄LogisticRegression)⁄DecisionTreeClassifier⁄export_text⁄export_graphviz)⁄Dict⁄List⁄Tuple⁄Union)⁄is_api_available⁄ignore)⁄levelc                   @   sv   e Zd ZdZdddÑZddÑ ZddÑ Zd	d
Ñ ZddÑ ZddÑ Z	ddÑ Z
ddÑ ZdddÑZddÑ ZddÑ ZdddÑZdS ) ⁄Explainabilityzä
    Class for generating explanations for fraud detection predictions
    Implements SHAP, LIME, and other explainability techniques
    Nc                 C   s&   |pi | _ i | _i | _i | _d| _dS )zÄ
        Initialize Explainability
        
        Args:
            config (dict, optional): Configuration parameters
        FN)⁄config⁄
explainers⁄explanations⁄feature_names⁄fitted)⁄selfr   © r   ˙mC:\Users\Tranquil Abyss\fraud-detection-platform\app\..\src\fraud_detection_engine\analysis\explainability.py⁄__init__   s
    
zExplainability.__init__c                 C   s  z÷ddÑ |j D É}|| †d°}|| _i }|†° D ]|\}}z4|j|df }	|	dkrl| †||||	|°}
|
||< W q2 ty¨ } z&t†d|õ dt	|Éõ ù° W Y d}~q2d}~0 0 q2|| _
d	| _t†d
t|Éõ dù° |W S  têy } z"t†dt	|Éõ ù° Ç W Y d}~n
d}~0 0 dS )a=  
        Generate explanations for all transactions
        
        Args:
            df (DataFrame): Input data
            risk_scores (DataFrame): Risk scores
            model_results (dict): Results from different models
            
        Returns:
            dict: Explanations for each transaction
        c                 S   s   g | ]}|d vr|ëqS ))⁄transaction_id⁄	sender_id⁄receiver_id⁄
fraud_flagr   )⁄.0⁄colr   r   r   ⁄
<listcomp>:   s   
ˇz8Explainability.generate_explanations.<locals>.<listcomp>r   ⁄
risk_scoreÁ      ‡?˙-Error generating explanation for transaction ˙: NTzGenerated explanations for z transactionszError generating explanations: )⁄columns⁄fillnar   ⁄iterrows⁄loc⁄!_generate_transaction_explanation⁄	Exception⁄logger⁄warning⁄strr   r   ⁄info⁄len⁄error)r   ⁄df⁄risk_scores⁄model_results⁄feature_cols⁄Xr   ⁄idx⁄rowr    ⁄explanation⁄er   r   r   ⁄generate_explanations,   s,    
ˇ2z$Explainability.generate_explanationsc              
   C   s˙  êzö|† d|°|g g i i ddú}i }d|v rê| †|||d °}|†|° |d †° D ]:\}	}
d|
v rT|t|
d Ék rT|
d | |d d|	õ ù< qTd|v r¸| †|||d °}|†|° |d †° D ]:\}	}
d	|
v r¿|t|
d	 Ék r¿|
d	 | |d d
|	õ ù< q¿d|v êrZ| †|||d °}|†|° d|d v êrZ|t|d d Ék êrZ|d d | |d< ||d< t|†° ddÑ ddçddÖ }||d< | †||°|d< |W S  t	êyÙ } z>t
†d|õ dt|Éõ ù° |† d|°|t|ÉdúW  Y d}~S d}~0 0 dS )a}  
        Generate explanation for a single transaction
        
        Args:
            idx (int): Transaction index
            row (Series): Transaction data
            X (DataFrame): Feature data
            risk_score (float): Risk score
            model_results (dict): Model results
            
        Returns:
            dict: Explanation for the transaction
        r   ⁄ )r   r    ⁄top_factors⁄rule_violations⁄model_predictions⁄feature_contributions⁄text_explanation⁄unsupervised⁄scoresr=   ⁄unsupervised_⁄
supervised⁄probabilities⁄supervised_⁄rule⁄violated_rule_namesr<   r>   c                 S   s   t | d ÉS )NÈ   )⁄abs)⁄xr   r   r   ⁄<lambda>§   Û    zBExplainability._generate_transaction_explanation.<locals>.<lambda>T)⁄key⁄reverseNÈ
   r;   r?   r"   r#   )r   r    r/   )⁄get⁄_get_unsupervised_contributions⁄update⁄itemsr.   ⁄_get_supervised_contributions⁄_get_rule_contributions⁄sorted⁄_generate_text_explanationr)   r*   r/   r,   )r   r5   r6   r4   r    r2   r7   r>   Zunsupervised_contributions⁄
model_name⁄
model_dataZsupervised_contributionsZrule_contributionsr;   r8   r   r   r   r(   _   sh    
˘
ˇ

ˇ


ˇ
$˝¸ˇ
˝z0Explainability._generate_transaction_explanationc              
   C   s÷  êzêi }|† ° D ê]z\}}d|v rd|v r|d }|d }|j||d Ö|f }	|dkröt|dÉrò|j}
t|ÉD ]&\}}|t|
Ék rp|
| ||õ dù< qpq|dkêr$t|dÉêråt|d	Éêrå|j| }t|ÉD ]R\}}||	jd k rŒ|	jd
|f }t	†
||jddÖ|f †°  °}|||õ dù< qŒq|dkrt|dÉr|†|	°}t	†|	j| d°}t|ÉD ]0\}}||jd k êrZ|d
|f ||õ dù< êqZq|W S  têy– } z$t†dt|Éõ ù° i W  Y d}~S d}~0 0 dS )a6  
        Get feature contributions from unsupervised models
        
        Args:
            idx (int): Transaction index
            X (DataFrame): Feature data
            unsupervised_results (dict): Unsupervised model results
            
        Returns:
            dict: Feature contributions
        ⁄modelr   rH   ⁄isolation_forest⁄feature_importances_Z_isolation_forest⁄local_outlier_factor⁄_fit_X⁄
_distancesr   N⁄_lof⁄autoencoder⁄predictÈ   Z_autoencoderz*Error getting unsupervised contributions: )rS   r'   ⁄hasattrr\   ⁄	enumerater.   r_   ⁄shape⁄iloc⁄nprI   r^   ⁄meanrb   ⁄power⁄valuesr)   r*   r/   r,   )r   r5   r4   ⁄unsupervised_results⁄contributionsrX   rY   rZ   r   rJ   ⁄feature_importance⁄i⁄feature⁄	distances⁄feature_valueZfeature_diffZreconstructionZerror_per_featurer8   r   r   r   rQ   π   s@    


 

z.Explainability._get_unsupervised_contributionsc              
   C   s^  êzi }|† ° D ê]\}}d|v rd|v r|d }|d }|j||d Ö|f }	d|v r–|d }
t|
tÉrít|
ÉdkrÑ|
d | }qö|
d | }n|
| }t|ÉD ]*\}}|t|Ék r¢|| ||õ d|õ ù< q¢qd|v r|d }|†° D ]*\}}|d }|d	 }|||õ d|õ ù< qËq|W S  têyX } z$t†	d
t
|Éõ ù° i W  Y d}~S d}~0 0 dS )a0  
        Get feature contributions from supervised models
        
        Args:
            idx (int): Transaction index
            X (DataFrame): Feature data
            supervised_results (dict): Supervised model results
            
        Returns:
            dict: Feature contributions
        rZ   r   rH   ⁄shap_valuesr   ⁄_rn   rp   ⁄
importancez(Error getting supervised contributions: N)rS   r'   ⁄
isinstance⁄listr.   re   r&   r)   r*   r/   r,   )r   r5   r4   ⁄supervised_resultsrm   rX   rY   rZ   r   rJ   rs   Z	shap_valsro   rp   ⁄importance_dfrt   r6   ru   r8   r   r   r   rT   ˆ   s6    
z,Explainability._get_supervised_contributionsc                 C   s4  zi }d|v rÏ|t |d Ék rÏ|d | }|†di °}dgddgddgdgddgddgdgddgddgdgdgdgdgdgddgddgddgddgd	ú}|D ]N}|†|d
°}	|†|g °}
|
D ],}||v rº|†|õ dùd°|	 ||õ dù< qºqú|W S  têy. } z$t†dt|Éõ ù° i W  Y d}~S d}~0 0 dS )a&  
        Get feature contributions from rule-based model
        
        Args:
            idx (int): Transaction index
            row (Series): Transaction data
            rule_results (dict): Rule model results
            
        Returns:
            dict: Feature contributions
        rG   ⁄rule_weights⁄amountr   r   ⁄	timestamp⁄sender_location⁄receiver_location)⁄high_amount⁄unusual_amount_for_sender⁄unusual_amount_for_receiver⁄round_amount⁄high_frequency_sender⁄high_frequency_receiver⁄rapid_succession⁄cross_border⁄high_risk_country⁄unusual_location_for_sender⁄unusual_hour⁄weekend⁄
new_sender⁄new_receiver⁄sanctions_check⁄tax_compliance⁄bank_verification⁄identity_verificationg      ?⁄_ruler   z"Error getting rule contributions: N)r.   rP   r)   r*   r/   r,   )r   r5   r6   ⁄rule_resultsrm   ⁄violated_rulesrz   Zrule_to_featuresrF   ⁄weight⁄featuresrp   r8   r   r   r   rU   ,  sD    Ó$z&Explainability._get_rule_contributionsc           
   
   C   sj  êz&|d }|dkrd}n |dkr(d}n|dkr6d}nd}d	|õ d
|dõdù}|d rÍ|d ddÖ }ddÑ |D É}t |Édkrñ|d|d õ dù7 }nTt |Édkr¬|d|d õ d|d õ dù7 }n(|dd†|ddÖ °õ d|d õ dù7 }|d êrPt |d Édkêr |d|d d õ dù7 }n0|dt |d Éõ dd†|d ddÖ °õ dù7 }d|v êrn|d |d õ dù7 }d!|v êr†d"|v êr†|d#|d! õ d$|d" õ dù7 }d%|v êræ|d&|d% õ dù7 }d'}td(Éêr“d)}ntd*Éêr‡d)}|êrÙ|d+|õ ù7 }|dkêr|d,7 }n|dkêr|d-7 }n|d.7 }|W S  têyd }	 z"t†d/t|	Éõ ù° W Y d}	~	d0S d}	~	0 0 dS )1zÍ
        Generate natural language explanation
        
        Args:
            explanation (dict): Explanation data
            row (Series): Transaction data
            
        Returns:
            str: Text explanation
        r    gÕÃÃÃÃÃÏ?⁄criticalgffffffÊ?⁄highr!   ⁄medium⁄lowzThis transaction has a z risk score of z.2fz. r;   NÈ   c                 S   s   g | ]}|d  † d°d  ëqS )r   rt   )⁄split)r   ⁄factorr   r   r   r   Ñ  rL   z=Explainability._generate_text_explanation.<locals>.<listcomp>rH   zThe primary factor is r   rc   zThe main factors are z and z, Èˇˇˇˇz, and r<   zIt violates the rule: zIt violates z rules: r{   zThe transaction amount is r   r   zIt involves sender z and receiver r|   zThe transaction occurred at r:   ⁄geminiz$AI analysis pending implementation. ⁄openaizAI analysis: z0This transaction should be reviewed immediately.z$This transaction should be reviewed.z(This transaction appears to be low risk.z#Error generating text explanation: zUnable to generate explanation.)r.   ⁄joinr   r)   r*   r/   r,   )
r   r7   r6   r    ⁄
risk_level⁄textr;   Zfactor_namesZai_explanationr8   r   r   r   rW   g  sX     (
0







z)Explainability._generate_text_explanationc                 C   s<   | j stdÉÇ| j†° D ]\}}|†d°|kr|  S qdS )z‘
        Get explanation for a specific transaction
        
        Args:
            transaction_id (str): Transaction ID
            
        Returns:
            dict: Explanation for the transaction
        z=Explanations not generated. Call generate_explanations first.r   N)r   ⁄
ValueErrorr   rS   rP   )r   r   r5   r7   r   r   r   ⁄get_explanation∫  s    

zExplainability.get_explanationÈ   c           
   
   C   s$  z‚i }| j †° D ]J}d|v r|d †° D ]0\}}|du s@||v r(|†|d°t|É ||< q(q|spt†d° W dS t†t	|†
° Ét	|†° Édú°jdddç}|†|°}tjd	d
ç tjdd|dç t†d|õ dù° t†°  t†° W S  têy }	 z"t†dt|	Éõ ù° Ç W Y d}	~	n
d}	~	0 0 dS )zæ
        Plot feature importance for a model
        
        Args:
            model_name (str, optional): Name of the model
            top_n (int): Number of top features to show
        r>   Nr   zNo feature contributions found)rp   ru   ru   F)⁄	ascending©rO   È   ©⁄figsizerp   )rJ   ⁄y⁄datazTop z Feature Importancez#Error plotting feature importance: )r   rk   rS   rP   rI   r*   r+   ⁄pd⁄	DataFramerw   ⁄keys⁄sort_values⁄head⁄plt⁄figure⁄sns⁄barplot⁄title⁄tight_layout⁄gcfr)   r/   r,   )
r   rX   ⁄top_nr>   r7   rp   ⁄contributionry   ⁄top_featuresr8   r   r   r   ⁄plot_feature_importanceŒ  s2    


˛˝

z&Explainability.plot_feature_importancec              
   C   sÑ   zDt jddç t †d|õ ù° t jdddddddç t †°  t †° W S  ty~ } z"t†d	t	|Éõ ù° Ç W Y d
}~n
d
}~0 0 d
S )zû
        Plot SHAP summary for a model
        
        Args:
            model_name (str): Name of the model
            X (DataFrame): Feature data
        rß   r©   zSHAP Summary for r!   z)SHAP summary plot would be displayed here⁄centerÈ   ©⁄ha⁄va⁄fontsizezError plotting SHAP summary: N©
r≤   r≥   r∂   r¢   r∑   r∏   r)   r*   r/   r,   )r   rX   r4   r8   r   r   r   ⁄plot_shap_summary˘  s    
ˇ
z Explainability.plot_shap_summaryc              
   C   sÑ   zDt jddç t †d|õ ù° t jdddddddç t †°  t †° W S  ty~ } z"t†d	t	|Éõ ù° Ç W Y d
}~n
d
}~0 0 d
S )z”
        Plot decision path for a tree-based model
        
        Args:
            model_name (str): Name of the model
            X (DataFrame): Feature data
            idx (int): Transaction index
        rß   r©   zDecision Path for r!   z%Decision path would be displayed hererΩ   ræ   rø   zError plotting decision path: Nr√   )r   rX   r4   r5   r8   r   r   r   ⁄plot_decision_path  s    	
ˇ
z!Explainability.plot_decision_pathr   c           	   
   C   s¶   zd|j ||d Ö }|†° }|jD ]}|dv r"|| d ||< q"|†d°d |†d°d ddidúW S  ty† } z$t†d	t|Éõ ù° i W  Y d
}~S d
}~0 0 d
S )ad  
        Generate counterfactual explanation
        
        Args:
            idx (int): Transaction index
            X (DataFrame): Feature data
            model_name (str): Name of the model
            target_class (int): Target class (0 for non-fraud, 1 for fraud)
            
        Returns:
            dict: Counterfactual explanation
        rH   )r{   r!   ⁄recordsr   r{   zReduced by 50%)⁄original⁄counterfactual⁄changesz-Error generating counterfactual explanation: N)r'   ⁄copyr$   ⁄to_dictr)   r*   r/   r,   )	r   r5   r4   rX   ⁄target_classZoriginal_rowr»   r   r8   r   r   r   ⁄get_counterfactual_explanation*  s    
ˇ˝z-Explainability.get_counterfactual_explanation)N)Nr•   )r   )⁄__name__⁄
__module__⁄__qualname__⁄__doc__r   r9   r(   rQ   rT   rU   rW   r§   rº   rƒ   r≈   rÕ   r   r   r   r   r      s   
3Z=6;S
+r   )%r—   ⁄pandasr≠   ⁄numpyrh   ⁄shap⁄matplotlib.pyplot⁄pyplotr≤   ⁄seabornr¥   ⁄sklearn.inspectionr   r   ⁄sklearn.linear_modelr   Zsklearn.treer   r   r   ⁄graphviz⁄warnings⁄logging⁄typingr   r	   r
   r   ⁄&fraud_detection_engine.utils.api_utilsr   ⁄filterwarnings⁄basicConfig⁄INFO⁄	getLoggerrŒ   r*   r   r   r   r   r   ⁄<module>   s"   



=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\analysis\__pycache__\risk_scorer.cpython-39.pyc ===
a
    èöhπ?  „                   @   sà   d Z ddlZddlZddlmZ ddlmZ ddl	Z	ddl
Z
ddlmZmZmZmZ e	†d° e
je
jdç e
†e°ZG dd	Ñ d	ÉZdS )
z@
Risk Scorer Module
Implements risk scoring for fraud detection
È    N)⁄MinMaxScaler)⁄CalibratedClassifierCV)⁄Dict⁄List⁄Tuple⁄Union⁄ignore)⁄levelc                   @   sl   e Zd ZdZdddÑZddÑ Zdd	Ñ Zd
dÑ ZddÑ ZddÑ Z	ddÑ Z
ddÑ ZddÑ ZddÑ ZdddÑZdS )⁄
RiskScorerzy
    Class for calculating risk scores from multiple models
    Implements weighted combination of algorithm outputs
    ⁄weighted_averageNc                 C   s:   |† ° †dd°| _|p ddddú| _i | _i | _d| _dS )zÿ
        Initialize RiskScorer
        
        Args:
            method (str): Scoring method ('weighted_average', 'maximum', 'custom')
            custom_weights (dict, optional): Custom weights for models
        ˙ ⁄_ÁöôôôôôŸ?Áöôôôôô…?©⁄unsupervised⁄
supervised⁄ruleFN)⁄lower⁄replace⁄method⁄custom_weights⁄scalers⁄calibrators⁄fitted)⁄selfr   r   © r   ˙jC:\Users\Tranquil Abyss\fraud-detection-platform\app\..\src\fraud_detection_engine\analysis\risk_scorer.py⁄__init__   s    	˝zRiskScorer.__init__c           	   
   C   sp  êz,|† ° }d}d}d}d|v r.| †|d °}d|v rD| †|d °}d|v rZ| †|d °}| jdkrt| †|||°}nD| jdkré| †|||°}n*| jdkr®| †|||°}ntd| jõ ùÉÇ||d	< |dur–||d
< |dur‡||d< |dur||d< |d	 j	ddç|d< t
j|d g d¢g d¢dç|d< d| _|W S  têyj } z"t†dt|Éõ ù° Ç W Y d}~n
d}~0 0 dS )z¸
        Calculate risk scores from model results
        
        Args:
            model_results (dict): Results from different models
            df (DataFrame): Original data
            
        Returns:
            DataFrame: Risk scores
        Nr   r   r   r   ⁄maximum⁄customzUnknown scoring method: ⁄
risk_scoreZunsupervised_scoreZsupervised_score⁄
rule_scoreT)⁄pctZrisk_percentile)r   gffffffÊ?gÕÃÃÃÃÃÏ?gffffffÓ?g      ?)⁄Low⁄Medium⁄HighZCritical)⁄bins⁄labelsZ
risk_levelzError calculating risk scores: )⁄copy⁄_process_unsupervised_results⁄_process_supervised_results⁄_process_rule_resultsr   ⁄_weighted_average_combination⁄_maximum_combination⁄_custom_combination⁄
ValueError⁄rank⁄pd⁄cutr   ⁄	Exception⁄logger⁄error⁄str)	r   ⁄model_results⁄df⁄	result_df⁄unsupervised_scores⁄supervised_scores⁄rule_scores⁄combined_scores⁄er   r   r   ⁄calculate_scores)   sT    
ˇ
ˇ
ˇ˝
zRiskScorer.calculate_scoresc              
   C   sŒ   zég }g }|† ° D ],\}}d|v r|d }|†|° |†|° q|sJW dS t†|°j}tj|ddç}tÉ }	|	†|†dd°°†	° }
|	| j
d< |
W S  ty» } z"t†dt|Éõ ù° Ç W Y d}~n
d}~0 0 dS )z„
        Process unsupervised model results
        
        Args:
            unsupervised_results (dict): Results from unsupervised models
            
        Returns:
            array: Combined unsupervised scores
        ⁄scoresNÈ   ©⁄axisÈˇˇˇˇr   z'Error processing unsupervised results: )⁄items⁄append⁄np⁄array⁄T⁄meanr   ⁄fit_transform⁄reshape⁄flattenr   r4   r5   r6   r7   )r   ⁄unsupervised_results⁄
all_scores⁄model_names⁄
model_name⁄
model_datarA   Zscores_arrayZ
avg_scores⁄scaler⁄normalized_scoresr?   r   r   r   r*   w   s&    


z(RiskScorer._process_unsupervised_resultsc              
   C   sÓ   zÆg }g }|† ° D ],\}}d|v r|d }|†|° |†|° q|sJW dS t†|°j}tj|ddç}d| jv rî| jd †|†dd°°ddÖdf }	n|}	t	ddd	ç| jd< |	W S  t
yË }
 z"t†d
t|
Éõ ù° Ç W Y d}
~
n
d}
~
0 0 dS )z€
        Process supervised model results
        
        Args:
            supervised_results (dict): Results from supervised models
            
        Returns:
            array: Combined supervised scores
        ⁄probabilitiesNrB   rC   r   rE   ⁄isotonic⁄prefit©r   ⁄cvz%Error processing supervised results: )rF   rG   rH   rI   rJ   rK   r   ⁄predict_probarM   r   r4   r5   r6   r7   )r   ⁄supervised_results⁄all_probabilitiesrQ   rR   rS   rV   Zprobabilities_arrayZavg_probabilities⁄calibrated_scoresr?   r   r   r   r+   ¢   s,    


&ˇz&RiskScorer._process_supervised_resultsc              
   C   sä   zJd|v r*|d }t |tjÉs0t†|°}nW dS |}tdddç| jd< |W S  tyÑ } z"t†dt	|Éõ ù° Ç W Y d}~n
d}~0 0 dS )z√
        Process rule-based results
        
        Args:
            rule_results (dict): Results from rule engine
            
        Returns:
            array: Combined rule scores
        rU   NrW   rX   rY   r   zError processing rule results: )
⁄
isinstancerH   ⁄ndarrayrI   r   r   r4   r5   r6   r7   )r   ⁄rule_resultsr=   r^   r?   r   r   r   r,   —   s    
ˇz RiskScorer._process_rule_resultsc              
   C   sƒ  êzÄ| j †dd°}| j †dd°}| j †dd°}g }|durD|†|° |durV|†|° |durh|†|° t|É}|dkr®|durà|| }|durò|| }|dur®|| }|dur∫t|É}	n0|durÃt|É}	n|durﬁt|É}	nt†g °W S t†|	°}
|duêr"t|tj	Éêst†|°}|
|| 7 }
|duêrPt|tj	ÉêsDt†|°}|
|| 7 }
|duêr~t|tj	Éêsrt†|°}|
|| 7 }
|
W S  t
êyæ } z"t†dt|Éõ ù° Ç W Y d}~n
d}~0 0 dS )	aD  
        Combine scores using weighted average
        
        Args:
            unsupervised_scores (array): Unsupervised model scores
            supervised_scores (array): Supervised model scores
            rule_scores (array): Rule-based scores
            
        Returns:
            array: Combined scores
        r   r   r   r   r   Nr   z'Error in weighted average combination: )r   ⁄getrG   ⁄sum⁄lenrH   rI   ⁄zerosr_   r`   r4   r5   r6   r7   )r   r;   r<   r=   ⁄unsupervised_weight⁄supervised_weight⁄rule_weightZavailable_weights⁄total_weight⁄lengthr>   r?   r   r   r   r-   Ú   sV    












z(RiskScorer._weighted_average_combinationc              
   C   sû   z^g }|dur|† |° |dur*|† |° |dur<|† |° |sLt†g °W S tj|ddç}|W S  tyò } z"t†dt|Éõ ù° Ç W Y d}~n
d}~0 0 dS )a;  
        Combine scores using maximum
        
        Args:
            unsupervised_scores (array): Unsupervised model scores
            supervised_scores (array): Supervised model scores
            rule_scores (array): Rule-based scores
            
        Returns:
            array: Combined scores
        Nr   rC   zError in maximum combination: )rG   rH   rI   ⁄maxr4   r5   r6   r7   )r   r;   r<   r=   rP   r>   r?   r   r   r   r.   =  s    


zRiskScorer._maximum_combinationc           
   
   C   sÆ   zn| j †dd°}| j †dd°}| j †dd°}|durBt†|d°}nd}|dur\t†|d°}nd}| †|||°W S  ty® }	 z"t†d	t|	Éõ ù° Ç W Y d}	~	n
d}	~	0 0 dS )
aA  
        Combine scores using custom method
        
        Args:
            unsupervised_scores (array): Unsupervised model scores
            supervised_scores (array): Supervised model scores
            rule_scores (array): Rule-based scores
            
        Returns:
            array: Combined scores
        r   r   r   r   r   Ng      ¯?g       @zError in custom combination: )	r   rb   rH   ⁄powerr-   r4   r5   r6   r7   )
r   r;   r<   r=   rf   rg   rh   Zsupervised_transformedZrule_transformedr?   r   r   r   r/   b  s     ˇzRiskScorer._custom_combinationc                 C   s"   | j †|° t†d| j õ ù° dS )zá
        Update the weights for combining scores
        
        Args:
            new_weights (dict): New weights for models
        zUpdated weights: N)r   ⁄updater5   ⁄info)r   ⁄new_weightsr   r   r   ⁄update_weightsã  s    zRiskScorer.update_weightsc                 C   s
   | j †° S )za
        Get current weights
        
        Returns:
            dict: Current weights
        )r   r)   )r   r   r   r   ⁄get_weightsï  s    zRiskScorer.get_weightsrW   c              
   C   sz   z:dD ] }|| j v rt|ddç| j |< qt†d|õ ù° W n: tyt } z"t†dt|Éõ ù° Ç W Y d}~n
d}~0 0 dS )z£
        Calibrate scores to better reflect true probabilities
        
        Args:
            method (str): Calibration method ('isotonic', 'sigmoid')
        r   rX   rY   z!Updated calibrators with method: zError calibrating scores: N)r   r   r5   rn   r4   r6   r7   )r   r   ⁄
model_typer?   r   r   r   ⁄calibrate_scoresû  s    
ˇzRiskScorer.calibrate_scores)r   N)rW   )⁄__name__⁄
__module__⁄__qualname__⁄__doc__r   r@   r*   r+   r,   r-   r.   r/   rp   rq   rs   r   r   r   r   r
      s   
N+/!K%)
	r
   )rw   ⁄pandasr2   ⁄numpyrH   ⁄sklearn.preprocessingr   Zsklearn.calibrationr   ⁄warnings⁄logging⁄typingr   r   r   r   ⁄filterwarnings⁄basicConfig⁄INFO⁄	getLoggerrt   r5   r
   r   r   r   r   ⁄<module>   s   



=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\analysis\__pycache__\__init__.cpython-39.pyc ===
a
    [îëh    „                   @   s   d S )N© r   r   r   ˙gC:\Users\Tranquil Abyss\fraud-detection-platform\app\..\src\fraud_detection_engine\analysis\__init__.py⁄<module>   Û    

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\features\graph_features.py ===
"""
Graph Features Module
Implements graph-based feature extraction techniques for fraud detection
"""

import pandas as pd
import numpy as np
import networkx as nx
from collections import defaultdict, Counter
import community as community_louvain
from sklearn.preprocessing import MinMaxScaler
import warnings
import logging
from typing import Dict, List, Tuple, Union

warnings.filterwarnings('ignore')
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class GraphFeatures:
    """
    Class for extracting graph-based features from transaction data
    Implements techniques like centrality measures, clustering coefficients, etc.
    """
    
    def __init__(self, config=None):
        """
        Initialize GraphFeatures
        
        Args:
            config (dict, optional): Configuration parameters
        """
        self.config = config or {}
        self.graph = None
        self.sender_graph = None
        self.receiver_graph = None
        self.bipartite_graph = None
        self.feature_names = []
        self.scaler = MinMaxScaler()
        self.fitted = False
        
    def extract_features(self, df):
        """
        Extract all graph features from the dataframe
        
        Args:
            df (DataFrame): Input transaction data
            
        Returns:
            DataFrame: DataFrame with extracted features
        """
        try:
            # Create a copy to avoid modifying the original
            result_df = df.copy()
            
            # Build graphs
            self._build_graphs(df)
            
            # Extract different types of graph features
            result_df = self._extract_centrality_features(result_df)
            result_df = self._extract_clustering_features(result_df)
            result_df = self._extract_community_features(result_df)
            result_df = self._extract_path_features(result_df)
            result_df = self._extract_subgraph_features(result_df)
            result_df = self._extract_temporal_graph_features(result_df)
            
            # Store feature names
            self.feature_names = [col for col in result_df.columns if col not in df.columns]
            
            logger.info(f"Extracted {len(self.feature_names)} graph features")
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting graph features: {str(e)}")
            raise
    
    def _build_graphs(self, df):
        """
        Build various graphs from the transaction data
        
        Args:
            df (DataFrame): Input transaction data
        """
        try:
            # Build transaction graph (directed)
            self.graph = nx.DiGraph()
            
            # Add edges with attributes
            for _, row in df.iterrows():
                if 'sender_id' in row and 'receiver_id' in row:
                    sender = row['sender_id']
                    receiver = row['receiver_id']
                    
                    # Get edge attributes
                    attrs = {}
                    if 'amount' in row:
                        attrs['amount'] = row['amount']
                    if 'timestamp' in row:
                        attrs['timestamp'] = row['timestamp']
                    if 'transaction_id' in row:
                        attrs['transaction_id'] = row['transaction_id']
                    
                    # Add edge or update existing edge
                    if self.graph.has_edge(sender, receiver):
                        # Update existing edge
                        edge_data = self.graph[sender][receiver]
                        if 'amount' in attrs:
                            edge_data['total_amount'] = edge_data.get('total_amount', 0) + attrs['amount']
                        edge_data['transaction_count'] = edge_data.get('transaction_count', 0) + 1
                        edge_data['transactions'].append(attrs)
                    else:
                        # Add new edge
                        attrs['total_amount'] = attrs.get('amount', 0)
                        attrs['transaction_count'] = 1
                        attrs['transactions'] = [attrs]
                        self.graph.add_edge(sender, receiver, **attrs)
            
            # Build sender graph (undirected, senders connected if they have common receivers)
            self.sender_graph = nx.Graph()
            
            # Create a mapping from receivers to senders
            receiver_to_senders = defaultdict(set)
            for _, row in df.iterrows():
                if 'sender_id' in row and 'receiver_id' in row:
                    receiver_to_senders[row['receiver_id']].add(row['sender_id'])
            
            # Connect senders with common receivers
            for receiver, senders in receiver_to_senders.items():
                senders_list = list(senders)
                for i in range(len(senders_list)):
                    for j in range(i+1, len(senders_list)):
                        sender1 = senders_list[i]
                        sender2 = senders_list[j]
                        
                        if self.sender_graph.has_edge(sender1, sender2):
                            self.sender_graph[sender1][sender2]['common_receivers'] += 1
                        else:
                            self.sender_graph.add_edge(sender1, sender2, common_receivers=1)
            
            # Build receiver graph (undirected, receivers connected if they have common senders)
            self.receiver_graph = nx.Graph()
            
            # Create a mapping from senders to receivers
            sender_to_receivers = defaultdict(set)
            for _, row in df.iterrows():
                if 'sender_id' in row and 'receiver_id' in row:
                    sender_to_receivers[row['sender_id']].add(row['receiver_id'])
            
            # Connect receivers with common senders
            for sender, receivers in sender_to_receivers.items():
                receivers_list = list(receivers)
                for i in range(len(receivers_list)):
                    for j in range(i+1, len(receivers_list)):
                        receiver1 = receivers_list[i]
                        receiver2 = receivers_list[j]
                        
                        if self.receiver_graph.has_edge(receiver1, receiver2):
                            self.receiver_graph[receiver1][receiver2]['common_senders'] += 1
                        else:
                            self.receiver_graph.add_edge(receiver1, receiver2, common_senders=1)
            
            # Build bipartite graph (senders and receivers as two separate sets)
            self.bipartite_graph = nx.Graph()
            
            # Add nodes with bipartite attribute
            for _, row in df.iterrows():
                if 'sender_id' in row:
                    self.bipartite_graph.add_node(row['sender_id'], bipartite=0)
                if 'receiver_id' in row:
                    self.bipartite_graph.add_node(row['receiver_id'], bipartite=1)
            
            # Add edges
            for _, row in df.iterrows():
                if 'sender_id' in row and 'receiver_id' in row:
                    sender = row['sender_id']
                    receiver = row['receiver_id']
                    
                    # Get edge attributes
                    attrs = {}
                    if 'amount' in row:
                        attrs['amount'] = row['amount']
                    if 'timestamp' in row:
                        attrs['timestamp'] = row['timestamp']
                    if 'transaction_id' in row:
                        attrs['transaction_id'] = row['transaction_id']
                    
                    # Add edge
                    self.bipartite_graph.add_edge(sender, receiver, **attrs)
            
            logger.info("Graphs built successfully")
            
        except Exception as e:
            logger.error(f"Error building graphs: {str(e)}")
            raise
    
    def _extract_centrality_features(self, df):
        """
        Extract centrality-based features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with centrality features
        """
        try:
            result_df = df.copy()
            
            if self.graph is None:
                logger.warning("Graph not built. Skipping centrality features.")
                return result_df
            
            # Calculate degree centrality
            in_degree_centrality = nx.in_degree_centrality(self.graph)
            out_degree_centrality = nx.out_degree_centrality(self.graph)
            degree_centrality = nx.degree_centrality(self.graph.to_undirected())
            
            # Map to dataframe
            if 'sender_id' in df.columns:
                result_df['sender_in_degree_centrality'] = df['sender_id'].map(in_degree_centrality).fillna(0)
                result_df['sender_out_degree_centrality'] = df['sender_id'].map(out_degree_centrality).fillna(0)
                result_df['sender_degree_centrality'] = df['sender_id'].map(degree_centrality).fillna(0)
            
            if 'receiver_id' in df.columns:
                result_df['receiver_in_degree_centrality'] = df['receiver_id'].map(in_degree_centrality).fillna(0)
                result_df['receiver_out_degree_centrality'] = df['receiver_id'].map(out_degree_centrality).fillna(0)
                result_df['receiver_degree_centrality'] = df['receiver_id'].map(degree_centrality).fillna(0)
            
            # Calculate betweenness centrality (sample for large graphs)
            if len(self.graph.nodes()) <= 1000:
                betweenness_centrality = nx.betweenness_centrality(self.graph)
            else:
                # Sample nodes for betweenness calculation
                sample_nodes = list(self.graph.nodes())[:1000]
                betweenness_centrality = nx.betweenness_centrality(self.graph, k=len(sample_nodes))
            
            # Map to dataframe
            if 'sender_id' in df.columns:
                result_df['sender_betweenness_centrality'] = df['sender_id'].map(betweenness_centrality).fillna(0)
            
            if 'receiver_id' in df.columns:
                result_df['receiver_betweenness_centrality'] = df['receiver_id'].map(betweenness_centrality).fillna(0)
            
            # Calculate closeness centrality (sample for large graphs)
            if len(self.graph.nodes()) <= 1000:
                closeness_centrality = nx.closeness_centrality(self.graph)
            else:
                # Use approximate closeness for large graphs
                closeness_centrality = nx.closeness_centrality(self.graph, distance='weight')
            
            # Map to dataframe
            if 'sender_id' in df.columns:
                result_df['sender_closeness_centrality'] = df['sender_id'].map(closeness_centrality).fillna(0)
            
            if 'receiver_id' in df.columns:
                result_df['receiver_closeness_centrality'] = df['receiver_id'].map(closeness_centrality).fillna(0)
            
            # Calculate eigenvector centrality (sample for large graphs)
            if len(self.graph.nodes()) <= 1000:
                try:
                    eigenvector_centrality = nx.eigenvector_centrality(self.graph, max_iter=1000)
                except:
                    eigenvector_centrality = {}
            else:
                eigenvector_centrality = {}
            
            # Map to dataframe
            if 'sender_id' in df.columns:
                result_df['sender_eigenvector_centrality'] = df['sender_id'].map(eigenvector_centrality).fillna(0)
            
            if 'receiver_id' in df.columns:
                result_df['receiver_eigenvector_centrality'] = df['receiver_id'].map(eigenvector_centrality).fillna(0)
            
            # Calculate PageRank
            pagerank = nx.pagerank(self.graph)
            
            # Map to dataframe
            if 'sender_id' in df.columns:
                result_df['sender_pagerank'] = df['sender_id'].map(pagerank).fillna(0)
            
            if 'receiver_id' in df.columns:
                result_df['receiver_pagerank'] = df['receiver_id'].map(pagerank).fillna(0)
            
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting centrality features: {str(e)}")
            return df
    
    def _extract_clustering_features(self, df):
        """
        Extract clustering-based features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with clustering features
        """
        try:
            result_df = df.copy()
            
            if self.graph is None:
                logger.warning("Graph not built. Skipping clustering features.")
                return result_df
            
            # Calculate clustering coefficient for transaction graph
            clustering_coeff = nx.clustering(self.graph.to_undirected())
            
            # Map to dataframe
            if 'sender_id' in df.columns:
                result_df['sender_clustering_coefficient'] = df['sender_id'].map(clustering_coeff).fillna(0)
            
            if 'receiver_id' in df.columns:
                result_df['receiver_clustering_coefficient'] = df['receiver_id'].map(clustering_coeff).fillna(0)
            
            # Calculate clustering coefficient for sender graph
            if self.sender_graph is not None and len(self.sender_graph.nodes()) > 0:
                sender_clustering_coeff = nx.clustering(self.sender_graph)
                
                # Map to dataframe
                if 'sender_id' in df.columns:
                    result_df['sender_graph_clustering_coefficient'] = df['sender_id'].map(sender_clustering_coeff).fillna(0)
            
            # Calculate clustering coefficient for receiver graph
            if self.receiver_graph is not None and len(self.receiver_graph.nodes()) > 0:
                receiver_clustering_coeff = nx.clustering(self.receiver_graph)
                
                # Map to dataframe
                if 'receiver_id' in df.columns:
                    result_df['receiver_graph_clustering_coefficient'] = df['receiver_id'].map(receiver_clustering_coeff).fillna(0)
            
            # Calculate average neighbor degree
            avg_neighbor_degree = nx.average_neighbor_degree(self.graph.to_undirected())
            
            # Map to dataframe
            if 'sender_id' in df.columns:
                result_df['sender_avg_neighbor_degree'] = df['sender_id'].map(avg_neighbor_degree).fillna(0)
            
            if 'receiver_id' in df.columns:
                result_df['receiver_avg_neighbor_degree'] = df['receiver_id'].map(avg_neighbor_degree).fillna(0)
            
            # Calculate square clustering
            square_clustering = nx.square_clustering(self.graph.to_undirected())
            
            # Map to dataframe
            if 'sender_id' in df.columns:
                result_df['sender_square_clustering'] = df['sender_id'].map(square_clustering).fillna(0)
            
            if 'receiver_id' in df.columns:
                result_df['receiver_square_clustering'] = df['receiver_id'].map(square_clustering).fillna(0)
            
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting clustering features: {str(e)}")
            return df
    
    def _extract_community_features(self, df):
        """
        Extract community-based features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with community features
        """
        try:
            result_df = df.copy()
            
            if self.graph is None:
                logger.warning("Graph not built. Skipping community features.")
                return result_df
            
            # Detect communities using Louvain algorithm
            undirected_graph = self.graph.to_undirected()
            communities = community_louvain.best_partition(undirected_graph)
            
            # Map to dataframe
            if 'sender_id' in df.columns:
                result_df['sender_community'] = df['sender_id'].map(communities).fillna(-1)
            
            if 'receiver_id' in df.columns:
                result_df['receiver_community'] = df['receiver_id'].map(communities).fillna(-1)
            
            # Calculate community size for each node
            community_sizes = Counter(communities.values())
            node_community_sizes = {node: community_sizes[comm] for node, comm in communities.items()}
            
            # Map to dataframe
            if 'sender_id' in df.columns:
                result_df['sender_community_size'] = df['sender_id'].map(node_community_sizes).fillna(0)
            
            if 'receiver_id' in df.columns:
                result_df['receiver_community_size'] = df['receiver_id'].map(node_community_sizes).fillna(0)
            
            # Calculate community degree centrality
            community_degree_centrality = {}
            for comm, nodes in community_sizes.items():
                comm_nodes = [node for node, c in communities.items() if c == comm]
                subgraph = undirected_graph.subgraph(comm_nodes)
                if len(subgraph.nodes()) > 0:
                    comm_centrality = nx.degree_centrality(subgraph)
                    for node in comm_nodes:
                        community_degree_centrality[node] = comm_centrality.get(node, 0)
            
            # Map to dataframe
            if 'sender_id' in df.columns:
                result_df['sender_community_degree_centrality'] = df['sender_id'].map(community_degree_centrality).fillna(0)
            
            if 'receiver_id' in df.columns:
                result_df['receiver_community_degree_centrality'] = df['receiver_id'].map(community_degree_centrality).fillna(0)
            
            # Check if sender and receiver are in the same community
            if 'sender_id' in df.columns and 'receiver_id' in df.columns:
                same_community = []
                for _, row in df.iterrows():
                    sender = row['sender_id']
                    receiver = row['receiver_id']
                    
                    if sender in communities and receiver in communities:
                        same_community.append(int(communities[sender] == communities[receiver]))
                    else:
                        same_community.append(0)
                
                result_df['same_community'] = same_community
            
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting community features: {str(e)}")
            return df
    
    def _extract_path_features(self, df):
        """
        Extract path-based features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with path features
        """
        try:
            result_df = df.copy()
            
            if self.graph is None:
                logger.warning("Graph not built. Skipping path features.")
                return result_df
            
            # Calculate shortest path lengths (sample for large graphs)
            if len(self.graph.nodes()) <= 500:
                # For small graphs, calculate all pairs shortest paths
                shortest_paths = dict(nx.all_pairs_shortest_path_length(self.graph))
                
                # Map to dataframe
                if 'sender_id' in df.columns and 'receiver_id' in df.columns:
                    path_lengths = []
                    for _, row in df.iterrows():
                        sender = row['sender_id']
                        receiver = row['receiver_id']
                        
                        if sender in shortest_paths and receiver in shortest_paths[sender]:
                            path_lengths.append(shortest_paths[sender][receiver])
                        else:
                            path_lengths.append(float('inf'))
                    
                    result_df['shortest_path_length'] = path_lengths
            else:
                # For large graphs, sample or use approximation
                if 'sender_id' in df.columns and 'receiver_id' in df.columns:
                    # Use BFS for a limited depth
                    path_lengths = []
                    for _, row in df.iterrows():
                        sender = row['sender_id']
                        receiver = row['receiver_id']
                        
                        try:
                            path_length = nx.shortest_path_length(self.graph, sender, receiver)
                            path_lengths.append(path_length)
                        except:
                            path_lengths.append(float('inf'))
                    
                    result_df['shortest_path_length'] = path_lengths
            
            # Calculate number of common neighbors
            if 'sender_id' in df.columns and 'receiver_id' in df.columns:
                common_neighbors = []
                for _, row in df.iterrows():
                    sender = row['sender_id']
                    receiver = row['receiver_id']
                    
                    try:
                        sender_neighbors = set(self.graph.predecessors(sender)) | set(self.graph.successors(sender))
                        receiver_neighbors = set(self.graph.predecessors(receiver)) | set(self.graph.successors(receiver))
                        common_neighbors.append(len(sender_neighbors & receiver_neighbors))
                    except:
                        common_neighbors.append(0)
                
                result_df['common_neighbors_count'] = common_neighbors
            
            # Calculate Jaccard coefficient
            if 'sender_id' in df.columns and 'receiver_id' in df.columns:
                jaccard_coeffs = []
                for _, row in df.iterrows():
                    sender = row['sender_id']
                    receiver = row['receiver_id']
                    
                    try:
                        sender_neighbors = set(self.graph.predecessors(sender)) | set(self.graph.successors(sender))
                        receiver_neighbors = set(self.graph.predecessors(receiver)) | set(self.graph.successors(receiver))
                        
                        union = sender_neighbors | receiver_neighbors
                        intersection = sender_neighbors & receiver_neighbors
                        
                        if len(union) > 0:
                            jaccard = len(intersection) / len(union)
                        else:
                            jaccard = 0
                        
                        jaccard_coeffs.append(jaccard)
                    except:
                        jaccard_coeffs.append(0)
                
                result_df['jaccard_coefficient'] = jaccard_coeffs
            
            # Calculate Adamic-Adar index
            if 'sender_id' in df.columns and 'receiver_id' in df.columns:
                adamic_adar = []
                for _, row in df.iterrows():
                    sender = row['sender_id']
                    receiver = row['receiver_id']
                    
                    try:
                        sender_neighbors = set(self.graph.predecessors(sender)) | set(self.graph.successors(sender))
                        receiver_neighbors = set(self.graph.predecessors(receiver)) | set(self.graph.successors(receiver))
                        common = sender_neighbors & receiver_neighbors
                        
                        # Calculate sum of 1/log(degree) for common neighbors
                        aa_index = 0
                        for node in common:
                            degree = self.graph.degree(node)
                            if degree > 1:
                                aa_index += 1 / np.log(degree)
                        
                        adamic_adar.append(aa_index)
                    except:
                        adamic_adar.append(0)
                
                result_df['adamic_adar_index'] = adamic_adar
            
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting path features: {str(e)}")
            return df
    
    def _extract_subgraph_features(self, df):
        """
        Extract subgraph-based features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with subgraph features
        """
        try:
            result_df = df.copy()
            
            if self.graph is None:
                logger.warning("Graph not built. Skipping subgraph features.")
                return result_df
            
            # Calculate ego network features for senders
            if 'sender_id' in df.columns:
                sender_ego_sizes = []
                sender_ego_densities = []
                
                for sender in df['sender_id']:
                    try:
                        # Get ego network
                        ego_graph = nx.ego_graph(self.graph.to_undirected(), sender, radius=1)
                        
                        # Calculate size and density
                        ego_size = len(ego_graph.nodes())
                        ego_density = nx.density(ego_graph)
                        
                        sender_ego_sizes.append(ego_size)
                        sender_ego_densities.append(ego_density)
                    except:
                        sender_ego_sizes.append(0)
                        sender_ego_densities.append(0)
                
                result_df['sender_ego_network_size'] = sender_ego_sizes
                result_df['sender_ego_network_density'] = sender_ego_densities
            
            # Calculate ego network features for receivers
            if 'receiver_id' in df.columns:
                receiver_ego_sizes = []
                receiver_ego_densities = []
                
                for receiver in df['receiver_id']:
                    try:
                        # Get ego network
                        ego_graph = nx.ego_graph(self.graph.to_undirected(), receiver, radius=1)
                        
                        # Calculate size and density
                        ego_size = len(ego_graph.nodes())
                        ego_density = nx.density(ego_graph)
                        
                        receiver_ego_sizes.append(ego_size)
                        receiver_ego_densities.append(ego_density)
                    except:
                        receiver_ego_sizes.append(0)
                        receiver_ego_densities.append(0)
                
                result_df['receiver_ego_network_size'] = receiver_ego_sizes
                result_df['receiver_ego_network_density'] = receiver_ego_densities
            
            # Calculate bipartite projection features
            if self.bipartite_graph is not None:
                # Get sender and receiver sets
                senders = {n for n, d in self.bipartite_graph.nodes(data=True) if d['bipartite'] == 0}
                receivers = {n for n, d in self.bipartite_graph.nodes(data=True) if d['bipartite'] == 1}
                
                # Project to sender graph
                sender_projection = nx.bipartite.projected_graph(self.bipartite_graph, senders)
                
                # Calculate features for sender projection
                if 'sender_id' in df.columns:
                    sender_proj_degrees = []
                    for sender in df['sender_id']:
                        try:
                            degree = sender_projection.degree(sender)
                            sender_proj_degrees.append(degree)
                        except:
                            sender_proj_degrees.append(0)
                    
                    result_df['sender_projection_degree'] = sender_proj_degrees
                
                # Project to receiver graph
                receiver_projection = nx.bipartite.projected_graph(self.bipartite_graph, receivers)
                
                # Calculate features for receiver projection
                if 'receiver_id' in df.columns:
                    receiver_proj_degrees = []
                    for receiver in df['receiver_id']:
                        try:
                            degree = receiver_projection.degree(receiver)
                            receiver_proj_degrees.append(degree)
                        except:
                            receiver_proj_degrees.append(0)
                    
                    result_df['receiver_projection_degree'] = receiver_proj_degrees
            
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting subgraph features: {str(e)}")
            return df
    
    def _extract_temporal_graph_features(self, df):
        """
        Extract temporal graph features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with temporal graph features
        """
        try:
            result_df = df.copy()
            
            if self.graph is None or 'timestamp' not in df.columns:
                logger.warning("Graph not built or timestamp not available. Skipping temporal graph features.")
                return result_df
            
            # Convert timestamp to datetime if needed
            if not pd.api.types.is_datetime64_any_dtype(df['timestamp']):
                df['timestamp'] = pd.to_datetime(df['timestamp'])
            
            # Sort by timestamp
            df_sorted = df.sort_values('timestamp')
            
            # Calculate time window features
            time_windows = ['1H', '6H', '24H', '7D']
            
            for window in time_windows:
                # Calculate sender activity in time window
                if 'sender_id' in df.columns:
                    sender_activity = []
                    
                    for _, row in df_sorted.iterrows():
                        sender = row['sender_id']
                        timestamp = row['timestamp']
                        
                        # Get transactions in time window before current transaction
                        window_start = timestamp - pd.Timedelta(window)
                        window_end = timestamp
                        
                        window_transactions = df_sorted[
                            (df_sorted['timestamp'] >= window_start) &
                            (df_sorted['timestamp'] < window_end) &
                            (df_sorted['sender_id'] == sender)
                        ]
                        
                        # Calculate activity metrics
                        activity_count = len(window_transactions)
                        activity_amount = window_transactions['amount'].sum() if 'amount' in window_transactions.columns else 0
                        
                        sender_activity.append({
                            f'sender_activity_count_{window}': activity_count,
                            f'sender_activity_amount_{window}': activity_amount
                        })
                    
                    # Add to result dataframe
                    activity_df = pd.DataFrame(sender_activity)
                    for col in activity_df.columns:
                        result_df[col] = activity_df[col].values
                
                # Calculate receiver activity in time window
                if 'receiver_id' in df.columns:
                    receiver_activity = []
                    
                    for _, row in df_sorted.iterrows():
                        receiver = row['receiver_id']
                        timestamp = row['timestamp']
                        
                        # Get transactions in time window before current transaction
                        window_start = timestamp - pd.Timedelta(window)
                        window_end = timestamp
                        
                        window_transactions = df_sorted[
                            (df_sorted['timestamp'] >= window_start) &
                            (df_sorted['timestamp'] < window_end) &
                            (df_sorted['receiver_id'] == receiver)
                        ]
                        
                        # Calculate activity metrics
                        activity_count = len(window_transactions)
                        activity_amount = window_transactions['amount'].sum() if 'amount' in window_transactions.columns else 0
                        
                        receiver_activity.append({
                            f'receiver_activity_count_{window}': activity_count,
                            f'receiver_activity_amount_{window}': activity_amount
                        })
                    
                    # Add to result dataframe
                    activity_df = pd.DataFrame(receiver_activity)
                    for col in activity_df.columns:
                        result_df[col] = activity_df[col].values
            
            # Calculate time since last transaction for sender
            if 'sender_id' in df.columns:
                time_since_last = []
                
                for _, row in df_sorted.iterrows():
                    sender = row['sender_id']
                    timestamp = row['timestamp']
                    
                    # Get previous transaction from same sender
                    prev_transactions = df_sorted[
                        (df_sorted['timestamp'] < timestamp) &
                        (df_sorted['sender_id'] == sender)
                    ]
                    
                    if len(prev_transactions) > 0:
                        last_timestamp = prev_transactions['timestamp'].max()
                        time_diff = (timestamp - last_timestamp).total_seconds()
                        time_since_last.append(time_diff)
                    else:
                        time_since_last.append(float('inf'))
                
                result_df['sender_time_since_last_transaction'] = time_since_last
            
            # Calculate time since last transaction for receiver
            if 'receiver_id' in df.columns:
                time_since_last = []
                
                for _, row in df_sorted.iterrows():
                    receiver = row['receiver_id']
                    timestamp = row['timestamp']
                    
                    # Get previous transaction to same receiver
                    prev_transactions = df_sorted[
                        (df_sorted['timestamp'] < timestamp) &
                        (df_sorted['receiver_id'] == receiver)
                    ]
                    
                    if len(prev_transactions) > 0:
                        last_timestamp = prev_transactions['timestamp'].max()
                        time_diff = (timestamp - last_timestamp).total_seconds()
                        time_since_last.append(time_diff)
                    else:
                        time_since_last.append(float('inf'))
                
                result_df['receiver_time_since_last_transaction'] = time_since_last
            
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting temporal graph features: {str(e)}")
            return df
    
    def fit_transform(self, df):
        """
        Fit the feature extractor and transform the data
        
        Args:
            df (DataFrame): Input data
            
        Returns:
            DataFrame: Transformed data with features
        """
        # Extract features
        result_df = self.extract_features(df)
        
        # Get feature columns
        feature_cols = [col for col in result_df.columns if col not in df.columns]
        
        if len(feature_cols) > 0:
            # Fit scaler
            self.scaler.fit(result_df[feature_cols])
            
            self.fitted = True
        
        return result_df
    
    def transform(self, df):
        """
        Transform new data using fitted feature extractor
        
        Args:
            df (DataFrame): Input data
            
        Returns:
            DataFrame: Transformed data with features
        """
        if not self.fitted:
            raise ValueError("Feature extractor not fitted. Call fit_transform first.")
        
        # Extract features
        result_df = self.extract_features(df)
        
        # Get feature columns
        feature_cols = [col for col in result_df.columns if col not in df.columns]
        
        if len(feature_cols) > 0:
            # Transform features using fitted scaler
            result_df[feature_cols] = self.scaler.transform(result_df[feature_cols])
        
        return result_df

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\features\nlp_features.py ===
"""
NLP Features Module
Implements natural language processing features for fraud detection
"""

import pandas as pd
import numpy as np
import re
import string
from collections import Counter
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.stem import WordNetLemmatizer, PorterStemmer
from nltk.sentiment import SentimentIntensityAnalyzer
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation
from textblob import TextBlob
import gensim
from gensim.models import Word2Vec, Doc2Vec
from gensim.models.doc2vec import TaggedDocument
import warnings
import logging
from typing import Dict, List, Tuple, Union

from fraud_detection_engine.utils.api_utils import is_api_available

warnings.filterwarnings('ignore')
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Download required NLTK data
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')
    
try:
    nltk.data.find('corpora/stopwords')
except LookupError:
    nltk.download('stopwords')
    
try:
    nltk.data.find('corpora/wordnet')
except LookupError:
    nltk.download('wordnet')
    
try:
    nltk.data.find('sentiment/vader_lexicon')
except LookupError:
    nltk.download('vader_lexicon')

class NLPFeatures:
    """
    Class for extracting NLP features from transaction data
    Implements techniques like sentiment analysis, topic modeling, etc.
    """
    
    def __init__(self, config=None):
        """
        Initialize NLPFeatures
        
        Args:
            config (dict, optional): Configuration parameters
        """
        self.config = config or {}
        self.stop_words = set(stopwords.words('english'))
        self.lemmatizer = WordNetLemmatizer()
        self.stemmer = PorterStemmer()
        self.sia = SentimentIntensityAnalyzer()
        self.tfidf_vectorizer = None
        self.count_vectorizer = None
        self.lda_model = None
        self.word2vec_model = None
        self.doc2vec_model = None
        self.feature_names = []
        self.fitted = False
        
        # Fraud-related keywords
        self.fraud_keywords = [
            'urgent', 'immediately', 'asap', 'hurry', 'quick', 'fast',
            'secret', 'confidential', 'private', 'hidden', 'discreet',
            'suspicious', 'unusual', 'strange', 'odd', 'weird',
            'illegal', 'fraud', 'scam', 'fake', 'counterfeit',
            'money', 'cash', 'payment', 'transfer', 'wire',
            'overseas', 'foreign', 'international', 'abroad',
            'inheritance', 'lottery', 'prize', 'winner', 'claim',
            'verify', 'confirm', 'update', 'account', 'information',
            'click', 'link', 'website', 'login', 'password',
            'bank', 'check', 'routing', 'account', 'number'
        ]
        
        # Suspicious patterns
        self.suspicious_patterns = [
            r'\$\d+,\d+\.\d{2}',  # Money format
            r'\d{4}[\s-]?\d{4}[\s-]?\d{4}[\s-]?\d{4}',  # Credit card pattern
            r'\b\d{3}-\d{2}-\d{4}\b',  # SSN pattern
            r'\b[A-Z0-9._%+-]+@[A-Z0-9.-]+\.[A-Z]{2,}\b',  # Email pattern
            r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+',  # URL pattern
            r'\b\d{10,}\b',  # Long numbers (could be account numbers)
            r'[A-Z]{2,}',  # All caps words
            r'\d+\.\d+\.\d+\.\d+',  # IP address pattern
        ]
    
    def extract_features(self, df):
        """
        Extract all NLP features from the dataframe
        
        Args:
            df (DataFrame): Input transaction data
            
        Returns:
            DataFrame: DataFrame with extracted features
        """
        try:
            # Create a copy to avoid modifying the original
            result_df = df.copy()
            
            # Extract different types of NLP features
            result_df = self._extract_basic_text_features(result_df)
            result_df = self._extract_sentiment_features(result_df)
            result_df = self._extract_keyword_features(result_df)
            result_df = self._extract_pattern_features(result_df)
            result_df = self._extract_topic_features(result_df)
            result_df = self._extract_embedding_features(result_df)
            
            # Store feature names
            self.feature_names = [col for col in result_df.columns if col not in df.columns]
            
            logger.info(f"Extracted {len(self.feature_names)} NLP features")
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting NLP features: {str(e)}")
            raise
    
    def _extract_basic_text_features(self, df):
        """
        Extract basic text features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with basic text features
        """
        try:
            result_df = df.copy()
            
            # Text columns to process
            text_columns = ['description', 'notes']
            
            for col in text_columns:
                if col in df.columns:
                    # Character count
                    result_df[f'{col}_char_count'] = df[col].fillna('').astype(str).apply(len)
                    
                    # Word count
                    result_df[f'{col}_word_count'] = df[col].fillna('').astype(str).apply(
                        lambda x: len(word_tokenize(x)) if x else 0
                    )
                    
                    # Sentence count
                    result_df[f'{col}_sentence_count'] = df[col].fillna('').astype(str).apply(
                        lambda x: len(sent_tokenize(x)) if x else 0
                    )
                    
                    # Average word length
                    result_df[f'{col}_avg_word_length'] = df[col].fillna('').astype(str).apply(
                        lambda x: np.mean([len(word) for word in word_tokenize(x)]) if word_tokenize(x) else 0
                    )
                    
                    # Average sentence length
                    result_df[f'{col}_avg_sentence_length'] = df[col].fillna('').astype(str).apply(
                        lambda x: np.mean([len(word_tokenize(sent)) for sent in sent_tokenize(x)]) if sent_tokenize(x) else 0
                    )
                    
                    # Punctuation count
                    result_df[f'{col}_punctuation_count'] = df[col].fillna('').astype(str).apply(
                        lambda x: sum(1 for char in x if char in string.punctuation)
                    )
                    
                    # Uppercase word count
                    result_df[f'{col}_uppercase_count'] = df[col].fillna('').astype(str).apply(
                        lambda x: sum(1 for word in word_tokenize(x) if word.isupper() and len(word) > 1)
                    )
                    
                    # Digit count
                    result_df[f'{col}_digit_count'] = df[col].fillna('').astype(str).apply(
                        lambda x: sum(1 for char in x if char.isdigit())
                    )
                    
                    # Unique word count
                    result_df[f'{col}_unique_word_count'] = df[col].fillna('').astype(str).apply(
                        lambda x: len(set(word_tokenize(x.lower()))) if x else 0
                    )
                    
                    # Lexical diversity (unique words / total words)
                    result_df[f'{col}_lexical_diversity'] = df[col].fillna('').astype(str).apply(
                        lambda x: len(set(word_tokenize(x.lower()))) / len(word_tokenize(x)) if word_tokenize(x) else 0
                    )
            
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting basic text features: {str(e)}")
            return df
    
    def _extract_sentiment_features(self, df):
        """
        Extract sentiment analysis features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with sentiment features
        """
        try:
            result_df = df.copy()
            
            # Text columns to process
            text_columns = ['description', 'notes']
            
            for col in text_columns:
                if col in df.columns:
                    # VADER sentiment scores
                    sentiment_scores = df[col].fillna('').astype(str).apply(
                        lambda x: self.sia.polarity_scores(x) if x else {'neg': 0, 'neu': 0, 'pos': 0, 'compound': 0}
                    )
                    
                    # Extract individual scores
                    result_df[f'{col}_sentiment_neg'] = sentiment_scores.apply(lambda x: x['neg'])
                    result_df[f'{col}_sentiment_neu'] = sentiment_scores.apply(lambda x: x['neu'])
                    result_df[f'{col}_sentiment_pos'] = sentiment_scores.apply(lambda x: x['pos'])
                    result_df[f'{col}_sentiment_compound'] = sentiment_scores.apply(lambda x: x['compound'])
                    
                    # TextBlob sentiment
                    textblob_sentiment = df[col].fillna('').astype(str).apply(
                        lambda x: TextBlob(x).sentiment if x else TextBlob('').sentiment
                    )
                    
                    result_df[f'{col}_textblob_polarity'] = textblob_sentiment.apply(lambda x: x.polarity)
                    result_df[f'{col}_textblob_subjectivity'] = textblob_sentiment.apply(lambda x: x.subjectivity)
                    
                    # Emotion indicators
                    result_df[f'{col}_is_negative'] = (result_df[f'{col}_sentiment_compound'] < -0.05).astype(int)
                    result_df[f'{col}_is_positive'] = (result_df[f'{col}_sentiment_compound'] > 0.05).astype(int)
                    result_df[f'{col}_is_neutral'] = (
                        (result_df[f'{col}_sentiment_compound'] >= -0.05) & 
                        (result_df[f'{col}_sentiment_compound'] <= 0.05)
                    ).astype(int)
            
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting sentiment features: {str(e)}")
            return df
    
    def _extract_keyword_features(self, df):
        """
        Extract keyword-based features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with keyword features
        """
        try:
            result_df = df.copy()
            
            # Text columns to process
            text_columns = ['description', 'notes']
            
            for col in text_columns:
                if col in df.columns:
                    # Preprocess text
                    processed_text = df[col].fillna('').astype(str).apply(self._preprocess_text)
                    
                    # Count fraud keywords
                    fraud_keyword_counts = processed_text.apply(
                        lambda x: sum(1 for word in x if word in self.fraud_keywords)
                    )
                    result_df[f'{col}_fraud_keyword_count'] = fraud_keyword_counts
                    
                    # Flag if any fraud keywords present
                    result_df[f'{col}_has_fraud_keywords'] = (fraud_keyword_counts > 0).astype(int)
                    
                    # Count specific keyword categories
                    urgency_keywords = ['urgent', 'immediately', 'asap', 'hurry', 'quick', 'fast']
                    secrecy_keywords = ['secret', 'confidential', 'private', 'hidden', 'discreet']
                    money_keywords = ['money', 'cash', 'payment', 'transfer', 'wire']
                    
                    result_df[f'{col}_urgency_keyword_count'] = processed_text.apply(
                        lambda x: sum(1 for word in x if word in urgency_keywords)
                    )
                    
                    result_df[f'{col}_secrecy_keyword_count'] = processed_text.apply(
                        lambda x: sum(1 for word in x if word in secrecy_keywords)
                    )
                    
                    result_df[f'{col}_money_keyword_count'] = processed_text.apply(
                        lambda x: sum(1 for word in x if word in money_keywords)
                    )
                    
                    # Calculate keyword density
                    result_df[f'{col}_fraud_keyword_density'] = fraud_keyword_counts / (
                        df[col].fillna('').astype(str).apply(lambda x: len(word_tokenize(x)) if x else 1)
                    )
                    
                    # TF-IDF for fraud keywords
                    if not self.fitted:
                        # Fit TF-IDF vectorizer
                        self.tfidf_vectorizer = TfidfVectorizer(
                            vocabulary=self.fraud_keywords,
                            ngram_range=(1, 2),
                            max_features=100
                        )
                        
                        # Fit on all text
                        all_text = pd.concat([df[col].fillna('').astype(str) for col in text_columns if col in df.columns])
                        self.tfidf_vectorizer.fit(all_text)
                    
                    # Transform text
                    tfidf_features = self.tfidf_vectorizer.transform(df[col].fillna('').astype(str))
                    
                    # Add top TF-IDF features
                    feature_names = self.tfidf_vectorizer.get_feature_names_out()
                    for i, feature in enumerate(feature_names[:10]):  # Top 10 features
                        result_df[f'{col}_tfidf_{feature}'] = tfidf_features[:, i].toarray().flatten()
            
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting keyword features: {str(e)}")
            return df
    
    def _extract_pattern_features(self, df):
        """
        Extract pattern-based features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with pattern features
        """
        try:
            result_df = df.copy()
            
            # Text columns to process
            text_columns = ['description', 'notes']
            
            for col in text_columns:
                if col in df.columns:
                    # Count suspicious patterns
                    pattern_counts = []
                    for text in df[col].fillna('').astype(str):
                        count = 0
                        for pattern in self.suspicious_patterns:
                            matches = re.findall(pattern, text, re.IGNORECASE)
                            count += len(matches)
                        pattern_counts.append(count)
                    
                    result_df[f'{col}_suspicious_pattern_count'] = pattern_counts
                    
                    # Flag if any suspicious patterns present
                    result_df[f'{col}_has_suspicious_patterns'] = (np.array(pattern_counts) > 0).astype(int)
                    
                    # Specific pattern counts
                    money_pattern = r'\$\d+,\d+\.\d{2}'
                    credit_card_pattern = r'\d{4}[\s-]?\d{4}[\s-]?\d{4}[\s-]?\d{4}'
                    ssn_pattern = r'\b\d{3}-\d{2}-\d{4}\b'
                    email_pattern = r'\b[A-Z0-9._%+-]+@[A-Z0-9.-]+\.[A-Z]{2,}\b'
                    url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'
                    
                    result_df[f'{col}_money_pattern_count'] = df[col].fillna('').astype(str).apply(
                        lambda x: len(re.findall(money_pattern, x))
                    )
                    
                    result_df[f'{col}_credit_card_pattern_count'] = df[col].fillna('').astype(str).apply(
                        lambda x: len(re.findall(credit_card_pattern, x))
                    )
                    
                    result_df[f'{col}_ssn_pattern_count'] = df[col].fillna('').astype(str).apply(
                        lambda x: len(re.findall(ssn_pattern, x))
                    )
                    
                    result_df[f'{col}_email_pattern_count'] = df[col].fillna('').astype(str).apply(
                        lambda x: len(re.findall(email_pattern, x))
                    )
                    
                    result_df[f'{col}_url_pattern_count'] = df[col].fillna('').astype(str).apply(
                        lambda x: len(re.findall(url_pattern, x))
                    )
                    
                    # Check for excessive punctuation
                    result_df[f'{col}_excessive_punctuation'] = (
                        df[col].fillna('').astype(str).apply(
                            lambda x: 1 if sum(1 for char in x if char in string.punctuation) / len(x) > 0.3 else 0
                        )
                    )
                    
                    # Check for excessive capitalization
                    result_df[f'{col}_excessive_capitalization'] = (
                        df[col].fillna('').astype(str).apply(
                            lambda x: 1 if sum(1 for char in x if char.isupper()) / len(x) > 0.5 else 0
                        )
                    )
            
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting pattern features: {str(e)}")
            return df
    
    def _extract_topic_features(self, df):
        """
        Extract topic modeling features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with topic features
        """
        try:
            result_df = df.copy()
            
            # Text columns to process
            text_columns = ['description', 'notes']
            
            # Combine text from all columns
            all_text = []
            for col in text_columns:
                if col in df.columns:
                    all_text.extend(df[col].fillna('').astype(str).tolist())
            
            if not all_text:
                return result_df
            
            # Preprocess text for topic modeling
            processed_docs = [self._preprocess_text(doc) for doc in all_text]
            processed_docs = [' '.join(doc) for doc in processed_docs if doc]
            
            if not processed_docs:
                return result_df
            
            # Fit CountVectorizer and LDA model if not fitted
            if not self.fitted:
                # Fit CountVectorizer
                self.count_vectorizer = CountVectorizer(
                    max_features=1000,
                    stop_words='english',
                    ngram_range=(1, 2)
                )
                doc_term_matrix = self.count_vectorizer.fit_transform(processed_docs)
                
                # Fit LDA model
                self.lda_model = LatentDirichletAllocation(
                    n_components=5,  # 5 topics
                    random_state=42,
                    max_iter=10,
                    learning_method='online'
                )
                self.lda_model.fit(doc_term_matrix)
            
            # Transform each text column
            for col in text_columns:
                if col in df.columns:
                    # Preprocess text
                    processed_text = df[col].fillna('').astype(str).apply(self._preprocess_text)
                    processed_text = processed_text.apply(lambda x: ' '.join(x) if x else '')
                    
                    # Transform to document-term matrix
                    doc_term_matrix = self.count_vectorizer.transform(processed_text)
                    
                    # Get topic distributions
                    topic_distributions = self.lda_model.transform(doc_term_matrix)
                    
                    # Add topic features
                    for i in range(topic_distributions.shape[1]):
                        result_df[f'{col}_topic_{i}_prob'] = topic_distributions[:, i]
                    
                    # Get dominant topic
                    dominant_topics = np.argmax(topic_distributions, axis=1)
                    result_df[f'{col}_dominant_topic'] = dominant_topics
            
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting topic features: {str(e)}")
            return df
    
    def _extract_embedding_features(self, df):
        """
        Extract word embedding features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with embedding features
        """
        try:
            result_df = df.copy()
            
            # Text columns to process
            text_columns = ['description', 'notes']
            
            # Combine text from all columns
            all_text = []
            for col in text_columns:
                if col in df.columns:
                    all_text.extend(df[col].fillna('').astype(str).tolist())
            
            if not all_text:
                return result_df
            
            # Preprocess text for embeddings
            processed_docs = [self._preprocess_text(doc) for doc in all_text]
            processed_docs = [doc for doc in processed_docs if doc]
            
            if not processed_docs:
                return result_df
            
            # Check if Gemini API is available
            if is_api_available('gemini'):
                # Here you would implement Gemini API calls for embeddings
                # For now, we'll skip and use local embeddings
                logger.info("Gemini API available, but implementation pending. Using local embeddings.")
            
            # Check if OpenAI API is available
            if is_api_available('openai'):
                # Here you would implement OpenAI API calls for embeddings
                # For now, we'll skip and use local embeddings
                logger.info("OpenAI API available, but implementation pending. Using local embeddings.")
            
            # Fit Word2Vec model (local implementation)
            self.word2vec_model = Word2Vec(
                sentences=processed_docs,
                vector_size=100,  # 100-dimensional vectors
                window=5,
                min_count=1,
                workers=4,
                sg=1  # Skip-gram model
            )
            
            # Train Doc2Vec model (local implementation)
            tagged_docs = [TaggedDocument(doc, [i]) for i, doc in enumerate(processed_docs)]
            self.doc2vec_model = Doc2Vec(
                documents=tagged_docs,
                vector_size=100,  # 100-dimensional vectors
                window=5,
                min_count=1,
                workers=4,
                epochs=10
            )
            
            # Transform each text column
            for col in text_columns:
                if col in df.columns:
                    # Preprocess text
                    processed_text = df[col].fillna('').astype(str).apply(self._preprocess_text)
                    
                    # Calculate average Word2Vec vectors
                    word2vec_vectors = []
                    for doc in processed_text:
                        if doc:
                            # Get vectors for words in document
                            word_vectors = [self.word2vec_model.wv[word] for word in doc if word in self.word2vec_model.wv]
                            
                            if word_vectors:
                                # Average the vectors
                                avg_vector = np.mean(word_vectors, axis=0)
                                word2vec_vectors.append(avg_vector)
                            else:
                                # Use zero vector if no words found
                                word2vec_vectors.append(np.zeros(100))
                        else:
                            # Use zero vector for empty documents
                            word2vec_vectors.append(np.zeros(100))
                    
                    # Add Word2Vec features (first 10 dimensions)
                    word2vec_vectors = np.array(word2vec_vectors)
                    for i in range(min(10, word2vec_vectors.shape[1])):
                        result_df[f'{col}_word2vec_dim_{i}'] = word2vec_vectors[:, i]
                    
                    # Calculate Doc2Vec vectors
                    doc2vec_vectors = []
                    for doc in processed_text:
                        if doc:
                            # Infer vector for document
                            vector = self.doc2vec_model.infer_vector(doc)
                            doc2vec_vectors.append(vector)
                        else:
                            # Use zero vector for empty documents
                            doc2vec_vectors.append(np.zeros(100))
                    
                    # Add Doc2Vec features (first 10 dimensions)
                    doc2vec_vectors = np.array(doc2vec_vectors)
                    for i in range(min(10, doc2vec_vectors.shape[1])):
                        result_df[f'{col}_doc2vec_dim_{i}'] = doc2vec_vectors[:, i]
            
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting embedding features: {str(e)}")
            return df
    
    def _preprocess_text(self, text):
        """
        Preprocess text for NLP analysis
        
        Args:
            text (str): Input text
            
        Returns:
            list: List of processed tokens
        """
        try:
            # Convert to lowercase
            text = text.lower()
            
            # Remove punctuation
            text = text.translate(str.maketrans('', '', string.punctuation))
            
            # Remove digits
            text = re.sub(r'\d+', '', text)
            
            # Tokenize
            tokens = word_tokenize(text)
            
            # Remove stop words
            tokens = [word for word in tokens if word not in self.stop_words]
            
            # Lemmatize
            tokens = [self.lemmatizer.lemmatize(word) for word in tokens]
            
            # Remove short words
            tokens = [word for word in tokens if len(word) > 2]
            
            return tokens
            
        except Exception as e:
            logger.error(f"Error preprocessing text: {str(e)}")
            return []
    
    def fit_transform(self, df):
        """
        Fit the feature extractor and transform the data
        
        Args:
            df (DataFrame): Input data
            
        Returns:
            DataFrame: Transformed data with features
        """
        # Extract features
        result_df = self.extract_features(df)
        
        # Get feature columns
        feature_cols = [col for col in result_df.columns if col not in df.columns]
        
        if len(feature_cols) > 0:
            self.fitted = True
        
        return result_df
    
    def transform(self, df):
        """
        Transform new data using fitted feature extractor
        
        Args:
            df (DataFrame): Input data
            
        Returns:
            DataFrame: Transformed data with features
        """
        if not self.fitted:
            raise ValueError("Feature extractor not fitted. Call fit_transform first.")
        
        # Extract features
        result_df = self.extract_features(df)
        
        return result_df

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\features\statistical_features.py ===
"""
Statistical Features Module
Implements various statistical feature extraction techniques for fraud detection
"""

import pandas as pd
import numpy as np
import scipy.stats as stats
from scipy.spatial.distance import mahalanobis
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
import warnings
import logging
from typing import Dict, List, Tuple, Union

warnings.filterwarnings('ignore')
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class StatisticalFeatures:
    """
    Class for extracting statistical features from transaction data
    Implements techniques like Benford's Law, Z-score, MAD, etc.
    """
    
    def __init__(self, config=None):
        """
        Initialize StatisticalFeatures
        
        Args:
            config (dict, optional): Configuration parameters
        """
        self.config = config or {}
        self.feature_names = []
        self.scaler = StandardScaler()
        self.pca = None
        self.fitted = False
        
    def extract_features(self, df):
        """
        Extract all statistical features from the dataframe
        
        Args:
            df (DataFrame): Input transaction data
            
        Returns:
            DataFrame: DataFrame with extracted features
        """
        try:
            # Create a copy to avoid modifying the original
            result_df = df.copy()
            
            # Extract different types of statistical features
            result_df = self._extract_benford_features(result_df)
            result_df = self._extract_zscore_features(result_df)
            result_df = self._extract_mad_features(result_df)
            result_df = self._extract_percentile_features(result_df)
            result_df = self._extract_distribution_features(result_df)
            result_df = self._extract_mahalanobis_features(result_df)
            result_df = self._extract_grubbs_features(result_df)
            result_df = self._extract_entropy_features(result_df)
            result_df = self._extract_correlation_features(result_df)
            
            # Store feature names
            self.feature_names = [col for col in result_df.columns if col not in df.columns]
            
            logger.info(f"Extracted {len(self.feature_names)} statistical features")
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting statistical features: {str(e)}")
            raise
    
    def _extract_benford_features(self, df):
        """
        Extract Benford's Law features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with Benford's Law features
        """
        try:
            result_df = df.copy()
            
            # Apply Benford's Law to amount column if it exists
            if 'amount' in df.columns:
                # Get first digit of amounts
                amounts = df['amount'].abs()
                first_digits = amounts.astype(str).str[0].replace('n', '0').astype(int)
                first_digits = first_digits[first_digits >= 1]  # Exclude 0
                
                if len(first_digits) > 0:
                    # Calculate actual distribution
                    actual_dist = first_digits.value_counts(normalize=True).sort_index()
                    
                    # Expected Benford's distribution
                    benford_dist = pd.Series([np.log10(1 + 1/d) for d in range(1, 10)], index=range(1, 10))
                    
                    # Calculate Chi-square statistic
                    chi_square = 0
                    for digit in range(1, 10):
                        expected_count = benford_dist[digit] * len(first_digits)
                        actual_count = actual_dist.get(digit, 0)
                        if expected_count > 0:
                            chi_square += (actual_count - expected_count) ** 2 / expected_count
                    
                    # Add features
                    result_df['benford_chi_square'] = chi_square
                    result_df['benford_p_value'] = 1 - stats.chi2.cdf(chi_square, 8)  # 8 degrees of freedom
                    
                    # Calculate deviation for each digit
                    for digit in range(1, 6):  # Just first 5 digits to avoid too many features
                        actual_pct = actual_dist.get(digit, 0)
                        expected_pct = benford_dist[digit]
                        result_df[f'benford_deviation_{digit}'] = abs(actual_pct - expected_pct)
            
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting Benford's Law features: {str(e)}")
            return df
    
    def _extract_zscore_features(self, df):
        """
        Extract Z-score based features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with Z-score features
        """
        try:
            result_df = df.copy()
            
            # Apply to amount column if it exists
            if 'amount' in df.columns:
                amounts = df['amount']
                
                # Calculate Z-scores
                mean_amount = amounts.mean()
                std_amount = amounts.std()
                
                if std_amount > 0:
                    z_scores = (amounts - mean_amount) / std_amount
                    result_df['amount_zscore'] = z_scores
                    
                    # Flag extreme values
                    result_df['amount_zscore_outlier'] = (np.abs(z_scores) > 3).astype(int)
                else:
                    result_df['amount_zscore'] = 0
                    result_df['amount_zscore_outlier'] = 0
            
            # Apply Z-score to sender and receiver if ID columns exist
            if 'sender_id' in df.columns and 'amount' in df.columns:
                # Calculate average amount per sender
                sender_avg = df.groupby('sender_id')['amount'].mean()
                sender_std = df.groupby('sender_id')['amount'].std()
                
                # Calculate Z-scores for each transaction relative to sender's history
                sender_zscores = []
                for _, row in df.iterrows():
                    sender_id = row['sender_id']
                    amount = row['amount']
                    
                    if sender_id in sender_avg and sender_id in sender_std and sender_std[sender_id] > 0:
                        z_score = (amount - sender_avg[sender_id]) / sender_std[sender_id]
                    else:
                        z_score = 0
                    
                    sender_zscores.append(z_score)
                
                result_df['sender_amount_zscore'] = sender_zscores
                result_df['sender_amount_zscore_outlier'] = (np.abs(sender_zscores) > 3).astype(int)
            
            if 'receiver_id' in df.columns and 'amount' in df.columns:
                # Calculate average amount per receiver
                receiver_avg = df.groupby('receiver_id')['amount'].mean()
                receiver_std = df.groupby('receiver_id')['amount'].std()
                
                # Calculate Z-scores for each transaction relative to receiver's history
                receiver_zscores = []
                for _, row in df.iterrows():
                    receiver_id = row['receiver_id']
                    amount = row['amount']
                    
                    if receiver_id in receiver_avg and receiver_id in receiver_std and receiver_std[receiver_id] > 0:
                        z_score = (amount - receiver_avg[receiver_id]) / receiver_std[receiver_id]
                    else:
                        z_score = 0
                    
                    receiver_zscores.append(z_score)
                
                result_df['receiver_amount_zscore'] = receiver_zscores
                result_df['receiver_amount_zscore_outlier'] = (np.abs(receiver_zscores) > 3).astype(int)
            
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting Z-score features: {str(e)}")
            return df
    
    def _extract_mad_features(self, df):
        """
        Extract Median Absolute Deviation (MAD) features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with MAD features
        """
        try:
            result_df = df.copy()
            
            # Apply to amount column if it exists
            if 'amount' in df.columns:
                amounts = df['amount']
                
                # Calculate MAD
                median_amount = amounts.median()
                abs_dev = np.abs(amounts - median_amount)
                mad = abs_dev.median()
                
                if mad > 0:
                    # Calculate modified Z-scores using MAD
                    modified_z_scores = 0.6745 * abs_dev / mad
                    result_df['amount_mad_zscore'] = modified_z_scores
                    
                    # Flag extreme values
                    result_df['amount_mad_outlier'] = (modified_z_scores > 3.5).astype(int)
                else:
                    result_df['amount_mad_zscore'] = 0
                    result_df['amount_mad_outlier'] = 0
            
            # Apply MAD to sender and receiver if ID columns exist
            if 'sender_id' in df.columns and 'amount' in df.columns:
                # Calculate MAD per sender
                sender_mad = df.groupby('sender_id')['amount'].apply(lambda x: np.median(np.abs(x - x.median())))
                sender_median = df.groupby('sender_id')['amount'].median()
                
                # Calculate MAD-based Z-scores for each transaction
                sender_mad_zscores = []
                for _, row in df.iterrows():
                    sender_id = row['sender_id']
                    amount = row['amount']
                    
                    if sender_id in sender_mad and sender_id in sender_median and sender_mad[sender_id] > 0:
                        abs_dev = abs(amount - sender_median[sender_id])
                        mad_z_score = 0.6745 * abs_dev / sender_mad[sender_id]
                    else:
                        mad_z_score = 0
                    
                    sender_mad_zscores.append(mad_z_score)
                
                result_df['sender_amount_mad_zscore'] = sender_mad_zscores
                result_df['sender_amount_mad_outlier'] = (np.array(sender_mad_zscores) > 3.5).astype(int)
            
            if 'receiver_id' in df.columns and 'amount' in df.columns:
                # Calculate MAD per receiver
                receiver_mad = df.groupby('receiver_id')['amount'].apply(lambda x: np.median(np.abs(x - x.median())))
                receiver_median = df.groupby('receiver_id')['amount'].median()
                
                # Calculate MAD-based Z-scores for each transaction
                receiver_mad_zscores = []
                for _, row in df.iterrows():
                    receiver_id = row['receiver_id']
                    amount = row['amount']
                    
                    if receiver_id in receiver_mad and receiver_id in receiver_median and receiver_mad[receiver_id] > 0:
                        abs_dev = abs(amount - receiver_median[receiver_id])
                        mad_z_score = 0.6745 * abs_dev / receiver_mad[receiver_id]
                    else:
                        mad_z_score = 0
                    
                    receiver_mad_zscores.append(mad_z_score)
                
                result_df['receiver_amount_mad_zscore'] = receiver_mad_zscores
                result_df['receiver_amount_mad_outlier'] = (np.array(receiver_mad_zscores) > 3.5).astype(int)
            
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting MAD features: {str(e)}")
            return df
    
    def _extract_percentile_features(self, df):
        """
        Extract percentile-based features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with percentile features
        """
        try:
            result_df = df.copy()
            
            # Apply to amount column if it exists
            if 'amount' in df.columns:
                amounts = df['amount']
                
                # Calculate percentiles
                percentiles = [5, 10, 25, 50, 75, 90, 95, 99]
                percentile_values = np.percentile(amounts, percentiles)
                
                # Add features for each percentile
                for i, p in enumerate(percentiles):
                    result_df[f'amount_above_{p}th_percentile'] = (amounts > percentile_values[i]).astype(int)
                
                # Calculate percentile rank for each amount
                result_df['amount_percentile_rank'] = amounts.rank(pct=True)
            
            # Apply percentiles to sender and receiver if ID columns exist
            if 'sender_id' in df.columns and 'amount' in df.columns:
                # Calculate percentile rank within each sender's transactions
                sender_percentile_ranks = df.groupby('sender_id')['amount'].rank(pct=True)
                result_df['sender_amount_percentile_rank'] = sender_percentile_ranks
                
                # Flag if amount is in top 5% for sender
                result_df['sender_amount_top_5pct'] = (sender_percentile_ranks > 0.95).astype(int)
            
            if 'receiver_id' in df.columns and 'amount' in df.columns:
                # Calculate percentile rank within each receiver's transactions
                receiver_percentile_ranks = df.groupby('receiver_id')['amount'].rank(pct=True)
                result_df['receiver_amount_percentile_rank'] = receiver_percentile_ranks
                
                # Flag if amount is in top 5% for receiver
                result_df['receiver_amount_top_5pct'] = (receiver_percentile_ranks > 0.95).astype(int)
            
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting percentile features: {str(e)}")
            return df
    
    def _extract_distribution_features(self, df):
        """
        Extract distribution-based features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with distribution features
        """
        try:
            result_df = df.copy()
            
            # Apply to amount column if it exists
            if 'amount' in df.columns:
                amounts = df['amount']
                
                # Skewness and kurtosis
                result_df['amount_skewness'] = stats.skew(amounts)
                result_df['amount_kurtosis'] = stats.kurtosis(amounts)
                
                # Normality tests
                _, normality_p = stats.normaltest(amounts)
                result_df['amount_normality_p'] = normality_p
                
                # Shapiro-Wilk test (for smaller samples)
                if len(amounts) <= 5000:
                    _, shapiro_p = stats.shapiro(amounts)
                    result_df['amount_shapiro_p'] = shapiro_p
                
                # Kolmogorov-Smirnov test against normal distribution
                _, ks_p = stats.kstest(amounts, 'norm', args=(amounts.mean(), amounts.std()))
                result_df['amount_ks_p'] = ks_p
                
                # Anderson-Darling test
                ad_result = stats.anderson(amounts)
                result_df['amount_ad_statistic'] = ad_result.statistic
                
                # Jarque-Bera test
                jb_stat, jb_p = stats.jarque_bera(amounts)
                result_df['amount_jb_statistic'] = jb_stat
                result_df['amount_jb_p'] = jb_p
            
            # Apply distribution features to sender and receiver if ID columns exist
            if 'sender_id' in df.columns and 'amount' in df.columns:
                # Calculate distribution features per sender
                sender_stats = df.groupby('sender_id')['amount'].agg([
                    ('skewness', lambda x: stats.skew(x) if len(x) >= 3 else 0),
                    ('kurtosis', lambda x: stats.kurtosis(x) if len(x) >= 4 else 0),
                    ('variance', 'var'),
                    ('range', lambda x: x.max() - x.min() if len(x) > 0 else 0)
                ])
                
                # Map back to each transaction
                for stat in ['skewness', 'kurtosis', 'variance', 'range']:
                    result_df[f'sender_amount_{stat}'] = df['sender_id'].map(sender_stats[stat]).fillna(0)
            
            if 'receiver_id' in df.columns and 'amount' in df.columns:
                # Calculate distribution features per receiver
                receiver_stats = df.groupby('receiver_id')['amount'].agg([
                    ('skewness', lambda x: stats.skew(x) if len(x) >= 3 else 0),
                    ('kurtosis', lambda x: stats.kurtosis(x) if len(x) >= 4 else 0),
                    ('variance', 'var'),
                    ('range', lambda x: x.max() - x.min() if len(x) > 0 else 0)
                ])
                
                # Map back to each transaction
                for stat in ['skewness', 'kurtosis', 'variance', 'range']:
                    result_df[f'receiver_amount_{stat}'] = df['receiver_id'].map(receiver_stats[stat]).fillna(0)
            
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting distribution features: {str(e)}")
            return df
    
    def _extract_mahalanobis_features(self, df):
        """
        Extract Mahalanobis distance features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with Mahalanobis features
        """
        try:
            result_df = df.copy()
            
            # Get numeric columns
            numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
            
            if len(numeric_cols) < 2:
                logger.warning("Not enough numeric columns for Mahalanobis distance calculation")
                return result_df
            
            # Prepare data
            X = df[numeric_cols].fillna(0)
            
            # Calculate covariance matrix
            cov_matrix = np.cov(X, rowvar=False)
            
            # Check if covariance matrix is invertible
            if np.linalg.det(cov_matrix) == 0:
                logger.warning("Covariance matrix is singular, using pseudo-inverse")
                inv_cov_matrix = np.linalg.pinv(cov_matrix)
            else:
                inv_cov_matrix = np.linalg.inv(cov_matrix)
            
            # Calculate mean vector
            mean_vector = np.mean(X, axis=0)
            
            # Calculate Mahalanobis distances
            mahalanobis_distances = []
            for i in range(len(X)):
                mahalanobis_distances.append(
                    mahalanobis(X.iloc[i], mean_vector, inv_cov_matrix)
                )
            
            result_df['mahalanobis_distance'] = mahalanobis_distances
            
            # Flag outliers based on Mahalanobis distance
            # Using chi-square distribution with degrees of freedom = number of features
            threshold = stats.chi2.ppf(0.975, df=len(numeric_cols))
            result_df['mahalanobis_outlier'] = (np.array(mahalanobis_distances) > threshold).astype(int)
            
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting Mahalanobis features: {str(e)}")
            return df
    
    def _extract_grubbs_features(self, df):
        """
        Extract Grubbs' test features for outliers
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with Grubbs' test features
        """
        try:
            result_df = df.copy()
            
            # Apply to amount column if it exists
            if 'amount' in df.columns:
                amounts = df['amount'].values
                
                # Calculate Grubbs' test statistic
                mean_amount = np.mean(amounts)
                std_amount = np.std(amounts)
                
                if std_amount > 0:
                    # Calculate absolute deviations
                    abs_deviations = np.abs(amounts - mean_amount)
                    max_deviation = np.max(abs_deviations)
                    
                    # Grubbs' test statistic
                    grubbs_stat = max_deviation / std_amount
                    result_df['grubbs_statistic'] = grubbs_stat
                    
                    # Calculate critical value for two-sided test
                    n = len(amounts)
                    t_critical = stats.t.ppf(1 - 0.025 / (2 * n), n - 2)
                    critical_value = (n - 1) * t_critical / np.sqrt(n * (n - 2 + t_critical**2))
                    
                    # Flag outliers
                    result_df['grubbs_outlier'] = (grubbs_stat > critical_value).astype(int)
                else:
                    result_df['grubbs_statistic'] = 0
                    result_df['grubbs_outlier'] = 0
            
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting Grubbs' test features: {str(e)}")
            return df
    
    def _extract_entropy_features(self, df):
        """
        Extract entropy-based features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with entropy features
        """
        try:
            result_df = df.copy()
            
            # Calculate entropy for categorical columns
            categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()
            
            for col in categorical_cols:
                # Calculate probability distribution
                value_counts = df[col].value_counts(normalize=True)
                
                # Calculate Shannon entropy
                entropy = -sum(p * np.log2(p) for p in value_counts if p > 0)
                result_df[f'{col}_entropy'] = entropy
                
                # Calculate normalized entropy (0 to 1)
                max_entropy = np.log2(len(value_counts))
                if max_entropy > 0:
                    normalized_entropy = entropy / max_entropy
                else:
                    normalized_entropy = 0
                result_df[f'{col}_normalized_entropy'] = normalized_entropy
            
            # Calculate entropy for numeric columns (after binning)
            numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
            
            for col in numeric_cols:
                # Bin the data
                try:
                    binned = pd.cut(df[col], bins=10, duplicates='drop')
                    
                    # Calculate probability distribution
                    value_counts = binned.value_counts(normalize=True)
                    
                    # Calculate Shannon entropy
                    entropy = -sum(p * np.log2(p) for p in value_counts if p > 0)
                    result_df[f'{col}_binned_entropy'] = entropy
                    
                    # Calculate normalized entropy (0 to 1)
                    max_entropy = np.log2(len(value_counts))
                    if max_entropy > 0:
                        normalized_entropy = entropy / max_entropy
                    else:
                        normalized_entropy = 0
                    result_df[f'{col}_binned_normalized_entropy'] = normalized_entropy
                except:
                    pass
            
            # Calculate transaction entropy for sender and receiver
            if 'sender_id' in df.columns:
                # Calculate entropy of transaction amounts per sender
                sender_entropy = df.groupby('sender_id')['amount'].apply(
                    lambda x: self._calculate_series_entropy(x)
                )
                result_df['sender_amount_entropy'] = df['sender_id'].map(sender_entropy).fillna(0)
            
            if 'receiver_id' in df.columns:
                # Calculate entropy of transaction amounts per receiver
                receiver_entropy = df.groupby('receiver_id')['amount'].apply(
                    lambda x: self._calculate_series_entropy(x)
                )
                result_df['receiver_amount_entropy'] = df['receiver_id'].map(receiver_entropy).fillna(0)
            
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting entropy features: {str(e)}")
            return df
    
    def _calculate_series_entropy(self, series):
        """
        Calculate entropy of a pandas Series
        
        Args:
            series (Series): Input series
            
        Returns:
            float: Entropy value
        """
        try:
            # Bin the data
            binned = pd.cut(series, bins=min(10, len(series)), duplicates='drop')
            
            # Calculate probability distribution
            value_counts = binned.value_counts(normalize=True)
            
            # Calculate Shannon entropy
            entropy = -sum(p * np.log2(p) for p in value_counts if p > 0)
            return entropy
        except:
            return 0
    
    def _extract_correlation_features(self, df):
        """
        Extract correlation-based features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with correlation features
        """
        try:
            result_df = df.copy()
            
            # Get numeric columns
            numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
            
            if len(numeric_cols) < 2:
                logger.warning("Not enough numeric columns for correlation calculation")
                return result_df
            
            # Calculate correlation matrix
            corr_matrix = df[numeric_cols].corr().abs()
            
            # Find highest correlation for each variable
            max_corr = {}
            for col in numeric_cols:
                # Get correlations with other variables
                corrs = corr_matrix[col].drop(col)
                max_corr[col] = corrs.max()
            
            # Add features
            for col in numeric_cols:
                result_df[f'{col}_max_correlation'] = max_corr[col]
            
            # Calculate average correlation
            avg_corr = {}
            for col in numeric_cols:
                # Get correlations with other variables
                corrs = corr_matrix[col].drop(col)
                avg_corr[col] = corrs.mean()
            
            # Add features
            for col in numeric_cols:
                result_df[f'{col}_avg_correlation'] = avg_corr[col]
            
            # Calculate variance inflation factor (VIF) for multicollinearity
            for i, col in enumerate(numeric_cols):
                # Prepare data for VIF calculation
                X = df[numeric_cols].copy()
                y = X[col]
                X = X.drop(col, axis=1)
                
                # Fit linear regression
                try:
                    from sklearn.linear_model import LinearRegression
                    model = LinearRegression()
                    model.fit(X, y)
                    
                    # Calculate R-squared
                    r_squared = model.score(X, y)
                    
                    # Calculate VIF
                    if r_squared < 1:
                        vif = 1 / (1 - r_squared)
                    else:
                        vif = float('inf')
                    
                    result_df[f'{col}_vif'] = vif
                except:
                    result_df[f'{col}_vif'] = 1
            
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting correlation features: {str(e)}")
            return df
    
    def fit_transform(self, df):
        """
        Fit the feature extractor and transform the data
        
        Args:
            df (DataFrame): Input data
            
        Returns:
            DataFrame: Transformed data with features
        """
        # Extract features
        result_df = self.extract_features(df)
        
        # Get feature columns
        feature_cols = [col for col in result_df.columns if col not in df.columns]
        
        if len(feature_cols) > 0:
            # Fit scaler
            self.scaler.fit(result_df[feature_cols])
            
            # Apply PCA if we have enough features
            if len(feature_cols) >= 10:
                self.pca = PCA(n_components=min(10, len(feature_cols)))
                self.pca.fit(result_df[feature_cols])
                
                # Add PCA components
                pca_components = self.pca.transform(result_df[feature_cols])
                for i in range(pca_components.shape[1]):
                    result_df[f'stat_pca_{i+1}'] = pca_components[:, i]
            
            self.fitted = True
        
        return result_df
    
    def transform(self, df):
        """
        Transform new data using fitted feature extractor
        
        Args:
            df (DataFrame): Input data
            
        Returns:
            DataFrame: Transformed data with features
        """
        if not self.fitted:
            raise ValueError("Feature extractor not fitted. Call fit_transform first.")
        
        # Extract features
        result_df = self.extract_features(df)
        
        # Get feature columns
        feature_cols = [col for col in result_df.columns if col not in df.columns]
        
        if len(feature_cols) > 0:
            # Transform features using fitted scaler
            result_df[feature_cols] = self.scaler.transform(result_df[feature_cols])
            
            # Apply PCA if it was fitted
            if self.pca is not None:
                pca_components = self.pca.transform(result_df[feature_cols])
                for i in range(pca_components.shape[1]):
                    result_df[f'stat_pca_{i+1}'] = pca_components[:, i]
        
        return result_df

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\features\timeseries_features.py ===
"""
Time Series Features Module
Implements time series feature extraction techniques for fraud detection
"""

import pandas as pd
import numpy as np
from scipy import stats
from scipy.signal import find_peaks
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tsa.stattools import acf, pacf
import warnings
import logging
from typing import Dict, List, Tuple, Union

warnings.filterwarnings('ignore')
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class TimeSeriesFeatures:
    """
    Class for extracting time series features from transaction data
    Implements techniques like burstiness analysis, gap analysis, etc.
    """
    
    def __init__(self, config=None):
        """
        Initialize TimeSeriesFeatures
        
        Args:
            config (dict, optional): Configuration parameters
        """
        self.config = config or {}
        self.feature_names = []
        self.fitted = False
        
    def extract_features(self, df):
        """
        Extract all time series features from the dataframe
        
        Args:
            df (DataFrame): Input transaction data
            
        Returns:
            DataFrame: DataFrame with extracted features
        """
        try:
            # Create a copy to avoid modifying the original
            result_df = df.copy()
            
            # Ensure timestamp is in datetime format
            if 'timestamp' in df.columns and not pd.api.types.is_datetime64_any_dtype(df['timestamp']):
                result_df['timestamp'] = pd.to_datetime(df['timestamp'])
            
            # Extract different types of time series features
            result_df = self._extract_temporal_features(result_df)
            result_df = self._extract_frequency_features(result_df)
            result_df = self._extract_burstiness_features(result_df)
            result_df = self._extract_gap_features(result_df)
            result_df = self._extract_seasonal_features(result_df)
            result_df = self._extract_autocorrelation_features(result_df)
            
            # Store feature names
            self.feature_names = [col for col in result_df.columns if col not in df.columns]
            
            logger.info(f"Extracted {len(self.feature_names)} time series features")
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting time series features: {str(e)}")
            raise
    
    def _extract_temporal_features(self, df):
        """
        Extract temporal features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with temporal features
        """
        try:
            result_df = df.copy()
            
            if 'timestamp' not in df.columns:
                logger.warning("Timestamp column not found. Skipping temporal features.")
                return result_df
            
            # Extract time components
            result_df['hour'] = result_df['timestamp'].dt.hour
            result_df['day'] = result_df['timestamp'].dt.day
            result_df['month'] = result_df['timestamp'].dt.month
            result_df['year'] = result_df['timestamp'].dt.year
            result_df['dayofweek'] = result_df['timestamp'].dt.dayofweek  # Monday=0, Sunday=6
            result_df['dayofyear'] = result_df['timestamp'].dt.dayofyear
            result_df['weekofyear'] = result_df['timestamp'].dt.isocalendar().week
            result_df['quarter'] = result_df['timestamp'].dt.quarter
            
            # Time-based flags
            result_df['is_weekend'] = (result_df['dayofweek'] >= 5).astype(int)
            result_df['is_month_start'] = result_df['timestamp'].dt.is_month_start.astype(int)
            result_df['is_month_end'] = result_df['timestamp'].dt.is_month_end.astype(int)
            result_df['is_quarter_start'] = result_df['timestamp'].dt.is_quarter_start.astype(int)
            result_df['is_quarter_end'] = result_df['timestamp'].dt.is_quarter_end.astype(int)
            result_df['is_year_start'] = result_df['timestamp'].dt.is_year_start.astype(int)
            result_df['is_year_end'] = result_df['timestamp'].dt.is_year_end.astype(int)
            
            # Time of day flags
            result_df['is_night'] = ((result_df['hour'] >= 22) | (result_df['hour'] < 6)).astype(int)
            result_df['is_morning'] = ((result_df['hour'] >= 6) & (result_df['hour'] < 12)).astype(int)
            result_df['is_afternoon'] = ((result_df['hour'] >= 12) & (result_df['hour'] < 18)).astype(int)
            result_df['is_evening'] = ((result_df['hour'] >= 18) & (result_df['hour'] < 22)).astype(int)
            
            # Business hours flag (9 AM to 5 PM, Monday to Friday)
            result_df['is_business_hours'] = (
                (result_df['hour'] >= 9) & 
                (result_df['hour'] < 17) & 
                (result_df['dayofweek'] < 5)
            ).astype(int)
            
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting temporal features: {str(e)}")
            return df
    
    def _extract_frequency_features(self, df):
        """
        Extract frequency-based features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with frequency features
        """
        try:
            result_df = df.copy()
            
            if 'timestamp' not in df.columns:
                logger.warning("Timestamp column not found. Skipping frequency features.")
                return result_df
            
            # Sort by timestamp
            df_sorted = result_df.sort_values('timestamp')
            
            # Time windows for frequency analysis
            time_windows = ['1H', '6H', '24H', '7D', '30D']
            
            for window in time_windows:
                # Calculate transaction frequency in time windows before each transaction
                window_counts = []
                
                for _, row in df_sorted.iterrows():
                    timestamp = row['timestamp']
                    
                    # Get transactions in time window before current transaction
                    window_start = timestamp - pd.Timedelta(window)
                    window_end = timestamp
                    
                    window_transactions = df_sorted[
                        (df_sorted['timestamp'] >= window_start) &
                        (df_sorted['timestamp'] < window_end)
                    ]
                    
                    # Count transactions in window
                    count = len(window_transactions)
                    window_counts.append(count)
                
                result_df[f'transaction_frequency_{window}'] = window_counts
            
            # Calculate frequency by sender
            if 'sender_id' in df.columns:
                for window in ['1H', '6H', '24H']:
                    sender_window_counts = []
                    
                    for _, row in df_sorted.iterrows():
                        sender = row['sender_id']
                        timestamp = row['timestamp']
                        
                        # Get transactions from same sender in time window
                        window_start = timestamp - pd.Timedelta(window)
                        window_end = timestamp
                        
                        window_transactions = df_sorted[
                            (df_sorted['timestamp'] >= window_start) &
                            (df_sorted['timestamp'] < window_end) &
                            (df_sorted['sender_id'] == sender)
                        ]
                        
                        # Count transactions in window
                        count = len(window_transactions)
                        sender_window_counts.append(count)
                    
                    result_df[f'sender_frequency_{window}'] = sender_window_counts
            
            # Calculate frequency by receiver
            if 'receiver_id' in df.columns:
                for window in ['1H', '6H', '24H']:
                    receiver_window_counts = []
                    
                    for _, row in df_sorted.iterrows():
                        receiver = row['receiver_id']
                        timestamp = row['timestamp']
                        
                        # Get transactions to same receiver in time window
                        window_start = timestamp - pd.Timedelta(window)
                        window_end = timestamp
                        
                        window_transactions = df_sorted[
                            (df_sorted['timestamp'] >= window_start) &
                            (df_sorted['timestamp'] < window_end) &
                            (df_sorted['receiver_id'] == receiver)
                        ]
                        
                        # Count transactions in window
                        count = len(window_transactions)
                        receiver_window_counts.append(count)
                    
                    result_df[f'receiver_frequency_{window}'] = receiver_window_counts
            
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting frequency features: {str(e)}")
            return df
    
    def _extract_burstiness_features(self, df):
        """
        Extract burstiness analysis features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with burstiness features
        """
        try:
            result_df = df.copy()
            
            if 'timestamp' not in df.columns:
                logger.warning("Timestamp column not found. Skipping burstiness features.")
                return result_df
            
            # Sort by timestamp
            df_sorted = result_df.sort_values('timestamp')
            
            # Calculate inter-transaction times
            inter_times = df_sorted['timestamp'].diff().dt.total_seconds().fillna(0)
            
            # Calculate burstiness coefficient
            if len(inter_times) > 1 and inter_times.std() > 0:
                burstiness = (inter_times.std() - inter_times.mean()) / (inter_times.std() + inter_times.mean())
            else:
                burstiness = 0
            
            result_df['burstiness_coefficient'] = burstiness
            
            # Calculate local burstiness (in sliding windows)
            window_sizes = [10, 50, 100]  # Number of transactions
            
            for window_size in window_sizes:
                local_burstiness = []
                
                for i in range(len(df_sorted)):
                    # Get window around current transaction
                    start_idx = max(0, i - window_size // 2)
                    end_idx = min(len(df_sorted), i + window_size // 2 + 1)
                    
                    window_inter_times = inter_times.iloc[start_idx:end_idx]
                    
                    # Calculate burstiness for window
                    if len(window_inter_times) > 1 and window_inter_times.std() > 0:
                        local_b = (window_inter_times.std() - window_inter_times.mean()) / (
                            window_inter_times.std() + window_inter_times.mean()
                        )
                    else:
                        local_b = 0
                    
                    local_burstiness.append(local_b)
                
                result_df[f'local_burstiness_{window_size}'] = local_burstiness
            
            # Detect burst periods
            # A burst is defined as a period with inter-transaction times significantly lower than average
            avg_inter_time = inter_times.mean()
            std_inter_time = inter_times.std()
            
            # Threshold for burst detection (2 standard deviations below mean)
            burst_threshold = max(0, avg_inter_time - 2 * std_inter_time)
            
            # Flag transactions in bursts
            result_df['is_in_burst'] = (inter_times < burst_threshold).astype(int)
            
            # Calculate burst duration (consecutive transactions in burst)
            burst_durations = []
            current_duration = 0
            
            for is_burst in result_df['is_in_burst']:
                if is_burst:
                    current_duration += 1
                else:
                    burst_durations.append(current_duration)
                    current_duration = 0
            
            # Add the last duration if we're still in a burst
            burst_durations.append(current_duration)
            
            # Map burst durations back to transactions
            burst_duration_map = []
            idx = 0
            for i, is_burst in enumerate(result_df['is_in_burst']):
                if is_burst:
                    burst_duration_map.append(burst_durations[idx])
                else:
                    burst_duration_map.append(0)
                    if i > 0 and not result_df['is_in_burst'].iloc[i-1]:
                        idx += 1
            
            result_df['burst_duration'] = burst_duration_map
            
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting burstiness features: {str(e)}")
            return df
    
    def _extract_gap_features(self, df):
        """
        Extract gap analysis features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with gap features
        """
        try:
            result_df = df.copy()
            
            if 'timestamp' not in df.columns:
                logger.warning("Timestamp column not found. Skipping gap features.")
                return result_df
            
            # Sort by timestamp
            df_sorted = result_df.sort_values('timestamp')
            
            # Calculate inter-transaction times
            inter_times = df_sorted['timestamp'].diff().dt.total_seconds().fillna(0)
            
            # Time since last transaction
            result_df['time_since_last_transaction'] = inter_times
            
            # Time until next transaction
            time_until_next = df_sorted['timestamp'].diff(-1).dt.total_seconds().abs().fillna(0)
            result_df['time_until_next_transaction'] = time_until_next
            
            # Detect gaps (unusually long inter-transaction times)
            if len(inter_times) > 1 and inter_times.std() > 0:
                # Threshold for gap detection (2 standard deviations above mean)
                gap_threshold = inter_times.mean() + 2 * inter_times.std()
                
                # Flag transactions after gaps
                result_df['is_after_gap'] = (inter_times > gap_threshold).astype(int)
                
                # Calculate gap sizes
                result_df['gap_size'] = np.where(inter_times > gap_threshold, inter_times, 0)
            else:
                result_df['is_after_gap'] = 0
                result_df['gap_size'] = 0
            
            # Calculate gap statistics by sender
            if 'sender_id' in df.columns:
                sender_gaps = []
                sender_gap_stats = {}
                
                # Calculate average gap for each sender
                for sender in df_sorted['sender_id'].unique():
                    sender_transactions = df_sorted[df_sorted['sender_id'] == sender].sort_values('timestamp')
                    if len(sender_transactions) > 1:
                        sender_inter_times = sender_transactions['timestamp'].diff().dt.total_seconds().fillna(0)
                        sender_gap_stats[sender] = {
                            'mean': sender_inter_times.mean(),
                            'std': sender_inter_times.std()
                        }
                
                # Calculate gap features for each transaction
                for _, row in df_sorted.iterrows():
                    sender = row['sender_id']
                    
                    if sender in sender_gap_stats:
                        stats = sender_gap_stats[sender]
                        
                        # Time since last transaction for this sender
                        sender_transactions = df_sorted[
                            (df_sorted['sender_id'] == sender) &
                            (df_sorted['timestamp'] < row['timestamp'])
                        ]
                        
                        if len(sender_transactions) > 0:
                            last_sender_time = sender_transactions['timestamp'].max()
                            sender_inter_time = (row['timestamp'] - last_sender_time).total_seconds()
                        else:
                            sender_inter_time = float('inf')
                        
                        # Calculate Z-score for this gap
                        if stats['std'] > 0:
                            gap_z_score = (sender_inter_time - stats['mean']) / stats['std']
                        else:
                            gap_z_score = 0
                        
                        sender_gaps.append({
                            'sender_time_since_last': sender_inter_time,
                            'sender_gap_z_score': gap_z_score
                        })
                    else:
                        sender_gaps.append({
                            'sender_time_since_last': float('inf'),
                            'sender_gap_z_score': 0
                        })
                
                # Add to result dataframe
                gap_df = pd.DataFrame(sender_gaps)
                result_df['sender_time_since_last'] = gap_df['sender_time_since_last']
                result_df['sender_gap_z_score'] = gap_df['sender_gap_z_score']
            
            # Calculate gap statistics by receiver
            if 'receiver_id' in df.columns:
                receiver_gaps = []
                receiver_gap_stats = {}
                
                # Calculate average gap for each receiver
                for receiver in df_sorted['receiver_id'].unique():
                    receiver_transactions = df_sorted[df_sorted['receiver_id'] == receiver].sort_values('timestamp')
                    if len(receiver_transactions) > 1:
                        receiver_inter_times = receiver_transactions['timestamp'].diff().dt.total_seconds().fillna(0)
                        receiver_gap_stats[receiver] = {
                            'mean': receiver_inter_times.mean(),
                            'std': receiver_inter_times.std()
                        }
                
                # Calculate gap features for each transaction
                for _, row in df_sorted.iterrows():
                    receiver = row['receiver_id']
                    
                    if receiver in receiver_gap_stats:
                        stats = receiver_gap_stats[receiver]
                        
                        # Time since last transaction to this receiver
                        receiver_transactions = df_sorted[
                            (df_sorted['receiver_id'] == receiver) &
                            (df_sorted['timestamp'] < row['timestamp'])
                        ]
                        
                        if len(receiver_transactions) > 0:
                            last_receiver_time = receiver_transactions['timestamp'].max()
                            receiver_inter_time = (row['timestamp'] - last_receiver_time).total_seconds()
                        else:
                            receiver_inter_time = float('inf')
                        
                        # Calculate Z-score for this gap
                        if stats['std'] > 0:
                            gap_z_score = (receiver_inter_time - stats['mean']) / stats['std']
                        else:
                            gap_z_score = 0
                        
                        receiver_gaps.append({
                            'receiver_time_since_last': receiver_inter_time,
                            'receiver_gap_z_score': gap_z_score
                        })
                    else:
                        receiver_gaps.append({
                            'receiver_time_since_last': float('inf'),
                            'receiver_gap_z_score': 0
                        })
                
                # Add to result dataframe
                gap_df = pd.DataFrame(receiver_gaps)
                result_df['receiver_time_since_last'] = gap_df['receiver_time_since_last']
                result_df['receiver_gap_z_score'] = gap_df['receiver_gap_z_score']
            
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting gap features: {str(e)}")
            return df
    
    def _extract_seasonal_features(self, df):
        """
        Extract seasonal features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with seasonal features
        """
        try:
            result_df = df.copy()
            
            if 'timestamp' not in df.columns:
                logger.warning("Timestamp column not found. Skipping seasonal features.")
                return result_df
            
            # Sort by timestamp
            df_sorted = result_df.sort_values('timestamp')
            
            # Create a time series of transaction counts
            # Resample to different frequencies
            frequencies = ['1H', '6H', '1D', '1W']
            
            for freq in frequencies:
                # Count transactions per time period
                ts = df_sorted.set_index('timestamp').resample(freq).size()
                
                if len(ts) > 10:  # Need enough data points for decomposition
                    try:
                        # Perform seasonal decomposition
                        decomposition = seasonal_decompose(ts, model='additive', period=min(24, len(ts)//2))
                        
                        # Extract components
                        trend = decomposition.trend
                        seasonal = decomposition.seasonal
                        residual = decomposition.resid
                        
                        # Map back to original dataframe
                        trend_map = {}
                        seasonal_map = {}
                        residual_map = {}
                        
                        for timestamp in df_sorted['timestamp']:
                            # Find the closest time period
                            period_start = timestamp.floor(freq)
                            
                            if period_start in trend.index:
                                trend_map[timestamp] = trend[period_start]
                                seasonal_map[timestamp] = seasonal[period_start]
                                residual_map[timestamp] = residual[period_start]
                            else:
                                trend_map[timestamp] = 0
                                seasonal_map[timestamp] = 0
                                residual_map[timestamp] = 0
                        
                        # Add to result dataframe
                        result_df[f'trend_{freq}'] = df_sorted['timestamp'].map(trend_map).fillna(0)
                        result_df[f'seasonal_{freq}'] = df_sorted['timestamp'].map(seasonal_map).fillna(0)
                        result_df[f'residual_{freq}'] = df_sorted['timestamp'].map(residual_map).fillna(0)
                        
                        # Calculate anomaly score based on residual
                        residual_mean = residual.mean()
                        residual_std = residual.std()
                        
                        if residual_std > 0:
                            anomaly_scores = (df_sorted['timestamp'].map(residual_map) - residual_mean) / residual_std
                            result_df[f'seasonal_anomaly_{freq}'] = anomaly_scores.fillna(0)
                        else:
                            result_df[f'seasonal_anomaly_{freq}'] = 0
                            
                    except Exception as e:
                        logger.warning(f"Error in seasonal decomposition for {freq}: {str(e)}")
            
            # Calculate periodicity features
            # Check for daily, weekly, and monthly patterns
            periodicity_features = {}
            
            # Daily pattern (hour of day)
            if 'hour' in result_df.columns:
                hourly_counts = result_df.groupby('hour').size()
                # Calculate entropy to measure uniformity
                hourly_probs = hourly_counts / hourly_counts.sum()
                daily_entropy = -sum(p * np.log2(p) for p in hourly_probs if p > 0)
                max_entropy = np.log2(24)  # Maximum possible entropy for 24 hours
                daily_uniformity = daily_entropy / max_entropy if max_entropy > 0 else 0
                
                periodicity_features['daily_pattern_strength'] = 1 - daily_uniformity  # Higher means stronger pattern
                
                # Calculate peak hour
                peak_hour = hourly_counts.idxmax()
                periodicity_features['peak_hour'] = peak_hour
            
            # Weekly pattern (day of week)
            if 'dayofweek' in result_df.columns:
                weekly_counts = result_df.groupby('dayofweek').size()
                # Calculate entropy to measure uniformity
                weekly_probs = weekly_counts / weekly_counts.sum()
                weekly_entropy = -sum(p * np.log2(p) for p in weekly_probs if p > 0)
                max_entropy = np.log2(7)  # Maximum possible entropy for 7 days
                weekly_uniformity = weekly_entropy / max_entropy if max_entropy > 0 else 0
                
                periodicity_features['weekly_pattern_strength'] = 1 - weekly_uniformity  # Higher means stronger pattern
                
                # Calculate peak day
                peak_day = weekly_counts.idxmax()
                periodicity_features['peak_day'] = peak_day
            
            # Add periodicity features to all rows
            for feature, value in periodicity_features.items():
                result_df[feature] = value
            
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting seasonal features: {str(e)}")
            return df
    
    def _extract_autocorrelation_features(self, df):
        """
        Extract autocorrelation features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with autocorrelation features
        """
        try:
            result_df = df.copy()
            
            if 'timestamp' not in df.columns:
                logger.warning("Timestamp column not found. Skipping autocorrelation features.")
                return result_df
            
            # Sort by timestamp
            df_sorted = result_df.sort_values('timestamp')
            
            # Create time series of transaction counts
            # Resample to different frequencies
            frequencies = ['1H', '6H', '1D']
            
            for freq in frequencies:
                # Count transactions per time period
                ts = df_sorted.set_index('timestamp').resample(freq).size()
                
                if len(ts) > 10:  # Need enough data points for autocorrelation
                    try:
                        # Calculate autocorrelation function
                        nlags = min(10, len(ts) // 2)
                        autocorr = acf(ts, nlags=nlags, fft=True)
                        
                        # Add autocorrelation features
                        for i in range(1, min(6, len(autocorr))):  # First 5 lags
                            result_df[f'autocorr_{freq}_lag_{i}'] = autocorr[i]
                        
                        # Calculate partial autocorrelation
                        pacf_values = pacf(ts, nlags=nlags)
                        
                        # Add partial autocorrelation features
                        for i in range(1, min(6, len(pacf_values))):  # First 5 lags
                            result_df[f'pacf_{freq}_lag_{i}'] = pacf_values[i]
                        
                        # Detect periodicity in autocorrelation
                        # Look for peaks in autocorrelation
                        peaks, _ = find_peaks(autocorr[1:], height=0.2)  # Ignore lag 0
                        
                        if len(peaks) > 0:
                            # Find the most significant peak
                            peak_lag = peaks[0] + 1  # +1 because we ignored lag 0
                            result_df[f'periodicity_{freq}'] = peak_lag
                            result_df[f'periodicity_strength_{freq}'] = autocorr[peak_lag]
                        else:
                            result_df[f'periodicity_{freq}'] = 0
                            result_df[f'periodicity_strength_{freq}'] = 0
                            
                    except Exception as e:
                        logger.warning(f"Error in autocorrelation analysis for {freq}: {str(e)}")
            
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting autocorrelation features: {str(e)}")
            return df
    
    def fit_transform(self, df):
        """
        Fit the feature extractor and transform the data
        
        Args:
            df (DataFrame): Input data
            
        Returns:
            DataFrame: Transformed data with features
        """
        # Extract features
        result_df = self.extract_features(df)
        
        # Get feature columns
        feature_cols = [col for col in result_df.columns if col not in df.columns]
        
        if len(feature_cols) > 0:
            self.fitted = True
        
        return result_df
    
    def transform(self, df):
        """
        Transform new data using fitted feature extractor
        
        Args:
            df (DataFrame): Input data
            
        Returns:
            DataFrame: Transformed data with features
        """
        if not self.fitted:
            raise ValueError("Feature extractor not fitted. Call fit_transform first.")
        
        # Extract features
        result_df = self.extract_features(df)
        
        return result_df

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\features\__init__.py ===

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\features\.ipynb_checkpoints\graph_features-checkpoint.py ===
"""
Graph Features Module
Implements graph-based feature extraction techniques for fraud detection
"""

import pandas as pd
import numpy as np
import networkx as nx
from collections import defaultdict, Counter
import community as community_louvain
from sklearn.preprocessing import MinMaxScaler
import warnings
import logging
from typing import Dict, List, Tuple, Union

warnings.filterwarnings('ignore')
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class GraphFeatures:
    """
    Class for extracting graph-based features from transaction data
    Implements techniques like centrality measures, clustering coefficients, etc.
    """
    
    def __init__(self, config=None):
        """
        Initialize GraphFeatures
        
        Args:
            config (dict, optional): Configuration parameters
        """
        self.config = config or {}
        self.graph = None
        self.sender_graph = None
        self.receiver_graph = None
        self.bipartite_graph = None
        self.feature_names = []
        self.scaler = MinMaxScaler()
        self.fitted = False
        
    def extract_features(self, df):
        """
        Extract all graph features from the dataframe
        
        Args:
            df (DataFrame): Input transaction data
            
        Returns:
            DataFrame: DataFrame with extracted features
        """
        try:
            # Create a copy to avoid modifying the original
            result_df = df.copy()
            
            # Build graphs
            self._build_graphs(df)
            
            # Extract different types of graph features
            result_df = self._extract_centrality_features(result_df)
            result_df = self._extract_clustering_features(result_df)
            result_df = self._extract_community_features(result_df)
            result_df = self._extract_path_features(result_df)
            result_df = self._extract_subgraph_features(result_df)
            result_df = self._extract_temporal_graph_features(result_df)
            
            # Store feature names
            self.feature_names = [col for col in result_df.columns if col not in df.columns]
            
            logger.info(f"Extracted {len(self.feature_names)} graph features")
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting graph features: {str(e)}")
            raise
    
    def _build_graphs(self, df):
        """
        Build various graphs from the transaction data
        
        Args:
            df (DataFrame): Input transaction data
        """
        try:
            # Build transaction graph (directed)
            self.graph = nx.DiGraph()
            
            # Add edges with attributes
            for _, row in df.iterrows():
                if 'sender_id' in row and 'receiver_id' in row:
                    sender = row['sender_id']
                    receiver = row['receiver_id']
                    
                    # Get edge attributes
                    attrs = {}
                    if 'amount' in row:
                        attrs['amount'] = row['amount']
                    if 'timestamp' in row:
                        attrs['timestamp'] = row['timestamp']
                    if 'transaction_id' in row:
                        attrs['transaction_id'] = row['transaction_id']
                    
                    # Add edge or update existing edge
                    if self.graph.has_edge(sender, receiver):
                        # Update existing edge
                        edge_data = self.graph[sender][receiver]
                        if 'amount' in attrs:
                            edge_data['total_amount'] = edge_data.get('total_amount', 0) + attrs['amount']
                        edge_data['transaction_count'] = edge_data.get('transaction_count', 0) + 1
                        edge_data['transactions'].append(attrs)
                    else:
                        # Add new edge
                        attrs['total_amount'] = attrs.get('amount', 0)
                        attrs['transaction_count'] = 1
                        attrs['transactions'] = [attrs]
                        self.graph.add_edge(sender, receiver, **attrs)
            
            # Build sender graph (undirected, senders connected if they have common receivers)
            self.sender_graph = nx.Graph()
            
            # Create a mapping from receivers to senders
            receiver_to_senders = defaultdict(set)
            for _, row in df.iterrows():
                if 'sender_id' in row and 'receiver_id' in row:
                    receiver_to_senders[row['receiver_id']].add(row['sender_id'])
            
            # Connect senders with common receivers
            for receiver, senders in receiver_to_senders.items():
                senders_list = list(senders)
                for i in range(len(senders_list)):
                    for j in range(i+1, len(senders_list)):
                        sender1 = senders_list[i]
                        sender2 = senders_list[j]
                        
                        if self.sender_graph.has_edge(sender1, sender2):
                            self.sender_graph[sender1][sender2]['common_receivers'] += 1
                        else:
                            self.sender_graph.add_edge(sender1, sender2, common_receivers=1)
            
            # Build receiver graph (undirected, receivers connected if they have common senders)
            self.receiver_graph = nx.Graph()
            
            # Create a mapping from senders to receivers
            sender_to_receivers = defaultdict(set)
            for _, row in df.iterrows():
                if 'sender_id' in row and 'receiver_id' in row:
                    sender_to_receivers[row['sender_id']].add(row['receiver_id'])
            
            # Connect receivers with common senders
            for sender, receivers in sender_to_receivers.items():
                receivers_list = list(receivers)
                for i in range(len(receivers_list)):
                    for j in range(i+1, len(receivers_list)):
                        receiver1 = receivers_list[i]
                        receiver2 = receivers_list[j]
                        
                        if self.receiver_graph.has_edge(receiver1, receiver2):
                            self.receiver_graph[receiver1][receiver2]['common_senders'] += 1
                        else:
                            self.receiver_graph.add_edge(receiver1, receiver2, common_senders=1)
            
            # Build bipartite graph (senders and receivers as two separate sets)
            self.bipartite_graph = nx.Graph()
            
            # Add nodes with bipartite attribute
            for _, row in df.iterrows():
                if 'sender_id' in row:
                    self.bipartite_graph.add_node(row['sender_id'], bipartite=0)
                if 'receiver_id' in row:
                    self.bipartite_graph.add_node(row['receiver_id'], bipartite=1)
            
            # Add edges
            for _, row in df.iterrows():
                if 'sender_id' in row and 'receiver_id' in row:
                    sender = row['sender_id']
                    receiver = row['receiver_id']
                    
                    # Get edge attributes
                    attrs = {}
                    if 'amount' in row:
                        attrs['amount'] = row['amount']
                    if 'timestamp' in row:
                        attrs['timestamp'] = row['timestamp']
                    if 'transaction_id' in row:
                        attrs['transaction_id'] = row['transaction_id']
                    
                    # Add edge
                    self.bipartite_graph.add_edge(sender, receiver, **attrs)
            
            logger.info("Graphs built successfully")
            
        except Exception as e:
            logger.error(f"Error building graphs: {str(e)}")
            raise
    
    def _extract_centrality_features(self, df):
        """
        Extract centrality-based features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with centrality features
        """
        try:
            result_df = df.copy()
            
            if self.graph is None:
                logger.warning("Graph not built. Skipping centrality features.")
                return result_df
            
            # Calculate degree centrality
            in_degree_centrality = nx.in_degree_centrality(self.graph)
            out_degree_centrality = nx.out_degree_centrality(self.graph)
            degree_centrality = nx.degree_centrality(self.graph.to_undirected())
            
            # Map to dataframe
            if 'sender_id' in df.columns:
                result_df['sender_in_degree_centrality'] = df['sender_id'].map(in_degree_centrality).fillna(0)
                result_df['sender_out_degree_centrality'] = df['sender_id'].map(out_degree_centrality).fillna(0)
                result_df['sender_degree_centrality'] = df['sender_id'].map(degree_centrality).fillna(0)
            
            if 'receiver_id' in df.columns:
                result_df['receiver_in_degree_centrality'] = df['receiver_id'].map(in_degree_centrality).fillna(0)
                result_df['receiver_out_degree_centrality'] = df['receiver_id'].map(out_degree_centrality).fillna(0)
                result_df['receiver_degree_centrality'] = df['receiver_id'].map(degree_centrality).fillna(0)
            
            # Calculate betweenness centrality (sample for large graphs)
            if len(self.graph.nodes()) <= 1000:
                betweenness_centrality = nx.betweenness_centrality(self.graph)
            else:
                # Sample nodes for betweenness calculation
                sample_nodes = list(self.graph.nodes())[:1000]
                betweenness_centrality = nx.betweenness_centrality(self.graph, k=len(sample_nodes))
            
            # Map to dataframe
            if 'sender_id' in df.columns:
                result_df['sender_betweenness_centrality'] = df['sender_id'].map(betweenness_centrality).fillna(0)
            
            if 'receiver_id' in df.columns:
                result_df['receiver_betweenness_centrality'] = df['receiver_id'].map(betweenness_centrality).fillna(0)
            
            # Calculate closeness centrality (sample for large graphs)
            if len(self.graph.nodes()) <= 1000:
                closeness_centrality = nx.closeness_centrality(self.graph)
            else:
                # Use approximate closeness for large graphs
                closeness_centrality = nx.closeness_centrality(self.graph, distance='weight')
            
            # Map to dataframe
            if 'sender_id' in df.columns:
                result_df['sender_closeness_centrality'] = df['sender_id'].map(closeness_centrality).fillna(0)
            
            if 'receiver_id' in df.columns:
                result_df['receiver_closeness_centrality'] = df['receiver_id'].map(closeness_centrality).fillna(0)
            
            # Calculate eigenvector centrality (sample for large graphs)
            if len(self.graph.nodes()) <= 1000:
                try:
                    eigenvector_centrality = nx.eigenvector_centrality(self.graph, max_iter=1000)
                except:
                    eigenvector_centrality = {}
            else:
                eigenvector_centrality = {}
            
            # Map to dataframe
            if 'sender_id' in df.columns:
                result_df['sender_eigenvector_centrality'] = df['sender_id'].map(eigenvector_centrality).fillna(0)
            
            if 'receiver_id' in df.columns:
                result_df['receiver_eigenvector_centrality'] = df['receiver_id'].map(eigenvector_centrality).fillna(0)
            
            # Calculate PageRank
            pagerank = nx.pagerank(self.graph)
            
            # Map to dataframe
            if 'sender_id' in df.columns:
                result_df['sender_pagerank'] = df['sender_id'].map(pagerank).fillna(0)
            
            if 'receiver_id' in df.columns:
                result_df['receiver_pagerank'] = df['receiver_id'].map(pagerank).fillna(0)
            
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting centrality features: {str(e)}")
            return df
    
    def _extract_clustering_features(self, df):
        """
        Extract clustering-based features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with clustering features
        """
        try:
            result_df = df.copy()
            
            if self.graph is None:
                logger.warning("Graph not built. Skipping clustering features.")
                return result_df
            
            # Calculate clustering coefficient for transaction graph
            clustering_coeff = nx.clustering(self.graph.to_undirected())
            
            # Map to dataframe
            if 'sender_id' in df.columns:
                result_df['sender_clustering_coefficient'] = df['sender_id'].map(clustering_coeff).fillna(0)
            
            if 'receiver_id' in df.columns:
                result_df['receiver_clustering_coefficient'] = df['receiver_id'].map(clustering_coeff).fillna(0)
            
            # Calculate clustering coefficient for sender graph
            if self.sender_graph is not None and len(self.sender_graph.nodes()) > 0:
                sender_clustering_coeff = nx.clustering(self.sender_graph)
                
                # Map to dataframe
                if 'sender_id' in df.columns:
                    result_df['sender_graph_clustering_coefficient'] = df['sender_id'].map(sender_clustering_coeff).fillna(0)
            
            # Calculate clustering coefficient for receiver graph
            if self.receiver_graph is not None and len(self.receiver_graph.nodes()) > 0:
                receiver_clustering_coeff = nx.clustering(self.receiver_graph)
                
                # Map to dataframe
                if 'receiver_id' in df.columns:
                    result_df['receiver_graph_clustering_coefficient'] = df['receiver_id'].map(receiver_clustering_coeff).fillna(0)
            
            # Calculate average neighbor degree
            avg_neighbor_degree = nx.average_neighbor_degree(self.graph.to_undirected())
            
            # Map to dataframe
            if 'sender_id' in df.columns:
                result_df['sender_avg_neighbor_degree'] = df['sender_id'].map(avg_neighbor_degree).fillna(0)
            
            if 'receiver_id' in df.columns:
                result_df['receiver_avg_neighbor_degree'] = df['receiver_id'].map(avg_neighbor_degree).fillna(0)
            
            # Calculate square clustering
            square_clustering = nx.square_clustering(self.graph.to_undirected())
            
            # Map to dataframe
            if 'sender_id' in df.columns:
                result_df['sender_square_clustering'] = df['sender_id'].map(square_clustering).fillna(0)
            
            if 'receiver_id' in df.columns:
                result_df['receiver_square_clustering'] = df['receiver_id'].map(square_clustering).fillna(0)
            
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting clustering features: {str(e)}")
            return df
    
    def _extract_community_features(self, df):
        """
        Extract community-based features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with community features
        """
        try:
            result_df = df.copy()
            
            if self.graph is None:
                logger.warning("Graph not built. Skipping community features.")
                return result_df
            
            # Detect communities using Louvain algorithm
            undirected_graph = self.graph.to_undirected()
            communities = community_louvain.best_partition(undirected_graph)
            
            # Map to dataframe
            if 'sender_id' in df.columns:
                result_df['sender_community'] = df['sender_id'].map(communities).fillna(-1)
            
            if 'receiver_id' in df.columns:
                result_df['receiver_community'] = df['receiver_id'].map(communities).fillna(-1)
            
            # Calculate community size for each node
            community_sizes = Counter(communities.values())
            node_community_sizes = {node: community_sizes[comm] for node, comm in communities.items()}
            
            # Map to dataframe
            if 'sender_id' in df.columns:
                result_df['sender_community_size'] = df['sender_id'].map(node_community_sizes).fillna(0)
            
            if 'receiver_id' in df.columns:
                result_df['receiver_community_size'] = df['receiver_id'].map(node_community_sizes).fillna(0)
            
            # Calculate community degree centrality
            community_degree_centrality = {}
            for comm, nodes in community_sizes.items():
                comm_nodes = [node for node, c in communities.items() if c == comm]
                subgraph = undirected_graph.subgraph(comm_nodes)
                if len(subgraph.nodes()) > 0:
                    comm_centrality = nx.degree_centrality(subgraph)
                    for node in comm_nodes:
                        community_degree_centrality[node] = comm_centrality.get(node, 0)
            
            # Map to dataframe
            if 'sender_id' in df.columns:
                result_df['sender_community_degree_centrality'] = df['sender_id'].map(community_degree_centrality).fillna(0)
            
            if 'receiver_id' in df.columns:
                result_df['receiver_community_degree_centrality'] = df['receiver_id'].map(community_degree_centrality).fillna(0)
            
            # Check if sender and receiver are in the same community
            if 'sender_id' in df.columns and 'receiver_id' in df.columns:
                same_community = []
                for _, row in df.iterrows():
                    sender = row['sender_id']
                    receiver = row['receiver_id']
                    
                    if sender in communities and receiver in communities:
                        same_community.append(int(communities[sender] == communities[receiver]))
                    else:
                        same_community.append(0)
                
                result_df['same_community'] = same_community
            
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting community features: {str(e)}")
            return df
    
    def _extract_path_features(self, df):
        """
        Extract path-based features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with path features
        """
        try:
            result_df = df.copy()
            
            if self.graph is None:
                logger.warning("Graph not built. Skipping path features.")
                return result_df
            
            # Calculate shortest path lengths (sample for large graphs)
            if len(self.graph.nodes()) <= 500:
                # For small graphs, calculate all pairs shortest paths
                shortest_paths = dict(nx.all_pairs_shortest_path_length(self.graph))
                
                # Map to dataframe
                if 'sender_id' in df.columns and 'receiver_id' in df.columns:
                    path_lengths = []
                    for _, row in df.iterrows():
                        sender = row['sender_id']
                        receiver = row['receiver_id']
                        
                        if sender in shortest_paths and receiver in shortest_paths[sender]:
                            path_lengths.append(shortest_paths[sender][receiver])
                        else:
                            path_lengths.append(float('inf'))
                    
                    result_df['shortest_path_length'] = path_lengths
            else:
                # For large graphs, sample or use approximation
                if 'sender_id' in df.columns and 'receiver_id' in df.columns:
                    # Use BFS for a limited depth
                    path_lengths = []
                    for _, row in df.iterrows():
                        sender = row['sender_id']
                        receiver = row['receiver_id']
                        
                        try:
                            path_length = nx.shortest_path_length(self.graph, sender, receiver)
                            path_lengths.append(path_length)
                        except:
                            path_lengths.append(float('inf'))
                    
                    result_df['shortest_path_length'] = path_lengths
            
            # Calculate number of common neighbors
            if 'sender_id' in df.columns and 'receiver_id' in df.columns:
                common_neighbors = []
                for _, row in df.iterrows():
                    sender = row['sender_id']
                    receiver = row['receiver_id']
                    
                    try:
                        sender_neighbors = set(self.graph.predecessors(sender)) | set(self.graph.successors(sender))
                        receiver_neighbors = set(self.graph.predecessors(receiver)) | set(self.graph.successors(receiver))
                        common_neighbors.append(len(sender_neighbors & receiver_neighbors))
                    except:
                        common_neighbors.append(0)
                
                result_df['common_neighbors_count'] = common_neighbors
            
            # Calculate Jaccard coefficient
            if 'sender_id' in df.columns and 'receiver_id' in df.columns:
                jaccard_coeffs = []
                for _, row in df.iterrows():
                    sender = row['sender_id']
                    receiver = row['receiver_id']
                    
                    try:
                        sender_neighbors = set(self.graph.predecessors(sender)) | set(self.graph.successors(sender))
                        receiver_neighbors = set(self.graph.predecessors(receiver)) | set(self.graph.successors(receiver))
                        
                        union = sender_neighbors | receiver_neighbors
                        intersection = sender_neighbors & receiver_neighbors
                        
                        if len(union) > 0:
                            jaccard = len(intersection) / len(union)
                        else:
                            jaccard = 0
                        
                        jaccard_coeffs.append(jaccard)
                    except:
                        jaccard_coeffs.append(0)
                
                result_df['jaccard_coefficient'] = jaccard_coeffs
            
            # Calculate Adamic-Adar index
            if 'sender_id' in df.columns and 'receiver_id' in df.columns:
                adamic_adar = []
                for _, row in df.iterrows():
                    sender = row['sender_id']
                    receiver = row['receiver_id']
                    
                    try:
                        sender_neighbors = set(self.graph.predecessors(sender)) | set(self.graph.successors(sender))
                        receiver_neighbors = set(self.graph.predecessors(receiver)) | set(self.graph.successors(receiver))
                        common = sender_neighbors & receiver_neighbors
                        
                        # Calculate sum of 1/log(degree) for common neighbors
                        aa_index = 0
                        for node in common:
                            degree = self.graph.degree(node)
                            if degree > 1:
                                aa_index += 1 / np.log(degree)
                        
                        adamic_adar.append(aa_index)
                    except:
                        adamic_adar.append(0)
                
                result_df['adamic_adar_index'] = adamic_adar
            
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting path features: {str(e)}")
            return df
    
    def _extract_subgraph_features(self, df):
        """
        Extract subgraph-based features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with subgraph features
        """
        try:
            result_df = df.copy()
            
            if self.graph is None:
                logger.warning("Graph not built. Skipping subgraph features.")
                return result_df
            
            # Calculate ego network features for senders
            if 'sender_id' in df.columns:
                sender_ego_sizes = []
                sender_ego_densities = []
                
                for sender in df['sender_id']:
                    try:
                        # Get ego network
                        ego_graph = nx.ego_graph(self.graph.to_undirected(), sender, radius=1)
                        
                        # Calculate size and density
                        ego_size = len(ego_graph.nodes())
                        ego_density = nx.density(ego_graph)
                        
                        sender_ego_sizes.append(ego_size)
                        sender_ego_densities.append(ego_density)
                    except:
                        sender_ego_sizes.append(0)
                        sender_ego_densities.append(0)
                
                result_df['sender_ego_network_size'] = sender_ego_sizes
                result_df['sender_ego_network_density'] = sender_ego_densities
            
            # Calculate ego network features for receivers
            if 'receiver_id' in df.columns:
                receiver_ego_sizes = []
                receiver_ego_densities = []
                
                for receiver in df['receiver_id']:
                    try:
                        # Get ego network
                        ego_graph = nx.ego_graph(self.graph.to_undirected(), receiver, radius=1)
                        
                        # Calculate size and density
                        ego_size = len(ego_graph.nodes())
                        ego_density = nx.density(ego_graph)
                        
                        receiver_ego_sizes.append(ego_size)
                        receiver_ego_densities.append(ego_density)
                    except:
                        receiver_ego_sizes.append(0)
                        receiver_ego_densities.append(0)
                
                result_df['receiver_ego_network_size'] = receiver_ego_sizes
                result_df['receiver_ego_network_density'] = receiver_ego_densities
            
            # Calculate bipartite projection features
            if self.bipartite_graph is not None:
                # Get sender and receiver sets
                senders = {n for n, d in self.bipartite_graph.nodes(data=True) if d['bipartite'] == 0}
                receivers = {n for n, d in self.bipartite_graph.nodes(data=True) if d['bipartite'] == 1}
                
                # Project to sender graph
                sender_projection = nx.bipartite.projected_graph(self.bipartite_graph, senders)
                
                # Calculate features for sender projection
                if 'sender_id' in df.columns:
                    sender_proj_degrees = []
                    for sender in df['sender_id']:
                        try:
                            degree = sender_projection.degree(sender)
                            sender_proj_degrees.append(degree)
                        except:
                            sender_proj_degrees.append(0)
                    
                    result_df['sender_projection_degree'] = sender_proj_degrees
                
                # Project to receiver graph
                receiver_projection = nx.bipartite.projected_graph(self.bipartite_graph, receivers)
                
                # Calculate features for receiver projection
                if 'receiver_id' in df.columns:
                    receiver_proj_degrees = []
                    for receiver in df['receiver_id']:
                        try:
                            degree = receiver_projection.degree(receiver)
                            receiver_proj_degrees.append(degree)
                        except:
                            receiver_proj_degrees.append(0)
                    
                    result_df['receiver_projection_degree'] = receiver_proj_degrees
            
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting subgraph features: {str(e)}")
            return df
    
    def _extract_temporal_graph_features(self, df):
        """
        Extract temporal graph features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with temporal graph features
        """
        try:
            result_df = df.copy()
            
            if self.graph is None or 'timestamp' not in df.columns:
                logger.warning("Graph not built or timestamp not available. Skipping temporal graph features.")
                return result_df
            
            # Convert timestamp to datetime if needed
            if not pd.api.types.is_datetime64_any_dtype(df['timestamp']):
                df['timestamp'] = pd.to_datetime(df['timestamp'])
            
            # Sort by timestamp
            df_sorted = df.sort_values('timestamp')
            
            # Calculate time window features
            time_windows = ['1H', '6H', '24H', '7D']
            
            for window in time_windows:
                # Calculate sender activity in time window
                if 'sender_id' in df.columns:
                    sender_activity = []
                    
                    for _, row in df_sorted.iterrows():
                        sender = row['sender_id']
                        timestamp = row['timestamp']
                        
                        # Get transactions in time window before current transaction
                        window_start = timestamp - pd.Timedelta(window)
                        window_end = timestamp
                        
                        window_transactions = df_sorted[
                            (df_sorted['timestamp'] >= window_start) &
                            (df_sorted['timestamp'] < window_end) &
                            (df_sorted['sender_id'] == sender)
                        ]
                        
                        # Calculate activity metrics
                        activity_count = len(window_transactions)
                        activity_amount = window_transactions['amount'].sum() if 'amount' in window_transactions.columns else 0
                        
                        sender_activity.append({
                            f'sender_activity_count_{window}': activity_count,
                            f'sender_activity_amount_{window}': activity_amount
                        })
                    
                    # Add to result dataframe
                    activity_df = pd.DataFrame(sender_activity)
                    for col in activity_df.columns:
                        result_df[col] = activity_df[col].values
                
                # Calculate receiver activity in time window
                if 'receiver_id' in df.columns:
                    receiver_activity = []
                    
                    for _, row in df_sorted.iterrows():
                        receiver = row['receiver_id']
                        timestamp = row['timestamp']
                        
                        # Get transactions in time window before current transaction
                        window_start = timestamp - pd.Timedelta(window)
                        window_end = timestamp
                        
                        window_transactions = df_sorted[
                            (df_sorted['timestamp'] >= window_start) &
                            (df_sorted['timestamp'] < window_end) &
                            (df_sorted['receiver_id'] == receiver)
                        ]
                        
                        # Calculate activity metrics
                        activity_count = len(window_transactions)
                        activity_amount = window_transactions['amount'].sum() if 'amount' in window_transactions.columns else 0
                        
                        receiver_activity.append({
                            f'receiver_activity_count_{window}': activity_count,
                            f'receiver_activity_amount_{window}': activity_amount
                        })
                    
                    # Add to result dataframe
                    activity_df = pd.DataFrame(receiver_activity)
                    for col in activity_df.columns:
                        result_df[col] = activity_df[col].values
            
            # Calculate time since last transaction for sender
            if 'sender_id' in df.columns:
                time_since_last = []
                
                for _, row in df_sorted.iterrows():
                    sender = row['sender_id']
                    timestamp = row['timestamp']
                    
                    # Get previous transaction from same sender
                    prev_transactions = df_sorted[
                        (df_sorted['timestamp'] < timestamp) &
                        (df_sorted['sender_id'] == sender)
                    ]
                    
                    if len(prev_transactions) > 0:
                        last_timestamp = prev_transactions['timestamp'].max()
                        time_diff = (timestamp - last_timestamp).total_seconds()
                        time_since_last.append(time_diff)
                    else:
                        time_since_last.append(float('inf'))
                
                result_df['sender_time_since_last_transaction'] = time_since_last
            
            # Calculate time since last transaction for receiver
            if 'receiver_id' in df.columns:
                time_since_last = []
                
                for _, row in df_sorted.iterrows():
                    receiver = row['receiver_id']
                    timestamp = row['timestamp']
                    
                    # Get previous transaction to same receiver
                    prev_transactions = df_sorted[
                        (df_sorted['timestamp'] < timestamp) &
                        (df_sorted['receiver_id'] == receiver)
                    ]
                    
                    if len(prev_transactions) > 0:
                        last_timestamp = prev_transactions['timestamp'].max()
                        time_diff = (timestamp - last_timestamp).total_seconds()
                        time_since_last.append(time_diff)
                    else:
                        time_since_last.append(float('inf'))
                
                result_df['receiver_time_since_last_transaction'] = time_since_last
            
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting temporal graph features: {str(e)}")
            return df
    
    def fit_transform(self, df):
        """
        Fit the feature extractor and transform the data
        
        Args:
            df (DataFrame): Input data
            
        Returns:
            DataFrame: Transformed data with features
        """
        # Extract features
        result_df = self.extract_features(df)
        
        # Get feature columns
        feature_cols = [col for col in result_df.columns if col not in df.columns]
        
        if len(feature_cols) > 0:
            # Fit scaler
            self.scaler.fit(result_df[feature_cols])
            
            self.fitted = True
        
        return result_df
    
    def transform(self, df):
        """
        Transform new data using fitted feature extractor
        
        Args:
            df (DataFrame): Input data
            
        Returns:
            DataFrame: Transformed data with features
        """
        if not self.fitted:
            raise ValueError("Feature extractor not fitted. Call fit_transform first.")
        
        # Extract features
        result_df = self.extract_features(df)
        
        # Get feature columns
        feature_cols = [col for col in result_df.columns if col not in df.columns]
        
        if len(feature_cols) > 0:
            # Transform features using fitted scaler
            result_df[feature_cols] = self.scaler.transform(result_df[feature_cols])
        
        return result_df

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\features\.ipynb_checkpoints\nlp_features-checkpoint.py ===
"""
NLP Features Module
Implements natural language processing features for fraud detection
"""

import pandas as pd
import numpy as np
import re
import string
from collections import Counter
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.stem import WordNetLemmatizer, PorterStemmer
from nltk.sentiment import SentimentIntensityAnalyzer
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation
from textblob import TextBlob
import gensim
from gensim.models import Word2Vec, Doc2Vec
from gensim.models.doc2vec import TaggedDocument
import warnings
import logging
from typing import Dict, List, Tuple, Union

from fraud_detection_engine.utils.api_utils import is_api_available

warnings.filterwarnings('ignore')
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Download required NLTK data
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')
    
try:
    nltk.data.find('corpora/stopwords')
except LookupError:
    nltk.download('stopwords')
    
try:
    nltk.data.find('corpora/wordnet')
except LookupError:
    nltk.download('wordnet')
    
try:
    nltk.data.find('sentiment/vader_lexicon')
except LookupError:
    nltk.download('vader_lexicon')

class NLPFeatures:
    """
    Class for extracting NLP features from transaction data
    Implements techniques like sentiment analysis, topic modeling, etc.
    """
    
    def __init__(self, config=None):
        """
        Initialize NLPFeatures
        
        Args:
            config (dict, optional): Configuration parameters
        """
        self.config = config or {}
        self.stop_words = set(stopwords.words('english'))
        self.lemmatizer = WordNetLemmatizer()
        self.stemmer = PorterStemmer()
        self.sia = SentimentIntensityAnalyzer()
        self.tfidf_vectorizer = None
        self.count_vectorizer = None
        self.lda_model = None
        self.word2vec_model = None
        self.doc2vec_model = None
        self.feature_names = []
        self.fitted = False
        
        # Fraud-related keywords
        self.fraud_keywords = [
            'urgent', 'immediately', 'asap', 'hurry', 'quick', 'fast',
            'secret', 'confidential', 'private', 'hidden', 'discreet',
            'suspicious', 'unusual', 'strange', 'odd', 'weird',
            'illegal', 'fraud', 'scam', 'fake', 'counterfeit',
            'money', 'cash', 'payment', 'transfer', 'wire',
            'overseas', 'foreign', 'international', 'abroad',
            'inheritance', 'lottery', 'prize', 'winner', 'claim',
            'verify', 'confirm', 'update', 'account', 'information',
            'click', 'link', 'website', 'login', 'password',
            'bank', 'check', 'routing', 'account', 'number'
        ]
        
        # Suspicious patterns
        self.suspicious_patterns = [
            r'\$\d+,\d+\.\d{2}',  # Money format
            r'\d{4}[\s-]?\d{4}[\s-]?\d{4}[\s-]?\d{4}',  # Credit card pattern
            r'\b\d{3}-\d{2}-\d{4}\b',  # SSN pattern
            r'\b[A-Z0-9._%+-]+@[A-Z0-9.-]+\.[A-Z]{2,}\b',  # Email pattern
            r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+',  # URL pattern
            r'\b\d{10,}\b',  # Long numbers (could be account numbers)
            r'[A-Z]{2,}',  # All caps words
            r'\d+\.\d+\.\d+\.\d+',  # IP address pattern
        ]
    
    def extract_features(self, df):
        """
        Extract all NLP features from the dataframe
        
        Args:
            df (DataFrame): Input transaction data
            
        Returns:
            DataFrame: DataFrame with extracted features
        """
        try:
            # Create a copy to avoid modifying the original
            result_df = df.copy()
            
            # Extract different types of NLP features
            result_df = self._extract_basic_text_features(result_df)
            result_df = self._extract_sentiment_features(result_df)
            result_df = self._extract_keyword_features(result_df)
            result_df = self._extract_pattern_features(result_df)
            result_df = self._extract_topic_features(result_df)
            result_df = self._extract_embedding_features(result_df)
            
            # Store feature names
            self.feature_names = [col for col in result_df.columns if col not in df.columns]
            
            logger.info(f"Extracted {len(self.feature_names)} NLP features")
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting NLP features: {str(e)}")
            raise
    
    def _extract_basic_text_features(self, df):
        """
        Extract basic text features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with basic text features
        """
        try:
            result_df = df.copy()
            
            # Text columns to process
            text_columns = ['description', 'notes']
            
            for col in text_columns:
                if col in df.columns:
                    # Character count
                    result_df[f'{col}_char_count'] = df[col].fillna('').astype(str).apply(len)
                    
                    # Word count
                    result_df[f'{col}_word_count'] = df[col].fillna('').astype(str).apply(
                        lambda x: len(word_tokenize(x)) if x else 0
                    )
                    
                    # Sentence count
                    result_df[f'{col}_sentence_count'] = df[col].fillna('').astype(str).apply(
                        lambda x: len(sent_tokenize(x)) if x else 0
                    )
                    
                    # Average word length
                    result_df[f'{col}_avg_word_length'] = df[col].fillna('').astype(str).apply(
                        lambda x: np.mean([len(word) for word in word_tokenize(x)]) if word_tokenize(x) else 0
                    )
                    
                    # Average sentence length
                    result_df[f'{col}_avg_sentence_length'] = df[col].fillna('').astype(str).apply(
                        lambda x: np.mean([len(word_tokenize(sent)) for sent in sent_tokenize(x)]) if sent_tokenize(x) else 0
                    )
                    
                    # Punctuation count
                    result_df[f'{col}_punctuation_count'] = df[col].fillna('').astype(str).apply(
                        lambda x: sum(1 for char in x if char in string.punctuation)
                    )
                    
                    # Uppercase word count
                    result_df[f'{col}_uppercase_count'] = df[col].fillna('').astype(str).apply(
                        lambda x: sum(1 for word in word_tokenize(x) if word.isupper() and len(word) > 1)
                    )
                    
                    # Digit count
                    result_df[f'{col}_digit_count'] = df[col].fillna('').astype(str).apply(
                        lambda x: sum(1 for char in x if char.isdigit())
                    )
                    
                    # Unique word count
                    result_df[f'{col}_unique_word_count'] = df[col].fillna('').astype(str).apply(
                        lambda x: len(set(word_tokenize(x.lower()))) if x else 0
                    )
                    
                    # Lexical diversity (unique words / total words)
                    result_df[f'{col}_lexical_diversity'] = df[col].fillna('').astype(str).apply(
                        lambda x: len(set(word_tokenize(x.lower()))) / len(word_tokenize(x)) if word_tokenize(x) else 0
                    )
            
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting basic text features: {str(e)}")
            return df
    
    def _extract_sentiment_features(self, df):
        """
        Extract sentiment analysis features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with sentiment features
        """
        try:
            result_df = df.copy()
            
            # Text columns to process
            text_columns = ['description', 'notes']
            
            for col in text_columns:
                if col in df.columns:
                    # VADER sentiment scores
                    sentiment_scores = df[col].fillna('').astype(str).apply(
                        lambda x: self.sia.polarity_scores(x) if x else {'neg': 0, 'neu': 0, 'pos': 0, 'compound': 0}
                    )
                    
                    # Extract individual scores
                    result_df[f'{col}_sentiment_neg'] = sentiment_scores.apply(lambda x: x['neg'])
                    result_df[f'{col}_sentiment_neu'] = sentiment_scores.apply(lambda x: x['neu'])
                    result_df[f'{col}_sentiment_pos'] = sentiment_scores.apply(lambda x: x['pos'])
                    result_df[f'{col}_sentiment_compound'] = sentiment_scores.apply(lambda x: x['compound'])
                    
                    # TextBlob sentiment
                    textblob_sentiment = df[col].fillna('').astype(str).apply(
                        lambda x: TextBlob(x).sentiment if x else TextBlob('').sentiment
                    )
                    
                    result_df[f'{col}_textblob_polarity'] = textblob_sentiment.apply(lambda x: x.polarity)
                    result_df[f'{col}_textblob_subjectivity'] = textblob_sentiment.apply(lambda x: x.subjectivity)
                    
                    # Emotion indicators
                    result_df[f'{col}_is_negative'] = (result_df[f'{col}_sentiment_compound'] < -0.05).astype(int)
                    result_df[f'{col}_is_positive'] = (result_df[f'{col}_sentiment_compound'] > 0.05).astype(int)
                    result_df[f'{col}_is_neutral'] = (
                        (result_df[f'{col}_sentiment_compound'] >= -0.05) & 
                        (result_df[f'{col}_sentiment_compound'] <= 0.05)
                    ).astype(int)
            
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting sentiment features: {str(e)}")
            return df
    
    def _extract_keyword_features(self, df):
        """
        Extract keyword-based features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with keyword features
        """
        try:
            result_df = df.copy()
            
            # Text columns to process
            text_columns = ['description', 'notes']
            
            for col in text_columns:
                if col in df.columns:
                    # Preprocess text
                    processed_text = df[col].fillna('').astype(str).apply(self._preprocess_text)
                    
                    # Count fraud keywords
                    fraud_keyword_counts = processed_text.apply(
                        lambda x: sum(1 for word in x if word in self.fraud_keywords)
                    )
                    result_df[f'{col}_fraud_keyword_count'] = fraud_keyword_counts
                    
                    # Flag if any fraud keywords present
                    result_df[f'{col}_has_fraud_keywords'] = (fraud_keyword_counts > 0).astype(int)
                    
                    # Count specific keyword categories
                    urgency_keywords = ['urgent', 'immediately', 'asap', 'hurry', 'quick', 'fast']
                    secrecy_keywords = ['secret', 'confidential', 'private', 'hidden', 'discreet']
                    money_keywords = ['money', 'cash', 'payment', 'transfer', 'wire']
                    
                    result_df[f'{col}_urgency_keyword_count'] = processed_text.apply(
                        lambda x: sum(1 for word in x if word in urgency_keywords)
                    )
                    
                    result_df[f'{col}_secrecy_keyword_count'] = processed_text.apply(
                        lambda x: sum(1 for word in x if word in secrecy_keywords)
                    )
                    
                    result_df[f'{col}_money_keyword_count'] = processed_text.apply(
                        lambda x: sum(1 for word in x if word in money_keywords)
                    )
                    
                    # Calculate keyword density
                    result_df[f'{col}_fraud_keyword_density'] = fraud_keyword_counts / (
                        df[col].fillna('').astype(str).apply(lambda x: len(word_tokenize(x)) if x else 1)
                    )
                    
                    # TF-IDF for fraud keywords
                    if not self.fitted:
                        # Fit TF-IDF vectorizer
                        self.tfidf_vectorizer = TfidfVectorizer(
                            vocabulary=self.fraud_keywords,
                            ngram_range=(1, 2),
                            max_features=100
                        )
                        
                        # Fit on all text
                        all_text = pd.concat([df[col].fillna('').astype(str) for col in text_columns if col in df.columns])
                        self.tfidf_vectorizer.fit(all_text)
                    
                    # Transform text
                    tfidf_features = self.tfidf_vectorizer.transform(df[col].fillna('').astype(str))
                    
                    # Add top TF-IDF features
                    feature_names = self.tfidf_vectorizer.get_feature_names_out()
                    for i, feature in enumerate(feature_names[:10]):  # Top 10 features
                        result_df[f'{col}_tfidf_{feature}'] = tfidf_features[:, i].toarray().flatten()
            
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting keyword features: {str(e)}")
            return df
    
    def _extract_pattern_features(self, df):
        """
        Extract pattern-based features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with pattern features
        """
        try:
            result_df = df.copy()
            
            # Text columns to process
            text_columns = ['description', 'notes']
            
            for col in text_columns:
                if col in df.columns:
                    # Count suspicious patterns
                    pattern_counts = []
                    for text in df[col].fillna('').astype(str):
                        count = 0
                        for pattern in self.suspicious_patterns:
                            matches = re.findall(pattern, text, re.IGNORECASE)
                            count += len(matches)
                        pattern_counts.append(count)
                    
                    result_df[f'{col}_suspicious_pattern_count'] = pattern_counts
                    
                    # Flag if any suspicious patterns present
                    result_df[f'{col}_has_suspicious_patterns'] = (np.array(pattern_counts) > 0).astype(int)
                    
                    # Specific pattern counts
                    money_pattern = r'\$\d+,\d+\.\d{2}'
                    credit_card_pattern = r'\d{4}[\s-]?\d{4}[\s-]?\d{4}[\s-]?\d{4}'
                    ssn_pattern = r'\b\d{3}-\d{2}-\d{4}\b'
                    email_pattern = r'\b[A-Z0-9._%+-]+@[A-Z0-9.-]+\.[A-Z]{2,}\b'
                    url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'
                    
                    result_df[f'{col}_money_pattern_count'] = df[col].fillna('').astype(str).apply(
                        lambda x: len(re.findall(money_pattern, x))
                    )
                    
                    result_df[f'{col}_credit_card_pattern_count'] = df[col].fillna('').astype(str).apply(
                        lambda x: len(re.findall(credit_card_pattern, x))
                    )
                    
                    result_df[f'{col}_ssn_pattern_count'] = df[col].fillna('').astype(str).apply(
                        lambda x: len(re.findall(ssn_pattern, x))
                    )
                    
                    result_df[f'{col}_email_pattern_count'] = df[col].fillna('').astype(str).apply(
                        lambda x: len(re.findall(email_pattern, x))
                    )
                    
                    result_df[f'{col}_url_pattern_count'] = df[col].fillna('').astype(str).apply(
                        lambda x: len(re.findall(url_pattern, x))
                    )
                    
                    # Check for excessive punctuation
                    result_df[f'{col}_excessive_punctuation'] = (
                        df[col].fillna('').astype(str).apply(
                            lambda x: 1 if sum(1 for char in x if char in string.punctuation) / len(x) > 0.3 else 0
                        )
                    )
                    
                    # Check for excessive capitalization
                    result_df[f'{col}_excessive_capitalization'] = (
                        df[col].fillna('').astype(str).apply(
                            lambda x: 1 if sum(1 for char in x if char.isupper()) / len(x) > 0.5 else 0
                        )
                    )
            
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting pattern features: {str(e)}")
            return df
    
    def _extract_topic_features(self, df):
        """
        Extract topic modeling features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with topic features
        """
        try:
            result_df = df.copy()
            
            # Text columns to process
            text_columns = ['description', 'notes']
            
            # Combine text from all columns
            all_text = []
            for col in text_columns:
                if col in df.columns:
                    all_text.extend(df[col].fillna('').astype(str).tolist())
            
            if not all_text:
                return result_df
            
            # Preprocess text for topic modeling
            processed_docs = [self._preprocess_text(doc) for doc in all_text]
            processed_docs = [' '.join(doc) for doc in processed_docs if doc]
            
            if not processed_docs:
                return result_df
            
            # Fit CountVectorizer and LDA model if not fitted
            if not self.fitted:
                # Fit CountVectorizer
                self.count_vectorizer = CountVectorizer(
                    max_features=1000,
                    stop_words='english',
                    ngram_range=(1, 2)
                )
                doc_term_matrix = self.count_vectorizer.fit_transform(processed_docs)
                
                # Fit LDA model
                self.lda_model = LatentDirichletAllocation(
                    n_components=5,  # 5 topics
                    random_state=42,
                    max_iter=10,
                    learning_method='online'
                )
                self.lda_model.fit(doc_term_matrix)
            
            # Transform each text column
            for col in text_columns:
                if col in df.columns:
                    # Preprocess text
                    processed_text = df[col].fillna('').astype(str).apply(self._preprocess_text)
                    processed_text = processed_text.apply(lambda x: ' '.join(x) if x else '')
                    
                    # Transform to document-term matrix
                    doc_term_matrix = self.count_vectorizer.transform(processed_text)
                    
                    # Get topic distributions
                    topic_distributions = self.lda_model.transform(doc_term_matrix)
                    
                    # Add topic features
                    for i in range(topic_distributions.shape[1]):
                        result_df[f'{col}_topic_{i}_prob'] = topic_distributions[:, i]
                    
                    # Get dominant topic
                    dominant_topics = np.argmax(topic_distributions, axis=1)
                    result_df[f'{col}_dominant_topic'] = dominant_topics
            
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting topic features: {str(e)}")
            return df
    
    def _extract_embedding_features(self, df):
        """
        Extract word embedding features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with embedding features
        """
        try:
            result_df = df.copy()
            
            # Text columns to process
            text_columns = ['description', 'notes']
            
            # Combine text from all columns
            all_text = []
            for col in text_columns:
                if col in df.columns:
                    all_text.extend(df[col].fillna('').astype(str).tolist())
            
            if not all_text:
                return result_df
            
            # Preprocess text for embeddings
            processed_docs = [self._preprocess_text(doc) for doc in all_text]
            processed_docs = [doc for doc in processed_docs if doc]
            
            if not processed_docs:
                return result_df
            
            # Check if Gemini API is available
            if is_api_available('gemini'):
                # Here you would implement Gemini API calls for embeddings
                # For now, we'll skip and use local embeddings
                logger.info("Gemini API available, but implementation pending. Using local embeddings.")
            
            # Check if OpenAI API is available
            if is_api_available('openai'):
                # Here you would implement OpenAI API calls for embeddings
                # For now, we'll skip and use local embeddings
                logger.info("OpenAI API available, but implementation pending. Using local embeddings.")
            
            # Fit Word2Vec model (local implementation)
            self.word2vec_model = Word2Vec(
                sentences=processed_docs,
                vector_size=100,  # 100-dimensional vectors
                window=5,
                min_count=1,
                workers=4,
                sg=1  # Skip-gram model
            )
            
            # Train Doc2Vec model (local implementation)
            tagged_docs = [TaggedDocument(doc, [i]) for i, doc in enumerate(processed_docs)]
            self.doc2vec_model = Doc2Vec(
                documents=tagged_docs,
                vector_size=100,  # 100-dimensional vectors
                window=5,
                min_count=1,
                workers=4,
                epochs=10
            )
            
            # Transform each text column
            for col in text_columns:
                if col in df.columns:
                    # Preprocess text
                    processed_text = df[col].fillna('').astype(str).apply(self._preprocess_text)
                    
                    # Calculate average Word2Vec vectors
                    word2vec_vectors = []
                    for doc in processed_text:
                        if doc:
                            # Get vectors for words in document
                            word_vectors = [self.word2vec_model.wv[word] for word in doc if word in self.word2vec_model.wv]
                            
                            if word_vectors:
                                # Average the vectors
                                avg_vector = np.mean(word_vectors, axis=0)
                                word2vec_vectors.append(avg_vector)
                            else:
                                # Use zero vector if no words found
                                word2vec_vectors.append(np.zeros(100))
                        else:
                            # Use zero vector for empty documents
                            word2vec_vectors.append(np.zeros(100))
                    
                    # Add Word2Vec features (first 10 dimensions)
                    word2vec_vectors = np.array(word2vec_vectors)
                    for i in range(min(10, word2vec_vectors.shape[1])):
                        result_df[f'{col}_word2vec_dim_{i}'] = word2vec_vectors[:, i]
                    
                    # Calculate Doc2Vec vectors
                    doc2vec_vectors = []
                    for doc in processed_text:
                        if doc:
                            # Infer vector for document
                            vector = self.doc2vec_model.infer_vector(doc)
                            doc2vec_vectors.append(vector)
                        else:
                            # Use zero vector for empty documents
                            doc2vec_vectors.append(np.zeros(100))
                    
                    # Add Doc2Vec features (first 10 dimensions)
                    doc2vec_vectors = np.array(doc2vec_vectors)
                    for i in range(min(10, doc2vec_vectors.shape[1])):
                        result_df[f'{col}_doc2vec_dim_{i}'] = doc2vec_vectors[:, i]
            
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting embedding features: {str(e)}")
            return df
    
    def _preprocess_text(self, text):
        """
        Preprocess text for NLP analysis
        
        Args:
            text (str): Input text
            
        Returns:
            list: List of processed tokens
        """
        try:
            # Convert to lowercase
            text = text.lower()
            
            # Remove punctuation
            text = text.translate(str.maketrans('', '', string.punctuation))
            
            # Remove digits
            text = re.sub(r'\d+', '', text)
            
            # Tokenize
            tokens = word_tokenize(text)
            
            # Remove stop words
            tokens = [word for word in tokens if word not in self.stop_words]
            
            # Lemmatize
            tokens = [self.lemmatizer.lemmatize(word) for word in tokens]
            
            # Remove short words
            tokens = [word for word in tokens if len(word) > 2]
            
            return tokens
            
        except Exception as e:
            logger.error(f"Error preprocessing text: {str(e)}")
            return []
    
    def fit_transform(self, df):
        """
        Fit the feature extractor and transform the data
        
        Args:
            df (DataFrame): Input data
            
        Returns:
            DataFrame: Transformed data with features
        """
        # Extract features
        result_df = self.extract_features(df)
        
        # Get feature columns
        feature_cols = [col for col in result_df.columns if col not in df.columns]
        
        if len(feature_cols) > 0:
            self.fitted = True
        
        return result_df
    
    def transform(self, df):
        """
        Transform new data using fitted feature extractor
        
        Args:
            df (DataFrame): Input data
            
        Returns:
            DataFrame: Transformed data with features
        """
        if not self.fitted:
            raise ValueError("Feature extractor not fitted. Call fit_transform first.")
        
        # Extract features
        result_df = self.extract_features(df)
        
        return result_df

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\features\.ipynb_checkpoints\statistical_features-checkpoint.py ===
"""
Statistical Features Module
Implements various statistical feature extraction techniques for fraud detection
"""

import pandas as pd
import numpy as np
import scipy.stats as stats
from scipy.spatial.distance import mahalanobis
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
import warnings
import logging
from typing import Dict, List, Tuple, Union

warnings.filterwarnings('ignore')
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class StatisticalFeatures:
    """
    Class for extracting statistical features from transaction data
    Implements techniques like Benford's Law, Z-score, MAD, etc.
    """
    
    def __init__(self, config=None):
        """
        Initialize StatisticalFeatures
        
        Args:
            config (dict, optional): Configuration parameters
        """
        self.config = config or {}
        self.feature_names = []
        self.scaler = StandardScaler()
        self.pca = None
        self.fitted = False
        
    def extract_features(self, df):
        """
        Extract all statistical features from the dataframe
        
        Args:
            df (DataFrame): Input transaction data
            
        Returns:
            DataFrame: DataFrame with extracted features
        """
        try:
            # Create a copy to avoid modifying the original
            result_df = df.copy()
            
            # Extract different types of statistical features
            result_df = self._extract_benford_features(result_df)
            result_df = self._extract_zscore_features(result_df)
            result_df = self._extract_mad_features(result_df)
            result_df = self._extract_percentile_features(result_df)
            result_df = self._extract_distribution_features(result_df)
            result_df = self._extract_mahalanobis_features(result_df)
            result_df = self._extract_grubbs_features(result_df)
            result_df = self._extract_entropy_features(result_df)
            result_df = self._extract_correlation_features(result_df)
            
            # Store feature names
            self.feature_names = [col for col in result_df.columns if col not in df.columns]
            
            logger.info(f"Extracted {len(self.feature_names)} statistical features")
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting statistical features: {str(e)}")
            raise
    
    def _extract_benford_features(self, df):
        """
        Extract Benford's Law features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with Benford's Law features
        """
        try:
            result_df = df.copy()
            
            # Apply Benford's Law to amount column if it exists
            if 'amount' in df.columns:
                # Get first digit of amounts
                amounts = df['amount'].abs()
                first_digits = amounts.astype(str).str[0].replace('n', '0').astype(int)
                first_digits = first_digits[first_digits >= 1]  # Exclude 0
                
                if len(first_digits) > 0:
                    # Calculate actual distribution
                    actual_dist = first_digits.value_counts(normalize=True).sort_index()
                    
                    # Expected Benford's distribution
                    benford_dist = pd.Series([np.log10(1 + 1/d) for d in range(1, 10)], index=range(1, 10))
                    
                    # Calculate Chi-square statistic
                    chi_square = 0
                    for digit in range(1, 10):
                        expected_count = benford_dist[digit] * len(first_digits)
                        actual_count = actual_dist.get(digit, 0)
                        if expected_count > 0:
                            chi_square += (actual_count - expected_count) ** 2 / expected_count
                    
                    # Add features
                    result_df['benford_chi_square'] = chi_square
                    result_df['benford_p_value'] = 1 - stats.chi2.cdf(chi_square, 8)  # 8 degrees of freedom
                    
                    # Calculate deviation for each digit
                    for digit in range(1, 6):  # Just first 5 digits to avoid too many features
                        actual_pct = actual_dist.get(digit, 0)
                        expected_pct = benford_dist[digit]
                        result_df[f'benford_deviation_{digit}'] = abs(actual_pct - expected_pct)
            
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting Benford's Law features: {str(e)}")
            return df
    
    def _extract_zscore_features(self, df):
        """
        Extract Z-score based features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with Z-score features
        """
        try:
            result_df = df.copy()
            
            # Apply to amount column if it exists
            if 'amount' in df.columns:
                amounts = df['amount']
                
                # Calculate Z-scores
                mean_amount = amounts.mean()
                std_amount = amounts.std()
                
                if std_amount > 0:
                    z_scores = (amounts - mean_amount) / std_amount
                    result_df['amount_zscore'] = z_scores
                    
                    # Flag extreme values
                    result_df['amount_zscore_outlier'] = (np.abs(z_scores) > 3).astype(int)
                else:
                    result_df['amount_zscore'] = 0
                    result_df['amount_zscore_outlier'] = 0
            
            # Apply Z-score to sender and receiver if ID columns exist
            if 'sender_id' in df.columns and 'amount' in df.columns:
                # Calculate average amount per sender
                sender_avg = df.groupby('sender_id')['amount'].mean()
                sender_std = df.groupby('sender_id')['amount'].std()
                
                # Calculate Z-scores for each transaction relative to sender's history
                sender_zscores = []
                for _, row in df.iterrows():
                    sender_id = row['sender_id']
                    amount = row['amount']
                    
                    if sender_id in sender_avg and sender_id in sender_std and sender_std[sender_id] > 0:
                        z_score = (amount - sender_avg[sender_id]) / sender_std[sender_id]
                    else:
                        z_score = 0
                    
                    sender_zscores.append(z_score)
                
                result_df['sender_amount_zscore'] = sender_zscores
                result_df['sender_amount_zscore_outlier'] = (np.abs(sender_zscores) > 3).astype(int)
            
            if 'receiver_id' in df.columns and 'amount' in df.columns:
                # Calculate average amount per receiver
                receiver_avg = df.groupby('receiver_id')['amount'].mean()
                receiver_std = df.groupby('receiver_id')['amount'].std()
                
                # Calculate Z-scores for each transaction relative to receiver's history
                receiver_zscores = []
                for _, row in df.iterrows():
                    receiver_id = row['receiver_id']
                    amount = row['amount']
                    
                    if receiver_id in receiver_avg and receiver_id in receiver_std and receiver_std[receiver_id] > 0:
                        z_score = (amount - receiver_avg[receiver_id]) / receiver_std[receiver_id]
                    else:
                        z_score = 0
                    
                    receiver_zscores.append(z_score)
                
                result_df['receiver_amount_zscore'] = receiver_zscores
                result_df['receiver_amount_zscore_outlier'] = (np.abs(receiver_zscores) > 3).astype(int)
            
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting Z-score features: {str(e)}")
            return df
    
    def _extract_mad_features(self, df):
        """
        Extract Median Absolute Deviation (MAD) features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with MAD features
        """
        try:
            result_df = df.copy()
            
            # Apply to amount column if it exists
            if 'amount' in df.columns:
                amounts = df['amount']
                
                # Calculate MAD
                median_amount = amounts.median()
                abs_dev = np.abs(amounts - median_amount)
                mad = abs_dev.median()
                
                if mad > 0:
                    # Calculate modified Z-scores using MAD
                    modified_z_scores = 0.6745 * abs_dev / mad
                    result_df['amount_mad_zscore'] = modified_z_scores
                    
                    # Flag extreme values
                    result_df['amount_mad_outlier'] = (modified_z_scores > 3.5).astype(int)
                else:
                    result_df['amount_mad_zscore'] = 0
                    result_df['amount_mad_outlier'] = 0
            
            # Apply MAD to sender and receiver if ID columns exist
            if 'sender_id' in df.columns and 'amount' in df.columns:
                # Calculate MAD per sender
                sender_mad = df.groupby('sender_id')['amount'].apply(lambda x: np.median(np.abs(x - x.median())))
                sender_median = df.groupby('sender_id')['amount'].median()
                
                # Calculate MAD-based Z-scores for each transaction
                sender_mad_zscores = []
                for _, row in df.iterrows():
                    sender_id = row['sender_id']
                    amount = row['amount']
                    
                    if sender_id in sender_mad and sender_id in sender_median and sender_mad[sender_id] > 0:
                        abs_dev = abs(amount - sender_median[sender_id])
                        mad_z_score = 0.6745 * abs_dev / sender_mad[sender_id]
                    else:
                        mad_z_score = 0
                    
                    sender_mad_zscores.append(mad_z_score)
                
                result_df['sender_amount_mad_zscore'] = sender_mad_zscores
                result_df['sender_amount_mad_outlier'] = (np.array(sender_mad_zscores) > 3.5).astype(int)
            
            if 'receiver_id' in df.columns and 'amount' in df.columns:
                # Calculate MAD per receiver
                receiver_mad = df.groupby('receiver_id')['amount'].apply(lambda x: np.median(np.abs(x - x.median())))
                receiver_median = df.groupby('receiver_id')['amount'].median()
                
                # Calculate MAD-based Z-scores for each transaction
                receiver_mad_zscores = []
                for _, row in df.iterrows():
                    receiver_id = row['receiver_id']
                    amount = row['amount']
                    
                    if receiver_id in receiver_mad and receiver_id in receiver_median and receiver_mad[receiver_id] > 0:
                        abs_dev = abs(amount - receiver_median[receiver_id])
                        mad_z_score = 0.6745 * abs_dev / receiver_mad[receiver_id]
                    else:
                        mad_z_score = 0
                    
                    receiver_mad_zscores.append(mad_z_score)
                
                result_df['receiver_amount_mad_zscore'] = receiver_mad_zscores
                result_df['receiver_amount_mad_outlier'] = (np.array(receiver_mad_zscores) > 3.5).astype(int)
            
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting MAD features: {str(e)}")
            return df
    
    def _extract_percentile_features(self, df):
        """
        Extract percentile-based features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with percentile features
        """
        try:
            result_df = df.copy()
            
            # Apply to amount column if it exists
            if 'amount' in df.columns:
                amounts = df['amount']
                
                # Calculate percentiles
                percentiles = [5, 10, 25, 50, 75, 90, 95, 99]
                percentile_values = np.percentile(amounts, percentiles)
                
                # Add features for each percentile
                for i, p in enumerate(percentiles):
                    result_df[f'amount_above_{p}th_percentile'] = (amounts > percentile_values[i]).astype(int)
                
                # Calculate percentile rank for each amount
                result_df['amount_percentile_rank'] = amounts.rank(pct=True)
            
            # Apply percentiles to sender and receiver if ID columns exist
            if 'sender_id' in df.columns and 'amount' in df.columns:
                # Calculate percentile rank within each sender's transactions
                sender_percentile_ranks = df.groupby('sender_id')['amount'].rank(pct=True)
                result_df['sender_amount_percentile_rank'] = sender_percentile_ranks
                
                # Flag if amount is in top 5% for sender
                result_df['sender_amount_top_5pct'] = (sender_percentile_ranks > 0.95).astype(int)
            
            if 'receiver_id' in df.columns and 'amount' in df.columns:
                # Calculate percentile rank within each receiver's transactions
                receiver_percentile_ranks = df.groupby('receiver_id')['amount'].rank(pct=True)
                result_df['receiver_amount_percentile_rank'] = receiver_percentile_ranks
                
                # Flag if amount is in top 5% for receiver
                result_df['receiver_amount_top_5pct'] = (receiver_percentile_ranks > 0.95).astype(int)
            
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting percentile features: {str(e)}")
            return df
    
    def _extract_distribution_features(self, df):
        """
        Extract distribution-based features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with distribution features
        """
        try:
            result_df = df.copy()
            
            # Apply to amount column if it exists
            if 'amount' in df.columns:
                amounts = df['amount']
                
                # Skewness and kurtosis
                result_df['amount_skewness'] = stats.skew(amounts)
                result_df['amount_kurtosis'] = stats.kurtosis(amounts)
                
                # Normality tests
                _, normality_p = stats.normaltest(amounts)
                result_df['amount_normality_p'] = normality_p
                
                # Shapiro-Wilk test (for smaller samples)
                if len(amounts) <= 5000:
                    _, shapiro_p = stats.shapiro(amounts)
                    result_df['amount_shapiro_p'] = shapiro_p
                
                # Kolmogorov-Smirnov test against normal distribution
                _, ks_p = stats.kstest(amounts, 'norm', args=(amounts.mean(), amounts.std()))
                result_df['amount_ks_p'] = ks_p
                
                # Anderson-Darling test
                ad_result = stats.anderson(amounts)
                result_df['amount_ad_statistic'] = ad_result.statistic
                
                # Jarque-Bera test
                jb_stat, jb_p = stats.jarque_bera(amounts)
                result_df['amount_jb_statistic'] = jb_stat
                result_df['amount_jb_p'] = jb_p
            
            # Apply distribution features to sender and receiver if ID columns exist
            if 'sender_id' in df.columns and 'amount' in df.columns:
                # Calculate distribution features per sender
                sender_stats = df.groupby('sender_id')['amount'].agg([
                    ('skewness', lambda x: stats.skew(x) if len(x) >= 3 else 0),
                    ('kurtosis', lambda x: stats.kurtosis(x) if len(x) >= 4 else 0),
                    ('variance', 'var'),
                    ('range', lambda x: x.max() - x.min() if len(x) > 0 else 0)
                ])
                
                # Map back to each transaction
                for stat in ['skewness', 'kurtosis', 'variance', 'range']:
                    result_df[f'sender_amount_{stat}'] = df['sender_id'].map(sender_stats[stat]).fillna(0)
            
            if 'receiver_id' in df.columns and 'amount' in df.columns:
                # Calculate distribution features per receiver
                receiver_stats = df.groupby('receiver_id')['amount'].agg([
                    ('skewness', lambda x: stats.skew(x) if len(x) >= 3 else 0),
                    ('kurtosis', lambda x: stats.kurtosis(x) if len(x) >= 4 else 0),
                    ('variance', 'var'),
                    ('range', lambda x: x.max() - x.min() if len(x) > 0 else 0)
                ])
                
                # Map back to each transaction
                for stat in ['skewness', 'kurtosis', 'variance', 'range']:
                    result_df[f'receiver_amount_{stat}'] = df['receiver_id'].map(receiver_stats[stat]).fillna(0)
            
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting distribution features: {str(e)}")
            return df
    
    def _extract_mahalanobis_features(self, df):
        """
        Extract Mahalanobis distance features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with Mahalanobis features
        """
        try:
            result_df = df.copy()
            
            # Get numeric columns
            numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
            
            if len(numeric_cols) < 2:
                logger.warning("Not enough numeric columns for Mahalanobis distance calculation")
                return result_df
            
            # Prepare data
            X = df[numeric_cols].fillna(0)
            
            # Calculate covariance matrix
            cov_matrix = np.cov(X, rowvar=False)
            
            # Check if covariance matrix is invertible
            if np.linalg.det(cov_matrix) == 0:
                logger.warning("Covariance matrix is singular, using pseudo-inverse")
                inv_cov_matrix = np.linalg.pinv(cov_matrix)
            else:
                inv_cov_matrix = np.linalg.inv(cov_matrix)
            
            # Calculate mean vector
            mean_vector = np.mean(X, axis=0)
            
            # Calculate Mahalanobis distances
            mahalanobis_distances = []
            for i in range(len(X)):
                mahalanobis_distances.append(
                    mahalanobis(X.iloc[i], mean_vector, inv_cov_matrix)
                )
            
            result_df['mahalanobis_distance'] = mahalanobis_distances
            
            # Flag outliers based on Mahalanobis distance
            # Using chi-square distribution with degrees of freedom = number of features
            threshold = stats.chi2.ppf(0.975, df=len(numeric_cols))
            result_df['mahalanobis_outlier'] = (np.array(mahalanobis_distances) > threshold).astype(int)
            
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting Mahalanobis features: {str(e)}")
            return df
    
    def _extract_grubbs_features(self, df):
        """
        Extract Grubbs' test features for outliers
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with Grubbs' test features
        """
        try:
            result_df = df.copy()
            
            # Apply to amount column if it exists
            if 'amount' in df.columns:
                amounts = df['amount'].values
                
                # Calculate Grubbs' test statistic
                mean_amount = np.mean(amounts)
                std_amount = np.std(amounts)
                
                if std_amount > 0:
                    # Calculate absolute deviations
                    abs_deviations = np.abs(amounts - mean_amount)
                    max_deviation = np.max(abs_deviations)
                    
                    # Grubbs' test statistic
                    grubbs_stat = max_deviation / std_amount
                    result_df['grubbs_statistic'] = grubbs_stat
                    
                    # Calculate critical value for two-sided test
                    n = len(amounts)
                    t_critical = stats.t.ppf(1 - 0.025 / (2 * n), n - 2)
                    critical_value = (n - 1) * t_critical / np.sqrt(n * (n - 2 + t_critical**2))
                    
                    # Flag outliers
                    result_df['grubbs_outlier'] = (grubbs_stat > critical_value).astype(int)
                else:
                    result_df['grubbs_statistic'] = 0
                    result_df['grubbs_outlier'] = 0
            
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting Grubbs' test features: {str(e)}")
            return df
    
    def _extract_entropy_features(self, df):
        """
        Extract entropy-based features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with entropy features
        """
        try:
            result_df = df.copy()
            
            # Calculate entropy for categorical columns
            categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()
            
            for col in categorical_cols:
                # Calculate probability distribution
                value_counts = df[col].value_counts(normalize=True)
                
                # Calculate Shannon entropy
                entropy = -sum(p * np.log2(p) for p in value_counts if p > 0)
                result_df[f'{col}_entropy'] = entropy
                
                # Calculate normalized entropy (0 to 1)
                max_entropy = np.log2(len(value_counts))
                if max_entropy > 0:
                    normalized_entropy = entropy / max_entropy
                else:
                    normalized_entropy = 0
                result_df[f'{col}_normalized_entropy'] = normalized_entropy
            
            # Calculate entropy for numeric columns (after binning)
            numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
            
            for col in numeric_cols:
                # Bin the data
                try:
                    binned = pd.cut(df[col], bins=10, duplicates='drop')
                    
                    # Calculate probability distribution
                    value_counts = binned.value_counts(normalize=True)
                    
                    # Calculate Shannon entropy
                    entropy = -sum(p * np.log2(p) for p in value_counts if p > 0)
                    result_df[f'{col}_binned_entropy'] = entropy
                    
                    # Calculate normalized entropy (0 to 1)
                    max_entropy = np.log2(len(value_counts))
                    if max_entropy > 0:
                        normalized_entropy = entropy / max_entropy
                    else:
                        normalized_entropy = 0
                    result_df[f'{col}_binned_normalized_entropy'] = normalized_entropy
                except:
                    pass
            
            # Calculate transaction entropy for sender and receiver
            if 'sender_id' in df.columns:
                # Calculate entropy of transaction amounts per sender
                sender_entropy = df.groupby('sender_id')['amount'].apply(
                    lambda x: self._calculate_series_entropy(x)
                )
                result_df['sender_amount_entropy'] = df['sender_id'].map(sender_entropy).fillna(0)
            
            if 'receiver_id' in df.columns:
                # Calculate entropy of transaction amounts per receiver
                receiver_entropy = df.groupby('receiver_id')['amount'].apply(
                    lambda x: self._calculate_series_entropy(x)
                )
                result_df['receiver_amount_entropy'] = df['receiver_id'].map(receiver_entropy).fillna(0)
            
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting entropy features: {str(e)}")
            return df
    
    def _calculate_series_entropy(self, series):
        """
        Calculate entropy of a pandas Series
        
        Args:
            series (Series): Input series
            
        Returns:
            float: Entropy value
        """
        try:
            # Bin the data
            binned = pd.cut(series, bins=min(10, len(series)), duplicates='drop')
            
            # Calculate probability distribution
            value_counts = binned.value_counts(normalize=True)
            
            # Calculate Shannon entropy
            entropy = -sum(p * np.log2(p) for p in value_counts if p > 0)
            return entropy
        except:
            return 0
    
    def _extract_correlation_features(self, df):
        """
        Extract correlation-based features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with correlation features
        """
        try:
            result_df = df.copy()
            
            # Get numeric columns
            numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
            
            if len(numeric_cols) < 2:
                logger.warning("Not enough numeric columns for correlation calculation")
                return result_df
            
            # Calculate correlation matrix
            corr_matrix = df[numeric_cols].corr().abs()
            
            # Find highest correlation for each variable
            max_corr = {}
            for col in numeric_cols:
                # Get correlations with other variables
                corrs = corr_matrix[col].drop(col)
                max_corr[col] = corrs.max()
            
            # Add features
            for col in numeric_cols:
                result_df[f'{col}_max_correlation'] = max_corr[col]
            
            # Calculate average correlation
            avg_corr = {}
            for col in numeric_cols:
                # Get correlations with other variables
                corrs = corr_matrix[col].drop(col)
                avg_corr[col] = corrs.mean()
            
            # Add features
            for col in numeric_cols:
                result_df[f'{col}_avg_correlation'] = avg_corr[col]
            
            # Calculate variance inflation factor (VIF) for multicollinearity
            for i, col in enumerate(numeric_cols):
                # Prepare data for VIF calculation
                X = df[numeric_cols].copy()
                y = X[col]
                X = X.drop(col, axis=1)
                
                # Fit linear regression
                try:
                    from sklearn.linear_model import LinearRegression
                    model = LinearRegression()
                    model.fit(X, y)
                    
                    # Calculate R-squared
                    r_squared = model.score(X, y)
                    
                    # Calculate VIF
                    if r_squared < 1:
                        vif = 1 / (1 - r_squared)
                    else:
                        vif = float('inf')
                    
                    result_df[f'{col}_vif'] = vif
                except:
                    result_df[f'{col}_vif'] = 1
            
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting correlation features: {str(e)}")
            return df
    
    def fit_transform(self, df):
        """
        Fit the feature extractor and transform the data
        
        Args:
            df (DataFrame): Input data
            
        Returns:
            DataFrame: Transformed data with features
        """
        # Extract features
        result_df = self.extract_features(df)
        
        # Get feature columns
        feature_cols = [col for col in result_df.columns if col not in df.columns]
        
        if len(feature_cols) > 0:
            # Fit scaler
            self.scaler.fit(result_df[feature_cols])
            
            # Apply PCA if we have enough features
            if len(feature_cols) >= 10:
                self.pca = PCA(n_components=min(10, len(feature_cols)))
                self.pca.fit(result_df[feature_cols])
                
                # Add PCA components
                pca_components = self.pca.transform(result_df[feature_cols])
                for i in range(pca_components.shape[1]):
                    result_df[f'stat_pca_{i+1}'] = pca_components[:, i]
            
            self.fitted = True
        
        return result_df
    
    def transform(self, df):
        """
        Transform new data using fitted feature extractor
        
        Args:
            df (DataFrame): Input data
            
        Returns:
            DataFrame: Transformed data with features
        """
        if not self.fitted:
            raise ValueError("Feature extractor not fitted. Call fit_transform first.")
        
        # Extract features
        result_df = self.extract_features(df)
        
        # Get feature columns
        feature_cols = [col for col in result_df.columns if col not in df.columns]
        
        if len(feature_cols) > 0:
            # Transform features using fitted scaler
            result_df[feature_cols] = self.scaler.transform(result_df[feature_cols])
            
            # Apply PCA if it was fitted
            if self.pca is not None:
                pca_components = self.pca.transform(result_df[feature_cols])
                for i in range(pca_components.shape[1]):
                    result_df[f'stat_pca_{i+1}'] = pca_components[:, i]
        
        return result_df

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\features\.ipynb_checkpoints\timeseries_features-checkpoint.py ===
"""
Time Series Features Module
Implements time series feature extraction techniques for fraud detection
"""

import pandas as pd
import numpy as np
from scipy import stats
from scipy.signal import find_peaks
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tsa.stattools import acf, pacf
import warnings
import logging
from typing import Dict, List, Tuple, Union

warnings.filterwarnings('ignore')
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class TimeSeriesFeatures:
    """
    Class for extracting time series features from transaction data
    Implements techniques like burstiness analysis, gap analysis, etc.
    """
    
    def __init__(self, config=None):
        """
        Initialize TimeSeriesFeatures
        
        Args:
            config (dict, optional): Configuration parameters
        """
        self.config = config or {}
        self.feature_names = []
        self.fitted = False
        
    def extract_features(self, df):
        """
        Extract all time series features from the dataframe
        
        Args:
            df (DataFrame): Input transaction data
            
        Returns:
            DataFrame: DataFrame with extracted features
        """
        try:
            # Create a copy to avoid modifying the original
            result_df = df.copy()
            
            # Ensure timestamp is in datetime format
            if 'timestamp' in df.columns and not pd.api.types.is_datetime64_any_dtype(df['timestamp']):
                result_df['timestamp'] = pd.to_datetime(df['timestamp'])
            
            # Extract different types of time series features
            result_df = self._extract_temporal_features(result_df)
            result_df = self._extract_frequency_features(result_df)
            result_df = self._extract_burstiness_features(result_df)
            result_df = self._extract_gap_features(result_df)
            result_df = self._extract_seasonal_features(result_df)
            result_df = self._extract_autocorrelation_features(result_df)
            
            # Store feature names
            self.feature_names = [col for col in result_df.columns if col not in df.columns]
            
            logger.info(f"Extracted {len(self.feature_names)} time series features")
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting time series features: {str(e)}")
            raise
    
    def _extract_temporal_features(self, df):
        """
        Extract temporal features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with temporal features
        """
        try:
            result_df = df.copy()
            
            if 'timestamp' not in df.columns:
                logger.warning("Timestamp column not found. Skipping temporal features.")
                return result_df
            
            # Extract time components
            result_df['hour'] = result_df['timestamp'].dt.hour
            result_df['day'] = result_df['timestamp'].dt.day
            result_df['month'] = result_df['timestamp'].dt.month
            result_df['year'] = result_df['timestamp'].dt.year
            result_df['dayofweek'] = result_df['timestamp'].dt.dayofweek  # Monday=0, Sunday=6
            result_df['dayofyear'] = result_df['timestamp'].dt.dayofyear
            result_df['weekofyear'] = result_df['timestamp'].dt.isocalendar().week
            result_df['quarter'] = result_df['timestamp'].dt.quarter
            
            # Time-based flags
            result_df['is_weekend'] = (result_df['dayofweek'] >= 5).astype(int)
            result_df['is_month_start'] = result_df['timestamp'].dt.is_month_start.astype(int)
            result_df['is_month_end'] = result_df['timestamp'].dt.is_month_end.astype(int)
            result_df['is_quarter_start'] = result_df['timestamp'].dt.is_quarter_start.astype(int)
            result_df['is_quarter_end'] = result_df['timestamp'].dt.is_quarter_end.astype(int)
            result_df['is_year_start'] = result_df['timestamp'].dt.is_year_start.astype(int)
            result_df['is_year_end'] = result_df['timestamp'].dt.is_year_end.astype(int)
            
            # Time of day flags
            result_df['is_night'] = ((result_df['hour'] >= 22) | (result_df['hour'] < 6)).astype(int)
            result_df['is_morning'] = ((result_df['hour'] >= 6) & (result_df['hour'] < 12)).astype(int)
            result_df['is_afternoon'] = ((result_df['hour'] >= 12) & (result_df['hour'] < 18)).astype(int)
            result_df['is_evening'] = ((result_df['hour'] >= 18) & (result_df['hour'] < 22)).astype(int)
            
            # Business hours flag (9 AM to 5 PM, Monday to Friday)
            result_df['is_business_hours'] = (
                (result_df['hour'] >= 9) & 
                (result_df['hour'] < 17) & 
                (result_df['dayofweek'] < 5)
            ).astype(int)
            
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting temporal features: {str(e)}")
            return df
    
    def _extract_frequency_features(self, df):
        """
        Extract frequency-based features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with frequency features
        """
        try:
            result_df = df.copy()
            
            if 'timestamp' not in df.columns:
                logger.warning("Timestamp column not found. Skipping frequency features.")
                return result_df
            
            # Sort by timestamp
            df_sorted = result_df.sort_values('timestamp')
            
            # Time windows for frequency analysis
            time_windows = ['1H', '6H', '24H', '7D', '30D']
            
            for window in time_windows:
                # Calculate transaction frequency in time windows before each transaction
                window_counts = []
                
                for _, row in df_sorted.iterrows():
                    timestamp = row['timestamp']
                    
                    # Get transactions in time window before current transaction
                    window_start = timestamp - pd.Timedelta(window)
                    window_end = timestamp
                    
                    window_transactions = df_sorted[
                        (df_sorted['timestamp'] >= window_start) &
                        (df_sorted['timestamp'] < window_end)
                    ]
                    
                    # Count transactions in window
                    count = len(window_transactions)
                    window_counts.append(count)
                
                result_df[f'transaction_frequency_{window}'] = window_counts
            
            # Calculate frequency by sender
            if 'sender_id' in df.columns:
                for window in ['1H', '6H', '24H']:
                    sender_window_counts = []
                    
                    for _, row in df_sorted.iterrows():
                        sender = row['sender_id']
                        timestamp = row['timestamp']
                        
                        # Get transactions from same sender in time window
                        window_start = timestamp - pd.Timedelta(window)
                        window_end = timestamp
                        
                        window_transactions = df_sorted[
                            (df_sorted['timestamp'] >= window_start) &
                            (df_sorted['timestamp'] < window_end) &
                            (df_sorted['sender_id'] == sender)
                        ]
                        
                        # Count transactions in window
                        count = len(window_transactions)
                        sender_window_counts.append(count)
                    
                    result_df[f'sender_frequency_{window}'] = sender_window_counts
            
            # Calculate frequency by receiver
            if 'receiver_id' in df.columns:
                for window in ['1H', '6H', '24H']:
                    receiver_window_counts = []
                    
                    for _, row in df_sorted.iterrows():
                        receiver = row['receiver_id']
                        timestamp = row['timestamp']
                        
                        # Get transactions to same receiver in time window
                        window_start = timestamp - pd.Timedelta(window)
                        window_end = timestamp
                        
                        window_transactions = df_sorted[
                            (df_sorted['timestamp'] >= window_start) &
                            (df_sorted['timestamp'] < window_end) &
                            (df_sorted['receiver_id'] == receiver)
                        ]
                        
                        # Count transactions in window
                        count = len(window_transactions)
                        receiver_window_counts.append(count)
                    
                    result_df[f'receiver_frequency_{window}'] = receiver_window_counts
            
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting frequency features: {str(e)}")
            return df
    
    def _extract_burstiness_features(self, df):
        """
        Extract burstiness analysis features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with burstiness features
        """
        try:
            result_df = df.copy()
            
            if 'timestamp' not in df.columns:
                logger.warning("Timestamp column not found. Skipping burstiness features.")
                return result_df
            
            # Sort by timestamp
            df_sorted = result_df.sort_values('timestamp')
            
            # Calculate inter-transaction times
            inter_times = df_sorted['timestamp'].diff().dt.total_seconds().fillna(0)
            
            # Calculate burstiness coefficient
            if len(inter_times) > 1 and inter_times.std() > 0:
                burstiness = (inter_times.std() - inter_times.mean()) / (inter_times.std() + inter_times.mean())
            else:
                burstiness = 0
            
            result_df['burstiness_coefficient'] = burstiness
            
            # Calculate local burstiness (in sliding windows)
            window_sizes = [10, 50, 100]  # Number of transactions
            
            for window_size in window_sizes:
                local_burstiness = []
                
                for i in range(len(df_sorted)):
                    # Get window around current transaction
                    start_idx = max(0, i - window_size // 2)
                    end_idx = min(len(df_sorted), i + window_size // 2 + 1)
                    
                    window_inter_times = inter_times.iloc[start_idx:end_idx]
                    
                    # Calculate burstiness for window
                    if len(window_inter_times) > 1 and window_inter_times.std() > 0:
                        local_b = (window_inter_times.std() - window_inter_times.mean()) / (
                            window_inter_times.std() + window_inter_times.mean()
                        )
                    else:
                        local_b = 0
                    
                    local_burstiness.append(local_b)
                
                result_df[f'local_burstiness_{window_size}'] = local_burstiness
            
            # Detect burst periods
            # A burst is defined as a period with inter-transaction times significantly lower than average
            avg_inter_time = inter_times.mean()
            std_inter_time = inter_times.std()
            
            # Threshold for burst detection (2 standard deviations below mean)
            burst_threshold = max(0, avg_inter_time - 2 * std_inter_time)
            
            # Flag transactions in bursts
            result_df['is_in_burst'] = (inter_times < burst_threshold).astype(int)
            
            # Calculate burst duration (consecutive transactions in burst)
            burst_durations = []
            current_duration = 0
            
            for is_burst in result_df['is_in_burst']:
                if is_burst:
                    current_duration += 1
                else:
                    burst_durations.append(current_duration)
                    current_duration = 0
            
            # Add the last duration if we're still in a burst
            burst_durations.append(current_duration)
            
            # Map burst durations back to transactions
            burst_duration_map = []
            idx = 0
            for i, is_burst in enumerate(result_df['is_in_burst']):
                if is_burst:
                    burst_duration_map.append(burst_durations[idx])
                else:
                    burst_duration_map.append(0)
                    if i > 0 and not result_df['is_in_burst'].iloc[i-1]:
                        idx += 1
            
            result_df['burst_duration'] = burst_duration_map
            
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting burstiness features: {str(e)}")
            return df
    
    def _extract_gap_features(self, df):
        """
        Extract gap analysis features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with gap features
        """
        try:
            result_df = df.copy()
            
            if 'timestamp' not in df.columns:
                logger.warning("Timestamp column not found. Skipping gap features.")
                return result_df
            
            # Sort by timestamp
            df_sorted = result_df.sort_values('timestamp')
            
            # Calculate inter-transaction times
            inter_times = df_sorted['timestamp'].diff().dt.total_seconds().fillna(0)
            
            # Time since last transaction
            result_df['time_since_last_transaction'] = inter_times
            
            # Time until next transaction
            time_until_next = df_sorted['timestamp'].diff(-1).dt.total_seconds().abs().fillna(0)
            result_df['time_until_next_transaction'] = time_until_next
            
            # Detect gaps (unusually long inter-transaction times)
            if len(inter_times) > 1 and inter_times.std() > 0:
                # Threshold for gap detection (2 standard deviations above mean)
                gap_threshold = inter_times.mean() + 2 * inter_times.std()
                
                # Flag transactions after gaps
                result_df['is_after_gap'] = (inter_times > gap_threshold).astype(int)
                
                # Calculate gap sizes
                result_df['gap_size'] = np.where(inter_times > gap_threshold, inter_times, 0)
            else:
                result_df['is_after_gap'] = 0
                result_df['gap_size'] = 0
            
            # Calculate gap statistics by sender
            if 'sender_id' in df.columns:
                sender_gaps = []
                sender_gap_stats = {}
                
                # Calculate average gap for each sender
                for sender in df_sorted['sender_id'].unique():
                    sender_transactions = df_sorted[df_sorted['sender_id'] == sender].sort_values('timestamp')
                    if len(sender_transactions) > 1:
                        sender_inter_times = sender_transactions['timestamp'].diff().dt.total_seconds().fillna(0)
                        sender_gap_stats[sender] = {
                            'mean': sender_inter_times.mean(),
                            'std': sender_inter_times.std()
                        }
                
                # Calculate gap features for each transaction
                for _, row in df_sorted.iterrows():
                    sender = row['sender_id']
                    
                    if sender in sender_gap_stats:
                        stats = sender_gap_stats[sender]
                        
                        # Time since last transaction for this sender
                        sender_transactions = df_sorted[
                            (df_sorted['sender_id'] == sender) &
                            (df_sorted['timestamp'] < row['timestamp'])
                        ]
                        
                        if len(sender_transactions) > 0:
                            last_sender_time = sender_transactions['timestamp'].max()
                            sender_inter_time = (row['timestamp'] - last_sender_time).total_seconds()
                        else:
                            sender_inter_time = float('inf')
                        
                        # Calculate Z-score for this gap
                        if stats['std'] > 0:
                            gap_z_score = (sender_inter_time - stats['mean']) / stats['std']
                        else:
                            gap_z_score = 0
                        
                        sender_gaps.append({
                            'sender_time_since_last': sender_inter_time,
                            'sender_gap_z_score': gap_z_score
                        })
                    else:
                        sender_gaps.append({
                            'sender_time_since_last': float('inf'),
                            'sender_gap_z_score': 0
                        })
                
                # Add to result dataframe
                gap_df = pd.DataFrame(sender_gaps)
                result_df['sender_time_since_last'] = gap_df['sender_time_since_last']
                result_df['sender_gap_z_score'] = gap_df['sender_gap_z_score']
            
            # Calculate gap statistics by receiver
            if 'receiver_id' in df.columns:
                receiver_gaps = []
                receiver_gap_stats = {}
                
                # Calculate average gap for each receiver
                for receiver in df_sorted['receiver_id'].unique():
                    receiver_transactions = df_sorted[df_sorted['receiver_id'] == receiver].sort_values('timestamp')
                    if len(receiver_transactions) > 1:
                        receiver_inter_times = receiver_transactions['timestamp'].diff().dt.total_seconds().fillna(0)
                        receiver_gap_stats[receiver] = {
                            'mean': receiver_inter_times.mean(),
                            'std': receiver_inter_times.std()
                        }
                
                # Calculate gap features for each transaction
                for _, row in df_sorted.iterrows():
                    receiver = row['receiver_id']
                    
                    if receiver in receiver_gap_stats:
                        stats = receiver_gap_stats[receiver]
                        
                        # Time since last transaction to this receiver
                        receiver_transactions = df_sorted[
                            (df_sorted['receiver_id'] == receiver) &
                            (df_sorted['timestamp'] < row['timestamp'])
                        ]
                        
                        if len(receiver_transactions) > 0:
                            last_receiver_time = receiver_transactions['timestamp'].max()
                            receiver_inter_time = (row['timestamp'] - last_receiver_time).total_seconds()
                        else:
                            receiver_inter_time = float('inf')
                        
                        # Calculate Z-score for this gap
                        if stats['std'] > 0:
                            gap_z_score = (receiver_inter_time - stats['mean']) / stats['std']
                        else:
                            gap_z_score = 0
                        
                        receiver_gaps.append({
                            'receiver_time_since_last': receiver_inter_time,
                            'receiver_gap_z_score': gap_z_score
                        })
                    else:
                        receiver_gaps.append({
                            'receiver_time_since_last': float('inf'),
                            'receiver_gap_z_score': 0
                        })
                
                # Add to result dataframe
                gap_df = pd.DataFrame(receiver_gaps)
                result_df['receiver_time_since_last'] = gap_df['receiver_time_since_last']
                result_df['receiver_gap_z_score'] = gap_df['receiver_gap_z_score']
            
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting gap features: {str(e)}")
            return df
    
    def _extract_seasonal_features(self, df):
        """
        Extract seasonal features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with seasonal features
        """
        try:
            result_df = df.copy()
            
            if 'timestamp' not in df.columns:
                logger.warning("Timestamp column not found. Skipping seasonal features.")
                return result_df
            
            # Sort by timestamp
            df_sorted = result_df.sort_values('timestamp')
            
            # Create a time series of transaction counts
            # Resample to different frequencies
            frequencies = ['1H', '6H', '1D', '1W']
            
            for freq in frequencies:
                # Count transactions per time period
                ts = df_sorted.set_index('timestamp').resample(freq).size()
                
                if len(ts) > 10:  # Need enough data points for decomposition
                    try:
                        # Perform seasonal decomposition
                        decomposition = seasonal_decompose(ts, model='additive', period=min(24, len(ts)//2))
                        
                        # Extract components
                        trend = decomposition.trend
                        seasonal = decomposition.seasonal
                        residual = decomposition.resid
                        
                        # Map back to original dataframe
                        trend_map = {}
                        seasonal_map = {}
                        residual_map = {}
                        
                        for timestamp in df_sorted['timestamp']:
                            # Find the closest time period
                            period_start = timestamp.floor(freq)
                            
                            if period_start in trend.index:
                                trend_map[timestamp] = trend[period_start]
                                seasonal_map[timestamp] = seasonal[period_start]
                                residual_map[timestamp] = residual[period_start]
                            else:
                                trend_map[timestamp] = 0
                                seasonal_map[timestamp] = 0
                                residual_map[timestamp] = 0
                        
                        # Add to result dataframe
                        result_df[f'trend_{freq}'] = df_sorted['timestamp'].map(trend_map).fillna(0)
                        result_df[f'seasonal_{freq}'] = df_sorted['timestamp'].map(seasonal_map).fillna(0)
                        result_df[f'residual_{freq}'] = df_sorted['timestamp'].map(residual_map).fillna(0)
                        
                        # Calculate anomaly score based on residual
                        residual_mean = residual.mean()
                        residual_std = residual.std()
                        
                        if residual_std > 0:
                            anomaly_scores = (df_sorted['timestamp'].map(residual_map) - residual_mean) / residual_std
                            result_df[f'seasonal_anomaly_{freq}'] = anomaly_scores.fillna(0)
                        else:
                            result_df[f'seasonal_anomaly_{freq}'] = 0
                            
                    except Exception as e:
                        logger.warning(f"Error in seasonal decomposition for {freq}: {str(e)}")
            
            # Calculate periodicity features
            # Check for daily, weekly, and monthly patterns
            periodicity_features = {}
            
            # Daily pattern (hour of day)
            if 'hour' in result_df.columns:
                hourly_counts = result_df.groupby('hour').size()
                # Calculate entropy to measure uniformity
                hourly_probs = hourly_counts / hourly_counts.sum()
                daily_entropy = -sum(p * np.log2(p) for p in hourly_probs if p > 0)
                max_entropy = np.log2(24)  # Maximum possible entropy for 24 hours
                daily_uniformity = daily_entropy / max_entropy if max_entropy > 0 else 0
                
                periodicity_features['daily_pattern_strength'] = 1 - daily_uniformity  # Higher means stronger pattern
                
                # Calculate peak hour
                peak_hour = hourly_counts.idxmax()
                periodicity_features['peak_hour'] = peak_hour
            
            # Weekly pattern (day of week)
            if 'dayofweek' in result_df.columns:
                weekly_counts = result_df.groupby('dayofweek').size()
                # Calculate entropy to measure uniformity
                weekly_probs = weekly_counts / weekly_counts.sum()
                weekly_entropy = -sum(p * np.log2(p) for p in weekly_probs if p > 0)
                max_entropy = np.log2(7)  # Maximum possible entropy for 7 days
                weekly_uniformity = weekly_entropy / max_entropy if max_entropy > 0 else 0
                
                periodicity_features['weekly_pattern_strength'] = 1 - weekly_uniformity  # Higher means stronger pattern
                
                # Calculate peak day
                peak_day = weekly_counts.idxmax()
                periodicity_features['peak_day'] = peak_day
            
            # Add periodicity features to all rows
            for feature, value in periodicity_features.items():
                result_df[feature] = value
            
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting seasonal features: {str(e)}")
            return df
    
    def _extract_autocorrelation_features(self, df):
        """
        Extract autocorrelation features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with autocorrelation features
        """
        try:
            result_df = df.copy()
            
            if 'timestamp' not in df.columns:
                logger.warning("Timestamp column not found. Skipping autocorrelation features.")
                return result_df
            
            # Sort by timestamp
            df_sorted = result_df.sort_values('timestamp')
            
            # Create time series of transaction counts
            # Resample to different frequencies
            frequencies = ['1H', '6H', '1D']
            
            for freq in frequencies:
                # Count transactions per time period
                ts = df_sorted.set_index('timestamp').resample(freq).size()
                
                if len(ts) > 10:  # Need enough data points for autocorrelation
                    try:
                        # Calculate autocorrelation function
                        nlags = min(10, len(ts) // 2)
                        autocorr = acf(ts, nlags=nlags, fft=True)
                        
                        # Add autocorrelation features
                        for i in range(1, min(6, len(autocorr))):  # First 5 lags
                            result_df[f'autocorr_{freq}_lag_{i}'] = autocorr[i]
                        
                        # Calculate partial autocorrelation
                        pacf_values = pacf(ts, nlags=nlags)
                        
                        # Add partial autocorrelation features
                        for i in range(1, min(6, len(pacf_values))):  # First 5 lags
                            result_df[f'pacf_{freq}_lag_{i}'] = pacf_values[i]
                        
                        # Detect periodicity in autocorrelation
                        # Look for peaks in autocorrelation
                        peaks, _ = find_peaks(autocorr[1:], height=0.2)  # Ignore lag 0
                        
                        if len(peaks) > 0:
                            # Find the most significant peak
                            peak_lag = peaks[0] + 1  # +1 because we ignored lag 0
                            result_df[f'periodicity_{freq}'] = peak_lag
                            result_df[f'periodicity_strength_{freq}'] = autocorr[peak_lag]
                        else:
                            result_df[f'periodicity_{freq}'] = 0
                            result_df[f'periodicity_strength_{freq}'] = 0
                            
                    except Exception as e:
                        logger.warning(f"Error in autocorrelation analysis for {freq}: {str(e)}")
            
            return result_df
            
        except Exception as e:
            logger.error(f"Error extracting autocorrelation features: {str(e)}")
            return df
    
    def fit_transform(self, df):
        """
        Fit the feature extractor and transform the data
        
        Args:
            df (DataFrame): Input data
            
        Returns:
            DataFrame: Transformed data with features
        """
        # Extract features
        result_df = self.extract_features(df)
        
        # Get feature columns
        feature_cols = [col for col in result_df.columns if col not in df.columns]
        
        if len(feature_cols) > 0:
            self.fitted = True
        
        return result_df
    
    def transform(self, df):
        """
        Transform new data using fitted feature extractor
        
        Args:
            df (DataFrame): Input data
            
        Returns:
            DataFrame: Transformed data with features
        """
        if not self.fitted:
            raise ValueError("Feature extractor not fitted. Call fit_transform first.")
        
        # Extract features
        result_df = self.extract_features(df)
        
        return result_df

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\features\.ipynb_checkpoints\__init__-checkpoint.py ===

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\features\__pycache__\graph_features.cpython-313.pyc ===
Û
    dÄöhπñ  „                   Û¸   ï S r SSKrSSKrSSKrSSKJrJ	r	  SSK
rSSKJr  SSKrSSKrSSKJrJrJrJr  \R*                  " S5        \R,                  " \R.                  S9  \R0                  " \5      r " S S	5      rg)
z`
Graph Features Module
Implements graph-based feature extraction techniques for fraud detection
È    N)⁄defaultdict⁄Counter)⁄MinMaxScaler)⁄Dict⁄List⁄Tuple⁄Union⁄ignore)⁄levelc                   Û^   ï \ rS rSrSrSS jrS rS rS rS r	S	 r
S
 rS rS rS rS rSrg)⁄GraphFeaturesÈ   zè
Class for extracting graph-based features from transaction data
Implements techniques like centrality measures, clustering coefficients, etc.
Nc                 Ûñ   ï U=(       d    0 U l         SU l        SU l        SU l        SU l        / U l        [        5       U l        SU l        g)zW
Initialize GraphFeatures

Args:
    config (dict, optional): Configuration parameters
NF)	⁄config⁄graph⁄sender_graph⁄receiver_graph⁄bipartite_graph⁄feature_namesr   ⁄scaler⁄fitted)⁄selfr   s     ⁄mC:\Users\Tranquil Abyss\fraud-detection-platform\app\..\src\fraud_detection_engine\features\graph_features.py⁄__init__⁄GraphFeatures.__init__   sF   Ä  ól†àåÿàå
ÿ à‘ÿ"à‘ÿ#à‘ÿà‘‹"ìnàåÿàçÛ    c                 ÛF  ï  UR                  5       nU R                  U5        U R                  U5      nU R                  U5      nU R	                  U5      nU R                  U5      nU R                  U5      nU R                  U5      nUR                   Vs/ s H  o3UR                  ;  d  M  UPM     snU l	        [        R                  S[        U R                  5       S35        U$ s  snf ! [         a'  n[        R                  S[        U5       35        e SnAff = f)z†
Extract all graph features from the dataframe

Args:
    df (DataFrame): Input transaction data
    
Returns:
    DataFrame: DataFrame with extracted features
z
Extracted z graph featuresz!Error extracting graph features: N)⁄copy⁄_build_graphs⁄_extract_centrality_features⁄_extract_clustering_features⁄_extract_community_features⁄_extract_path_features⁄_extract_subgraph_features⁄ _extract_temporal_graph_features⁄columnsr   ⁄logger⁄info⁄len⁄	Exception⁄error⁄str)r   ⁄df⁄	result_df⁄col⁄es        r   ⁄extract_features⁄GraphFeatures.extract_features*   s  Ä 	‡üôõ	àI ◊—òr‘" ◊9—9∏)”DàIÿ◊9—9∏)”DàIÿ◊8—8∏”CàIÿ◊3—3∞I”>àIÿ◊7—7∏	”BàIÿ◊=—=∏i”HàI 2;◊1B“1B”!\“1B®#–QS◊Q[—Q[—F[ß#—1B—!\àD‘‰èKâKò*§S®◊);—);”%<–$=∏_–M‘Nÿ–˘Ú "]¯Ù
 Û 	‹èLâL–<ºS¿ªV∏H–E‘Fÿ˚	˙s0   ÇBC/ ¬C*¬.C*¬45C/ √*C/ √/
D √9"DƒD c           	      Û™	  ï  [         R                  " 5       U l        UR                  5        GH
  u  p#SU;   d  M  SU;   d  M  US   nUS   n0 nSU;   a  US   US'   SU;   a  US   US'   SU;   a  US   US'   U R                  R	                  XE5      (       aa  U R                  U   U   nSU;   a  UR                  SS5      US   -   US'   UR                  SS5      S	-   US'   US
   R                  U5        MÕ  UR                  SS5      US'   S	US'   U/US
'   U R                  R                  " XE40 UD6  GM     [         R                  " 5       U l	        [        [        5      nUR                  5        H.  u  p#SU;   d  M  SU;   d  M  XÉS      R                  US   5        M0     UR                  5        H´  u  pY[        U	5      n
[        [!        U
5      5       HÉ  n[        US	-   [!        U
5      5       Hd  nX´   nX¨   nU R                  R	                  Xﬁ5      (       a  U R                  U   U   S==   S	-  ss'   MJ  U R                  R                  XﬁS	S9  Mf     MÖ     M≠     [         R                  " 5       U l        [        [        5      nUR                  5        H.  u  p#SU;   d  M  SU;   d  M  XÛS      R                  US   5        M0     UR                  5        H∞  u  nn[        U5      n[        [!        U5      5       Há  n[        US	-   [!        U5      5       Hh  nUU   nUU   nU R"                  R	                  UU5      (       a  U R"                  U   U   S==   S	-  ss'   MM  U R"                  R                  UUS	S9  Mj     Mâ     M≤     [         R                  " 5       U l        UR                  5        HM  u  p#SU;   a  U R$                  R'                  US   SS9  SU;   d  M0  U R$                  R'                  US   S	S9  MO     UR                  5        Hh  u  p#SU;   d  M  SU;   d  M  US   nUS   n0 nSU;   a  US   US'   SU;   a  US   US'   SU;   a  US   US'   U R$                  R                  " XE40 UD6  Mj     [(        R+                  S5        g! [,         a'  n[(        R/                  S[1        U5       35        e SnAff = f)zb
Build various graphs from the transaction data

Args:
    df (DataFrame): Input transaction data
⁄	sender_id⁄receiver_id⁄amount⁄	timestamp⁄transaction_id⁄total_amountr   ⁄transaction_countÈ   ⁄transactions⁄common_receivers)r=   ⁄common_senders)r>   )⁄	bipartitezGraphs built successfullyzError building graphs: N)⁄nx⁄DiGraphr   ⁄iterrows⁄has_edge⁄get⁄append⁄add_edge⁄Graphr   r   ⁄set⁄add⁄items⁄list⁄ranger)   r   r   ⁄add_noder'   r(   r*   r+   r,   )r   r-   ⁄_⁄row⁄sender⁄receiver⁄attrs⁄	edge_data⁄receiver_to_senders⁄senders⁄senders_list⁄i⁄j⁄sender1⁄sender2⁄sender_to_receivers⁄	receivers⁄receivers_list⁄	receiver1⁄	receiver2r0   s                        r   r   ⁄GraphFeatures._build_graphsM   s…  Ä n	‰üöõàDåJ ü+ô+ü-ëêÿ†#’%®-∏3’*>ÿ †—-êFÿ"†=—1êH êEÿ†3ìÿ*-®h©-òòhôÿ"†c”)ÿ-0∞—-=òòk—*ÿ'®3”.ÿ25–6F—2Gò–.—/ ózëz◊*—*®6◊<—<‡$(ßJ°J®v—$6∞x—$@ò	ÿ#†u”,ÿ8Aøπ¿n–VW”8X–[`–ai—[j—8jòI†n—5ÿ9Bøπ–GZ–\]”9^–ab—9bò	–"5—6ÿ!†.—1◊8—8∏÷? 16∑	±	∏(¿A”0Fòòn—-ÿ56ò–1—2ÿ16∞òòn—-ÿü
ô
◊+“+®F—F¿’FÒ7 (Ù< !#ß¢£
àD‘Ù #.¨c”"2–ÿü+ô+û-ëêÿ†#’%®-∏3’*>ÿ'®M—(:—;◊?—?¿¿K—@P÷QÒ (
 &9◊%>—%>÷%@—!ê‹#†Gõ}ê‹ús†<”0÷1êA‹"†1†Q°3¨®L”(9÷:òÿ".°/òÿ".°/ò‡◊,—,◊5—5∞g◊G—Gÿ ◊-—-®g—6∞w—?–@R”S–WX—X’S‡ ◊-—-◊6—6∞w–Z[–6”\Û ;Û 2Ò &AÙ #%ß(¢(£*àD‘Ù #.¨c”"2–ÿü+ô+û-ëêÿ†#’%®-∏3’*>ÿ'®K—(8—9◊=—=∏c¿-—>P÷QÒ (
 &9◊%>—%>÷%@—!êò	‹!%†i£ê‹ús†>”2÷3êA‹"†1†Q°3¨®N”(;÷<òÿ$2∞1—$5ò	ÿ$2∞1—$5ò	‡◊.—.◊7—7∏	¿9◊M—Mÿ ◊/—/∞	—:∏9—E–FV”W–[\—\’W‡ ◊/—/◊8—8∏¿I–^_–8”`Û =Û 4Ò &AÙ $&ß8¢8£:àD‘  ü+ô+û-ëêÿ†#”%ÿ◊(—(◊1—1∞#∞k—2B»a–1—Pÿ †C’'ÿ◊(—(◊1—1∞#∞m—2D–PQ–1”RÒ	 ( ü+ô+û-ëêÿ†#’%®-∏3’*>ÿ †—-êFÿ"†=—1êH êEÿ†3ìÿ*-®h©-òòhôÿ"†c”)ÿ-0∞—-=òòk—*ÿ'®3”.ÿ25–6F—2Gò–.—/ ◊(—(◊1“1∞&—L¿e‘LÒ (Ù" èKâK–3’4¯‰Û 	‹èLâL–2¥3∞q≥6∞(–;‘<ÿ˚	˙sP   Ç4R! ∫R! ¡D;R! ∆R! ∆	DR!  +R!  3D7R! œ.:R! –,R! –4A,R! “!
S“+"S”Sc                 Û¯	  ï  UR                  5       nU R                  c  [        R                  S5        U$ [        R
                  " U R                  5      n[        R                  " U R                  5      n[        R                  " U R                  R                  5       5      nSUR                  ;   ar  US   R                  U5      R                  S5      US'   US   R                  U5      R                  S5      US'   US   R                  U5      R                  S5      US'   SUR                  ;   ar  US   R                  U5      R                  S5      US	'   US   R                  U5      R                  S5      US
'   US   R                  U5      R                  S5      US'   [        U R                  R                  5       5      S::  a!  [        R                  " U R                  5      nOE[        U R                  R                  5       5      SS n[        R                  " U R                  US9nSUR                  ;   a&  US   R                  U5      R                  S5      US'   SUR                  ;   a&  US   R                  U5      R                  S5      US'   [        U R                  R                  5       5      S::  a!  [        R                   " U R                  5      nO[        R                   " U R                  SS9nSUR                  ;   a&  US   R                  U5      R                  S5      US'   SUR                  ;   a&  US   R                  U5      R                  S5      US'   [        U R                  R                  5       5      S::  a!   [        R"                  " U R                  SS9n	O0 n	SUR                  ;   a&  US   R                  U	5      R                  S5      US'   SUR                  ;   a&  US   R                  U	5      R                  S5      US'   [        R$                  " U R                  5      n
SUR                  ;   a&  US   R                  U
5      R                  S5      US'   SUR                  ;   a&  US   R                  U
5      R                  S5      US'   U$ !   0 n	 GN= f! [&         a-  n[        R)                  S[+        U5       35        Us SnA$ SnAff = f)zé
Extract centrality-based features

Args:
    df (DataFrame): Input dataframe
    
Returns:
    DataFrame: DataFrame with centrality features
Nz.Graph not built. Skipping centrality features.r4   r   ⁄sender_in_degree_centrality⁄sender_out_degree_centrality⁄sender_degree_centralityr5   ⁄receiver_in_degree_centrality⁄receiver_out_degree_centrality⁄receiver_degree_centralityiË  )⁄k⁄sender_betweenness_centrality⁄receiver_betweenness_centrality⁄weight)⁄distance⁄sender_closeness_centrality⁄receiver_closeness_centrality)⁄max_iter⁄sender_eigenvector_centrality⁄receiver_eigenvector_centrality⁄sender_pagerank⁄receiver_pagerankz&Error extracting centrality features: )r   r   r'   ⁄warningr@   ⁄in_degree_centrality⁄out_degree_centrality⁄degree_centrality⁄to_undirectedr&   ⁄map⁄fillnar)   ⁄nodes⁄betweenness_centralityrK   ⁄closeness_centrality⁄eigenvector_centrality⁄pagerankr*   r+   r,   )r   r-   r.   ru   rv   rw   r|   ⁄sample_nodesr}   r~   r   r0   s               r   r    ⁄*GraphFeatures._extract_centrality_featuresƒ   sm  Ä R	ÿüôõ	àI‡èzâz—!‹óë–O‘Pÿ – Ù $&◊#:“#:∏4ø:π:”#F– ‹$&◊$<“$<∏TøZπZ”$H–!‹ "◊ 4“ 4∞T∑Z±Z◊5M—5M”5O” P– òbüjôj”(ÿ;=∏kπ?◊;N—;N–Oc”;d◊;k—;k–lm”;nê	–7—8ÿ<>∏{πO◊<O—<O–Pe”<f◊<m—<m–no”<pê	–8—9ÿ8:∏;π◊8K—8K–L]”8^◊8e—8e–fg”8hê	–4—5‡†ß
°
”*ÿ=?¿—=N◊=R—=R–Sg”=h◊=o—=o–pq”=rê	–9—:ÿ>@¿—>O◊>S—>S–Ti”>j◊>q—>q–rs”>tê	–:—;ÿ:<∏]—:K◊:O—:O–Pa”:b◊:i—:i–jk”:lê	–6—7Ù ê4ó:ë:◊#—#”%”&®$”.‹)+◊)B“)B¿4«:¡:”)N—&Ù  $†DßJ°J◊$4—$4”$6”7∏∏–>ê‹)+◊)B“)B¿4«:¡:–Q]—)^–& òbüjôj”(ÿ=?¿π_◊=P—=P–Qg”=h◊=o—=o–pq”=rê	–9—:‡†ß
°
”*ÿ?A¿-—?P◊?T—?T–Uk”?l◊?s—?s–tu”?vê	–;—<Ù ê4ó:ë:◊#—#”%”&®$”.‹')◊'>“'>∏tøzπz”'J—$Ù (*◊'>“'>∏tøzπz–T\—']–$ òbüjôj”(ÿ;=∏kπ?◊;N—;N–Oc”;d◊;k—;k–lm”;nê	–7—8‡†ß
°
”*ÿ=?¿—=N◊=R—=R–Sg”=h◊=o—=o–pq”=rê	–9—:Ù ê4ó:ë:◊#—#”%”&®$”.0‹-/◊-F“-F¿t«z¡z–\`—-a—* *,–& òbüjôj”(ÿ=?¿π_◊=P—=P–Qg”=h◊=o—=o–pq”=rê	–9—:‡†ß
°
”*ÿ?A¿-—?P◊?T—?T–Uk”?l◊?s—?s–tu”?vê	–;—<Ù ó{í{†4ß:°:”.àH òbüjôj”(ÿ/1∞+©◊/B—/B¿8”/L◊/S—/S–TU”/Vê	–+—,‡†ß
°
”*ÿ13∞M—1B◊1F—1F¿x”1P◊1W—1W–XY”1Zê	–-—.‡–¯-0ÿ-/”*˚Ù. Û 	‹èLâL–Aƒ#¿a√&¿–J‘KÿçI˚	˙s<   Ç3S ∂M%S ŒR8 Œ;C<S “8R?“<S ”
S9”"S4”.S9”4S9c                 Û®  ï  UR                  5       nU R                  c  [        R                  S5        U$ [        R
                  " U R                  R                  5       5      nSUR                  ;   a&  US   R                  U5      R                  S5      US'   SUR                  ;   a&  US   R                  U5      R                  S5      US'   U R                  b}  [        U R                  R                  5       5      S:î  aV  [        R
                  " U R                  5      nSUR                  ;   a&  US   R                  U5      R                  S5      US'   U R                  b}  [        U R                  R                  5       5      S:î  aV  [        R
                  " U R                  5      nSUR                  ;   a&  US   R                  U5      R                  S5      US	'   [        R                  " U R                  R                  5       5      nSUR                  ;   a&  US   R                  U5      R                  S5      US
'   SUR                  ;   a&  US   R                  U5      R                  S5      US'   [        R                  " U R                  R                  5       5      nSUR                  ;   a&  US   R                  U5      R                  S5      US'   SUR                  ;   a&  US   R                  U5      R                  S5      US'   U$ ! [          a-  n[        R#                  S[%        U5       35        Us SnA$ SnAff = f)zé
Extract clustering-based features

Args:
    df (DataFrame): Input dataframe
    
Returns:
    DataFrame: DataFrame with clustering features
Nz.Graph not built. Skipping clustering features.r4   r   ⁄sender_clustering_coefficientr5   ⁄receiver_clustering_coefficient⁄#sender_graph_clustering_coefficient⁄%receiver_graph_clustering_coefficient⁄sender_avg_neighbor_degree⁄receiver_avg_neighbor_degree⁄sender_square_clustering⁄receiver_square_clusteringz&Error extracting clustering features: )r   r   r'   rt   r@   ⁄
clusteringrx   r&   ry   rz   r   r)   r{   r   ⁄average_neighbor_degree⁄square_clusteringr*   r+   r,   )	r   r-   r.   ⁄clustering_coeff⁄sender_clustering_coeff⁄receiver_clustering_coeff⁄avg_neighbor_degreerç   r0   s	            r   r!   ⁄*GraphFeatures._extract_clustering_features"  sÙ  Ä 9	ÿüôõ	àI‡èzâz—!‹óë–O‘Pÿ – Ù  "ü}ö}®TØZ©Z◊-E—-E”-G”H– òbüjôj”(ÿ=?¿π_◊=P—=P–Qa”=b◊=i—=i–jk”=lê	–9—:‡†ß
°
”*ÿ?A¿-—?P◊?T—?T–Ue”?f◊?m—?m–no”?pê	–;—< ◊ — —,¥∞T◊5F—5F◊5L—5L”5N”1O–RS”1S‹*,Ø-™-∏◊8I—8I”*J–' †"ß*°*”,ÿGI»+¡◊GZ—GZ–[r”Gs◊Gz—Gz–{|”G}êI–C—D ◊"—"—.¥3∞t◊7J—7J◊7P—7P”7R”3S–VW”3W‹,.ØM™M∏$◊:M—:M”,N–) !†BßJ°J”.ÿIK»M—IZ◊I^—I^–_x”Iy˜  JAÒ  JA  BCÛ  JDêI–E—FÙ #%◊"<“"<∏TøZπZ◊=U—=U”=W”"X– òbüjôj”(ÿ:<∏[π/◊:M—:M–Na”:b◊:i—:i–jk”:lê	–6—7‡†ß
°
”*ÿ<>∏}—<M◊<Q—<Q–Re”<f◊<m—<m–no”<pê	–8—9Ù !#◊ 4“ 4∞T∑Z±Z◊5M—5M”5O” P– òbüjôj”(ÿ8:∏;π◊8K—8K–L]”8^◊8e—8e–fg”8hê	–4—5‡†ß
°
”*ÿ:<∏]—:K◊:O—:O–Pa”:b◊:i—:i–jk”:lê	–6—7‡–¯‰Û 	‹èLâL–Aƒ#¿a√&¿–J‘KÿçI˚	˙s#   Ç3L ∂K#L Ã
MÃ$"MÕMÕMc                 ÛŒ  ï  UR                  5       nU R                  c  [        R                  S5        U$ U R                  R	                  5       n[
        R                  " U5      nSUR                  ;   a&  US   R                  U5      R                  S5      US'   SUR                  ;   a&  US   R                  U5      R                  S5      US'   [        UR                  5       5      nUR                  5        VVs0 s H
  u  pgXeU   _M     nnnSUR                  ;   a&  US   R                  U5      R                  S5      US	'   SUR                  ;   a&  US   R                  U5      R                  S5      US
'   0 n	UR                  5        Hì  u  pzUR                  5        VVs/ s H  u  pkX∑:X  d  M  UPM     nnnUR                  U5      n[        UR                  5       5      S:î  d  M`  [         R"                  " U5      nU H  nUR%                  US5      Xñ'   M     Mï     SUR                  ;   a&  US   R                  U	5      R                  S5      US'   SUR                  ;   a&  US   R                  U	5      R                  S5      US'   SUR                  ;   a|  SUR                  ;   al  / nUR'                  5        HR  u  nnUS   nUS   nUU;   a+  UU;   a%  UR)                  [+        UU   UU   :H  5      5        MA  UR)                  S5        MT     XÚS'   U$ s  snnf s  snnf ! [,         a-  n[        R/                  S[1        U5       35        Us SnA$ SnAff = f)zå
Extract community-based features

Args:
    df (DataFrame): Input dataframe
    
Returns:
    DataFrame: DataFrame with community features
Nz-Graph not built. Skipping community features.r4   Èˇˇˇˇ⁄sender_communityr5   ⁄receiver_communityr   ⁄sender_community_size⁄receiver_community_size⁄"sender_community_degree_centrality⁄$receiver_community_degree_centrality⁄same_communityz%Error extracting community features: )r   r   r'   rt   rx   ⁄community_louvain⁄best_partitionr&   ry   rz   r   ⁄valuesrJ   ⁄subgraphr)   r{   r@   rw   rD   rB   rE   ⁄intr*   r+   r,   )r   r-   r.   ⁄undirected_graph⁄communities⁄community_sizes⁄node⁄comm⁄node_community_sizes⁄community_degree_centralityr{   ⁄c⁄
comm_nodesrü   ⁄comm_centralityrõ   rN   rO   rP   rQ   r0   s                        r   r"   ⁄)GraphFeatures._extract_community_featuresg  s=  Ä @	ÿüôõ	àI‡èzâz—!‹óë–N‘Oÿ –   $üzôz◊7—7”9–‹+◊:“:–;K”LàK òbüjôj”(ÿ02∞;±◊0C—0C¿K”0P◊0W—0W–XZ”0[ê	–,—-‡†ß
°
”*ÿ24∞]—2C◊2G—2G»”2T◊2[—2[–\^”2_ê	–.—/Ù &†k◊&8—&8”&:”;àOÿR]◊Rc—Rc‘Re‘#f“Re¡J¿D†D∏$—*?“$?—Re– —#f òbüjôj”(ÿ57∏±_◊5H—5H–I]”5^◊5e—5e–fg”5hê	–1—2‡†ß
°
”*ÿ79∏-—7H◊7L—7L–Ma”7b◊7i—7i–jk”7lê	–3—4 +-–'ÿ.◊4—4÷6ëêÿ2=◊2C—2C‘2E‘S“2E°w†t»…üd—2Eê
—Sÿ+◊4—4∞Z”@ê‹êxó~ë~”'”(®1’,‹&(◊&:“&:∏8”&DêO€ *òÿ<K◊<O—<O–PT–VW”<X–3”9Û !+Ò  7 òbüjôj”(ÿBD¿[¡/◊BU—BU–Vq”Br◊By—By–z{”B|ê	–>—?‡†ß
°
”*ÿDF¿}—DU◊DY—DY–Zu”Dv◊D}—D}–~Û  EAê	–@—A òbüjôj”(®]∏bøjπj”-Hÿ!#êÿ ükôkûmëFêAêsÿ †—-êFÿ"†=—1êH‡†”,∞∏[”1Hÿ&◊-—-¨c∞+∏f—2E»–U]—I^—2^”._÷`‡&◊-—-®a÷0Ò , /=–*—+‡–˘ÛQ $g˘Û T¯Ù< Û 	‹èLâL–@ƒ¿Q√¿–I‘JÿçI˚	˙sO   Ç3L- ∂CL- √>L!ƒBL- ∆(L'∆7L'∆=/L- «0D0L- Ã!L- Ã-
M$Ã7"MÕM$ÕM$c                 Û  ï  UR                  5       nU R                  c  [        R                  S5        U$ [	        U R                  R                  5       5      S::  a≤  [        [        R                  " U R                  5      5      nSUR                  ;   ax  SUR                  ;   ah  / nUR                  5        HN  u  pVUS   nUS   nXs;   a   XÉU   ;   a  UR                  X7   U   5        M4  UR                  [        S5      5        MP     XBS'   O|SUR                  ;   al  SUR                  ;   a\  / nUR                  5        HB  u  pVUS   nUS   n [        R                  " U R                  Xx5      n	UR                  U	5        MD     XBS'   SUR                  ;   aË  SUR                  ;   aÿ  / n
UR                  5        Hæ  u  pVUS   nUS   n [        U R                  R                  U5      5      [        U R                  R!                  U5      5      -  n[        U R                  R                  U5      5      [        U R                  R!                  U5      5      -  nU
R                  [	        Xº-  5      5        M¿     X¢S	'   SUR                  ;   Ga  SUR                  ;   a˛  / nUR                  5        H‰  u  pVUS   nUS   n [        U R                  R                  U5      5      [        U R                  R!                  U5      5      -  n[        U R                  R                  U5      5      [        U R                  R!                  U5      5      -  nXº-  nXº-  n[	        U5      S:î  a  [	        U5      [	        U5      -  nOSnUR                  U5        MÊ     X“S
'   SUR                  ;   Ga/  SUR                  ;   Ga  / nUR                  5        GH  u  pVUS   nUS   n [        U R                  R                  U5      5      [        U R                  R!                  U5      5      -  n[        U R                  R                  U5      5      [        U R                  R!                  U5      5      -  nXº-  nSnU HB  nU R                  R#                  U5      nUS:î  d  M&  US[$        R&                  " U5      -  -  nMD     UR                  U5        GM     UUS'   U$ !   UR                  [        S5      5         GM√  = f!   U
R                  S5         GMa  = f!   UR                  S5         GMÇ  = f!   UR                  S5         GM|  = f! [(         a-  n[        R+                  S[-        U5       35        Us SnA$ SnAff = f)zÇ
Extract path-based features

Args:
    df (DataFrame): Input dataframe
    
Returns:
    DataFrame: DataFrame with path features
Nz(Graph not built. Skipping path features.iÙ  r4   r5   ⁄inf⁄shortest_path_lengthr   ⁄common_neighbors_count⁄jaccard_coefficientr;   ⁄adamic_adar_indexz Error extracting path features: )r   r   r'   rt   r)   r{   ⁄dictr@   ⁄all_pairs_shortest_path_lengthr&   rB   rE   ⁄floatrÆ   rH   ⁄predecessors⁄
successors⁄degree⁄np⁄logr*   r+   r,   )r   r-   r.   ⁄shortest_paths⁄path_lengthsrN   rO   rP   rQ   ⁄path_length⁄common_neighbors⁄sender_neighbors⁄receiver_neighbors⁄jaccard_coeffs⁄union⁄intersection⁄jaccard⁄adamic_adar⁄common⁄aa_indexr§   r∑   r0   s                          r   r#   ⁄$GraphFeatures._extract_path_features≥  s  Ä p	ÿüôõ	àI‡èzâz—!‹óë–I‘Jÿ – Ù ê4ó:ë:◊#—#”%”&®#”-‰!%§b◊&G“&G»œ
…
”&S”!Tê †"ß*°*”,∞¿"«*¡*”1Lÿ#%êLÿ"$ß+°+¶-ôòÿ!$†[—!1òÿ#&†}—#5ò‡!”3∏–SY—DZ”8Zÿ(◊/—/∞—0F¿x—0P÷Q‡(◊/—/¥∞e≥÷=Ò #0 9E–4—5¯ †"ß*°*”,∞¿"«*¡*”1L‡#%êLÿ"$ß+°+¶-ôòÿ!$†[—!1òÿ#&†}—#5ò>‹*,◊*A“*A¿$«*¡*»f”*_òKÿ(◊/—/∞÷<Ò #0 9E–4—5 òbüjôj”(®]∏bøjπj”-Hÿ#%– ÿ ükôkûmëFêAÿ †—-êFÿ"†=—1êH3‹+.®tØz©z◊/F—/F¿v”/N”+O‘RU–VZ◊V`—V`◊Vk—Vk–lr”Vs”Rt—+t–(‹-0∞∑±◊1H—1H»”1R”-S‘VY–Z^◊Zd—Zd◊Zo—Zo–px”Zy”Vz—-z–*ÿ(◊/—/¥–4D—4Y”0Z÷[Ò , 7G–2—3 òbüjôj‘(®]∏bøjπj”-Hÿ!#êÿ ükôkûmëFêAÿ †—-êFÿ"†=—1êH1‹+.®tØz©z◊/F—/F¿v”/N”+O‘RU–VZ◊V`—V`◊Vk—Vk–lr”Vs”Rt—+t–(‹-0∞∑±◊1H—1H»”1R”-S‘VY–Z^◊Zd—Zd◊Zo—Zo–px”Zy”Vz—-z–*‡ 0— Eòÿ'7—'Lò‰òuõ:®õ>‹&)®,”&7º#∏eª*—&DôG‡&'òG‡&◊-—-®g÷6Ò! ,( 4B–/—0 òbüjôj‘(®]∏bøjπj‘-Hÿ êÿ ükôkümëFêAÿ †—-êFÿ"†=—1êH.‹+.®tØz©z◊/F—/F¿v”/N”+O‘RU–VZ◊V`—V`◊Vk—Vk–lr”Vs”Rt—+t–(‹-0∞∑±◊1H—1H»”1R”-S‘VY–Z^◊Zd—Zd◊Zo—Zo–px”Zy”Vz—-z–*ÿ!1—!Fò $%ò€$*òDÿ%)ßZ°Z◊%6—%6∞t”%<òFÿ%®ùzÿ (®A¥∑≤∞v≥—,>— >¢Ò %+
 $◊*—*®8◊4Ò! ,( 2=ê	–-—.‡–¯O>ÿ(◊/—/¥∞e≥◊=–=˚3ÿ(◊/—/∞◊2–2˚01ÿ&◊-—-®a◊0–0˚0.ÿ#◊*—*®1◊-–-˚Ù Û 	‹èLâL–;ºC¿ªF∏8–D‘EÿçI˚	˙sè   Ç3U ∂DU ≈2S$∆A	U «B.T…;A
U ÀCT!ŒAU œ'B<T;“'1T;”U ”$T‘ U ‘T‘U ‘!T8‘4U ‘;U’U ’
V’"V÷V÷Vc                 Ûj  ï  UR                  5       nU R                  c  [        R                  S5        U$ SUR                  ;   aò  / n/ nUS    HÉ  n [
        R                  " U R                  R                  5       USS9n[        UR                  5       5      n[
        R                  " U5      nUR                  U5        UR                  U5        MÖ     X2S'   XBS'   S	UR                  ;   aò  / n	/ n
US	    HÉ  n [
        R                  " U R                  R                  5       USS9n[        UR                  5       5      n[
        R                  " U5      nU	R                  U5        U
R                  U5        MÖ     XíS
'   X¢S'   U R                  GbP  U R                  R                  SS9 VVs1 s H  u  pÕUS   S:X  d  M  UiM     nnnU R                  R                  SS9 VVs1 s H  u  pÕUS   S:X  d  M  UiM     nnn[
        R                  R                  U R                  U5      nSUR                  ;   a6  / nUS    H&  n UR                  U5      nUR                  U5        M(     UUS'   [
        R                  R                  U R                  U5      nS	UR                  ;   a6  / nUS	    H&  n UR                  U5      nUR                  U5        M(     UUS'   U$ !   UR                  S5        UR                  S5         GMø  = f!   U	R                  S5        U
R                  S5         GMB  = fs  snnf s  snnf !   UR                  S5         GM  = f!   UR                  S5         M√  = f! [          a-  n[        R#                  S[%        U5       35        Us SnA$ SnAff = f)zä
Extract subgraph-based features

Args:
    df (DataFrame): Input dataframe
    
Returns:
    DataFrame: DataFrame with subgraph features
Nz,Graph not built. Skipping subgraph features.r4   r;   )⁄radiusr   ⁄sender_ego_network_size⁄sender_ego_network_densityr5   ⁄receiver_ego_network_size⁄receiver_ego_network_densityT)⁄datar?   ⁄sender_projection_degree⁄receiver_projection_degreez$Error extracting subgraph features: )r   r   r'   rt   r&   r@   ⁄	ego_graphrx   r)   r{   ⁄densityrE   r   r?   ⁄projected_graphr∑   r*   r+   r,   )r   r-   r.   ⁄sender_ego_sizes⁄sender_ego_densitiesrP   r—   ⁄ego_size⁄ego_density⁄receiver_ego_sizes⁄receiver_ego_densitiesrQ   ⁄n⁄drU   r\   ⁄sender_projection⁄sender_proj_degreesr∑   ⁄receiver_projection⁄receiver_proj_degreesr0   s                         r   r$   ⁄(GraphFeatures._extract_subgraph_features/  sé  Ä ]	ÿüôõ	àI‡èzâz—!‹óë–M‘Nÿ –  òbüjôj”(ÿ#%– ÿ')–$‡ †úoêF7‰$&ßL¢L∞∑±◊1I—1I”1K»V–\]—$^ò	Ù $'†yß°”'8”#9ò‹&(ßj¢j∞”&;ò‡(◊/—/∞‘9ÿ,◊3—3∞K÷@Ò . 8H–3—4ÿ:N–6—7 †ß
°
”*ÿ%'–"ÿ)+–&‡ "†=‘ 1êH9‰$&ßL¢L∞∑±◊1I—1I”1K»X–^_—$`ò	Ù $'†yß°”'8”#9ò‹&(ßj¢j∞”&;ò‡*◊1—1∞(‘;ÿ.◊5—5∞k÷BÒ !2 :L–5—6ÿ<R–8—9 ◊#—#“/‡)-◊)=—)=◊)C—)C»–)C—)N‘f“)N°†–RS–T_—R`–de—Reü1—)Nê—fÿ+/◊+?—+?◊+E—+E»4–+E—+P‘h“+P°4†1–TU–Va—Tb–fg—TgüQ—+Pê	—hÙ %'ßL°L◊$@—$@¿◊AU—AU–W^”$_–! †"ß*°*”,ÿ*,–'ÿ"$†[§/ò:ÿ%6◊%=—%=∏f”%EòFÿ/◊6—6∞v÷>Ò #2 =PêI–8—9Ù ')ßl°l◊&B—&B¿4◊CW—CW–Yb”&c–# !†BßJ°J”.ÿ,.–)ÿ$&†}‘$5ò<ÿ%8◊%?—%?¿”%IòFÿ1◊8—8∏÷@Ò %6 ?TêI–:—;‡–¯E7ÿ(◊/—/∞‘2ÿ,◊3—3∞A◊6–6˚*9ÿ*◊1—1∞!‘4ÿ.◊5—5∞a◊8–8¸Û g˘€h¯:ÿ/◊6—6∞q◊9–9˚<ÿ1◊8—8∏◊;˚Ù Û 	‹èLâL–?ƒ¿A√∏x–H‘IÿçI˚	˙s™   Ç3M; ∂M; ¡A?K&√(M; √;A?L≈:6M; ∆0L<«L<«	M; «(M«;M»AM; …	"M…+AM;  9"M"À
M; À&$LÃ
M; Ã$L9Ã5M; ÕMÕM; Õ"M8Õ5M; Õ;
N2Œ"N-Œ'N2Œ-N2c                 Ûp  ï  UR                  5       nU R                  b  SUR                  ;  a  [        R	                  S5        U$ [
        R                  R                  R                  US   5      (       d  [
        R                  " US   5      US'   UR                  S5      n/ SQnU GHÌ  nSUR                  ;   a„  / nUR                  5        Hí  u  pxUS   n	US   n
U
[
        R                  " U5      -
  nU
nUUS   U:¨  US   U:  -  US   U	:H  -     n[        U5      nSUR                  ;   a  US   R                  5       OSnUR                  SU 3US	U 3U05        Mî     [
        R                   " U5      nUR                   H  nUU   R"                  UU'   M     S
UR                  ;   d  GM
  / nUR                  5        Hí  u  pxUS
   nUS   n
U
[
        R                  " U5      -
  nU
nUUS   U:¨  US   U:  -  US
   U:H  -     n[        U5      nSUR                  ;   a  US   R                  5       OSnUR                  SU 3USU 3U05        Mî     [
        R                   " U5      nUR                   H  nUU   R"                  UU'   M     GM     SUR                  ;   a†  / nUR                  5        HÖ  u  pxUS   n	US   n
UUS   U
:  US   U	:H  -     n[        U5      S:î  a9  US   R%                  5       nU
U-
  R'                  5       nUR                  U5        Mk  UR                  [)        S5      5        Má     UUS'   S
UR                  ;   a†  / nUR                  5        HÖ  u  pxUS
   nUS   n
UUS   U
:  US
   U:H  -     n[        U5      S:î  a9  US   R%                  5       nU
U-
  R'                  5       nUR                  U5        Mk  UR                  [)        S5      5        Má     UUS'   U$ ! [*         a-  n[        R-                  S[/        U5       35        Us SnA$ SnAff = f)zê
Extract temporal graph features

Args:
    df (DataFrame): Input dataframe
    
Returns:
    DataFrame: DataFrame with temporal graph features
Nr7   zMGraph not built or timestamp not available. Skipping temporal graph features.)⁄1H⁄6H⁄24H⁄7Dr4   r6   r   ⁄sender_activity_count_⁄sender_activity_amount_r5   ⁄receiver_activity_count_⁄receiver_activity_amount_r≠   ⁄"sender_time_since_last_transaction⁄$receiver_time_since_last_transactionz*Error extracting temporal graph features: )r   r   r&   r'   rt   ⁄pd⁄api⁄types⁄is_datetime64_any_dtype⁄to_datetime⁄sort_valuesrB   ⁄	Timedeltar)   ⁄sumrE   ⁄	DataFramerû   ⁄max⁄total_secondsr¥   r*   r+   r,   )r   r-   r.   ⁄	df_sorted⁄time_windows⁄window⁄sender_activityrN   rO   rP   r7   ⁄window_start⁄
window_end⁄window_transactions⁄activity_count⁄activity_amount⁄activity_dfr/   ⁄receiver_activityrQ   ⁄time_since_last⁄prev_transactions⁄last_timestamp⁄	time_diffr0   s                            r   r%   ⁄.GraphFeatures._extract_temporal_graph_featuresò  s©  Ä D	ÿüôõ	àI‡èzâz—!†[∏ø
π
”%B‹óë–n‘oÿ – Ù ó6ë6ó<ë<◊7—7∏∏;π◊H—H‹"$ß.¢.∞∞K±”"Aêê;ë üô†{”3àIÚ 5àL‰&ê‡†"ß*°*”,ÿ&(êO‡"+◊"4—"4÷"6ôòÿ!$†[—!1òÿ$'®—$4ò	 (1¥2∑<≤<¿”3G—'Gòÿ%.ò
‡.7ÿ&†{—3∞|—Cÿ&†{—3∞j—@ÒB‡&†{—3∞v—=Ò?Ò/–+Ù *-–-@”)AòÿQY–]p◊]x—]x”Qx–*=∏h—*G◊*K—*K‘*M–~ò‡'◊.—.ÿ4∞V∞H–=∏~ÿ5∞f∞X–>¿0ˆ Ò% #7Ù0 #%ß,¢,®”"?êKÿ*◊2‘2òÿ)4∞S—)9◊)@—)@ò	†#õÒ  3 !†BßJ°J÷.ÿ(*–%‡"+◊"4—"4÷"6ôòÿ#&†}—#5òÿ$'®—$4ò	 (1¥2∑<≤<¿”3G—'Gòÿ%.ò
‡.7ÿ&†{—3∞|—Cÿ&†{—3∞j—@ÒB‡&†}—5∏—AÒCÒ/–+Ù *-–-@”)AòÿQY–]p◊]x—]x”Qx–*=∏h—*G◊*K—*K‘*M–~ò‡)◊0—0ÿ6∞v∞h–?¿ÿ7∏∞x–@¿/2ˆ Ò% #7Ù0 #%ß,¢,–/@”"AêKÿ*◊2‘2òÿ)4∞S—)9◊)@—)@ò	†#õÙ  3Ò} 'D òbüjôj”(ÿ"$ê‡'◊0—0÷2ëFêAÿ †—-êFÿ #†K— 0êI )2ÿ"†;—/∞)—;ÿ"†;—/∞6—9Ò;Ò)–%Ù
 –,”-∞”1ÿ):∏;—)G◊)K—)K”)Mòÿ%.∞—%?◊$N—$N”$Pò	ÿ'◊.—.®y÷9‡'◊.—.¨u∞U´|÷<Ò 3" CRê	–>—? †ß
°
”*ÿ"$ê‡'◊0—0÷2ëFêAÿ"†=—1êHÿ #†K— 0êI )2ÿ"†;—/∞)—;ÿ"†=—1∞X—=Ò?Ò)–%Ù
 –,”-∞”1ÿ):∏;—)G◊)K—)K”)Mòÿ%.∞—%?◊$N—$N”$Pò	ÿ'◊.—.®y÷9‡'◊.—.¨u∞U´|÷<Ò 3" ETê	–@—A‡–¯‰Û 	‹èLâL–Eƒc»!√f¿X–N‘OÿçI˚	˙s,   ÇAO> ¡E)O> ∆4I	O> œ>
P5–"P0–*P5–0P5c                 ÛÚ   ï U R                  U5      nUR                   Vs/ s H  o3UR                  ;  d  M  UPM     nn[        U5      S:î  a$  U R                  R	                  X$   5        SU l        U$ s  snf )zî
Fit the feature extractor and transform the data

Args:
    df (DataFrame): Input data
    
Returns:
    DataFrame: Transformed data with features
r   T)r1   r&   r)   r   ⁄fitr   ©r   r-   r.   r/   ⁄feature_colss        r   ⁄fit_transform⁄GraphFeatures.fit_transform(  sn   Ä  ◊)—)®"”-à	 (1◊'8“'8”R“'8†¿r«z¡z—<Qü—'8à–R‰à|”òq” ‡èKâKèOâOòI—3‘4‡àDåK‡–˘Ú Ss
   †A4∑A4c                 Û   ï U R                   (       d  [        S5      eU R                  U5      nUR                   Vs/ s H  o3UR                  ;  d  M  UPM     nn[	        U5      S:î  a  U R
                  R                  X$   5      X$'   U$ s  snf )zï
Transform new data using fitted feature extractor

Args:
    df (DataFrame): Input data
    
Returns:
    DataFrame: Transformed data with features
z7Feature extractor not fitted. Call fit_transform first.r   )r   ⁄
ValueErrorr1   r&   r)   r   ⁄	transformr	  s        r   r  ⁄GraphFeatures.transform@  sÉ   Ä  è{è{‹–V”W–W ◊)—)®"”-à	 (1◊'8“'8”R“'8†¿r«z¡z—<Qü—'8à–R‰à|”òq” ‡&*ßk°k◊&;—&;∏I—<S”&TàI—#‡–˘Ú Ss   ºB¡B)r   r   r   r   r   r   r   r   )N)⁄__name__⁄
__module__⁄__qualname__⁄__firstlineno__⁄__doc__r   r1   r   r    r!   r"   r#   r$   r%   r  r  ⁄__static_attributes__© r   r   r   r      sM   Ü ÒÙ
Ú !ÚFuÚn\Ú|CÚJJÚXzÚxgÚRNÚ`ı0r   r   )r  ⁄pandasrÏ   ⁄numpyr∏   ⁄networkxr@   ⁄collectionsr   r   ⁄	communityrú   ⁄sklearn.preprocessingr   ⁄warnings⁄logging⁄typingr   r   r   r	   ⁄filterwarnings⁄basicConfig⁄INFO⁄	getLoggerr  r'   r   r  r   r   ⁄<module>r%     sf   ÒÛ
 € € ﬂ ,€ %› .€ € ﬂ +” +‡ ◊ “ ò‘ !ÿ ◊ “ ò'ü,ô,“ 'ÿ	◊	“	ò8”	$Ä˜CÚ Cr   

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\features\__pycache__\graph_features.cpython-39.pyc ===
a
    [¨hæñ  „                   @   sú   d Z ddlZddlZddlZddlmZm	Z	 ddl
ZddlmZ ddlZddlZddlmZmZmZmZ e†d° ejejdç e†e°ZG dd	Ñ d	ÉZdS )
z`
Graph Features Module
Implements graph-based feature extraction techniques for fraud detection
È    N)⁄defaultdict⁄Counter)⁄MinMaxScaler)⁄Dict⁄List⁄Tuple⁄Union⁄ignore)⁄levelc                   @   sj   e Zd ZdZdddÑZddÑ ZddÑ Zd	d
Ñ ZddÑ ZddÑ Z	ddÑ Z
ddÑ ZddÑ ZddÑ ZddÑ ZdS )⁄GraphFeatureszõ
    Class for extracting graph-based features from transaction data
    Implements techniques like centrality measures, clustering coefficients, etc.
    Nc                 C   s:   |pi | _ d| _d| _d| _d| _g | _tÉ | _d| _dS )z
        Initialize GraphFeatures
        
        Args:
            config (dict, optional): Configuration parameters
        NF)	⁄config⁄graph⁄sender_graph⁄receiver_graph⁄bipartite_graph⁄feature_namesr   ⁄scaler⁄fitted)⁄selfr   © r   ˙mC:\Users\Tranquil Abyss\fraud-detection-platform\app\..\src\fraud_detection_engine\features\graph_features.py⁄__init__   s    
zGraphFeatures.__init__c              
      s¬   zÇà † ° }| †à ° | †|°}| †|°}| †|°}| †|°}| †|°}| †|°}á fddÑ|jD É| _	t
†dt| j	Éõ dù° |W S  tyº } z"t
†dt|Éõ ù° Ç W Y d}~n
d}~0 0 dS )z‡
        Extract all graph features from the dataframe
        
        Args:
            df (DataFrame): Input transaction data
            
        Returns:
            DataFrame: DataFrame with extracted features
        c                    s   g | ]}|à j vr|ëqS r   ©⁄columns©⁄.0⁄col©⁄dfr   r   ⁄
<listcomp>D   Û    z2GraphFeatures.extract_features.<locals>.<listcomp>z
Extracted z graph featuresz!Error extracting graph features: N)⁄copy⁄_build_graphs⁄_extract_centrality_features⁄_extract_clustering_features⁄_extract_community_features⁄_extract_path_features⁄_extract_subgraph_features⁄ _extract_temporal_graph_featuresr   r   ⁄logger⁄info⁄len⁄	Exception⁄error⁄str)r   r   ⁄	result_df⁄er   r   r   ⁄extract_features*   s    







zGraphFeatures.extract_featuresc              
   C   s&  êz‚t †° | _|†° D ê] \}}d|v rd|v r|d }|d }i }d|v rX|d |d< d|v rl|d |d< d|v rÄ|d |d< | j†||°r‡| j| | }d|v rº|†dd°|d  |d< |†dd°d	 |d< |d
 †|° q|†dd°|d< d	|d< |g|d
< | jj||fi |§é qt †° | _	t
tÉ}|†° D ]4\}}d|v êr4d|v êr4||d  †|d ° êq4|†° D ]é\}}	t|	É}
tt|
ÉÉD ]n}t|d	 t|
ÉÉD ]T}|
| }|
| }| j	†||°êr‰| j	| | d  d	7  < n| j	j||d	dç êq§êqéêqrt †° | _t
tÉ}|†° D ]4\}}d|v êrd|v êr||d  †|d ° êq|†° D ]é\}}t|É}tt|ÉÉD ]n}t|d	 t|ÉÉD ]T}|| }|| }| j†||°êrÃ| j| | d  d	7  < n| jj||d	dç êqåêqvêqZt †° | _|†° D ]F\}}d|v êr"| jj|d ddç d|v êr¸| jj|d d	dç êq¸|†° D ]ä\}}d|v êrLd|v êrL|d }|d }i }d|v êrí|d |d< d|v êr®|d |d< d|v êræ|d |d< | jj||fi |§é êqLt†d° W n< têy  } z"t†dt|Éõ ù° Ç W Y d}~n
d}~0 0 dS )zä
        Build various graphs from the transaction data
        
        Args:
            df (DataFrame): Input transaction data
        ⁄	sender_id⁄receiver_id⁄amount⁄	timestamp⁄transaction_idZtotal_amountr   Ztransaction_countÈ   Ztransactions⁄common_receivers)r8   ⁄common_senders)r9   )⁄	bipartitezGraphs built successfullyzError building graphs: N)⁄nxZDiGraphr   ⁄iterrowsZhas_edge⁄get⁄appendZadd_edge⁄Graphr   r   ⁄set⁄add⁄items⁄list⁄ranger+   r   r   Zadd_noder)   r*   r,   r-   r.   )r   r   ⁄_⁄row⁄sender⁄receiver⁄attrsZ	edge_dataZreceiver_to_senders⁄sendersZsenders_list⁄i⁄jZsender1Zsender2Zsender_to_receivers⁄	receiversZreceivers_listZ	receiver1Z	receiver2r0   r   r   r   r"   M   sí    









zGraphFeatures._build_graphsc              
   C   s$  êzﬁ|† ° }| jdu r&t†d° |W S t†| j°}t†| j°}t†| j†° °}d|j	v r†|d †
|°†d°|d< |d †
|°†d°|d< |d †
|°†d°|d< d|j	v rÚ|d †
|°†d°|d	< |d †
|°†d°|d
< |d †
|°†d°|d< t| j†° Édkêrt†| j°}n*t| j†° ÉddÖ }tj| jt|Édç}d|j	v êrb|d †
|°†d°|d< d|j	v êrÜ|d †
|°†d°|d< t| j†° Édkêr®t†| j°}ntj| jddç}d|j	v êr‹|d †
|°†d°|d< d|j	v êr |d †
|°†d°|d< t| j†° Édkêr<ztj| jddç}	W n   i }	Y n0 ni }	d|j	v êrd|d †
|	°†d°|d< d|j	v êrà|d †
|	°†d°|d< t†| j°}
d|j	v êr∏|d †
|
°†d°|d< d|j	v êr‹|d †
|
°†d°|d< |W S  têy } z$t†dt|Éõ ù° |W  Y d}~S d}~0 0 dS )zŒ
        Extract centrality-based features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with centrality features
        Nz.Graph not built. Skipping centrality features.r2   r   Zsender_in_degree_centralityZsender_out_degree_centralityZsender_degree_centralityr3   Zreceiver_in_degree_centralityZreceiver_out_degree_centralityZreceiver_degree_centralityiË  )⁄kZsender_betweenness_centralityZreceiver_betweenness_centrality⁄weight)⁄distanceZsender_closeness_centralityZreceiver_closeness_centrality)⁄max_iterZsender_eigenvector_centralityZreceiver_eigenvector_centralityZsender_pagerankZreceiver_pagerankz&Error extracting centrality features: )r!   r   r)   ⁄warningr;   ⁄in_degree_centrality⁄out_degree_centrality⁄degree_centrality⁄to_undirectedr   ⁄map⁄fillnar+   ⁄nodes⁄betweenness_centralityrC   ⁄closeness_centrality⁄eigenvector_centrality⁄pagerankr,   r-   r.   )r   r   r/   rS   rT   rU   rZ   Zsample_nodesr[   r\   r]   r0   r   r   r   r#   ƒ   sd    




z*GraphFeatures._extract_centrality_featuresc           	   
   C   s  êz∆|† ° }| jdu r&t†d° |W S t†| j†° °}d|jv rX|d †|°†	d°|d< d|jv rz|d †|°†	d°|d< | j
durƒt| j
†° Édkrƒt†| j
°}d|jv rƒ|d †|°†	d°|d< | jduêrt| j†° Édkêrt†| j°}d|jv êr|d †|°†	d°|d	< t†| j†° °}d|jv êrH|d †|°†	d°|d
< d|jv êrl|d †|°†	d°|d< t†| j†° °}d|jv êr†|d †|°†	d°|d< d|jv êrƒ|d †|°†	d°|d< |W S  têy } z$t†dt|Éõ ù° |W  Y d}~S d}~0 0 dS )zŒ
        Extract clustering-based features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with clustering features
        Nz.Graph not built. Skipping clustering features.r2   r   Zsender_clustering_coefficientr3   Zreceiver_clustering_coefficientZ#sender_graph_clustering_coefficientZ%receiver_graph_clustering_coefficientZsender_avg_neighbor_degreeZreceiver_avg_neighbor_degreeZsender_square_clusteringZreceiver_square_clusteringz&Error extracting clustering features: )r!   r   r)   rR   r;   Z
clusteringrV   r   rW   rX   r   r+   rY   r   Zaverage_neighbor_degree⁄square_clusteringr,   r-   r.   )	r   r   r/   Zclustering_coeffZsender_clustering_coeffZreceiver_clustering_coeffZavg_neighbor_degreer^   r0   r   r   r   r$   "  s@    





 z*GraphFeatures._extract_clustering_featuresc              
      sb  êz|† ° }| jdu r&t†d° |W S | j†° }t†|°}d|jv r\|d †|°†	d°|d< d|jv r~|d †|°†	d°|d< t
|†° Éâáfdd	Ñ|†° D É}d|jv r¬|d †|°†	d
°|d< d|jv r‰|d †|°†	d
°|d< i }à†° D ]^\â }á fddÑ|†° D É}|†|°}	t|	†° Éd
krt†|	°}
|D ]}|
†|d
°||< êq6qd|jv êrt|d †|°†	d
°|d< d|jv êrò|d †|°†	d
°|d< d|jv êrd|jv êrg }|†° D ]T\}}|d }|d }||v êr||v êr|†t|| || kÉ° n
|†d
° êqº||d< |W S  têy\ } z$t†dt|Éõ ù° |W  Y d}~S d}~0 0 dS )zÃ
        Extract community-based features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with community features
        Nz-Graph not built. Skipping community features.r2   ÈˇˇˇˇZsender_communityr3   Zreceiver_communityc                    s   i | ]\}}|à | ìqS r   r   )r   ⁄node⁄comm)⁄community_sizesr   r   ⁄
<dictcomp>Ö  r    z=GraphFeatures._extract_community_features.<locals>.<dictcomp>r   Zsender_community_sizeZreceiver_community_sizec                    s   g | ]\}}|à kr|ëqS r   r   )r   r`   ⁄c)ra   r   r   r   ë  r    z=GraphFeatures._extract_community_features.<locals>.<listcomp>Z"sender_community_degree_centralityZ$receiver_community_degree_centrality⁄same_communityz%Error extracting community features: )r!   r   r)   rR   rV   ⁄community_louvainZbest_partitionr   rW   rX   r   ⁄valuesrB   ⁄subgraphr+   rY   r;   rU   r=   r<   r>   ⁄intr,   r-   r.   )r   r   r/   Zundirected_graphZcommunitiesZnode_community_sizesZcommunity_degree_centralityrY   Z
comm_nodesrh   Zcomm_centralityr`   re   rE   rF   rG   rH   r0   r   )ra   rb   r   r%   g  sT    










z)GraphFeatures._extract_community_featuresc              
   C   s  êz |† ° }| jdu r&t†d° |W S t| j†° Édkr¬tt†| j°É}d|j	v r¿d|j	v r¿g }|†
° D ]N\}}|d }|d }||v r®||| v r®|†|| | ° qh|†tdÉ° qh||d< nÄd|j	v êrBd|j	v êrBg }|†
° D ]R\}}|d }|d }zt†| j||°}	|†|	° W qÊ   |†tdÉ° Y qÊ0 qÊ||d< d|j	v êr¯d|j	v êr¯g }
|†
° D ]à\}}|d }|d }zVt| j†|°Ét| j†|°ÉB }t| j†|°Ét| j†|°ÉB }|
†t||@ É° W n   |
†d° Y n0 êqf|
|d	< d|j	v êr⁄d|j	v êr⁄g }|†
° D ]¥\}}|d }|d }zÇt| j†|°Ét| j†|°ÉB }t| j†|°Ét| j†|°ÉB }||B }||@ }t|Édkêr¶t|Ét|É }nd}|†|° W n   |†d° Y n0 êq||d
< d|j	v êr»d|j	v êr»g }|†
° D ]¿\}}|d }|d }zét| j†|°Ét| j†|°ÉB }t| j†|°Ét| j†|°ÉB }||@ }d}|D ].}| j†|°}|dkêrh|dt†|° 7 }êqh|†|° W n   |†d° Y n0 êq˛||d< |W S  têy
 } z$t†dt|Éõ ù° |W  Y d}~S d}~0 0 dS )z¬
        Extract path-based features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with path features
        Nz(Graph not built. Skipping path features.iÙ  r2   r3   ⁄inf⁄shortest_path_lengthr   Zcommon_neighbors_countZjaccard_coefficientr7   Zadamic_adar_indexz Error extracting path features: )r!   r   r)   rR   r+   rY   ⁄dictr;   Zall_pairs_shortest_path_lengthr   r<   r>   ⁄floatrk   r@   ⁄predecessorsZ
successors⁄degree⁄np⁄logr,   r-   r.   )r   r   r/   Zshortest_pathsZpath_lengthsrE   rF   rG   rH   Zpath_lengthZcommon_neighborsZsender_neighborsZreceiver_neighborsZjaccard_coeffs⁄union⁄intersection⁄jaccardZadamic_adar⁄commonZaa_indexr`   ro   r0   r   r   r   r&   ≥  sú    



      
z$GraphFeatures._extract_path_featuresc              
   C   s®  êzb|† ° }| jdu r&t†d° |W S d|jv rºg }g }|d D ]j}zDtj| j†° |ddç}t|†	° É}t†
|°}|†|° |†|° W q@   |†d° |†d° Y q@0 q@||d< ||d< d	|jv êrTg }	g }
|d	 D ]j}zDtj| j†° |ddç}t|†	° É}t†
|°}|	†|° |
†|° W qÿ   |	†d° |
†d° Y qÿ0 qÿ|	|d
< |
|d< | jduêr`ddÑ | jj	ddçD É}ddÑ | jj	ddçD É}tj†| j|°}d|jv êr¯g }|d D ]6}z|†|°}|†|° W n   |†d° Y n0 êq∏||d< tj†| j|°}d	|jv êr`g }|d	 D ]6}z|†|°}|†|° W n   |†d° Y n0 êq ||d< |W S  têy¢ } z$t†dt|Éõ ù° |W  Y d}~S d}~0 0 dS )z 
        Extract subgraph-based features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with subgraph features
        Nz,Graph not built. Skipping subgraph features.r2   r7   )⁄radiusr   Zsender_ego_network_sizeZsender_ego_network_densityr3   Zreceiver_ego_network_sizeZreceiver_ego_network_densityc                 S   s    h | ]\}}|d  dkr|íqS )r:   r   r   ©r   ⁄n⁄dr   r   r   ⁄	<setcomp>q  r    z;GraphFeatures._extract_subgraph_features.<locals>.<setcomp>T)⁄datac                 S   s    h | ]\}}|d  dkr|íqS )r:   r7   r   rw   r   r   r   rz   r  r    Zsender_projection_degreeZreceiver_projection_degreez$Error extracting subgraph features: )r!   r   r)   rR   r   r;   ⁄	ego_graphrV   r+   rY   ⁄densityr>   r   r:   Zprojected_graphro   r,   r-   r.   )r   r   r/   Zsender_ego_sizesZsender_ego_densitiesrG   r|   Zego_sizeZego_densityZreceiver_ego_sizesZreceiver_ego_densitiesrH   rJ   rM   Zsender_projectionZsender_proj_degreesro   Zreceiver_projectionZreceiver_proj_degreesr0   r   r   r   r'   /  s|    











z(GraphFeatures._extract_subgraph_featuresc              
   C   sê  êzJ|† ° }| jdu s d|jvr0t†d° |W S tjj†|d °sTt†	|d °|d< |†
d°}g d¢}|D ê]™}d|jv êrBg }|†° D ]í\}}|d }	|d }
|
t†|° }|
}||d |k|d |k @ |d |	k@  }t|É}d|jv r¯|d †° nd}|†d|õ ù|d	|õ ù|i° qàt†|°}|jD ]}|| j||< êq,d
|jv rjg }|†° D ]ñ\}}|d
 }|d }
|
t†|° }|
}||d |k|d |k @ |d
 |k@  }t|É}d|jv êr |d †° nd}|†d|õ ù|d|õ ù|i° êqXt†|°}|jD ]}|| j||< êq qjd|jv êr∞g }|†° D ]v\}}|d }	|d }
||d |
k |d |	k@  }t|Édkêrñ|d †° }|
| †° }|†|° n|†tdÉ° êq0||d< d
|jv êrHg }|†° D ]v\}}|d
 }|d }
||d |
k |d
 |k@  }t|Édkêr.|d †° }|
| †° }|†|° n|†tdÉ° êq»||d< |W S  têyä } z$t†dt|Éõ ù° |W  Y d}~S d}~0 0 dS )z–
        Extract temporal graph features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with temporal graph features
        Nr5   zMGraph not built or timestamp not available. Skipping temporal graph features.)⁄1HZ6HZ24HZ7Dr2   r4   r   Zsender_activity_count_Zsender_activity_amount_r3   Zreceiver_activity_count_Zreceiver_activity_amount_rj   Z"sender_time_since_last_transactionZ$receiver_time_since_last_transactionz*Error extracting temporal graph features: )r!   r   r   r)   rR   ⁄pd⁄api⁄types⁄is_datetime64_any_dtype⁄to_datetime⁄sort_valuesr<   ⁄	Timedeltar+   ⁄sumr>   ⁄	DataFramerg   ⁄max⁄total_secondsrm   r,   r-   r.   )r   r   r/   ⁄	df_sortedZtime_windows⁄windowZsender_activityrE   rF   rG   r5   Zwindow_startZ
window_endZwindow_transactionsZactivity_countZactivity_amountZactivity_dfr   Zreceiver_activityrH   Ztime_since_lastZprev_transactionsZlast_timestampZ	time_diffr0   r   r   r   r(   ò  s∏    





ˇ
˛ˇ

˛




ˇ
˛ˇ

˛




ˇˇ

ˇˇz.GraphFeatures._extract_temporal_graph_featuresc                    sD   | † à °}á fddÑ|jD É}t|Édkr@| j†|| ° d| _|S )z‘
        Fit the feature extractor and transform the data
        
        Args:
            df (DataFrame): Input data
            
        Returns:
            DataFrame: Transformed data with features
        c                    s   g | ]}|à j vr|ëqS r   r   r   r   r   r   r   6  r    z/GraphFeatures.fit_transform.<locals>.<listcomp>r   T)r1   r   r+   r   ⁄fitr   ©r   r   r/   ⁄feature_colsr   r   r   ⁄fit_transform(  s    
zGraphFeatures.fit_transformc                    sP   | j stdÉÇ| †à °}á fddÑ|jD É}t|ÉdkrL| j†|| °||< |S )z’
        Transform new data using fitted feature extractor
        
        Args:
            df (DataFrame): Input data
            
        Returns:
            DataFrame: Transformed data with features
        z7Feature extractor not fitted. Call fit_transform first.c                    s   g | ]}|à j vr|ëqS r   r   r   r   r   r   r   Q  r    z+GraphFeatures.transform.<locals>.<listcomp>r   )r   ⁄
ValueErrorr1   r   r+   r   ⁄	transformrç   r   r   r   rë   @  s    

zGraphFeatures.transform)N)⁄__name__⁄
__module__⁄__qualname__⁄__doc__r   r1   r"   r#   r$   r%   r&   r'   r(   rè   rë   r   r   r   r   r      s   
#w^EL|i r   )rï   ⁄pandasr   ⁄numpyrp   Znetworkxr;   ⁄collectionsr   r   Z	communityrf   ⁄sklearn.preprocessingr   ⁄warnings⁄logging⁄typingr   r   r   r   ⁄filterwarnings⁄basicConfig⁄INFO⁄	getLoggerrí   r)   r   r   r   r   r   ⁄<module>   s   



=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\features\__pycache__\nlp_features.cpython-39.pyc ===
a
    èöhq  „                   @   s‰  d Z ddlZddlZddlZddlZddlmZ ddl	Z	ddl
mZ ddlmZmZ ddlmZmZ ddlmZ ddlmZmZ dd	lmZ dd
lmZ ddlZddlmZmZ ddlm Z  ddl!Z!ddl"Z"ddl#m$Z$m%Z%m&Z&m'Z' ddl(m)Z) e!†*d° e"j+e"j,dç e"†-e.°Z/ze	j0†1d° W n e2êy@   e	†3d° Y n0 ze	j0†1d° W n e2êyp   e	†3d° Y n0 ze	j0†1d° W n e2êy†   e	†3d° Y n0 ze	j0†1d° W n e2êy–   e	†3d° Y n0 G ddÑ dÉZ4dS )zY
NLP Features Module
Implements natural language processing features for fraud detection
È    N)⁄Counter)⁄	stopwords)⁄word_tokenize⁄sent_tokenize)⁄WordNetLemmatizer⁄PorterStemmer)⁄SentimentIntensityAnalyzer)⁄TfidfVectorizer⁄CountVectorizer)⁄LatentDirichletAllocation)⁄TextBlob)⁄Word2Vec⁄Doc2Vec©⁄TaggedDocument)⁄Dict⁄List⁄Tuple⁄Union)⁄is_api_available⁄ignore)⁄levelztokenizers/punktZpunktzcorpora/stopwordsr   zcorpora/wordnetZwordnetzsentiment/vader_lexiconZvader_lexiconc                   @   sj   e Zd ZdZdddÑZddÑ ZddÑ Zd	d
Ñ ZddÑ ZddÑ Z	ddÑ Z
ddÑ ZddÑ ZddÑ ZddÑ ZdS )⁄NLPFeatureszâ
    Class for extracting NLP features from transaction data
    Implements techniques like sentiment analysis, topic modeling, etc.
    Nc                 C   st   |pi | _ tt†d°É| _tÉ | _tÉ | _t	É | _
d| _d| _d| _d| _d| _g | _d| _g d¢| _g d¢| _dS )z}
        Initialize NLPFeatures
        
        Args:
            config (dict, optional): Configuration parameters
        ⁄englishNF)2⁄urgent⁄immediately⁄asap⁄hurry⁄quick⁄fast⁄secret⁄confidential⁄private⁄hidden⁄discreetZ
suspiciousZunusualZstrange⁄oddZweird⁄illegal⁄fraudZscamZfakeZcounterfeit⁄money⁄cash⁄payment⁄transfer⁄wireZoverseasZforeign⁄internationalZabroadZinheritanceZlotteryZprize⁄winnerZclaim⁄verify⁄confirm⁄update⁄accountZinformation⁄click⁄linkZwebsite⁄login⁄passwordZbank⁄check⁄routingr2   ⁄number)˙\$\d+,\d+\.\d{2}˙&\d{4}[\s-]?\d{4}[\s-]?\d{4}[\s-]?\d{4}˙\b\d{3}-\d{2}-\d{4}\b˙)\b[A-Z0-9._%+-]+@[A-Z0-9.-]+\.[A-Z]{2,}\b˙Phttp[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+z\b\d{10,}\bz	[A-Z]{2,}z\d+\.\d+\.\d+\.\d+)⁄config⁄setr   ⁄words⁄
stop_wordsr   ⁄
lemmatizerr   Zstemmerr   ⁄sia⁄tfidf_vectorizer⁄count_vectorizer⁄	lda_model⁄word2vec_model⁄doc2vec_model⁄feature_names⁄fitted⁄fraud_keywords⁄suspicious_patterns)⁄selfr?   © rO   ˙kC:\Users\Tranquil Abyss\fraud-detection-platform\app\..\src\fraud_detection_engine\features\nlp_features.py⁄__init__;   s    

zNLPFeatures.__init__c              
      s∏   zxà † ° }| †|°}| †|°}| †|°}| †|°}| †|°}| †|°}á fddÑ|jD É| _t	†
dt| jÉõ dù° |W S  ty≤ } z"t	†dt|Éõ ù° Ç W Y d}~n
d}~0 0 dS )zﬁ
        Extract all NLP features from the dataframe
        
        Args:
            df (DataFrame): Input transaction data
            
        Returns:
            DataFrame: DataFrame with extracted features
        c                    s   g | ]}|à j vr|ëqS rO   ©⁄columns©⁄.0⁄col©⁄dfrO   rP   ⁄
<listcomp>Ä   Û    z0NLPFeatures.extract_features.<locals>.<listcomp>z
Extracted z NLP featureszError extracting NLP features: N)⁄copy⁄_extract_basic_text_features⁄_extract_sentiment_features⁄_extract_keyword_features⁄_extract_pattern_features⁄_extract_topic_features⁄_extract_embedding_featuresrS   rJ   ⁄logger⁄info⁄len⁄	Exception⁄error⁄str)rN   rX   ⁄	result_df⁄erO   rW   rP   ⁄extract_featuresi   s    






zNLPFeatures.extract_featuresc              
   C   s˛  êz∏|† ° }ddg}|D ê]ö}||jv r|| †d°†t°†t°||õ dù< || †d°†t°†ddÑ °||õ dù< || †d°†t°†ddÑ °||õ d	ù< || †d°†t°†d
dÑ °||õ dù< || †d°†t°†ddÑ °||õ dù< || †d°†t°†ddÑ °||õ dù< || †d°†t°†ddÑ °||õ dù< || †d°†t°†ddÑ °||õ dù< || †d°†t°†ddÑ °||õ dù< || †d°†t°†ddÑ °||õ dù< q|W S  têy¯ } z$t†	dt|Éõ ù° |W  Y d}~S d}~0 0 dS )z»
        Extract basic text features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with basic text features
        ⁄description⁄notes⁄ Z_char_countc                 S   s   | rt t| ÉÉS dS ©Nr   ©rd   r   ©⁄xrO   rO   rP   ⁄<lambda>†   rZ   z:NLPFeatures._extract_basic_text_features.<locals>.<lambda>Z_word_countc                 S   s   | rt t| ÉÉS dS rn   )rd   r   rp   rO   rO   rP   rr   •   rZ   Z_sentence_countc                 S   s$   t | Ér t†ddÑ t | ÉD É°S dS )Nc                 S   s   g | ]}t |ÉëqS rO   ©rd   ©rU   ⁄wordrO   rO   rP   rY   ™   rZ   ˙NNLPFeatures._extract_basic_text_features.<locals>.<lambda>.<locals>.<listcomp>r   )r   ⁄np⁄meanrp   rO   rO   rP   rr   ™   rZ   Z_avg_word_lengthc                 S   s$   t | Ér t†ddÑ t | ÉD É°S dS )Nc                 S   s   g | ]}t t|ÉÉëqS rO   ro   )rU   ⁄sentrO   rO   rP   rY   Ø   rZ   rv   r   )r   rw   rx   rp   rO   rO   rP   rr   Ø   rZ   Z_avg_sentence_lengthc                 S   s   t ddÑ | D ÉÉS )Nc                 s   s   | ]}|t jv rd V  qdS ©È   N©⁄string⁄punctuation©rU   ⁄charrO   rO   rP   ⁄	<genexpr>¥   rZ   ˙MNLPFeatures._extract_basic_text_features.<locals>.<lambda>.<locals>.<genexpr>©⁄sumrp   rO   rO   rP   rr   ¥   rZ   Z_punctuation_countc                 S   s   t ddÑ t| ÉD ÉÉS )Nc                 s   s&   | ]}|† ° rt|Éd krd V  qdS rz   )⁄isupperrd   rt   rO   rO   rP   rÅ   π   rZ   rÇ   )rÑ   r   rp   rO   rO   rP   rr   π   rZ   Z_uppercase_countc                 S   s   t ddÑ | D ÉÉS )Nc                 s   s   | ]}|† ° rd V  qdS rz   )⁄isdigitr   rO   rO   rP   rÅ   æ   rZ   rÇ   rÉ   rp   rO   rO   rP   rr   æ   rZ   Z_digit_countc                 S   s   | rt tt| †° ÉÉÉS dS rn   )rd   r@   r   ⁄lowerrp   rO   rO   rP   rr   √   rZ   Z_unique_word_countc                 S   s,   t | Ér(ttt | †° ÉÉÉtt | ÉÉ S dS rn   )r   rd   r@   rá   rp   rO   rO   rP   rr   »   rZ   Z_lexical_diversityz&Error extracting basic text features: N)
r[   rS   ⁄fillna⁄astyperg   ⁄applyrd   re   rb   rf   )rN   rX   rh   ⁄text_columnsrV   ri   rO   rO   rP   r\   â   sJ    


$ˇˇˇˇˇˇˇˇˇz(NLPFeatures._extract_basic_text_featuresc              
      s∫  êzt|† ° }ddg}|D ê]V}||jv r|| †d°†t°†á fddÑ°}|†ddÑ °||õ dù< |†ddÑ °||õ d	ù< |†d
dÑ °||õ dù< |†ddÑ °||õ dù< || †d°†t°†ddÑ °}|†ddÑ °||õ dù< |†ddÑ °||õ dù< ||õ dù dk †t°||õ dù< ||õ dù dk†t°||õ dù< ||õ dù dk||õ dù dk@ †t°||õ dù< q|W S  têy¥ } z$t†	dt|Éõ ù° |W  Y d}~S d}~0 0 dS )zœ
        Extract sentiment analysis features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with sentiment features
        rk   rl   rm   c                    s   | rà j †| °S dddddúS )Nr   )⁄neg⁄neu⁄pos⁄compound)rD   Zpolarity_scoresrp   ©rN   rO   rP   rr   Â   rZ   z9NLPFeatures._extract_sentiment_features.<locals>.<lambda>c                 S   s   | d S )Nrå   rO   rp   rO   rO   rP   rr   È   rZ   Z_sentiment_negc                 S   s   | d S )Nrç   rO   rp   rO   rO   rP   rr   Í   rZ   Z_sentiment_neuc                 S   s   | d S )Nré   rO   rp   rO   rO   rP   rr   Î   rZ   Z_sentiment_posc                 S   s   | d S )Nrè   rO   rp   rO   rO   rP   rr   Ï   rZ   Z_sentiment_compoundc                 S   s   | rt | ÉjS t dÉjS )Nrm   )r   ⁄	sentimentrp   rO   rO   rP   rr      rZ   c                 S   s   | j S ©N)Zpolarityrp   rO   rO   rP   rr   Û   rZ   Z_textblob_polarityc                 S   s   | j S rí   )Zsubjectivityrp   rO   rO   rP   rr   Ù   rZ   Z_textblob_subjectivitygöôôôôô©øZ_is_negativegöôôôôô©?Z_is_positiveZ_is_neutralz%Error extracting sentiment features: N)
r[   rS   rà   râ   rg   rä   ⁄intre   rb   rf   )rN   rX   rh   rã   rV   Zsentiment_scoresZtextblob_sentimentri   rO   rê   rP   r]   —   s:    



ˇˇ""ˇˇ˝z'NLPFeatures._extract_sentiment_featuresc              
      s  êzæà † ° }ddg}|D ê]†}|à jv rà | †d°†t°†àj°}|†áfddÑ°}|||õ dù< |dk†t°||õ dù< g d	¢âg d
¢âg d¢â|†áfddÑ°||õ dù< |†áfddÑ°||õ dù< |†áfddÑ°||õ dù< |à | †d°†t°†ddÑ ° ||õ dù< àjêsRt	àj
dddçà_t†á fddÑ|D É°}àj†|° àj†à | †d°†t°°}àj†° }	t|	ddÖ ÉD ]0\}
}|ddÖ|
f †° †° ||õ d|õ ù< êqàq|W S  têy˛ } z$t†dt|Éõ ù° à W  Y d}~S d}~0 0 dS )z»
        Extract keyword-based features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with keyword features
        rk   rl   rm   c                    s   t á fddÑ| D ÉÉS )Nc                 3   s   | ]}|à j v rd V  qdS rz   )rL   rt   rê   rO   rP   rÅ     rZ   ˙JNLPFeatures._extract_keyword_features.<locals>.<lambda>.<locals>.<genexpr>rÉ   rp   rê   rO   rP   rr     rZ   z7NLPFeatures._extract_keyword_features.<locals>.<lambda>Z_fraud_keyword_countr   Z_has_fraud_keywords)r   r   r   r   r   r   )r    r!   r"   r#   r$   )r(   r)   r*   r+   r,   c                    s   t á fddÑ| D ÉÉS )Nc                 3   s   | ]}|à v rd V  qdS rz   rO   rt   ©⁄urgency_keywordsrO   rP   rÅ   (  rZ   rî   rÉ   rp   rï   rO   rP   rr   (  rZ   Z_urgency_keyword_countc                    s   t á fddÑ| D ÉÉS )Nc                 3   s   | ]}|à v rd V  qdS rz   rO   rt   ©⁄secrecy_keywordsrO   rP   rÅ   ,  rZ   rî   rÉ   rp   ró   rO   rP   rr   ,  rZ   Z_secrecy_keyword_countc                    s   t á fddÑ| D ÉÉS )Nc                 3   s   | ]}|à v rd V  qdS rz   rO   rt   ©⁄money_keywordsrO   rP   rÅ   0  rZ   rî   rÉ   rp   rô   rO   rP   rr   0  rZ   Z_money_keyword_countc                 S   s   | rt t| ÉÉS dS )Nr{   ro   rp   rO   rO   rP   rr   5  rZ   Z_fraud_keyword_density©r{   È   Èd   )Z
vocabulary⁄ngram_range⁄max_featuresc                    s*   g | ]"}|à j v rà | †d °†t°ëqS )rm   )rS   rà   râ   rg   rT   rW   rO   rP   rY   B  rZ   z9NLPFeatures._extract_keyword_features.<locals>.<listcomp>NÈ
   Z_tfidf_z#Error extracting keyword features: )r[   rS   rà   râ   rg   rä   ⁄_preprocess_textrì   rK   r	   rL   rE   ⁄pd⁄concat⁄fit⁄	transform⁄get_feature_names_out⁄	enumerate⁄toarray⁄flattenre   rb   rf   )rN   rX   rh   rã   rV   ⁄processed_textZfraud_keyword_counts⁄all_textZtfidf_featuresrJ   ⁄i⁄featureri   rO   )rX   rö   rò   rN   rñ   rP   r^     sT    



ˇ
ˇ
ˇ
ˇˇ˝
,z%NLPFeatures._extract_keyword_featuresc              
      s2  êzÏ|† ° }ddg}|D ê]Œ}||jv rg }|| †d°†t°D ]:}d}| jD ] }t†||tj°}	|t	|	É7 }qN|†
|° q@|||õ dù< t†|°dk†t°||õ dù< dâdâ d	âd
âdâ|| †d°†t°†áfddÑ°||õ dù< || †d°†t°†á fddÑ°||õ dù< || †d°†t°†áfddÑ°||õ dù< || †d°†t°†áfddÑ°||õ dù< || †d°†t°†áfddÑ°||õ dù< || †d°†t°†ddÑ °||õ dù< || †d°†t°†ddÑ °||õ dù< q|W S  têy, }
 z$t†dt|
Éõ ù° |W  Y d}
~
S d}
~
0 0 dS )z»
        Extract pattern-based features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with pattern features
        rk   rl   rm   r   Z_suspicious_pattern_countZ_has_suspicious_patternsr:   r;   r<   r=   r>   c                    s   t t†à | °ÉS rí   ©rd   ⁄re⁄findallrp   )⁄money_patternrO   rP   rr   {  rZ   z7NLPFeatures._extract_pattern_features.<locals>.<lambda>Z_money_pattern_countc                    s   t t†à | °ÉS rí   rÆ   rp   )⁄credit_card_patternrO   rP   rr     rZ   Z_credit_card_pattern_countc                    s   t t†à | °ÉS rí   rÆ   rp   )⁄ssn_patternrO   rP   rr   É  rZ   Z_ssn_pattern_countc                    s   t t†à | °ÉS rí   rÆ   rp   )⁄email_patternrO   rP   rr   á  rZ   Z_email_pattern_countc                    s   t t†à | °ÉS rí   rÆ   rp   )⁄url_patternrO   rP   rr   ã  rZ   Z_url_pattern_countc                 S   s&   t ddÑ | D ÉÉt| É dkr"dS dS )Nc                 s   s   | ]}|t jv rd V  qdS rz   r|   r   rO   rO   rP   rÅ   ë  rZ   ˙JNLPFeatures._extract_pattern_features.<locals>.<lambda>.<locals>.<genexpr>g333333”?r{   r   ©rÑ   rd   rp   rO   rO   rP   rr   ë  rZ   Z_excessive_punctuationc                 S   s&   t ddÑ | D ÉÉt| É dkr"dS dS )Nc                 s   s   | ]}|† ° rd V  qdS rz   )rÖ   r   rO   rO   rP   rÅ   ò  rZ   r∂   g      ‡?r{   r   r∑   rp   rO   rO   rP   rr   ò  rZ   Z_excessive_capitalizationz#Error extracting pattern features: N)r[   rS   rà   râ   rg   rM   rØ   r∞   ⁄
IGNORECASErd   ⁄appendrw   ⁄arrayrì   rä   re   rb   rf   )rN   rX   rh   rã   rV   Zpattern_counts⁄text⁄count⁄pattern⁄matchesri   rO   )r≤   r¥   r±   r≥   rµ   rP   r_   S  s\    




ˇ
ˇ
ˇ
ˇ
ˇˇˇˇˇz%NLPFeatures._extract_pattern_featuresc              
      sÆ  êzh|† ° }ddg}g }|D ],}||jv r|†|| †d°†t°†° ° q|sT|W S á fddÑ|D É}ddÑ |D É}|s~|W S à jsætddd	d
çà _	à j	†
|°}tdddddçà _à j†|° |D ]¢}||jv r¬|| †d°†t°†à j°}|†ddÑ °}à j	†|°}à j†|°}	t|	jd ÉD ]&}
|	ddÖ|
f ||õ d|
õ dù< êq tj|	ddç}|||õ dù< q¬|W S  têy® } z$t†dt|Éõ ù° |W  Y d}~S d}~0 0 dS )z«
        Extract topic modeling features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with topic features
        rk   rl   rm   c                    s   g | ]}à † |°ëqS rO   ©r°   ©rU   ⁄docrê   rO   rP   rY   º  rZ   z7NLPFeatures._extract_topic_features.<locals>.<listcomp>c                 S   s   g | ]}|rd † |°ëqS )˙ ©⁄joinr¿   rO   rO   rP   rY   Ω  rZ   iË  r   rõ   )rü   rB   rû   È   È*   r†   ⁄online)⁄n_components⁄random_state⁄max_iter⁄learning_methodc                 S   s   | rd† | °S dS )Nr¬   rm   r√   rp   rO   rO   rP   rr   ⁄  rZ   z5NLPFeatures._extract_topic_features.<locals>.<lambda>r{   NZ_topic_Z_prob©⁄axisZ_dominant_topicz!Error extracting topic features: )r[   rS   ⁄extendrà   râ   rg   ⁄tolistrK   r
   rF   ⁄fit_transformr   rG   r§   rä   r°   r•   ⁄range⁄shaperw   ⁄argmaxre   rb   rf   )rN   rX   rh   rã   r´   rV   ⁄processed_docsZdoc_term_matrixr™   Ztopic_distributionsr¨   Zdominant_topicsri   rO   rê   rP   r`   ¢  sR    

 ˝¸
$z#NLPFeatures._extract_topic_featuresc              
      sä  êzD|† ° }ddg}g }|D ],}||jv r|†|| †d°†t°†° ° q|sT|W S á fddÑ|D É}ddÑ |D É}|s~|W S tdÉrêt†	d° td	Ér¢t†	d
° t
|ddddddçà _ddÑ t|ÉD É}t|ddddddçà _|D ê]Z}||jv r‰|| †d°†t°†à j°}g }	|D ]`}
|
êrfá fddÑ|
D É}|êrTtj|ddç}|	†|° n|	†t†d°° n|	†t†d°° êqt†|	°}	ttd|	jd ÉÉD ]$}|	ddÖ|f ||õ d|õ ù< êqòg }|D ]4}
|
êrËà j†|
°}|†|° n|†t†d°° êq∆t†|°}ttd|jd ÉÉD ]$}|ddÖ|f ||õ d|õ ù< êqq‰|W S  têyÑ } z$t†dt|Éõ ù° |W  Y d}~S d}~0 0 dS )zÀ
        Extract word embedding features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with embedding features
        rk   rl   rm   c                    s   g | ]}à † |°ëqS rO   rø   r¿   rê   rO   rP   rY   
  rZ   z;NLPFeatures._extract_embedding_features.<locals>.<listcomp>c                 S   s   g | ]}|r|ëqS rO   rO   r¿   rO   rO   rP   rY     rZ   ⁄geminizIGemini API available, but implementation pending. Using local embeddings.⁄openaizIOpenAI API available, but implementation pending. Using local embeddings.rù   r≈   r{   È   )Z	sentences⁄vector_size⁄window⁄	min_count⁄workers⁄sgc                 S   s   g | ]\}}t ||gÉëqS rO   r   )rU   r¨   r¡   rO   rO   rP   rY   '  rZ   r†   )⁄	documentsrÿ   rŸ   r⁄   r€   Zepochsc                    s$   g | ]}|à j jv rà j j| ëqS rO   )rH   Zwvrt   rê   rO   rP   rY   <  rZ   r   rÃ   NZ_word2vec_dim_Z_doc2vec_dim_z%Error extracting embedding features: )r[   rS   rŒ   rà   râ   rg   rœ   r   rb   rc   r   rH   rß   r   rI   rä   r°   rw   rx   rπ   ⁄zerosr∫   r—   ⁄minr“   Zinfer_vectorre   rf   )rN   rX   rh   rã   r´   rV   r‘   Ztagged_docsr™   Zword2vec_vectorsr¡   Zword_vectorsZ
avg_vectorr¨   Zdoc2vec_vectors⁄vectorri   rO   rê   rP   ra     s|    

 

˙
˙



"
$z'NLPFeatures._extract_embedding_featuresc              
      sÆ   zl|† ° }|†t†ddtj°°}t†dd|°}t|É}á fddÑ|D É}á fddÑ|D É}ddÑ |D É}|W S  t	y® } z$t
†dt|Éõ ù° g W  Y d}~S d}~0 0 dS )	zµ
        Preprocess text for NLP analysis
        
        Args:
            text (str): Input text
            
        Returns:
            list: List of processed tokens
        rm   z\d+c                    s   g | ]}|à j vr|ëqS rO   )rB   rt   rê   rO   rP   rY   |  rZ   z0NLPFeatures._preprocess_text.<locals>.<listcomp>c                    s   g | ]}à j †|°ëqS rO   )rC   Z	lemmatizert   rê   rO   rP   rY     rZ   c                 S   s   g | ]}t |Éd kr|ëqS )rú   rs   rt   rO   rO   rP   rY   Ç  rZ   zError preprocessing text: N)rá   ⁄	translaterg   ⁄	maketransr}   r~   rØ   ⁄subr   re   rb   rf   )rN   rª   ⁄tokensri   rO   rê   rP   r°   d  s    
zNLPFeatures._preprocess_textc                    s4   | † à °}á fddÑ|jD É}t|Édkr0d| _|S )z‘
        Fit the feature extractor and transform the data
        
        Args:
            df (DataFrame): Input data
            
        Returns:
            DataFrame: Transformed data with features
        c                    s   g | ]}|à j vr|ëqS rO   rR   rT   rW   rO   rP   rY   ò  rZ   z-NLPFeatures.fit_transform.<locals>.<listcomp>r   T)rj   rS   rd   rK   )rN   rX   rh   ⁄feature_colsrO   rW   rP   r–   ä  s
    
zNLPFeatures.fit_transformc                 C   s   | j stdÉÇ| †|°}|S )z’
        Transform new data using fitted feature extractor
        
        Args:
            df (DataFrame): Input data
            
        Returns:
            DataFrame: Transformed data with features
        z7Feature extractor not fitted. Call fit_transform first.)rK   ⁄
ValueErrorrj   )rN   rX   rh   rO   rO   rP   r•   ü  s    

zNLPFeatures.transform)N)⁄__name__⁄
__module__⁄__qualname__⁄__doc__rQ   rj   r\   r]   r^   r_   r`   ra   r°   r–   r•   rO   rO   rO   rP   r   5   s   
. H3OONt&r   )5rÍ   ⁄pandasr¢   ⁄numpyrw   rØ   r}   ⁄collectionsr   ZnltkZnltk.corpusr   Znltk.tokenizer   r   Z	nltk.stemr   r   Znltk.sentimentr   Zsklearn.feature_extraction.textr	   r
   ⁄sklearn.decompositionr   Ztextblobr   ZgensimZgensim.modelsr   r   Zgensim.models.doc2vecr   ⁄warnings⁄logging⁄typingr   r   r   r   ⁄&fraud_detection_engine.utils.api_utilsr   ⁄filterwarnings⁄basicConfig⁄INFO⁄	getLoggerrÁ   rb   ⁄data⁄find⁄LookupError⁄downloadr   rO   rO   rO   rP   ⁄<module>   sP   



=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\features\__pycache__\statistical_features.cpython-313.pyc ===
Û
    dÄöh™|  „                   Û   ï S r SSKrSSKrSSKJr  SSKJr  SSK	J
r
  SSKJr  SSKrSSKrSSKJrJrJrJr  \R(                  " S5        \R*                  " \R,                  S9  \R.                  " \5      r " S	 S
5      rg)zn
Statistical Features Module
Implements various statistical feature extraction techniques for fraud detection
È    N)⁄mahalanobis)⁄StandardScaler)⁄PCA)⁄Dict⁄List⁄Tuple⁄Union⁄ignore)⁄levelc                   Ûp   ï \ rS rSrSrSS jrS rS rS rS r	S	 r
S
 rS rS rS rS rS rS rS rSrg)⁄StatisticalFeaturesÈ   z~
Class for extracting statistical features from transaction data
Implements techniques like Benford's Law, Z-score, MAD, etc.
Nc                 Ûl   ï U=(       d    0 U l         / U l        [        5       U l        SU l        SU l        g)z]
Initialize StatisticalFeatures

Args:
    config (dict, optional): Configuration parameters
NF)⁄config⁄feature_namesr   ⁄scaler⁄pca⁄fitted)⁄selfr   s     ⁄sC:\Users\Tranquil Abyss\fraud-detection-platform\app\..\src\fraud_detection_engine\features\statistical_features.py⁄__init__⁄StatisticalFeatures.__init__   s/   Ä  ól†àåÿà‘‹$”&àåÿàåÿàçÛ    c                 Ûä  ï  UR                  5       nU R                  U5      nU R                  U5      nU R                  U5      nU R	                  U5      nU R                  U5      nU R                  U5      nU R                  U5      nU R                  U5      nU R                  U5      nUR                   Vs/ s H  o3UR                  ;  d  M  UPM     snU l        [        R                  S[        U R                  5       S35        U$ s  snf ! [         a'  n[        R!                  S[#        U5       35        e SnAff = f)z¶
Extract all statistical features from the dataframe

Args:
    df (DataFrame): Input transaction data
    
Returns:
    DataFrame: DataFrame with extracted features
z
Extracted z statistical featuresz'Error extracting statistical features: N)⁄copy⁄_extract_benford_features⁄_extract_zscore_features⁄_extract_mad_features⁄_extract_percentile_features⁄_extract_distribution_features⁄_extract_mahalanobis_features⁄_extract_grubbs_features⁄_extract_entropy_features⁄_extract_correlation_features⁄columnsr   ⁄logger⁄info⁄len⁄	Exception⁄error⁄str)r   ⁄df⁄	result_df⁄col⁄es        r   ⁄extract_features⁄$StatisticalFeatures.extract_features'   s,  Ä 	‡üôõ	àI ◊6—6∞y”AàIÿ◊5—5∞i”@àIÿ◊2—2∞9”=àIÿ◊9—9∏)”DàIÿ◊;—;∏I”FàIÿ◊:—:∏9”EàIÿ◊5—5∞i”@àIÿ◊6—6∞y”AàIÿ◊:—:∏9”EàI 2;◊1B“1B”!\“1B®#–QS◊Q[—Q[—F[ß#—1B—!\àD‘‰èKâKò*§S®◊);—);”%<–$=–=R–S‘Tÿ–˘Ú "]¯Ù
 Û 	‹èLâL–Bƒ3¿q√6¿(–K‘Lÿ˚	˙s0   ÇB7D ¬9D√D√5D ƒD ƒ
Eƒ"D=ƒ=Ec           
      ÛÏ  ï  UR                  5       nSUR                  ;   Gaí  US   R                  5       nUR                  [        5      R                  S   R                  SS5      R                  [        5      nXDS:¨     n[        U5      S:î  Ga#  UR                  SS9R                  5       n[        R                  " [        SS5       Vs/ s H  n[        R                  " SSU-  -   5      PM!     sn[        SS5      S	9nSn[        SS5       H:  n	Xy   [        U5      -  n
UR                  U	S5      nU
S:î  d  M-  XãU
-
  S
-  U
-  -  nM<     XÇS'   S[         R"                  R%                  US5      -
  US'   [        SS5       H,  n	UR                  U	S5      nXy   n[        XÕ-
  5      USU	 3'   M.     U$ s  snf ! [&         a-  n[(        R+                  S[	        U5       35        Us SnA$ SnAff = f)zé
Extract Benford's Law features

Args:
    df (DataFrame): Input dataframe
    
Returns:
    DataFrame: DataFrame with Benford's Law features
⁄amountr   ⁄n⁄0È   T©⁄	normalizeÈ
   )⁄indexÈ   ⁄benford_chi_squareÈ   ⁄benford_p_valueÈ   ⁄benford_deviation_z)Error extracting Benford's Law features: N)r   r%   ⁄abs⁄astyper+   ⁄replace⁄intr(   ⁄value_counts⁄
sort_index⁄pd⁄Series⁄range⁄np⁄log10⁄get⁄stats⁄chi2⁄cdfr)   r&   r*   )r   r,   r-   ⁄amounts⁄first_digits⁄actual_dist⁄d⁄benford_dist⁄
chi_square⁄digit⁄expected_count⁄actual_count⁄
actual_pct⁄expected_pctr/   s                  r   r   ⁄-StatisticalFeatures._extract_benford_featuresJ   s„  Ä '	ÿüôõ	àI ò2ü:ô:‘%‡òXô,◊*—*”,êÿ&ü~ô~¨c”2◊6—6∞q—9◊A—A¿#¿s”K◊R—R‘SV”Wêÿ+∏A—,=—>ê‰ê|”$†q‘(‡".◊";—";¿d–";–"K◊"V—"V”"XêKÙ $&ß9¢9Ã»q–RTÃ”-V ¿A¨bØh™h∞q∏1∏Qπ3±w÷.?…—-V‘^c–de–gi”^j—#kêL "#êJ‹!&†q®"¶òÿ)5—)<ºs¿<”?P—)Pòÿ'2ß°∞u∏a”'@òÿ)®A’-ÿ&∏.—+H»Q—*N–Q_—*_—_öJÒ	 ". 7A–2—3ÿ34¥u∑z±z∑~±~¿j–RS”7T—3TêI–/—0Ù "'†q®!¶òÿ%0ß_°_∞U∏A”%>ò
ÿ'3—':ò‹BE¿j—F_”B`ò	–$6∞u∞g–">”?Ò "-
 –˘Ú) .W¯Ù, Û 	‹èLâL–DƒS»√V¿H–M‘NÿçI˚	˙s8   ÇCF< √&F7√3AF< ƒ>A8F< ∆7F< ∆<
G3«"G.«(G3«.G3c                 Û^  ï  UR                  5       nSUR                  ;   ap  US   nUR                  5       nUR                  5       nUS:î  a;  X4-
  U-  nXbS'   [        R
                  " U5      S:Ñ  R                  [        5      US'   O
SUS'   SUS'   SUR                  ;   aﬁ  SUR                  ;   aŒ  UR                  S5      S   R                  5       nUR                  S5      S   R                  5       n/ n	UR                  5        HA  u  p´US   nUS   nX«;   a  X»;   a  Xå   S:î  a  X◊U   -
  Xå   -  nOSnU	R                  U5        MC     XíS'   [        R
                  " U	5      S:Ñ  R                  [        5      US'   S	UR                  ;   a„  SUR                  ;   a”  UR                  S	5      S   R                  5       nUR                  S	5      S   R                  5       n/ nUR                  5        HE  u  p´US	   nUS   nUU;   a  UU;   a  UU   S:î  a  XﬂU   -
  UU   -  nOSnUR                  U5        MG     UUS
'   [        R
                  " U5      S:Ñ  R                  [        5      US'   U$ ! [         a-  n[        R                  S[        U5       35        Us SnA$ SnAff = f)zà
Extract Z-score based features

Args:
    df (DataFrame): Input dataframe
    
Returns:
    DataFrame: DataFrame with Z-score features
r3   r   ⁄amount_zscoreÈ   ⁄amount_zscore_outlier⁄	sender_id⁄sender_amount_zscore⁄sender_amount_zscore_outlier⁄receiver_id⁄receiver_amount_zscore⁄receiver_amount_zscore_outlierz#Error extracting Z-score features: N)r   r%   ⁄mean⁄stdrJ   rA   rB   rD   ⁄groupby⁄iterrows⁄appendr)   r&   r*   r+   )r   r,   r-   rP   ⁄mean_amount⁄
std_amount⁄z_scores⁄
sender_avg⁄
sender_std⁄sender_zscores⁄_⁄rowr`   r3   ⁄z_score⁄receiver_avg⁄receiver_std⁄receiver_zscoresrc   r/   s                       r   r   ⁄,StatisticalFeatures._extract_zscore_features}   s¥  Ä D	ÿüôõ	àI ò2ü:ô:”%ÿòXô,ê &ülôlõnêÿ$ü[ô[õ]ê
‡†ì>ÿ '— 5∏—CêHÿ19òo—.Ù ;=ø&∫&¿”:J»Q—:N◊9V—9V‘WZ”9[êI–5“6‡12êIòo—.ÿ9:êI–5—6 òbüjôj”(®X∏øπ”-C‡üZôZ®”4∞X—>◊C—C”Eê
ÿüZôZ®”4∞X—>◊B—B”Dê
 "$êÿ ükôkûmëFêAÿ #†K— 0êIÿ †ô]êF‡ ”.∞9”3J»z—Od–gh”Ohÿ#)∞y—,A—#A¿Z—EZ—"Zô‡"#ò‡"◊)—)®'÷2Ò , 5C–0—1‹=?øV∫V¿N”=S–VW—=W◊<_—<_‘`c”<dê	–8—9‡†ß
°
”*®x∏2ø:π:”/E‡!üzôz®-”8∏—B◊G—G”Iêÿ!üzôz®-”8∏—B◊F—F”Hê $&– ÿ ükôkûmëFêAÿ"%†m—"4êKÿ †ô]êF‡"†l”2∞{¿l”7R–Wc–do—Wp–st”Wtÿ#)∏—,E—#E»–Va—Ib—"bô‡"#ò‡$◊+—+®G÷4Ò , 7Gê	–2—3‹?Aøv∫v–FV”?W–Z[—?[◊>c—>c‘dg”>hê	–:—;‡–¯‰Û 	‹èLâL–>ºs¿1ªv∏h–G‘HÿçI˚	˙s   ÇI2I5 …5
J,…?"J' !J, 'J,c                 Û™  ï  UR                  5       nSUR                  ;   au  US   nUR                  5       n[        R                  " X4-
  5      nUR                  5       nUS:î  a(  SU-  U-  nXrS'   US:Ñ  R                  [        5      US'   O
SUS'   SUS'   SUR                  ;   aÓ  SUR                  ;   aﬁ  UR                  S5      S   R                  S 5      nUR                  S5      S   R                  5       n	/ n
UR                  5        HO  u  pºUS   nUS   nXÿ;   a(  XŸ;   a#  Xç   S:î  a  [	        XÈU   -
  5      nSU-  Xç   -  nOSnU
R                  U5        MQ     X¢S	'   [        R                  " U
5      S:Ñ  R                  [        5      US
'   SUR                  ;   aÙ  SUR                  ;   a‰  UR                  S5      S   R                  S 5      nUR                  S5      S   R                  5       n/ nUR                  5        HT  u  pºUS   nUS   nUU;   a,  UU;   a&  UU   S:î  a  [	        UUU   -
  5      nSU-  UU   -  nOSnUR                  U5        MV     UUS'   [        R                  " U5      S:Ñ  R                  [        5      US'   U$ ! [         a-  n[        R                  S[        U5       35        Us SnA$ SnAff = f)zñ
Extract Median Absolute Deviation (MAD) features

Args:
    df (DataFrame): Input dataframe
    
Returns:
    DataFrame: DataFrame with MAD features
r3   r   g/›$ÅïÂ?⁄amount_mad_zscoreg      @⁄amount_mad_outlierr`   c                 Ûv   ï [         R                  " [         R                  " X R                  5       -
  5      5      $ ©N©rJ   ⁄medianrA   ©⁄xs    r   ⁄<lambda>⁄;StatisticalFeatures._extract_mad_features.<locals>.<lambda>Ò   s&   Ä Ãrœy y‘Y[◊Y_“Y_–`a◊dl—dl”dn—`n”Yo‘Opr   ⁄sender_amount_mad_zscore⁄sender_amount_mad_outlierrc   c                 Ûv   ï [         R                  " [         R                  " X R                  5       -
  5      5      $ r|   r}   r   s    r   rÅ   rÇ     s)   Ä ‘SU◊S\“S\‘]_◊]c“]c–de◊hp—hp”hr—dr”]s‘Str   ⁄receiver_amount_mad_zscore⁄receiver_amount_mad_outlierzError extracting MAD features: N)r   r%   r~   rJ   rA   rB   rD   rh   ⁄applyri   rj   ⁄arrayr)   r&   r*   r+   )r   r,   r-   rP   ⁄median_amount⁄abs_dev⁄mad⁄modified_z_scores⁄
sender_mad⁄sender_median⁄sender_mad_zscoresrq   rr   r`   r3   ⁄mad_z_score⁄receiver_mad⁄receiver_median⁄receiver_mad_zscoresrc   r/   s                        r   r   ⁄)StatisticalFeatures._extract_mad_featuresÕ   sÏ  Ä H	ÿüôõ	àI ò2ü:ô:”%ÿòXô,ê !(ß°” 0ê‹ü&ö&†—!8”9êÿónën”&ê‡òì7‡(.∞—(8∏3—(>–%ÿ5F–1—2 8I»3—7N◊6V—6V‘WZ”6[êI–2“3‡56êI–1—2ÿ67êI–2—3 òbüjôj”(®X∏øπ”-C‡üZôZ®”4∞X—>◊D—D—Ep”qê
ÿ "ß
°
®;” 7∏— A◊ H— H” Jê &(–"ÿ ükôkûmëFêAÿ #†K— 0êIÿ †ô]êF‡ ”.∞9”3M–R\—Rg–jk”Rk‹"%†f∏Y—/G—&G”"Hòÿ&,®w—&6∏—9N—&Nô‡&'ò‡&◊-—-®k÷:Ò , 9K–4—5‹:<ø(∫(–CU”:V–Y\—:\◊9d—9d‘eh”9iê	–5—6‡†ß
°
”*®x∏2ø:π:”/E‡!üzôz®-”8∏—B◊H—H—It”uêÿ"$ß*°*®]”";∏H—"E◊"L—"L”"Nê (*–$ÿ ükôkûmëFêAÿ"%†m—"4êKÿ †ô]êF‡"†l”2∞{¿o”7U–Zf–gr—Zs–vw”Zw‹"%†f®∏{—/K—&K”"Lòÿ&,®w—&6∏¿k—9R—&Rô‡&'ò‡(◊/—/∞÷<Ò , ;Oê	–6—7‹<>øH∫H–EY”<Z–]`—<`◊;h—;h‘il”;mê	–7—8‡–¯‰Û 	‹èLâL–:º3∏qª6∏(–C‘DÿçI˚	˙s   ÇJJ  
K %"KÀKÀKc                 Û  ï  UR                  5       nSUR                  ;   af  US   n/ SQn[        R                  " X45      n[	        U5       H&  u  pgX5U   :Ñ  R                  [        5      USU S3'   M(     UR                  SS9US'   SUR                  ;   aP  SUR                  ;   a@  UR                  S5      S   R                  SS9nXÇS	'   US
:Ñ  R                  [        5      US'   SUR                  ;   aP  SUR                  ;   a@  UR                  S5      S   R                  SS9n	XíS'   U	S
:Ñ  R                  [        5      US'   U$ ! [         a-  n
[        R                  S[        U
5       35        Us Sn
A
$ Sn
A
ff = f)zé
Extract percentile-based features

Args:
    df (DataFrame): Input dataframe
    
Returns:
    DataFrame: DataFrame with percentile features
r3   )È   r9   È   È2   ÈK   ÈZ   È_   Èc   ⁄amount_above_⁄th_percentileT)⁄pct⁄amount_percentile_rankr`   ⁄sender_amount_percentile_rankgffffffÓ?⁄sender_amount_top_5pctrc   ⁄receiver_amount_percentile_rank⁄receiver_amount_top_5pctz&Error extracting percentile features: N)r   r%   rJ   ⁄
percentile⁄	enumeraterB   rD   ⁄rankrh   r)   r&   r*   r+   )r   r,   r-   rP   ⁄percentiles⁄percentile_values⁄i⁄p⁄sender_percentile_ranks⁄receiver_percentile_ranksr/   s              r   r   ⁄0StatisticalFeatures._extract_percentile_features!  sç  Ä '	ÿüôõ	àI ò2ü:ô:”%ÿòXô,êÚ >ê‹$&ßM¢M∞'”$G–!Ù &†k÷2ëDêAÿCJ–_`—Ma—Ca◊Bi—Bi‘jm”BnêI†®a®S∞–>”?Ò 3 7>∑l±l¿t∞l–6Lê	–2—3 òbüjôj”(®X∏øπ”-C‡*,Ø*©*∞[”*A¿(—*K◊*P—*P–UY–*P–*Z–'ÿ=T–9—: 8O–QU—7U◊6]—6]‘^a”6bê	–2—3‡†ß
°
”*®x∏2ø:π:”/E‡,.ØJ©J∞}”,E¿h—,O◊,T—,T–Y]–,T–,^–)ÿ?X–;—< :S–UY—9Y◊8a—8a‘be”8fê	–4—5‡–¯‰Û 	‹èLâL–Aƒ#¿a√&¿–J‘KÿçI˚	˙s   ÇEE
 ≈

F≈"E<≈6F≈<Fc                 Û∆  ï  UR                  5       nSUR                  ;   a˝  US   n[        R                  " U5      US'   [        R                  " U5      US'   [        R
                  " U5      u  pEXRS'   [        U5      S::  a  [        R                  " U5      u  pFXbS'   [        R                  " USUR                  5       UR                  5       4S9u  pGXrS	'   [        R                  " U5      nUR                  US
'   [        R                  " U5      u  pöXíS'   X¢S'   SUR                  ;   at  SUR                  ;   ad  UR                  S5      S   R                  SS 4SS 4SSS 4/5      nS H.  nUS   R!                  Xº   5      R#                  S5      USU 3'   M0     SUR                  ;   at  SUR                  ;   ad  UR                  S5      S   R                  SS 4SS 4SSS 4/5      nS H.  nUS   R!                  X‹   5      R#                  S5      USU 3'   M0     U$ ! [$         a-  n[&        R)                  S[+        U5       35        Us SnA$ SnAff = f)zí
Extract distribution-based features

Args:
    df (DataFrame): Input dataframe
    
Returns:
    DataFrame: DataFrame with distribution features
r3   ⁄amount_skewness⁄amount_kurtosis⁄amount_normality_pià  ⁄amount_shapiro_p⁄norm)⁄args⁄amount_ks_p⁄amount_ad_statistic⁄amount_jb_statistic⁄amount_jb_pr`   ⁄skewnessc                 ÛP   ï [        U 5      S:º  a  [        R                  " U 5      $ S$ ©Nr^   r   ©r(   rM   ⁄skewr   s    r   rÅ   ⁄DStatisticalFeatures._extract_distribution_features.<locals>.<lambda>É  Û   Ä ºC¿ªF¿aªK¨5Ø:™:∞a´=–+N»Q–+Nr   ⁄kurtosisc                 ÛP   ï [        U 5      S:º  a  [        R                  " U 5      $ S$ ©NÈ   r   ©r(   rM   r¬   r   s    r   rÅ   r¿   Ñ  Û    Ä ƒ¿A√»!√¨5Ø>™>∏!”+<–+R–QR–+Rr   )⁄variance⁄varrI   c                 Ûf   ï [        U 5      S:î  a!  U R                  5       U R                  5       -
  $ S$ ©Nr   ©r(   ⁄max⁄minr   s    r   rÅ   r¿   Ü  Û'   Ä ºS¿ªV¿aªZ®Ø©´∞!∑%±%≥'—(9–(N»Q–(Nr   )rª   r¬   r»   rI   r   ⁄sender_amount_rc   c                 ÛP   ï [        U 5      S:º  a  [        R                  " U 5      $ S$ rΩ   ræ   r   s    r   rÅ   r¿   ê  r¡   r   c                 ÛP   ï [        U 5      S:º  a  [        R                  " U 5      $ S$ rƒ   r∆   r   s    r   rÅ   r¿   ë  r«   r   c                 Ûf   ï [        U 5      S:î  a!  U R                  5       U R                  5       -
  $ S$ rÀ   rÃ   r   s    r   rÅ   r¿   ì  rœ   r   ⁄receiver_amount_z(Error extracting distribution features: N)r   r%   rM   rø   r¬   ⁄
normaltestr(   ⁄shapiro⁄kstestrf   rg   ⁄anderson⁄	statistic⁄jarque_berarh   ⁄agg⁄map⁄fillnar)   r&   r*   r+   )r   r,   r-   rP   rq   ⁄normality_p⁄	shapiro_p⁄ks_p⁄	ad_result⁄jb_stat⁄jb_p⁄sender_stats⁄stat⁄receiver_statsr/   s                  r   r    ⁄2StatisticalFeatures._extract_distribution_featuresT  su  Ä @	ÿüôõ	àI ò2ü:ô:”%ÿòXô,êÙ 05Øz™z∏'”/Bê	–+—,‹/4Ø~™~∏g”/Fê	–+—,Ù "'◊!1“!1∞'”!:ëêÿ2=–.—/Ù êwì<†4”'‹#(ß=¢=∞”#9ëLêAÿ4=–0—1Ù  ü,ö,†w∞∏gølπlªn»gœk…kÀm–=\—]ëêÿ+/ò-—(Ù "üNöN®7”3ê	ÿ3<◊3F—3Fê	–/—0Ù !&◊ 1“ 1∞'” :ëêÿ3:–/—0ÿ+/ò-—( òbüjôj”(®X∏øπ”-C‡!üzôz®+”6∞x—@◊D—Dÿ—!N–Oÿ—!R–Sÿ'ÿ—N–O	FÛ  êÛ JêDÿ9;∏Kπ◊9L—9L»\—M_”9`◊9g—9g–hi”9jêI†®t®f–5”6Ò J †ß
°
”*®x∏2ø:π:”/E‡!#ß°®M”!:∏8—!D◊!H—!Hÿ—!N–Oÿ—!R–Sÿ'ÿ—N–O	JÛ "êÛ JêDÿ;=∏m—;L◊;P—;P–Q_—Qe”;f◊;m—;m–no”;pêI– 0∞∞–7”8Ò J –¯‰Û 	‹èLâL–CƒC»√F¿8–L‘MÿçI˚	˙s   ÇH&H) »)
I »3"I…I …I c                 Û˙  ï  UR                  5       nUR                  [        R                  /S9R                  R                  5       n[        U5      S:  a  [        R                  S5        U$ X   R                  S5      n[        R                  " USS9n[        R                  R                  U5      S:X  a5  [        R                  S5        [        R                  R                  U5      nO[        R                  R                  U5      n[        R                  " USS9n/ n[!        [        U5      5       H+  n	UR#                  [%        UR&                  U	   Xv5      5        M-     XÇS	'   [(        R*                  R-                  S
[        U5      S9n
[        R.                  " U5      U
:Ñ  R1                  [2        5      US'   U$ ! [4         a-  n[        R7                  S[9        U5       35        Us SnA$ SnAff = f)zì
Extract Mahalanobis distance features

Args:
    df (DataFrame): Input dataframe
    
Returns:
    DataFrame: DataFrame with Mahalanobis features
©⁄includer;   z?Not enough numeric columns for Mahalanobis distance calculationr   F)⁄rowvarz3Covariance matrix is singular, using pseudo-inverse©⁄axis⁄mahalanobis_distanceg333333Ô?)r,   ⁄mahalanobis_outlierz'Error extracting Mahalanobis features: N)r   ⁄select_dtypesrJ   ⁄numberr%   ⁄tolistr(   r&   ⁄warningr›   ⁄cov⁄linalg⁄det⁄pinv⁄invrf   rI   rj   r   ⁄ilocrM   rN   ⁄ppfrâ   rB   rD   r)   r*   r+   )r   r,   r-   ⁄numeric_cols⁄X⁄
cov_matrix⁄inv_cov_matrix⁄mean_vector⁄mahalanobis_distancesr´   ⁄	thresholdr/   s               r   r!   ⁄1StatisticalFeatures._extract_mahalanobis_features†  sö  Ä ,	ÿüôõ	àI ◊+—+¥R∑Y±Y∞K–+–@◊H—H◊O—O”QàL‰ê<” †1”$‹óë–`‘aÿ –  — ◊'—'®”*àAÙ üö†®%—0àJÙ èyâyè}â}òZ”(®A”-‹óë–T‘U‹!#ß°ß°∞
”!;ë‰!#ß°ß°®z”!:êÙ ü'ö'†!®!—,àK %'–!‹ú3òqõ6ñ]êÿ%◊,—,‹†ß°†q°	®;”GˆÒ #
 1F–,—-Ù ü
ô
üô†u¥∞\”1Bò–CàI‹02∑≤–9N”0O–R[—0[◊/c—/c‘dg”/hàI–+—,‡–¯‰Û 	‹èLâL–Bƒ3¿q√6¿(–K‘LÿçI˚	˙s%   ÇA+G ¡.EG «
G:«"G5«/G:«5G:c                 Û∏  ï  UR                  5       nSUR                  ;   a˛  US   R                  n[        R                  " U5      n[        R
                  " U5      nUS:î  a≥  [        R                  " X4-
  5      n[        R                  " U5      nXu-  nXÇS'   [        U5      n	[        R                  R                  SSSU	-  -  -
  U	S-
  5      n
U	S-
  U
-  [        R                  " XôS-
  U
S-  -   -  5      -  nXã:Ñ  R                  [        5      US'   U$ SUS'   SUS'   U$ ! [         a-  n[         R#                  S[%        U5       35        Us S	nA$ S	nAff = f)
zô
Extract Grubbs' test features for outliers

Args:
    df (DataFrame): Input dataframe
    
Returns:
    DataFrame: DataFrame with Grubbs' test features
r3   r   ⁄grubbs_statisticr6   göôôôôôô?r;   ⁄grubbs_outlierz(Error extracting Grubbs' test features: N)r   r%   ⁄valuesrJ   rf   rg   rA   rÕ   r(   rM   ⁄tr˙   ⁄sqrtrB   rD   r)   r&   r*   r+   )r   r,   r-   rP   rk   rl   ⁄abs_deviations⁄max_deviation⁄grubbs_statr4   ⁄
t_critical⁄critical_valuer/   s                r   r"   ⁄,StatisticalFeatures._extract_grubbs_featuresÿ  sS  Ä #	ÿüôõ	àI ò2ü:ô:”%ÿòXô,◊-—-êÙ !ügög†g”.ê‹üVöV†Gõ_ê
‡†ì>‰%'ßV¢V®G—,A”%BêN‹$&ßF¢F®>”$:êM #0—"<êKÿ4?–0—1Ù òGõêA‹!&ß°ß°®Q∞∏!∏aπ%±—-@¿!¿a¡%”!HêJÿ&'®!°e®z—%9ºBøG∫G¿A»Q…–Q[–]^—Q^—I^—D_”<`—%`êN 4?—3O◊2W—2W‘X[”2\êI–.—/
 – 56êI–0—1ÿ23êI–.—/‡–¯‰Û 	‹èLâL–CƒC»√F¿8–L‘MÿçI˚	˙s$   ÇDD" ƒD" ƒ"
Eƒ,"E≈E≈Ec                 Û¨  ^ ï  UR                  5       nUR                  SS/S9R                  R                  5       nU Ha  nX   R	                  SS9n[        S U 5       5      * nXbU S3'   [        R                  " [        U5      5      nUS:î  a  Xg-  nOSnXÇU S	3'   Mc     UR                  [        R                  /S9R                  R                  5       n	U	 Hx  n [        R                  " X   S
SS9n
U
R	                  SS9n[        S U 5       5      * nXbU S3'   [        R                  " [        U5      5      nUS:î  a  Xg-  nOSnXÇU S3'   Mz     SUR                  ;   aM  UR                  S5      S   R                  U 4S j5      nUS   R                  U5      R                  S5      US'   SUR                  ;   aM  UR                  S5      S   R                  U 4S j5      nUS   R                  U5      R                  S5      US'   U$ !    GM>  = f! [          a-  n["        R%                  S['        U5       35        Us SnA$ SnAff = f)zà
Extract entropy-based features

Args:
    df (DataFrame): Input dataframe
    
Returns:
    DataFrame: DataFrame with entropy features
⁄object⁄categoryrÈ   Tr7   c              3   Ûb   #   ï U  H%  oS :î  d  M
  U[         R                  " U5      -  v ï  M'     g7f©r   N©rJ   ⁄log2©⁄.0r¨   s     r   ⁄	<genexpr>⁄@StatisticalFeatures._extract_entropy_features.<locals>.<genexpr>  s"   È Ä –M≤|∞!»1¡uõ~òq§2ß7¢7®1£:û~≤|˘Û   Ç	/è /⁄_entropyr   ⁄_normalized_entropyr9   ⁄drop©⁄bins⁄
duplicatesc              3   Ûb   #   ï U  H%  oS :î  d  M
  U[         R                  " U5      -  v ï  M'     g7fr  r  r  s     r   r  r  3  s"   È Ä –"Q∫<∞a»q…5£>†1§rßw¢w®q£z¶>∫<˘r  ⁄_binned_entropy⁄_binned_normalized_entropyr`   r3   c                 Û&   >ï TR                  U 5      $ r|   ©⁄_calculate_series_entropy©rÄ   r   s    Är   rÅ   ⁄?StatisticalFeatures._extract_entropy_features.<locals>.<lambda>D  Û   ¯Ä òd◊<—<∏Q‘?r   ⁄sender_amount_entropyrc   c                 Û&   >ï TR                  U 5      $ r|   r%  r'  s    Är   rÅ   r(  K  r)  r   ⁄receiver_amount_entropyz#Error extracting entropy features: N)r   r   r%   rÚ   rE   ⁄sumrJ   r  r(   rÒ   rG   ⁄cutrh   rà   r‹   r›   r)   r&   r*   r+   )r   r,   r-   ⁄categorical_colsr.   rE   ⁄entropy⁄max_entropy⁄normalized_entropyr˚   ⁄binned⁄sender_entropy⁄receiver_entropyr/   s   `             r   r#   ⁄-StatisticalFeatures._extract_entropy_features  sj  ¯Ä B	ÿüôõ	àI  "◊/—/∏¿:–8N–/–O◊W—W◊^—^”`–„'ê‡!ôw◊3—3∏d–3–CêÙ —M±|”M”M–Mêÿ.5òSòE†–*—+Ù !ügög§c®,”&7”8êÿ†ì?ÿ)0—)>—&‡)*–&ÿ9KòSòE–!4–5”6Ò (" ◊+—+¥R∑Y±Y∞K–+–@◊H—H◊O—O”QàL„#ê‹üVöV†B°G∞"¿—HêF $*◊#6—#6¿–#6–#FêLÙ  #—"Qπ<”"Q”Q–QêGÿ9@†††_–5—6Ù #%ß'¢'¨#®l”*;”"<êKÿ"†Qìÿ-4—-B—*‡-.–*ÿDV††–%?–@”AÒ% $. òbüjôj”(‡!#ß°®K”!8∏—!B◊!H—!H‹?Û"ê 68∏±_◊5H—5H»”5X◊5_—5_–`a”5bê	–1—2‡†ß
°
”*‡#%ß:°:®m”#<∏X—#F◊#L—#L‹?Û$–  8:∏-—7H◊7L—7L–M]”7^◊7e—7e–fg”7hê	–3—4‡–¯%€˚Ù& Û 	‹èLâL–>ºs¿1ªv∏h–G‘HÿçI˚	˙s8   ÉCH √A4H≈B?H »H»H »
I»&"I…I…Ic           	      Û¨   ï  [         R                  " U[        S[        U5      5      SS9nUR	                  SS9n[        S U 5       5      * nU$ !    g= f)zv
Calculate entropy of a pandas Series

Args:
    series (Series): Input series
    
Returns:
    float: Entropy value
r9   r  r  Tr7   c              3   Ûb   #   ï U  H%  oS :î  d  M
  U[         R                  " U5      -  v ï  M'     g7fr  r  r  s     r   r  ⁄@StatisticalFeatures._calculate_series_entropy.<locals>.<genexpr>g  s"   È Ä –I≤<®a¿q¡5õ>ò1úrüwöw†qõzû>≤<˘r  r   )rG   r.  rŒ   r(   rE   r-  )r   ⁄seriesr3  rE   r0  s        r   r&  ⁄-StatisticalFeatures._calculate_series_entropyU  s]   Ä 	‰óVíVòF¨®R¥∞V≥”)=»&—QàF "◊.—.∏–.–>àLÙ —I±<”I”I–IàGÿàN¯	Ÿ˙s   ÇAA ¡Ac                 Û–  ï  UR                  5       nUR                  [        R                  /S9R                  R                  5       n[        U5      S:  a  [        R                  S5        U$ X   R                  5       R                  5       n0 nU H(  nXF   R                  U5      nUR                  5       XV'   M*     U H  nXV   X& S3'   M     0 nU H(  nXF   R                  U5      nUR                  5       XÜ'   M*     U H  nXÜ   X& S3'   M     [        U5       H|  u  pñX   R                  5       n
X¶   nU
R                  USS9n
 SS	KJn  U" 5       nUR#                  X´5        UR%                  X´5      nUS:  a	  SSU-
  -  nO['        S
5      nXÚU S3'   M~     U$ !   SX& S3'    Mé  = f! [(         a-  n[        R+                  S[-        U5       35        Us SnA$ SnAff = f)zê
Extract correlation-based features

Args:
    df (DataFrame): Input dataframe
    
Returns:
    DataFrame: DataFrame with correlation features
rÈ   r;   z6Not enough numeric columns for correlation calculation⁄_max_correlation⁄_avg_correlationr6   rÏ   r   )⁄LinearRegression⁄inf⁄_vifz'Error extracting correlation features: N)r   r   rJ   rÒ   r%   rÚ   r(   r&   rÛ   ⁄corrrA   r  rÕ   rf   rß   ⁄sklearn.linear_modelr?  ⁄fit⁄score⁄floatr)   r*   r+   )r   r,   r-   r˚   ⁄corr_matrix⁄max_corrr.   ⁄corrs⁄avg_corrr´   r¸   ⁄yr?  ⁄model⁄	r_squared⁄vifr/   s                    r   r$   ⁄1StatisticalFeatures._extract_correlation_featuresl  s¯  Ä A	ÿüôõ	àI ◊+—+¥R∑Y±Y∞K–+–@◊H—H◊O—O”QàL‰ê<” †1”$‹óë–W‘Xÿ –  —*◊/—/”1◊5—5”7àK àH€#ê‡#—(◊-—-®c”2êÿ %ß	°	£êìÒ $Û $êÿ6>±mê	òE–!1–2”3Ò $ àH€#ê‡#—(◊-—-®c”2êÿ %ß
°
£êìÒ $Û $êÿ6>±mê	òE–!1–2”3Ò $Ù $†L÷1ëê‡—$◊)—)”+êÿëFêÿóFëFò3†QêF–'ê0›EŸ,”.êEÿóIëIòaîO !&ß°®A” 1êI !†1ì}ÿ†1†y°=—1ô‰#†Eõlò‡.1†††Tòl”+Ò+ 22 –¯0ÿ./êI††Tòl‘+˚Ù Û 	‹èLâL–Bƒ3¿q√6¿(–K‘LÿçI˚	˙s>   ÇA+F. ¡.CF. ≈	AF∆F. ∆	F+∆(F. ∆.
G%∆8"G «G%« G%c                 Û(  ï U R                  U5      nUR                   Vs/ s H  o3UR                  ;  d  M  UPM     nn[        U5      S:î  aø  U R                  R	                  X$   5        [        U5      S:º  aå  [        [        S[        U5      5      S9U l        U R                  R	                  X$   5        U R                  R                  X$   5      n[        UR                  S   5       H  nUSS2U4   USUS-    3'   M     SU l        U$ s  snf )zî
Fit the feature extractor and transform the data

Args:
    df (DataFrame): Input data
    
Returns:
    DataFrame: Transformed data with features
r   r9   )⁄n_componentsr6   N⁄	stat_pca_T)r0   r%   r(   r   rD  r   rŒ   r   ⁄	transformrI   ⁄shaper   ©r   r,   r-   r.   ⁄feature_cols⁄pca_componentsr´   s          r   ⁄fit_transform⁄!StatisticalFeatures.fit_transformπ  s˚   Ä  ◊)—)®"”-à	 (1◊'8“'8”R“'8†¿r«z¡z—<Qü—'8à–R‰à|”òq” ‡èKâKèOâOòI—3‘4Ù ê<” †B”&‹¨C∞¥C∏”4E”,F—GêîÿóëóëòY—4‘5 "&ß°◊!3—!3∞I—4K”!Lê‹ò~◊3—3∞A—6÷7êAÿ3A¬!¿Q¿$—3GêI†	®!®A©#®–/”0Ò 8 àDåK‡–˘Ú% Ss
   †D∑Dc                 Û÷  ï U R                   (       d  [        S5      eU R                  U5      nUR                   Vs/ s H  o3UR                  ;  d  M  UPM     nn[	        U5      S:î  az  U R
                  R                  X$   5      X$'   U R                  bN  U R                  R                  X$   5      n[        UR                  S   5       H  nUSS2U4   USUS-    3'   M     U$ s  snf )zï
Transform new data using fitted feature extractor

Args:
    df (DataFrame): Input data
    
Returns:
    DataFrame: Transformed data with features
z7Feature extractor not fitted. Call fit_transform first.r   Nr6   rR  )
r   ⁄
ValueErrorr0   r%   r(   r   rS  r   rI   rT  rU  s          r   rS  ⁄StatisticalFeatures.transform€  s‡   Ä  è{è{‹–V”W–W ◊)—)®"”-à	 (1◊'8“'8”R“'8†¿r«z¡z—<Qü—'8à–R‰à|”òq” ‡&*ßk°k◊&;—&;∏I—<S”&TàI—# èxâx—#ÿ!%ß°◊!3—!3∞I—4K”!Lê‹ò~◊3—3∞A—6÷7êAÿ3A¬!¿Q¿$—3GêI†	®!®A©#®–/”0Ò 8 –˘Ú Ss   ºC&¡C&)r   r   r   r   r   r|   )⁄__name__⁄
__module__⁄__qualname__⁄__firstlineno__⁄__doc__r   r0   r   r   r   r   r    r!   r"   r#   r&  r$   rX  rS  ⁄__static_attributes__© r   r   r   r      s]   Ü ÒÙ
Ú!ÚF1ÚfNÚ`RÚh1ÚfJÚX6Úp-Ú^LÚ\Ú.KÚZ ıDr   r   )ra  ⁄pandasrG   ⁄numpyrJ   ⁄scipy.statsrM   ⁄scipy.spatial.distancer   ⁄sklearn.preprocessingr   ⁄sklearn.decompositionr   ⁄warnings⁄logging⁄typingr   r   r   r	   ⁄filterwarnings⁄basicConfig⁄INFO⁄	getLoggerr]  r&   r   rc  r   r   ⁄<module>rq     sf   ÒÛ
 € › › .› 0› %€ € ﬂ +” +‡ ◊ “ ò‘ !ÿ ◊ “ ò'ü,ô,“ 'ÿ	◊	“	ò8”	$Ä˜dÚ dr   

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\features\__pycache__\statistical_features.cpython-39.pyc ===
a
    èöh™|  „                   @   s†   d Z ddlZddlZddlmZ ddlmZ ddl	m
Z
 ddlmZ ddlZddlZddlmZmZmZmZ e†d° ejejdç e†e°ZG d	d
Ñ d
ÉZdS )zn
Statistical Features Module
Implements various statistical feature extraction techniques for fraud detection
È    N)⁄mahalanobis)⁄StandardScaler)⁄PCA)⁄Dict⁄List⁄Tuple⁄Union⁄ignore)⁄levelc                   @   sÇ   e Zd ZdZdddÑZddÑ ZddÑ Zd	d
Ñ ZddÑ ZddÑ Z	ddÑ Z
ddÑ ZddÑ ZddÑ ZddÑ ZddÑ ZddÑ ZddÑ ZdS ) ⁄StatisticalFeatureszä
    Class for extracting statistical features from transaction data
    Implements techniques like Benford's Law, Z-score, MAD, etc.
    Nc                 C   s(   |pi | _ g | _tÉ | _d| _d| _dS )zÖ
        Initialize StatisticalFeatures
        
        Args:
            config (dict, optional): Configuration parameters
        NF)⁄config⁄feature_namesr   ⁄scaler⁄pca⁄fitted)⁄selfr   © r   ˙sC:\Users\Tranquil Abyss\fraud-detection-platform\app\..\src\fraud_detection_engine\features\statistical_features.py⁄__init__   s
    
zStatisticalFeatures.__init__c              
      s÷   zñà † ° }| †|°}| †|°}| †|°}| †|°}| †|°}| †|°}| †|°}| †|°}| †	|°}á fddÑ|j
D É| _t†dt| jÉõ dù° |W S  ty– } z"t†dt|Éõ ù° Ç W Y d}~n
d}~0 0 dS )zÊ
        Extract all statistical features from the dataframe
        
        Args:
            df (DataFrame): Input transaction data
            
        Returns:
            DataFrame: DataFrame with extracted features
        c                    s   g | ]}|à j vr|ëqS r   ©⁄columns©⁄.0⁄col©⁄dfr   r   ⁄
<listcomp>A   Û    z8StatisticalFeatures.extract_features.<locals>.<listcomp>z
Extracted z statistical featuresz'Error extracting statistical features: N)⁄copy⁄_extract_benford_features⁄_extract_zscore_features⁄_extract_mad_features⁄_extract_percentile_features⁄_extract_distribution_features⁄_extract_mahalanobis_features⁄_extract_grubbs_features⁄_extract_entropy_features⁄_extract_correlation_featuresr   r   ⁄logger⁄info⁄len⁄	Exception⁄error⁄str)r   r   ⁄	result_df⁄er   r   r   ⁄extract_features'   s"    









z$StatisticalFeatures.extract_featuresc              
   C   s~  êz8|† ° }d|jv êr6|d †° }|†t°jd †dd°†t°}||dk }t|Édkêr6|jddç†	° }t
jdd	Ñ tdd
ÉD Étdd
Édç}d}tdd
ÉD ]<}|| t|É }	|†|d°}
|	dkrû||
|	 d |	 7 }qû||d< dtj†|d° |d< tddÉD ]0}|†|d°}|| }t|| É|d|õ ù< êq|W S  têyx } z$t†dt|Éõ ù° |W  Y d}~S d}~0 0 dS )zŒ
        Extract Benford's Law features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with Benford's Law features
        ⁄amountr   ⁄n⁄0È   T©⁄	normalizec                 S   s   g | ]}t †d d |  °ëqS )r4   )⁄np⁄log10)r   ⁄dr   r   r   r   c   r   zAStatisticalFeatures._extract_benford_features.<locals>.<listcomp>È
   )⁄indexÈ   Zbenford_chi_squareÈ   Zbenford_p_valueÈ   Zbenford_deviation_z)Error extracting Benford's Law features: N)r   r   ⁄abs⁄astyper-   ⁄replace⁄intr*   ⁄value_counts⁄
sort_index⁄pd⁄Series⁄range⁄get⁄stats⁄chi2⁄cdfr+   r(   r,   )r   r   r.   ⁄amountsZfirst_digitsZactual_distZbenford_distZ
chi_square⁄digitZexpected_count⁄actual_countZ
actual_pctZexpected_pctr/   r   r   r   r   J   s2    
$z-StatisticalFeatures._extract_benford_featuresc              
   C   sN  êz|† ° }d|jv rt|d }|†° }|†° }|dkrd|| | }||d< t†|°dk†t°|d< nd|d< d|d< d|jv êr<d|jv êr<|†d°d †° }|†d°d †° }g }	|†	° D ]^\}
}|d }|d }||v êr||v êr|| dkêr|||  ||  }nd}|	†
|° qº|	|d< t†|	°dk†t°|d< d	|jv êrd|jv êr|†d	°d †° }|†d	°d †° }g }|†	° D ]`\}
}|d	 }|d }||v êr‘||v êr‘|| dkêr‘|||  ||  }nd}|†
|° êqÑ||d
< t†|°dk†t°|d< |W S  têyH } z$t†dt|Éõ ù° |W  Y d}~S d}~0 0 dS )z»
        Extract Z-score based features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with Z-score features
        r1   r   Zamount_zscoreÈ   Zamount_zscore_outlier⁄	sender_idZsender_amount_zscoreZsender_amount_zscore_outlier⁄receiver_idZreceiver_amount_zscoreZreceiver_amount_zscore_outlierz#Error extracting Z-score features: N)r   r   ⁄mean⁄stdr7   r?   r@   rB   ⁄groupby⁄iterrows⁄appendr+   r(   r,   r-   )r   r   r.   rL   ⁄mean_amount⁄
std_amountZz_scoresZ
sender_avgZ
sender_stdZsender_zscores⁄_⁄rowrP   r1   Zz_scoreZreceiver_avgZreceiver_stdZreceiver_zscoresrQ   r/   r   r   r   r    }   sT    

""z,StatisticalFeatures._extract_zscore_featuresc              
   C   sz  êz4|† ° }d|jv r||d }|†° }t†|| °}|†° }|dkrld| | }||d< |dk†t°|d< nd|d< d|d< d|jv êrVd|jv êrV|†d°d †dd	Ñ °}|†d°d †° }	g }
|†	° D ]j\}}|d }|d }||v êr&||	v êr&|| dkêr&t||	|  É}d| ||  }nd}|
†
|° q |
|d
< t†|
°dk†t°|d< d|jv êr2d|jv êr2|†d°d †dd	Ñ °}|†d°d †° }g }|†	° D ]l\}}|d }|d }||v êr ||v êr || dkêr t|||  É}d| ||  }nd}|†
|° êq§||d< t†|°dk†t°|d< |W S  têyt } z$t†dt|Éõ ù° |W  Y d}~S d}~0 0 dS )z÷
        Extract Median Absolute Deviation (MAD) features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with MAD features
        r1   r   g/›$ÅïÂ?Zamount_mad_zscoreg      @Zamount_mad_outlierrP   c                 S   s   t †t †| | †°  °°S ©N©r7   ⁄medianr?   ©⁄xr   r   r   ⁄<lambda>Ò   r   z;StatisticalFeatures._extract_mad_features.<locals>.<lambda>Zsender_amount_mad_zscoreZsender_amount_mad_outlierrQ   c                 S   s   t †t †| | †°  °°S r[   r\   r^   r   r   r   r`     r   Zreceiver_amount_mad_zscoreZreceiver_amount_mad_outlierzError extracting MAD features: N)r   r   r]   r7   r?   r@   rB   rT   ⁄applyrU   rV   ⁄arrayr+   r(   r,   r-   )r   r   r.   rL   Zmedian_amountZabs_devZmadZmodified_z_scoresZ
sender_madZsender_medianZsender_mad_zscoresrY   rZ   rP   r1   Zmad_z_scoreZreceiver_madZreceiver_medianZreceiver_mad_zscoresrQ   r/   r   r   r   r!   Õ   sZ    

""z)StatisticalFeatures._extract_mad_featuresc              
   C   sB  ê z¸|† ° }d|jv rr|d }g d¢}t†||°}t|ÉD ]&\}}||| k†t°|d|õ dù< q:|jddç|d< d|jv r∂d|jv r∂|†d°d jddç}||d	< |d
k†t°|d< d|jv r˙d|jv r˙|†d°d jddç}	|	|d< |	d
k†t°|d< |W S  t	êy< }
 z$t
†dt|
Éõ ù° |W  Y d}
~
S d}
~
0 0 dS )zŒ
        Extract percentile-based features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with percentile features
        r1   )È   r:   È   È2   ÈK   ÈZ   È_   Èc   Zamount_above_Zth_percentileT)⁄pctZamount_percentile_rankrP   Zsender_amount_percentile_rankgffffffÓ?Zsender_amount_top_5pctrQ   Zreceiver_amount_percentile_rankZreceiver_amount_top_5pctz&Error extracting percentile features: N)r   r   r7   ⁄
percentile⁄	enumerater@   rB   ⁄rankrT   r+   r(   r,   r-   )r   r   r.   rL   ⁄percentilesZpercentile_values⁄i⁄pZsender_percentile_ranksZreceiver_percentile_ranksr/   r   r   r   r"   !  s*    

 z0StatisticalFeatures._extract_percentile_featuresc              
   C   s  êz¬|† ° }d|jv rÃ|d }t†|°|d< t†|°|d< t†|°\}}||d< t|Édkrrt†|°\}}||d< tj|d|†	° |†
° fdç\}}||d	< t†|°}|j|d
< t†|°\}	}
|	|d< |
|d< d|jv êrFd|jv êrF|†d°d †dddÑ fdddÑ fddddÑ fg°}dD ](}|d †|| °†d°|d|õ ù< êqd|jv êr¿d|jv êr¿|†d°d †dddÑ fdddÑ fddddÑ fg°}dD ](}|d †|| °†d°|d|õ ù< êqñ|W S  têy } z$t†dt|Éõ ù° |W  Y d}~S d}~0 0 dS ) z“
        Extract distribution-based features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with distribution features
        r1   Zamount_skewnessZamount_kurtosisZamount_normality_pià  Zamount_shapiro_p⁄norm)⁄argsZamount_ks_pZamount_ad_statisticZamount_jb_statisticZamount_jb_prP   ⁄skewnessc                 S   s   t | Édkrt†| °S dS ©NrO   r   ©r*   rI   ⁄skewr^   r   r   r   r`   É  r   zDStatisticalFeatures._extract_distribution_features.<locals>.<lambda>⁄kurtosisc                 S   s   t | Édkrt†| °S dS ©NÈ   r   ©r*   rI   rw   r^   r   r   r   r`   Ñ  r   )⁄variance⁄varrG   c                 S   s    t | Édkr| †° | †°  S dS ©Nr   ©r*   ⁄max⁄minr^   r   r   r   r`   Ü  r   )rs   rw   r{   rG   r   Zsender_amount_rQ   c                 S   s   t | Édkrt†| °S dS rt   ru   r^   r   r   r   r`   ê  r   c                 S   s   t | Édkrt†| °S dS rx   rz   r^   r   r   r   r`   ë  r   c                 S   s    t | Édkr| †° | †°  S dS r}   r~   r^   r   r   r   r`   ì  r   Zreceiver_amount_z(Error extracting distribution features: N)r   r   rI   rv   rw   Z
normaltestr*   ZshapiroZkstestrR   rS   ZandersonZ	statisticZjarque_berarT   ⁄agg⁄map⁄fillnar+   r(   r,   r-   )r   r   r.   rL   rY   Znormality_pZ	shapiro_pZks_pZ	ad_resultZjb_statZjb_pZsender_stats⁄statZreceiver_statsr/   r   r   r   r#   T  sP    

 




¸&


¸&z2StatisticalFeatures._extract_distribution_featuresc              
   C   sF  êz |† ° }|jtjgdçj†° }t|Édk r>t†d° |W S || †	d°}tj
|ddç}tj†|°dkrÇt†d° tj†|°}ntj†|°}tj|ddç}g }tt|ÉÉD ]}	|†t|j|	 ||É° q¨||d	< tjjd
t|Édç}
t†|°|
k†t°|d< |W S  têy@ } z$t†dt|Éõ ù° |W  Y d}~S d}~0 0 dS )z”
        Extract Mahalanobis distance features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with Mahalanobis features
        ©⁄includer<   z?Not enough numeric columns for Mahalanobis distance calculationr   F)⁄rowvarz3Covariance matrix is singular, using pseudo-inverse©⁄axisZmahalanobis_distanceg333333Ô?r   Zmahalanobis_outlierz'Error extracting Mahalanobis features: N)r   ⁄select_dtypesr7   ⁄numberr   ⁄tolistr*   r(   ⁄warningrÉ   ⁄cov⁄linalg⁄det⁄pinv⁄invrR   rG   rV   r   ⁄ilocrI   rJ   ⁄ppfrb   r@   rB   r+   r,   r-   )r   r   r.   ⁄numeric_cols⁄XZ
cov_matrixZinv_cov_matrixZmean_vectorZmahalanobis_distancesro   ⁄	thresholdr/   r   r   r   r$   †  s2    


ˇz1StatisticalFeatures._extract_mahalanobis_featuresc              
   C   s  z÷|† ° }d|jv r“|d j}t†|°}t†|°}|dkr¬t†|| °}t†|°}|| }||d< t|É}	t	j
†ddd|	   |	d °}
|	d |
 t†|	|	d |
d   ° }||k†t°|d< nd|d< d|d< |W S  têy } z$t†dt|Éõ ù° |W  Y d	}~S d	}~0 0 d	S )
zŸ
        Extract Grubbs' test features for outliers
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with Grubbs' test features
        r1   r   Zgrubbs_statisticr4   göôôôôôô?r<   Zgrubbs_outlierz(Error extracting Grubbs' test features: N)r   r   ⁄valuesr7   rR   rS   r?   r   r*   rI   ⁄trî   ⁄sqrtr@   rB   r+   r(   r,   r-   )r   r   r.   rL   rW   rX   Zabs_deviationsZmax_deviationZgrubbs_statr2   Z
t_critical⁄critical_valuer/   r   r   r   r%   ÿ  s*    





&z,StatisticalFeatures._extract_grubbs_featuresc              
      s   êz∫|† ° }|jddgdçj†° }|D ]h}|| jddç}tddÑ |D ÉÉ }|||õ dù< t†t|É°}|d	kr||| }nd	}|||õ d
ù< q&|jtj	gdçj†° }	|	D ]å}zzt
j|| dddç}
|
jddç}tddÑ |D ÉÉ }|||õ dù< t†t|É°}|d	kêr|| }nd	}|||õ dù< W q™   Y q™0 q™d|jv êrx|†d°d †á fddÑ°}|d †|°†d	°|d< d|jv êr∏|†d°d †á fddÑ°}|d †|°†d	°|d< |W S  têy˙ } z$t†dt|Éõ ù° |W  Y d}~S d}~0 0 dS )z»
        Extract entropy-based features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with entropy features
        ⁄object⁄categoryrÖ   Tr5   c                 s   s$   | ]}|d kr|t †|° V  qdS ©r   N©r7   ⁄log2©r   rp   r   r   r   ⁄	<genexpr>  r   z@StatisticalFeatures._extract_entropy_features.<locals>.<genexpr>Z_entropyr   Z_normalized_entropyr:   ⁄drop©⁄bins⁄
duplicatesc                 s   s$   | ]}|d kr|t †|° V  qdS rû   rü   r°   r   r   r   r¢   3  r   Z_binned_entropyZ_binned_normalized_entropyrP   r1   c                    s
   à † | °S r[   ©⁄_calculate_series_entropyr^   ©r   r   r   r`   D  r   z?StatisticalFeatures._extract_entropy_features.<locals>.<lambda>Zsender_amount_entropyrQ   c                    s
   à † | °S r[   rß   r^   r©   r   r   r`   K  r   Zreceiver_amount_entropyz#Error extracting entropy features: N)r   rä   r   rå   rC   ⁄sumr7   r†   r*   rã   rE   ⁄cutrT   ra   rÇ   rÉ   r+   r(   r,   r-   )r   r   r.   Zcategorical_colsr   rC   ⁄entropyZmax_entropyZnormalized_entropyrï   ⁄binnedZsender_entropyZreceiver_entropyr/   r   r©   r   r&     sP    




ˇ
ˇz-StatisticalFeatures._extract_entropy_featuresc                 C   sT   z@t j|tdt|ÉÉddç}|jddç}tddÑ |D ÉÉ }|W S    Y dS 0 d	S )
z∂
        Calculate entropy of a pandas Series
        
        Args:
            series (Series): Input series
            
        Returns:
            float: Entropy value
        r:   r£   r§   Tr5   c                 s   s$   | ]}|d kr|t †|° V  qdS rû   rü   r°   r   r   r   r¢   g  r   z@StatisticalFeatures._calculate_series_entropy.<locals>.<genexpr>r   N)rE   r´   rÄ   r*   rC   r™   )r   ⁄seriesr≠   rC   r¨   r   r   r   r®   U  s    
z-StatisticalFeatures._calculate_series_entropyc              
   C   s   êzÑ|† ° }|jtjgdçj†° }t|Édk r>t†d° |W S || †	° †
° }i }|D ]}|| †|°}|†° ||< qV|D ]}|| ||õ dù< qzi }|D ]}|| †|°}|†° ||< qö|D ]}|| ||õ dù< qæt|ÉD ]¢\}	}|| † ° }
|
| }|
j|ddç}
z\dd	lm} |É }|†|
|° |†|
|°}|dk êrLdd|  }ntd
É}|||õ dù< W qﬁ   d||õ dù< Y qﬁ0 qﬁ|W S  têyƒ } z$t†dt|Éõ ù° |W  Y d}~S d}~0 0 dS )z–
        Extract correlation-based features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with correlation features
        rÖ   r<   z6Not enough numeric columns for correlation calculationZ_max_correlationZ_avg_correlationr4   rà   r   )⁄LinearRegression⁄infZ_vifz'Error extracting correlation features: N)r   rä   r7   rã   r   rå   r*   r(   rç   ⁄corrr?   r£   r   rR   rl   Zsklearn.linear_modelrØ   ⁄fit⁄score⁄floatr+   r,   r-   )r   r   r.   rï   Zcorr_matrixZmax_corrr   ZcorrsZavg_corrro   rñ   ⁄yrØ   ⁄modelZ	r_squaredZvifr/   r   r   r   r'   l  sL    


z1StatisticalFeatures._extract_correlation_featuresc                    s∏   | † à °}á fddÑ|jD É}t|Édkr¥| j†|| ° t|ÉdkrÆttdt|ÉÉdç| _| j†|| ° | j†|| °}t	|j
d ÉD ]"}|ddÖ|f |d|d õ ù< qäd	| _|S )
z‘
        Fit the feature extractor and transform the data
        
        Args:
            df (DataFrame): Input data
            
        Returns:
            DataFrame: Transformed data with features
        c                    s   g | ]}|à j vr|ëqS r   r   r   r   r   r   r   «  r   z5StatisticalFeatures.fit_transform.<locals>.<listcomp>r   r:   )⁄n_componentsr4   N⁄	stat_pca_T)r0   r   r*   r   r≤   r   rÄ   r   ⁄	transformrG   ⁄shaper   ©r   r   r.   Zfeature_colsZpca_componentsro   r   r   r   ⁄fit_transformπ  s    
 z!StatisticalFeatures.fit_transformc                    sú   | j stdÉÇ| †à °}á fddÑ|jD É}t|Édkrò| j†|| °||< | jdurò| j†|| °}t|j	d ÉD ]"}|ddÖ|f |d|d õ ù< qt|S )z’
        Transform new data using fitted feature extractor
        
        Args:
            df (DataFrame): Input data
            
        Returns:
            DataFrame: Transformed data with features
        z7Feature extractor not fitted. Call fit_transform first.c                    s   g | ]}|à j vr|ëqS r   r   r   r   r   r   r   Ï  r   z1StatisticalFeatures.transform.<locals>.<listcomp>r   Nr4   r∏   )
r   ⁄
ValueErrorr0   r   r*   r   rπ   r   rG   r∫   rª   r   r   r   rπ   €  s    


 zStatisticalFeatures.transform)N)⁄__name__⁄
__module__⁄__qualname__⁄__doc__r   r0   r   r    r!   r"   r#   r$   r%   r&   r®   r'   rº   rπ   r   r   r   r   r      s   
#3PT3L8/NM"r   )r¡   ⁄pandasrE   ⁄numpyr7   ⁄scipy.statsrI   Zscipy.spatial.distancer   Zsklearn.preprocessingr   Zsklearn.decompositionr   ⁄warnings⁄logging⁄typingr   r   r   r   ⁄filterwarnings⁄basicConfig⁄INFO⁄	getLoggerræ   r(   r   r   r   r   r   ⁄<module>   s   



=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\features\__pycache__\timeseries_features.cpython-39.pyc ===
a
    èöhµ|  „                   @   s§   d Z ddlZddlZddlmZ ddlmZ ddl	m
Z
 ddlmZmZ ddlZddlZddlmZmZmZmZ e†d° ejejd	ç e†e°ZG d
dÑ dÉZdS )zf
Time Series Features Module
Implements time series feature extraction techniques for fraud detection
È    N)⁄stats)⁄
find_peaks)⁄seasonal_decompose)⁄acf⁄pacf)⁄Dict⁄List⁄Tuple⁄Union⁄ignore)⁄levelc                   @   sb   e Zd ZdZdddÑZddÑ ZddÑ Zd	d
Ñ ZddÑ ZddÑ Z	ddÑ Z
ddÑ ZddÑ ZddÑ ZdS )⁄TimeSeriesFeatureszê
    Class for extracting time series features from transaction data
    Implements techniques like burstiness analysis, gap analysis, etc.
    Nc                 C   s   |pi | _ g | _d| _dS )zÑ
        Initialize TimeSeriesFeatures
        
        Args:
            config (dict, optional): Configuration parameters
        FN)⁄config⁄feature_names⁄fitted)⁄selfr   © r   ˙rC:\Users\Tranquil Abyss\fraud-detection-platform\app\..\src\fraud_detection_engine\features\timeseries_features.py⁄__init__   s    
zTimeSeriesFeatures.__init__c              
      sÊ   z¶à † ° }dà jv r8tjj†à d °s8t†à d °|d< | †|°}| †|°}| †	|°}| †
|°}| †|°}| †|°}á fddÑ|jD É| _t†dt| jÉõ dù° |W S  ty‡ } z"t†dt|Éõ ù° Ç W Y d}~n
d}~0 0 dS )zÊ
        Extract all time series features from the dataframe
        
        Args:
            df (DataFrame): Input transaction data
            
        Returns:
            DataFrame: DataFrame with extracted features
        ⁄	timestampc                    s   g | ]}|à j vr|ëqS r   ©⁄columns©⁄.0⁄col©⁄dfr   r   ⁄
<listcomp>@   Û    z7TimeSeriesFeatures.extract_features.<locals>.<listcomp>z
Extracted z time series featuresz'Error extracting time series features: N)⁄copyr   ⁄pd⁄api⁄types⁄is_datetime64_any_dtype⁄to_datetime⁄_extract_temporal_features⁄_extract_frequency_features⁄_extract_burstiness_features⁄_extract_gap_features⁄_extract_seasonal_features⁄!_extract_autocorrelation_featuresr   ⁄logger⁄info⁄len⁄	Exception⁄error⁄str©r   r   ⁄	result_df⁄er   r   r   ⁄extract_features%   s     






z#TimeSeriesFeatures.extract_featuresc              
   C   sB  êz¸|† ° }d|jvr&t†d° |W S |d jj|d< |d jj|d< |d jj|d< |d jj|d< |d jj	|d< |d jj
|d< |d j†° j|d	< |d jj|d
< |d dk†t°|d< |d jj†t°|d< |d jj†t°|d< |d jj†t°|d< |d jj†t°|d< |d jj†t°|d< |d jj†t°|d< |d dk|d dk B †t°|d< |d dk|d dk @ †t°|d< |d dk|d dk @ †t°|d< |d dk|d dk @ †t°|d< |d dk|d dk @ |d dk @ †t°|d< |W S  têy< } z$t†dt|Éõ ù° |W  Y d}~S d}~0 0 dS ) zƒ
        Extract temporal features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with temporal features
        r   z7Timestamp column not found. Skipping temporal features.⁄hour⁄day⁄month⁄year⁄	dayofweek⁄	dayofyear⁄
weekofyear⁄quarterÈ   Z
is_weekend⁄is_month_start⁄is_month_end⁄is_quarter_start⁄is_quarter_end⁄is_year_start⁄is_year_endÈ   È   Zis_nightÈ   Z
is_morningÈ   Zis_afternoonZ
is_eveningÈ	   È   Zis_business_hoursz$Error extracting temporal features: N)r   r   r+   ⁄warning⁄dtr5   r6   r7   r8   r9   r:   ⁄isocalendar⁄weekr<   ⁄astype⁄intr>   r?   r@   rA   rB   rC   r.   r/   r0   r1   r   r   r   r%   I   sH    


""""

ˇ
˛ˇ¸z-TimeSeriesFeatures._extract_temporal_featuresc              
   C   s&  êz‡|† ° }d|jvr&t†d° |W S |†d°}g d¢}|D ]p}g }|†° D ]P\}}|d }	|	t†|° }
|	}||d |
k|d |k @  }t|É}|†	|° qL||d|õ ù< q<d|jv êrDdD ]Ñ}g }|†° D ]d\}}|d }|d }	|	t†|° }
|	}||d |
k|d |k @ |d |k@  }t|É}|†	|° qŒ||d|õ ù< qæd|jv êrﬁdD ]à}g }|†° D ]f\}}|d }|d }	|	t†|° }
|	}||d |
k|d |k @ |d |k@  }t|É}|†	|° êqd||d	|õ ù< êqT|W S  t
êy  } z$t†d
t|Éõ ù° |W  Y d}~S d}~0 0 dS )zÃ
        Extract frequency-based features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with frequency features
        r   z8Timestamp column not found. Skipping frequency features.)⁄1H⁄6H⁄24H⁄7DZ30DZtransaction_frequency_⁄	sender_id)rP   rQ   rR   Zsender_frequency_⁄receiver_idZreceiver_frequency_z%Error extracting frequency features: N)r   r   r+   rJ   ⁄sort_values⁄iterrowsr    ⁄	Timedeltar-   ⁄appendr.   r/   r0   )r   r   r2   ⁄	df_sorted⁄time_windows⁄windowZwindow_counts⁄_⁄rowr   ⁄window_start⁄
window_end⁄window_transactions⁄countZsender_window_counts⁄senderZreceiver_window_counts⁄receiverr3   r   r   r   r&   Ä   sz    





ˇˇ

ˇ
˛ˇ

ˇ
˛ˇz.TimeSeriesFeatures._extract_frequency_featuresc              
   C   st  êz.|† ° }d|jvr&t†d° |W S |†d°}|d †° j†° †d°}t	|ÉdkrÇ|†
° dkrÇ|†
° |†°  |†
° |†°   }nd}||d< g d¢}|D ]Æ}g }tt	|ÉÉD ]ä}	td|	|d  É}
tt	|É|	|d  d É}|j|
|Ö }t	|Édkêr*|†
° dkêr*|†
° |†°  |†
° |†°   }nd}|†|° qÆ||d|õ ù< qö|†° }|†
° }td|d|  É}||k †t°|d	< g }d}|d	 D ]$}|êr¢|d7 }n|†|° d}êqé|†|° g }d}t|d	 ÉD ]P\}	}|êr|†|| ° n0|†d° |	dkêr“|d	 j|	d  ês“|d7 }êq“||d
< |W S  têyn } z$t†dt|Éõ ù° |W  Y d}~S d}~0 0 dS )z—
        Extract burstiness analysis features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with burstiness features
        r   z9Timestamp column not found. Skipping burstiness features.r   È   Zburstiness_coefficient)È
   È2   Èd   È   Zlocal_burstiness_Zis_in_burstZburst_durationz&Error extracting burstiness features: N)r   r   r+   rJ   rV   ⁄diffrK   ⁄total_seconds⁄fillnar-   ⁄std⁄mean⁄range⁄max⁄min⁄ilocrY   rN   rO   ⁄	enumerater.   r/   r0   )r   r   r2   rZ   ⁄inter_timesZ
burstinessZwindow_sizes⁄window_sizeZlocal_burstiness⁄i⁄	start_idx⁄end_idxZwindow_inter_timesZlocal_bZavg_inter_timeZstd_inter_timeZburst_thresholdZburst_durationsZcurrent_durationZis_burstZburst_duration_map⁄idxr3   r   r   r   r'   Â   sd    



"ˇ



z/TimeSeriesFeatures._extract_burstiness_featuresc              
   C   s¯  êz≤|† ° }d|jvr&t†d° |W S |†d°}|d †° j†° †d°}||d< |d †d°j†° †	° †d°}||d< t
|ÉdkrÃ|†° dkrÃ|†° d|†°   }||k†t°|d	< t†||k|d°|d
< nd|d	< d|d
< d|jv êrDg }i }|d †° D ]T}	||d |	k †d°}
t
|
Édkr¸|
d †° j†° †d°}|†° |†° dú||	< q¸|†° D ]∆\}}|d }	|	|v êr
||	 }||d |	k|d |d k @  }
t
|
Édkêr»|
d †° }|d | †° }ntdÉ}|d dkêrÙ||d  |d  }nd}|†||dú° n|†tdÉddú° êqZt†|°}|d |d< |d |d< d|jv êr∞g }i }|d †° D ]X}||d |k †d°}t
|Édkêrd|d †° j†° †d°}|†° |†° dú||< êqd|†° D ]∆\}}|d }||v êrv|| }||d |k|d |d k @  }t
|Édkêr4|d †° }|d | †° }ntdÉ}|d dkêr`||d  |d  }nd}|†||dú° n|†tdÉddú° êq∆t†|°}|d |d< |d |d< |W S  têyÚ } z$t†dt|Éõ ù° |W  Y d}~S d}~0 0 dS )z√
        Extract gap analysis features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with gap features
        r   z2Timestamp column not found. Skipping gap features.r   Ztime_since_last_transactionÈˇˇˇˇZtime_until_next_transactionre   ri   Zis_after_gap⁄gap_sizerT   )rn   rm   ⁄infrm   rn   )⁄sender_time_since_last⁄sender_gap_z_scorer}   r~   rU   )⁄receiver_time_since_last⁄receiver_gap_z_scorer   rÄ   zError extracting gap features: N)r   r   r+   rJ   rV   rj   rK   rk   rl   ⁄absr-   rm   rn   rN   rO   ⁄np⁄where⁄uniquerW   rp   ⁄floatrY   r    ⁄	DataFramer.   r/   r0   )r   r   r2   rZ   rt   Ztime_until_nextZgap_thresholdZsender_gapsZsender_gap_statsrc   Zsender_transactionsZsender_inter_timesr]   r^   r   Zlast_sender_timeZsender_inter_timeZgap_z_scoreZgap_dfZreceiver_gapsZreceiver_gap_statsrd   Zreceiver_transactionsZreceiver_inter_timesZlast_receiver_timeZreceiver_inter_timer3   r   r   r   r(   I  sº    



˛

ˇˇ˛
˛
˛

ˇˇ˛
˛
z(TimeSeriesFeatures._extract_gap_featuresc           "      C   sH  êz|† ° }d|jvr&t†d° |W S |†d°}g d¢}|D ê]¥}|†d°†|°†° }t|Édkr<êzLt	|dt
dt|Éd Édç}|j}|j}	|j}
i }i }i }|d D ]V}|†|°}||jv rÊ|| ||< |	| ||< |
| ||< q®d	||< d	||< d	||< q®|d †|°†d	°|d
|õ ù< |d †|°†d	°|d|õ ù< |d †|°†d	°|d|õ ù< |
†° }|
†° }|d	kêr†|d †|°| | }|†d	°|d|õ ù< nd	|d|õ ù< W q< têy } z&t†d|õ dt|Éõ ù° W Y d}~q<d}~0 0 q<i }d|jv êrn|†d°†° }||†°  }tddÑ |D ÉÉ }t†d°}|d	kêrN|| nd	}d| |d< |†° }||d< d|jv êr‰|†d°†° }||†°  }tddÑ |D ÉÉ }t†d°}|d	kêrƒ|| nd	}d| |d< |†° }||d< |†° D ]\} }!|!|| < êqÏ|W S  têyB } z$t†dt|Éõ ù° |W  Y d}~S d}~0 0 dS )zƒ
        Extract seasonal features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with seasonal features
        r   z7Timestamp column not found. Skipping seasonal features.)rP   rQ   ⁄1DZ1Wrf   ⁄additiveÈ   ri   )⁄model⁄periodr   Ztrend_Z	seasonal_Z	residual_Zseasonal_anomaly_z$Error in seasonal decomposition for ˙: Nr5   c                 s   s$   | ]}|d kr|t †|° V  qdS ©r   N©rÇ   ⁄log2©r   ⁄pr   r   r   ⁄	<genexpr><  r   z@TimeSeriesFeatures._extract_seasonal_features.<locals>.<genexpr>re   Zdaily_pattern_strength⁄	peak_hourr9   c                 s   s$   | ]}|d kr|t †|° V  qdS rç   ré   rê   r   r   r   rí   K  r   È   Zweekly_pattern_strength⁄peak_dayz$Error extracting seasonal features: )r   r   r+   rJ   rV   ⁄	set_index⁄resample⁄sizer-   r   rq   ⁄trend⁄seasonal⁄resid⁄floor⁄index⁄maprl   rn   rm   r.   r0   ⁄groupby⁄sumrÇ   rè   ⁄idxmax⁄itemsr/   )"r   r   r2   rZ   ⁄frequencies⁄freq⁄ts⁄decompositionrô   rö   ⁄residualZ	trend_mapZseasonal_mapZresidual_mapr   Zperiod_startZresidual_meanZresidual_stdZanomaly_scoresr3   Zperiodicity_featuresZhourly_countsZhourly_probsZdaily_entropy⁄max_entropyZdaily_uniformityrì   Zweekly_countsZweekly_probsZweekly_entropyZweekly_uniformityrï   ⁄feature⁄valuer   r   r   r)   È  s~    








2

z-TimeSeriesFeatures._extract_seasonal_featuresc                 C   sÙ  êzÆ|† ° }d|jvr&t†d° |W S |†d°}g d¢}|D ê]l}|†d°†|°†° }t|Édkr<êzt	dt|Éd É}t
||ddç}tdt	d	t|ÉÉÉD ]}	||	 |d
|õ d|	õ ù< qöt||dç}
tdt	d	t|
ÉÉÉD ]}	|
|	 |d|õ d|	õ ù< qÿt|ddÖ ddç\}}t|ÉdkêrJ|d d }||d|õ ù< || |d|õ ù< nd|d|õ ù< d|d|õ ù< W q< têy® } z&t†d|õ dt|Éõ ù° W Y d}~q<d}~0 0 q<|W S  têyÓ } z$t†dt|Éõ ù° |W  Y d}~S d}~0 0 dS )z“
        Extract autocorrelation features
        
        Args:
            df (DataFrame): Input dataframe
            
        Returns:
            DataFrame: DataFrame with autocorrelation features
        r   z>Timestamp column not found. Skipping autocorrelation features.)rP   rQ   rá   rf   ri   T)⁄nlags⁄fftre   rE   Z	autocorr_Z_lag_)r´   Zpacf_Ngöôôôôô…?)⁄heightr   Zperiodicity_Zperiodicity_strength_z&Error in autocorrelation analysis for rå   z+Error extracting autocorrelation features: )r   r   r+   rJ   rV   rñ   ró   rò   r-   rq   r   ro   r   r   r.   r0   r/   )r   r   r2   rZ   r£   r§   r•   r´   ⁄autocorrrv   Zpacf_valuesZpeaksr]   Zpeak_lagr3   r   r   r   r*   _  s>    




2z4TimeSeriesFeatures._extract_autocorrelation_featuresc                    s4   | † à °}á fddÑ|jD É}t|Édkr0d| _|S )z‘
        Fit the feature extractor and transform the data
        
        Args:
            df (DataFrame): Input data
            
        Returns:
            DataFrame: Transformed data with features
        c                    s   g | ]}|à j vr|ëqS r   r   r   r   r   r   r   ∞  r   z4TimeSeriesFeatures.fit_transform.<locals>.<listcomp>r   T)r4   r   r-   r   )r   r   r2   ⁄feature_colsr   r   r   ⁄fit_transform¢  s
    
z TimeSeriesFeatures.fit_transformc                 C   s   | j stdÉÇ| †|°}|S )z’
        Transform new data using fitted feature extractor
        
        Args:
            df (DataFrame): Input data
            
        Returns:
            DataFrame: Transformed data with features
        z7Feature extractor not fitted. Call fit_transform first.)r   ⁄
ValueErrorr4   )r   r   r2   r   r   r   ⁄	transform∑  s    

zTimeSeriesFeatures.transform)N)⁄__name__⁄
__module__⁄__qualname__⁄__doc__r   r4   r%   r&   r'   r(   r)   r*   r∞   r≤   r   r   r   r   r      s   
$7ed !vCr   )r∂   ⁄pandasr    ⁄numpyrÇ   ⁄scipyr   Zscipy.signalr   Zstatsmodels.tsa.seasonalr   Zstatsmodels.tsa.stattoolsr   r   ⁄warnings⁄logging⁄typingr   r   r	   r
   ⁄filterwarnings⁄basicConfig⁄INFO⁄	getLoggerr≥   r+   r   r   r   r   r   ⁄<module>   s   



=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\features\__pycache__\__init__.cpython-313.pyc ===
Û
    dÄöh    „                   Û   ï g )N© r   Û    ⁄gC:\Users\Tranquil Abyss\fraud-detection-platform\app\..\src\fraud_detection_engine\features\__init__.py⁄<module>r      s   Òr   

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\features\__pycache__\__init__.cpython-39.pyc ===
a
    èöh    „                   @   s   d S )N© r   r   r   ˙gC:\Users\Tranquil Abyss\fraud-detection-platform\app\..\src\fraud_detection_engine\features\__init__.py⁄<module>   Û    

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\ingestion\column_mapper.py ===
"""
Column Mapper Module
Handles intelligent mapping of user columns to expected format
"""
import pandas as pd
import numpy as np
import re
import logging
from collections import defaultdict
from difflib import SequenceMatcher
import yaml
import os
# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ColumnMapper:
    """
    Class for mapping user columns to expected format
    Uses AI-powered techniques to intelligently match columns
    """
    
    def __init__(self, config_path=None):
        """
        Initialize ColumnMapper
        
        Args:
            config_path (str, optional): Path to configuration file
        """
        self.expected_columns = self._get_expected_columns()
        self.synonyms = self._load_synonyms(config_path)
        self.patterns = self._load_patterns(config_path)
        self.mapping_history = []

    def get_expected_columns(self):
        """
        Return the list of expected column names
        
        Returns:
            list: Expected column names
        """
        return list(self.expected_columns.keys())

    
    def _get_expected_columns(self):
        """
        Get the expected columns for the fraud detection system
        
        Returns:
            dict: Expected columns with descriptions
        """
        return {
            'transaction_id': {
                'description': 'Unique identifier for the transaction',
                'data_type': 'string',
                'required': True
            },
            'timestamp': {
                'description': 'Date and time of the transaction',
                'data_type': 'datetime',
                'required': True
            },
            'amount': {
                'description': 'Transaction amount',
                'data_type': 'float',
                'required': True
            },
            'currency': {
                'description': 'Currency code (e.g., USD, EUR)',
                'data_type': 'string',
                'required': False
            },
            'sender_id': {
                'description': 'Identifier of the sender',
                'data_type': 'string',
                'required': True
            },
            'receiver_id': {
                'description': 'Identifier of the receiver',
                'data_type': 'string',
                'required': True
            },
            'sender_account_type': {
                'description': 'Type of sender account (e.g., personal, business)',
                'data_type': 'string',
                'required': False
            },
            'receiver_account_type': {
                'description': 'Type of receiver account (e.g., personal, business)',
                'data_type': 'string',
                'required': False
            },
            'sender_bank': {
                'description': 'Name of sender bank',
                'data_type': 'string',
                'required': False
            },
            'receiver_bank': {
                'description': 'Name of receiver bank',
                'data_type': 'string',
                'required': False
            },
            'sender_location': {
                'description': 'Location of sender (country, state, city)',
                'data_type': 'string',
                'required': False
            },
            'receiver_location': {
                'description': 'Location of receiver (country, state, city)',
                'data_type': 'string',
                'required': False
            },
            'transaction_type': {
                'description': 'Type of transaction (e.g., transfer, payment)',
                'data_type': 'string',
                'required': False
            },
            'transaction_category': {
                'description': 'Category of transaction (e.g., retail, services)',
                'data_type': 'string',
                'required': False
            },
            'merchant_id': {
                'description': 'Identifier of the merchant',
                'data_type': 'string',
                'required': False
            },
            'merchant_category': {
                'description': 'Category of the merchant',
                'data_type': 'string',
                'required': False
            },
            'ip_address': {
                'description': 'IP address used for the transaction',
                'data_type': 'string',
                'required': False
            },
            'device_id': {
                'description': 'Identifier of the device used',
                'data_type': 'string',
                'required': False
            },
            'description': {
                'description': 'Description of the transaction',
                'data_type': 'string',
                'required': False
            },
            'notes': {
                'description': 'Additional notes',
                'data_type': 'string',
                'required': False
            },
            'authorization_status': {
                'description': 'Authorization status (e.g., approved, declined)',
                'data_type': 'string',
                'required': False
            },
            'chargeback_flag': {
                'description': 'Whether the transaction was charged back',
                'data_type': 'boolean',
                'required': False
            },
            'fraud_flag': {
                'description': 'Whether the transaction is fraudulent (for supervised learning)',
                'data_type': 'boolean',
                'required': False
            }
        }
    
    def _load_synonyms(self, config_path=None):
        """
        Load synonyms for column names
        
        Args:
            config_path (str, optional): Path to configuration file
            
        Returns:
            dict: Synonyms for expected columns
        """
        # Default synonyms
        synonyms = {
            'transaction_id': ['id', 'transaction_id', 'tx_id', 'trans_id', 'reference', 'ref_no', 'transaction_no'],
            'timestamp': ['timestamp', 'date', 'time', 'datetime', 'trans_date', 'trans_time', 'transaction_date', 'transaction_time'],
            'amount': ['amount', 'value', 'sum', 'total', 'transaction_amount', 'amt', 'tx_amount'],
            'currency': ['currency', 'curr', 'ccy', 'currency_code'],
            'sender_id': ['sender_id', 'from_id', 'payer_id', 'source_id', 'originator_id'],
            'receiver_id': ['receiver_id', 'to_id', 'payee_id', 'destination_id', 'beneficiary_id'],
            'sender_account_type': ['sender_account_type', 'from_account_type', 'payer_account_type', 'source_account_type'],
            'receiver_account_type': ['receiver_account_type', 'to_account_type', 'payee_account_type', 'destination_account_type'],
            'sender_bank': ['sender_bank', 'from_bank', 'payer_bank', 'source_bank'],
            'receiver_bank': ['receiver_bank', 'to_bank', 'payee_bank', 'destination_bank'],
            'sender_location': ['sender_location', 'from_location', 'payer_location', 'source_location', 'sender_country', 'from_country'],
            'receiver_location': ['receiver_location', 'to_location', 'payee_location', 'destination_location', 'receiver_country', 'to_country'],
            'transaction_type': ['transaction_type', 'trans_type', 'type', 'tx_type'],
            'transaction_category': ['transaction_category', 'trans_category', 'category', 'tx_category'],
            'merchant_id': ['merchant_id', 'merchant', 'retailer_id', 'vendor_id'],
            'merchant_category': ['merchant_category', 'merchant_type', 'retailer_category', 'vendor_category'],
            'ip_address': ['ip_address', 'ip', 'ip_addr'],
            'device_id': ['device_id', 'device', 'device_identifier'],
            'description': ['description', 'desc', 'details', 'narrative'],
            'notes': ['notes', 'note', 'comments', 'remark'],
            'authorization_status': ['authorization_status', 'auth_status', 'status', 'approval_status'],
            'chargeback_flag': ['chargeback_flag', 'chargeback', 'is_chargeback'],
            'fraud_flag': ['fraud_flag', 'fraud', 'is_fraud', 'fraudulent']
        }
        
        # Try to load from config file
        if config_path and os.path.exists(config_path):
            try:
                with open(config_path, 'r') as f:
                    config = yaml.safe_load(f)
                    if 'column_synonyms' in config:
                        # Update with config synonyms
                        for col, syn_list in config['column_synonyms'].items():
                            if col in synonyms:
                                synonyms[col].extend(syn_list)
                            else:
                                synonyms[col] = syn_list
            except Exception as e:
                logger.warning(f"Error loading synonyms from config: {str(e)}")
        
        return synonyms
    
    def _load_patterns(self, config_path=None):
        """
        Load regex patterns for column names
        
        Args:
            config_path (str, optional): Path to configuration file
            
        Returns:
            dict: Regex patterns for expected columns
        """
        # Default patterns
        patterns = {
            'transaction_id': [r'transaction.?id', r'tx.?id', r'trans.?id', r'reference', r'ref.?(no|num)'],
            'timestamp': [r'timestamp', r'date.?time', r'trans.?date', r'trans.?time'],
            'amount': [r'amount', r'value', r'sum', r'total'],
            'currency': [r'currency', r'ccy', r'curr'],
            'sender_id': [r'sender.?id', r'from.?id', r'payer.?id', r'source.?id', r'originator.?id'],
            'receiver_id': [r'receiver.?id', r'to.?id', r'payee.?id', r'destination.?id', r'beneficiary.?id'],
            'sender_account_type': [r'sender.?account.?type', r'from.?account.?type', r'payer.?account.?type'],
            'receiver_account_type': [r'receiver.?account.?type', r'to.?account.?type', r'payee.?account.?type'],
            'sender_bank': [r'sender.?bank', r'from.?bank', r'payer.?bank', r'source.?bank'],
            'receiver_bank': [r'receiver.?bank', r'to.?bank', r'payee.?bank', r'destination.?bank'],
            'sender_location': [r'sender.?location', r'from.?location', r'payer.?location', r'source.?location', r'sender.?country'],
            'receiver_location': [r'receiver.?location', r'to.?location', r'payee.?location', r'destination.?location', r'receiver.?country'],
            'transaction_type': [r'transaction.?type', r'trans.?type', r'tx.?type'],
            'transaction_category': [r'transaction.?category', r'trans.?category', r'tx.?category'],
            'merchant_id': [r'merchant.?id', r'merchant', r'retailer.?id', r'vendor.?id'],
            'merchant_category': [r'merchant.?category', r'merchant.?type', r'retailer.?category'],
            'ip_address': [r'ip.?address', r'ip'],
            'device_id': [r'device.?id', r'device'],
            'description': [r'description', r'desc', r'details', r'narrative'],
            'notes': [r'notes?', r'comments?', r'remarks?'],
            'authorization_status': [r'authorization.?status', r'auth.?status', r'approval.?status'],
            'chargeback_flag': [r'chargeback.?(flag|is)'],
            'fraud_flag': [r'fraud.?(flag|is)', r'is.?fraud']
        }
        
        # Try to load from config file
        if config_path and os.path.exists(config_path):
            try:
                with open(config_path, 'r') as f:
                    config = yaml.safe_load(f)
                    if 'column_patterns' in config:
                        # Update with config patterns
                        for col, pat_list in config['column_patterns'].items():
                            if col in patterns:
                                patterns[col].extend(pat_list)
                            else:
                                patterns[col] = pat_list
            except Exception as e:
                logger.warning(f"Error loading patterns from config: {str(e)}")
        
        return patterns
    
    def auto_map_columns(self, user_columns, expected_columns=None):
        """
        Automatically map user columns to expected columns
        
        Args:
            user_columns (list): List of user column names
            expected_columns (list, optional): List of expected column names
            
        Returns:
            dict: Mapping from user columns to expected columns
        """
        if expected_columns is None:
            expected_columns = list(self.expected_columns.keys())
        
        mapping = {}
        used_columns = set()
        
        # First, try exact matches
        for user_col in user_columns:
            user_col_clean = self._clean_column_name(user_col)
            if user_col_clean in expected_columns and user_col_clean not in used_columns:
                mapping[user_col] = user_col_clean
                used_columns.add(user_col_clean)
        
        # Then, try synonym matches
        for user_col in user_columns:
            if user_col in mapping:
                continue
                
            user_col_clean = self._clean_column_name(user_col)
            
            for expected_col in expected_columns:
                if expected_col in used_columns:
                    continue
                    
                if user_col_clean in self.synonyms.get(expected_col, []):
                    mapping[user_col] = expected_col
                    used_columns.add(expected_col)
                    break
        
        # Then, try pattern matches
        for user_col in user_columns:
            if user_col in mapping:
                continue
                
            user_col_clean = self._clean_column_name(user_col)
            
            for expected_col in expected_columns:
                if expected_col in used_columns:
                    continue
                    
                for pattern in self.patterns.get(expected_col, []):
                    if re.search(pattern, user_col_clean, re.IGNORECASE):
                        mapping[user_col] = expected_col
                        used_columns.add(expected_col)
                        break
                else:
                    continue
                break
        
        # Finally, try fuzzy matching for remaining columns
        for user_col in user_columns:
            if user_col in mapping:
                continue
                
            user_col_clean = self._clean_column_name(user_col)
            
            best_match = None
            best_score = 0
            
            for expected_col in expected_columns:
                if expected_col in used_columns:
                    continue
                    
                # Calculate similarity score
                score = self._calculate_similarity(user_col_clean, expected_col)
                
                if score > best_score and score > 0.6:  # Threshold for fuzzy matching
                    best_score = score
                    best_match = expected_col
            
            if best_match:
                mapping[user_col] = best_match
                used_columns.add(best_match)
        
        # Store mapping history
        self.mapping_history.append({
            'timestamp': pd.Timestamp.now(),
            'user_columns': user_columns,
            'mapping': mapping
        })
        
        return mapping
    
    def _clean_column_name(self, column_name):
        """
        Clean column name for matching
        
        Args:
            column_name (str): Column name to clean
            
        Returns:
            str: Cleaned column name
        """
        # Convert to lowercase
        cleaned = column_name.lower()
        
        # Remove special characters and spaces
        cleaned = re.sub(r'[^a-z0-9]', '_', cleaned)
        
        # Remove consecutive underscores
        cleaned = re.sub(r'_+', '_', cleaned)
        
        # Remove leading and trailing underscores
        cleaned = cleaned.strip('_')
        
        return cleaned
    
    def _calculate_similarity(self, str1, str2):
        """
        Calculate similarity between two strings
        
        Args:
            str1 (str): First string
            str2 (str): Second string
            
        Returns:
            float: Similarity score (0-1)
        """
        # Use SequenceMatcher for similarity
        return SequenceMatcher(None, str1, str2).ratio()
    
    def apply_mapping(self, df, mapping):
        """
        Apply column mapping to a DataFrame
        
        Args:
            df (DataFrame): Input DataFrame
            mapping (dict): Column mapping
            
        Returns:
            DataFrame: DataFrame with mapped columns
        """
        try:
            # Create a copy of the DataFrame
            result_df = df.copy()
            
            # Create a new DataFrame with mapped columns
            mapped_df = pd.DataFrame()
            
            # Map columns
            for user_col, expected_col in mapping.items():
                if user_col in df.columns:
                    mapped_df[expected_col] = df[user_col]
            
            # Add unmapped columns with original names
            for col in df.columns:
                if col not in mapping:
                    mapped_df[col] = df[col]
            
            logger.info(f"Column mapping applied successfully. Mapped {len(mapping)} columns.")
            return mapped_df
            
        except Exception as e:
            logger.error(f"Error applying column mapping: {str(e)}")
            raise
    
    def validate_mapping(self, mapping, required_columns=None):
        """
        Validate a column mapping
        
        Args:
            mapping (dict): Column mapping
            required_columns (list, optional): List of required columns
            
        Returns:
            dict: Validation results
        """
        if required_columns is None:
            required_columns = [col for col, info in self.expected_columns.items() if info['required']]
        
        validation_result = {
            "valid": True,
            "errors": [],
            "warnings": [],
            "missing_required": [],
            "missing_optional": []
        }
        
        # Check for required columns
        mapped_columns = set(mapping.values())
        
        for col in required_columns:
            if col not in mapped_columns:
                validation_result["valid"] = False
                validation_result["missing_required"].append(col)
        
        # Check for optional columns
        optional_columns = [col for col in self.expected_columns if col not in required_columns]
        
        for col in optional_columns:
            if col not in mapped_columns:
                validation_result["missing_optional"].append(col)
        
        # Generate error messages
        if validation_result["missing_required"]:
            validation_result["errors"].append(
                f"Missing required columns: {', '.join(validation_result['missing_required'])}"
            )
        
        # Generate warning messages
        if validation_result["missing_optional"]:
            validation_result["warnings"].append(
                f"Missing optional columns: {', '.join(validation_result['missing_optional'])}"
            )
        
        return validation_result
    
    def save_mapping_template(self, file_path, mapping=None):
        """
        Save a mapping template to file
        
        Args:
            file_path (str): Path to save the template
            mapping (dict, optional): Mapping to save
        """
        if mapping is None:
            mapping = {}
        
        try:
            template = {
                "expected_columns": self.expected_columns,
                "synonyms": self.synonyms,
                "patterns": self.patterns,
                "current_mapping": mapping
            }
            
            with open(file_path, 'w') as f:
                yaml.dump(template, f, default_flow_style=False)
            
            logger.info(f"Mapping template saved to {file_path}")
            
        except Exception as e:
            logger.error(f"Error saving mapping template: {str(e)}")
            raise
    
    def load_mapping_template(self, file_path):
        """
        Load a mapping template from file
        
        Args:
            file_path (str): Path to the template file
            
        Returns:
            dict: Loaded mapping
        """
        try:
            with open(file_path, 'r') as f:
                template = yaml.safe_load(f)
            
            # Update instance variables
            if 'expected_columns' in template:
                self.expected_columns = template['expected_columns']
            
            if 'synonyms' in template:
                self.synonyms = template['synonyms']
            
            if 'patterns' in template:
                self.patterns = template['patterns']
            
            logger.info(f"Mapping template loaded from {file_path}")
            
            return template.get('current_mapping', {})
            
        except Exception as e:
            logger.error(f"Error loading mapping template: {str(e)}")
            raise
    
    def get_mapping_suggestions(self, user_columns, expected_columns=None):
        """
        Get mapping suggestions with confidence scores
        
        Args:
            user_columns (list): List of user column names
            expected_columns (list, optional): List of expected column names
            
        Returns:
            dict: Mapping suggestions with confidence scores
        """
        if expected_columns is None:
            expected_columns = list(self.expected_columns.keys())
        
        suggestions = {}
        
        for user_col in user_columns:
            user_col_clean = self._clean_column_name(user_col)
            
            col_suggestions = []
            
            # Check for exact matches
            if user_col_clean in expected_columns:
                col_suggestions.append({
                    "column": user_col_clean,
                    "confidence": 1.0,
                    "method": "exact_match"
                })
            
            # Check for synonyms
            for expected_col in expected_columns:
                if user_col_clean in self.synonyms.get(expected_col, []):
                    col_suggestions.append({
                        "column": expected_col,
                        "confidence": 0.9,
                        "method": "synonym_match"
                    })
            
            # Check for pattern matches
            for expected_col in expected_columns:
                for pattern in self.patterns.get(expected_col, []):
                    if re.search(pattern, user_col_clean, re.IGNORECASE):
                        col_suggestions.append({
                            "column": expected_col,
                            "confidence": 0.8,
                            "method": "pattern_match"
                        })
            
            # Check for fuzzy matches
            for expected_col in expected_columns:
                similarity = self._calculate_similarity(user_col_clean, expected_col)
                if similarity > 0.6:
                    col_suggestions.append({
                        "column": expected_col,
                        "confidence": similarity,
                        "method": "fuzzy_match"
                    })
            
            # Sort by confidence
            col_suggestions.sort(key=lambda x: x["confidence"], reverse=True)
            
            suggestions[user_col] = col_suggestions
        
        return suggestions

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\ingestion\data_loader.py ===
"""
Data Loader Module
Handles loading and preprocessing of transaction data
"""

import pandas as pd
import numpy as np
import dask.dataframe as dd
import os
import logging
from datetime import datetime
import chardet
import warnings
warnings.filterwarnings('ignore')

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class DataLoader:
    """
    Class for loading and preprocessing transaction data
    Supports CSV, Excel, and Parquet files
    Uses Dask for handling large files
    """
    
    def __init__(self, use_dask=True, chunk_size=100000):
        """
        Initialize DataLoader
        
        Args:
            use_dask (bool): Whether to use Dask for large files
            chunk_size (int): Chunk size for Dask processing
        """
        self.use_dask = use_dask
        self.chunk_size = chunk_size
        self.data = None
        self.original_data = None
        
    def load_data(self, file_path, file_type=None, **kwargs):
        """
        Load data from file
        
        Args:
            file_path (str): Path to the data file
            file_type (str, optional): Type of file (csv, excel, parquet)
            **kwargs: Additional parameters for pandas/dask read functions
            
        Returns:
            DataFrame: Loaded data
        """
        try:
            # Determine file type if not provided
            if file_type is None:
                file_type = os.path.splitext(file_path)[1][1:].lower()
            
            # Get file size to decide whether to use Dask
            file_size = os.path.getsize(file_path) / (1024 * 1024)  # Size in MB
            
            # Use Dask for large files
            if self.use_dask and file_size > 100:  # Use Dask for files > 100MB
                logger.info(f"Loading large file ({file_size:.2f} MB) using Dask")
                return self._load_with_dask(file_path, file_type, **kwargs)
            else:
                logger.info(f"Loading file ({file_size:.2f} MB) using pandas")
                return self._load_with_pandas(file_path, file_type, **kwargs)
                
        except Exception as e:
            logger.error(f"Error loading data: {str(e)}")
            raise
    
    def _load_with_pandas(self, file_path, file_type, **kwargs):
        """
        Load data using pandas
        
        Args:
            file_path (str): Path to the data file
            file_type (str): Type of file
            **kwargs: Additional parameters for pandas read functions
            
        Returns:
            DataFrame: Loaded data
        """
        try:
            # Detect encoding for CSV files
            if file_type == 'csv':
                with open(file_path, 'rb') as f:
                    result = chardet.detect(f.read())
                encoding = result['encoding']
                kwargs.setdefault('encoding', encoding)
            
            # Load data based on file type
            if file_type in ['csv', 'txt']:
                df = pd.read_csv(file_path, **kwargs)
            elif file_type in ['xlsx', 'xls']:
                df = pd.read_excel(file_path, **kwargs)
            elif file_type == 'parquet':
                df = pd.read_parquet(file_path, **kwargs)
            else:
                raise ValueError(f"Unsupported file type: {file_type}")
            
            # Store original data
            self.original_data = df.copy()
            
            # Basic preprocessing
            df = self._preprocess_data(df)
            
            self.data = df
            logger.info(f"Data loaded successfully. Shape: {df.shape}")
            return df
            
        except Exception as e:
            logger.error(f"Error loading data with pandas: {str(e)}")
            raise
    
    def _load_with_dask(self, file_path, file_type, **kwargs):
        """
        Load data using Dask
        
        Args:
            file_path (str): Path to the data file
            file_type (str): Type of file
            **kwargs: Additional parameters for dask read functions
            
        Returns:
            DataFrame: Loaded data
        """
        try:
            # Load data based on file type
            if file_type in ['csv', 'txt']:
                df = dd.read_csv(file_path, blocksize=self.chunk_size, **kwargs)
            elif file_type == 'parquet':
                df = dd.read_parquet(file_path, **kwargs)
            else:
                # For Excel files, we need to use pandas
                logger.warning("Dask does not support Excel files directly. Using pandas instead.")
                return self._load_with_pandas(file_path, file_type, **kwargs)
            
            # Store original data (compute a sample for inspection)
            self.original_data = df.head(1000)
            
            # Basic preprocessing
            df = df.map_partitions(self._preprocess_data)
            
            self.data = df
            logger.info(f"Data loaded successfully with Dask. Partitions: {df.npartitions}")
            return df
            
        except Exception as e:
            logger.error(f"Error loading data with Dask: {str(e)}")
            raise
    
    def _preprocess_data(self, df):
        """
        Basic preprocessing of the data
        
        Args:
            df (DataFrame): Input data
            
        Returns:
            DataFrame: Preprocessed data
        """
        try:
            # Remove duplicate rows
            initial_rows = len(df)
            df = df.drop_duplicates()
            removed_rows = initial_rows - len(df)
            if removed_rows > 0:
                logger.info(f"Removed {removed_rows} duplicate rows")
            
            # Handle missing values
            for col in df.columns:
                # For numeric columns, fill with median
                if pd.api.types.is_numeric_dtype(df[col]):
                    median_val = df[col].median()
                    df[col] = df[col].fillna(median_val)
                # For categorical columns, fill with mode or 'Unknown'
                elif pd.api.types.is_string_dtype(df[col]) or pd.api.types.is_categorical_dtype(df[col]):
                    mode_val = df[col].mode()
                    if len(mode_val) > 0:
                        df[col] = df[col].fillna(mode_val[0])
                    else:
                        df[col] = df[col].fillna('Unknown')
            
            # Convert timestamp columns to datetime
            for col in df.columns:
                if 'time' in col.lower() or 'date' in col.lower():
                    try:
                        df[col] = pd.to_datetime(df[col], errors='coerce')
                    except:
                        logger.warning(f"Could not convert {col} to datetime")
            
            # Convert amount columns to numeric
            for col in df.columns:
                if 'amount' in col.lower() or 'value' in col.lower() or 'sum' in col.lower():
                    try:
                        # Remove currency symbols and commas
                        if pd.api.types.is_string_dtype(df[col]):
                            df[col] = df[col].str.replace('[\$,‚Ç¨,¬£,¬•]', '', regex=True)
                            df[col] = df[col].str.replace(',', '')
                        df[col] = pd.to_numeric(df[col], errors='coerce')
                    except:
                        logger.warning(f"Could not convert {col} to numeric")
            
            # Convert ID columns to string
            for col in df.columns:
                if 'id' in col.lower():
                    df[col] = df[col].astype(str)
            
            # Convert boolean columns
            for col in df.columns:
                if df[col].dtype == 'object':
                    unique_vals = df[col].dropna().unique()
                    if len(unique_vals) <= 2 and all(val.lower() in ['true', 'false', 'yes', 'no', 'y', 'n', '0', '1'] for val in unique_vals):
                        df[col] = df[col].replace({
                            'true': True, 'yes': True, 'y': True, '1': True,
                            'false': False, 'no': False, 'n': False, '0': False
                        })
            
            # Convert categorical columns with many unique values to 'category' dtype
            for col in df.columns:
                if (pd.api.types.is_string_dtype(df[col]) and 
                    df[col].nunique() > 10 and 
                    df[col].nunique() < len(df) * 0.5):
                    df[col] = df[col].astype('category')
            
            logger.info("Data preprocessing completed")
            return df
            
        except Exception as e:
            logger.error(f"Error preprocessing data: {str(e)}")
            raise
    
    def get_data_info(self):
        """
        Get information about the loaded data
        
        Returns:
            dict: Data information
        """
        if self.data is None:
            return {"error": "No data loaded"}
        
        info = {
            "shape": self.data.shape,
            "columns": list(self.data.columns),
            "dtypes": dict(self.data.dtypes),
            "missing_values": dict(self.data.isnull().sum()),
            "memory_usage": dict(self.data.memory_usage(deep=True))
        }
        
        # Add statistics for numeric columns
        numeric_stats = {}
        for col in self.data.select_dtypes(include=[np.number]).columns:
            numeric_stats[col] = {
                "min": self.data[col].min(),
                "max": self.data[col].max(),
                "mean": self.data[col].mean(),
                "median": self.data[col].median(),
                "std": self.data[col].std()
            }
        
        info["numeric_stats"] = numeric_stats
        
        # Add unique counts for categorical columns
        categorical_stats = {}
        for col in self.data.select_dtypes(include=['object', 'category']).columns:
            categorical_stats[col] = {
                "unique_count": self.data[col].nunique(),
                "top_values": self.data[col].value_counts().head(5).to_dict()
            }
        
        info["categorical_stats"] = categorical_stats
        
        return info
    
    def get_sample(self, n=5):
        """
        Get a sample of the data
        
        Args:
            n (int): Number of rows to return
            
        Returns:
            DataFrame: Sample data
        """
        if self.data is None:
            return None
        
        if isinstance(self.data, dd.DataFrame):
            return self.data.head(n)
        else:
            return self.data.sample(n) if len(self.data) > n else self.data
    
    def validate_data(self, required_columns=None):
        """
        Validate the data against required columns
        
        Args:
            required_columns (list, optional): List of required columns
            
        Returns:
            dict: Validation results
        """
        if self.data is None:
            return {"valid": False, "error": "No data loaded"}
        
        if required_columns is None:
            required_columns = ['transaction_id', 'timestamp', 'amount', 'sender_id', 'receiver_id']
        
        validation_result = {
            "valid": True,
            "errors": [],
            "warnings": []
        }
        
        # Check for required columns
        missing_columns = [col for col in required_columns if col not in self.data.columns]
        if missing_columns:
            validation_result["valid"] = False
            validation_result["errors"].append(f"Missing required columns: {', '.join(missing_columns)}")
        
        # Check for duplicate transaction IDs
        if 'transaction_id' in self.data.columns:
            duplicate_ids = self.data['transaction_id'].duplicated().sum()
            if duplicate_ids > 0:
                validation_result["warnings"].append(f"Found {duplicate_ids} duplicate transaction IDs")
        
        # Check for missing values in critical columns
        critical_columns = ['transaction_id', 'timestamp', 'amount']
        for col in critical_columns:
            if col in self.data.columns:
                missing_count = self.data[col].isnull().sum()
                if missing_count > 0:
                    validation_result["warnings"].append(f"Found {missing_count} missing values in {col}")
        
        # Check for negative amounts
        if 'amount' in self.data.columns:
            negative_amounts = (self.data['amount'] < 0).sum()
            if negative_amounts > 0:
                validation_result["warnings"].append(f"Found {negative_amounts} transactions with negative amounts")
        
        # Check for future timestamps
        if 'timestamp' in self.data.columns and pd.api.types.is_datetime64_any_dtype(self.data['timestamp']):
            future_dates = (self.data['timestamp'] > datetime.now()).sum()
            if future_dates > 0:
                validation_result["warnings"].append(f"Found {future_dates} transactions with future timestamps")
        
        return validation_result
    
    def save_data(self, file_path, file_type=None, **kwargs):
        """
        Save the processed data to a file
        
        Args:
            file_path (str): Path to save the file
            file_type (str, optional): Type of file (csv, excel, parquet)
            **kwargs: Additional parameters for pandas save functions
        """
        if self.data is None:
            logger.error("No data to save")
            return
        
        try:
            # Determine file type if not provided
            if file_type is None:
                file_type = os.path.splitext(file_path)[1][1:].lower()
            
            # Save data based on file type
            if file_type == 'csv':
                self.data.to_csv(file_path, index=False, **kwargs)
            elif file_type in ['xlsx', 'xls']:
                self.data.to_excel(file_path, index=False, **kwargs)
            elif file_type == 'parquet':
                self.data.to_parquet(file_path, index=False, **kwargs)
            else:
                raise ValueError(f"Unsupported file type: {file_type}")
            
            logger.info(f"Data saved successfully to {file_path}")
            
        except Exception as e:
            logger.error(f"Error saving data: {str(e)}")
            raise

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\ingestion\__init__.py ===

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\ingestion\.ipynb_checkpoints\column_mapper-checkpoint.py ===
"""
Column Mapper Module
Handles intelligent mapping of user columns to expected format
"""
import pandas as pd
import numpy as np
import re
import logging
from collections import defaultdict
from difflib import SequenceMatcher
import yaml
import os
# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ColumnMapper:
    """
    Class for mapping user columns to expected format
    Uses AI-powered techniques to intelligently match columns
    """
    
    def __init__(self, config_path=None):
        """
        Initialize ColumnMapper
        
        Args:
            config_path (str, optional): Path to configuration file
        """
        self.expected_columns = self._get_expected_columns()
        self.synonyms = self._load_synonyms(config_path)
        self.patterns = self._load_patterns(config_path)
        self.mapping_history = []

    def get_expected_columns(self):
        """
        Return the list of expected column names
        
        Returns:
            list: Expected column names
        """
        return list(self.expected_columns.keys())

    
    def _get_expected_columns(self):
        """
        Get the expected columns for the fraud detection system
        
        Returns:
            dict: Expected columns with descriptions
        """
        return {
            'transaction_id': {
                'description': 'Unique identifier for the transaction',
                'data_type': 'string',
                'required': True
            },
            'timestamp': {
                'description': 'Date and time of the transaction',
                'data_type': 'datetime',
                'required': True
            },
            'amount': {
                'description': 'Transaction amount',
                'data_type': 'float',
                'required': True
            },
            'currency': {
                'description': 'Currency code (e.g., USD, EUR)',
                'data_type': 'string',
                'required': False
            },
            'sender_id': {
                'description': 'Identifier of the sender',
                'data_type': 'string',
                'required': True
            },
            'receiver_id': {
                'description': 'Identifier of the receiver',
                'data_type': 'string',
                'required': True
            },
            'sender_account_type': {
                'description': 'Type of sender account (e.g., personal, business)',
                'data_type': 'string',
                'required': False
            },
            'receiver_account_type': {
                'description': 'Type of receiver account (e.g., personal, business)',
                'data_type': 'string',
                'required': False
            },
            'sender_bank': {
                'description': 'Name of sender bank',
                'data_type': 'string',
                'required': False
            },
            'receiver_bank': {
                'description': 'Name of receiver bank',
                'data_type': 'string',
                'required': False
            },
            'sender_location': {
                'description': 'Location of sender (country, state, city)',
                'data_type': 'string',
                'required': False
            },
            'receiver_location': {
                'description': 'Location of receiver (country, state, city)',
                'data_type': 'string',
                'required': False
            },
            'transaction_type': {
                'description': 'Type of transaction (e.g., transfer, payment)',
                'data_type': 'string',
                'required': False
            },
            'transaction_category': {
                'description': 'Category of transaction (e.g., retail, services)',
                'data_type': 'string',
                'required': False
            },
            'merchant_id': {
                'description': 'Identifier of the merchant',
                'data_type': 'string',
                'required': False
            },
            'merchant_category': {
                'description': 'Category of the merchant',
                'data_type': 'string',
                'required': False
            },
            'ip_address': {
                'description': 'IP address used for the transaction',
                'data_type': 'string',
                'required': False
            },
            'device_id': {
                'description': 'Identifier of the device used',
                'data_type': 'string',
                'required': False
            },
            'description': {
                'description': 'Description of the transaction',
                'data_type': 'string',
                'required': False
            },
            'notes': {
                'description': 'Additional notes',
                'data_type': 'string',
                'required': False
            },
            'authorization_status': {
                'description': 'Authorization status (e.g., approved, declined)',
                'data_type': 'string',
                'required': False
            },
            'chargeback_flag': {
                'description': 'Whether the transaction was charged back',
                'data_type': 'boolean',
                'required': False
            },
            'fraud_flag': {
                'description': 'Whether the transaction is fraudulent (for supervised learning)',
                'data_type': 'boolean',
                'required': False
            }
        }
    
    def _load_synonyms(self, config_path=None):
        """
        Load synonyms for column names
        
        Args:
            config_path (str, optional): Path to configuration file
            
        Returns:
            dict: Synonyms for expected columns
        """
        # Default synonyms
        synonyms = {
            'transaction_id': ['id', 'transaction_id', 'tx_id', 'trans_id', 'reference', 'ref_no', 'transaction_no'],
            'timestamp': ['timestamp', 'date', 'time', 'datetime', 'trans_date', 'trans_time', 'transaction_date', 'transaction_time'],
            'amount': ['amount', 'value', 'sum', 'total', 'transaction_amount', 'amt', 'tx_amount'],
            'currency': ['currency', 'curr', 'ccy', 'currency_code'],
            'sender_id': ['sender_id', 'from_id', 'payer_id', 'source_id', 'originator_id'],
            'receiver_id': ['receiver_id', 'to_id', 'payee_id', 'destination_id', 'beneficiary_id'],
            'sender_account_type': ['sender_account_type', 'from_account_type', 'payer_account_type', 'source_account_type'],
            'receiver_account_type': ['receiver_account_type', 'to_account_type', 'payee_account_type', 'destination_account_type'],
            'sender_bank': ['sender_bank', 'from_bank', 'payer_bank', 'source_bank'],
            'receiver_bank': ['receiver_bank', 'to_bank', 'payee_bank', 'destination_bank'],
            'sender_location': ['sender_location', 'from_location', 'payer_location', 'source_location', 'sender_country', 'from_country'],
            'receiver_location': ['receiver_location', 'to_location', 'payee_location', 'destination_location', 'receiver_country', 'to_country'],
            'transaction_type': ['transaction_type', 'trans_type', 'type', 'tx_type'],
            'transaction_category': ['transaction_category', 'trans_category', 'category', 'tx_category'],
            'merchant_id': ['merchant_id', 'merchant', 'retailer_id', 'vendor_id'],
            'merchant_category': ['merchant_category', 'merchant_type', 'retailer_category', 'vendor_category'],
            'ip_address': ['ip_address', 'ip', 'ip_addr'],
            'device_id': ['device_id', 'device', 'device_identifier'],
            'description': ['description', 'desc', 'details', 'narrative'],
            'notes': ['notes', 'note', 'comments', 'remark'],
            'authorization_status': ['authorization_status', 'auth_status', 'status', 'approval_status'],
            'chargeback_flag': ['chargeback_flag', 'chargeback', 'is_chargeback'],
            'fraud_flag': ['fraud_flag', 'fraud', 'is_fraud', 'fraudulent']
        }
        
        # Try to load from config file
        if config_path and os.path.exists(config_path):
            try:
                with open(config_path, 'r') as f:
                    config = yaml.safe_load(f)
                    if 'column_synonyms' in config:
                        # Update with config synonyms
                        for col, syn_list in config['column_synonyms'].items():
                            if col in synonyms:
                                synonyms[col].extend(syn_list)
                            else:
                                synonyms[col] = syn_list
            except Exception as e:
                logger.warning(f"Error loading synonyms from config: {str(e)}")
        
        return synonyms
    
    def _load_patterns(self, config_path=None):
        """
        Load regex patterns for column names
        
        Args:
            config_path (str, optional): Path to configuration file
            
        Returns:
            dict: Regex patterns for expected columns
        """
        # Default patterns
        patterns = {
            'transaction_id': [r'transaction.?id', r'tx.?id', r'trans.?id', r'reference', r'ref.?(no|num)'],
            'timestamp': [r'timestamp', r'date.?time', r'trans.?date', r'trans.?time'],
            'amount': [r'amount', r'value', r'sum', r'total'],
            'currency': [r'currency', r'ccy', r'curr'],
            'sender_id': [r'sender.?id', r'from.?id', r'payer.?id', r'source.?id', r'originator.?id'],
            'receiver_id': [r'receiver.?id', r'to.?id', r'payee.?id', r'destination.?id', r'beneficiary.?id'],
            'sender_account_type': [r'sender.?account.?type', r'from.?account.?type', r'payer.?account.?type'],
            'receiver_account_type': [r'receiver.?account.?type', r'to.?account.?type', r'payee.?account.?type'],
            'sender_bank': [r'sender.?bank', r'from.?bank', r'payer.?bank', r'source.?bank'],
            'receiver_bank': [r'receiver.?bank', r'to.?bank', r'payee.?bank', r'destination.?bank'],
            'sender_location': [r'sender.?location', r'from.?location', r'payer.?location', r'source.?location', r'sender.?country'],
            'receiver_location': [r'receiver.?location', r'to.?location', r'payee.?location', r'destination.?location', r'receiver.?country'],
            'transaction_type': [r'transaction.?type', r'trans.?type', r'tx.?type'],
            'transaction_category': [r'transaction.?category', r'trans.?category', r'tx.?category'],
            'merchant_id': [r'merchant.?id', r'merchant', r'retailer.?id', r'vendor.?id'],
            'merchant_category': [r'merchant.?category', r'merchant.?type', r'retailer.?category'],
            'ip_address': [r'ip.?address', r'ip'],
            'device_id': [r'device.?id', r'device'],
            'description': [r'description', r'desc', r'details', r'narrative'],
            'notes': [r'notes?', r'comments?', r'remarks?'],
            'authorization_status': [r'authorization.?status', r'auth.?status', r'approval.?status'],
            'chargeback_flag': [r'chargeback.?(flag|is)'],
            'fraud_flag': [r'fraud.?(flag|is)', r'is.?fraud']
        }
        
        # Try to load from config file
        if config_path and os.path.exists(config_path):
            try:
                with open(config_path, 'r') as f:
                    config = yaml.safe_load(f)
                    if 'column_patterns' in config:
                        # Update with config patterns
                        for col, pat_list in config['column_patterns'].items():
                            if col in patterns:
                                patterns[col].extend(pat_list)
                            else:
                                patterns[col] = pat_list
            except Exception as e:
                logger.warning(f"Error loading patterns from config: {str(e)}")
        
        return patterns
    
    def auto_map_columns(self, user_columns, expected_columns=None):
        """
        Automatically map user columns to expected columns
        
        Args:
            user_columns (list): List of user column names
            expected_columns (list, optional): List of expected column names
            
        Returns:
            dict: Mapping from user columns to expected columns
        """
        if expected_columns is None:
            expected_columns = list(self.expected_columns.keys())
        
        mapping = {}
        used_columns = set()
        
        # First, try exact matches
        for user_col in user_columns:
            user_col_clean = self._clean_column_name(user_col)
            if user_col_clean in expected_columns and user_col_clean not in used_columns:
                mapping[user_col] = user_col_clean
                used_columns.add(user_col_clean)
        
        # Then, try synonym matches
        for user_col in user_columns:
            if user_col in mapping:
                continue
                
            user_col_clean = self._clean_column_name(user_col)
            
            for expected_col in expected_columns:
                if expected_col in used_columns:
                    continue
                    
                if user_col_clean in self.synonyms.get(expected_col, []):
                    mapping[user_col] = expected_col
                    used_columns.add(expected_col)
                    break
        
        # Then, try pattern matches
        for user_col in user_columns:
            if user_col in mapping:
                continue
                
            user_col_clean = self._clean_column_name(user_col)
            
            for expected_col in expected_columns:
                if expected_col in used_columns:
                    continue
                    
                for pattern in self.patterns.get(expected_col, []):
                    if re.search(pattern, user_col_clean, re.IGNORECASE):
                        mapping[user_col] = expected_col
                        used_columns.add(expected_col)
                        break
                else:
                    continue
                break
        
        # Finally, try fuzzy matching for remaining columns
        for user_col in user_columns:
            if user_col in mapping:
                continue
                
            user_col_clean = self._clean_column_name(user_col)
            
            best_match = None
            best_score = 0
            
            for expected_col in expected_columns:
                if expected_col in used_columns:
                    continue
                    
                # Calculate similarity score
                score = self._calculate_similarity(user_col_clean, expected_col)
                
                if score > best_score and score > 0.6:  # Threshold for fuzzy matching
                    best_score = score
                    best_match = expected_col
            
            if best_match:
                mapping[user_col] = best_match
                used_columns.add(best_match)
        
        # Store mapping history
        self.mapping_history.append({
            'timestamp': pd.Timestamp.now(),
            'user_columns': user_columns,
            'mapping': mapping
        })
        
        return mapping
    
    def _clean_column_name(self, column_name):
        """
        Clean column name for matching
        
        Args:
            column_name (str): Column name to clean
            
        Returns:
            str: Cleaned column name
        """
        # Convert to lowercase
        cleaned = column_name.lower()
        
        # Remove special characters and spaces
        cleaned = re.sub(r'[^a-z0-9]', '_', cleaned)
        
        # Remove consecutive underscores
        cleaned = re.sub(r'_+', '_', cleaned)
        
        # Remove leading and trailing underscores
        cleaned = cleaned.strip('_')
        
        return cleaned
    
    def _calculate_similarity(self, str1, str2):
        """
        Calculate similarity between two strings
        
        Args:
            str1 (str): First string
            str2 (str): Second string
            
        Returns:
            float: Similarity score (0-1)
        """
        # Use SequenceMatcher for similarity
        return SequenceMatcher(None, str1, str2).ratio()
    
    def apply_mapping(self, df, mapping):
        """
        Apply column mapping to a DataFrame
        
        Args:
            df (DataFrame): Input DataFrame
            mapping (dict): Column mapping
            
        Returns:
            DataFrame: DataFrame with mapped columns
        """
        try:
            # Create a copy of the DataFrame
            result_df = df.copy()
            
            # Create a new DataFrame with mapped columns
            mapped_df = pd.DataFrame()
            
            # Map columns
            for user_col, expected_col in mapping.items():
                if user_col in df.columns:
                    mapped_df[expected_col] = df[user_col]
            
            # Add unmapped columns with original names
            for col in df.columns:
                if col not in mapping:
                    mapped_df[col] = df[col]
            
            logger.info(f"Column mapping applied successfully. Mapped {len(mapping)} columns.")
            return mapped_df
            
        except Exception as e:
            logger.error(f"Error applying column mapping: {str(e)}")
            raise
    
    def validate_mapping(self, mapping, required_columns=None):
        """
        Validate a column mapping
        
        Args:
            mapping (dict): Column mapping
            required_columns (list, optional): List of required columns
            
        Returns:
            dict: Validation results
        """
        if required_columns is None:
            required_columns = [col for col, info in self.expected_columns.items() if info['required']]
        
        validation_result = {
            "valid": True,
            "errors": [],
            "warnings": [],
            "missing_required": [],
            "missing_optional": []
        }
        
        # Check for required columns
        mapped_columns = set(mapping.values())
        
        for col in required_columns:
            if col not in mapped_columns:
                validation_result["valid"] = False
                validation_result["missing_required"].append(col)
        
        # Check for optional columns
        optional_columns = [col for col in self.expected_columns if col not in required_columns]
        
        for col in optional_columns:
            if col not in mapped_columns:
                validation_result["missing_optional"].append(col)
        
        # Generate error messages
        if validation_result["missing_required"]:
            validation_result["errors"].append(
                f"Missing required columns: {', '.join(validation_result['missing_required'])}"
            )
        
        # Generate warning messages
        if validation_result["missing_optional"]:
            validation_result["warnings"].append(
                f"Missing optional columns: {', '.join(validation_result['missing_optional'])}"
            )
        
        return validation_result
    
    def save_mapping_template(self, file_path, mapping=None):
        """
        Save a mapping template to file
        
        Args:
            file_path (str): Path to save the template
            mapping (dict, optional): Mapping to save
        """
        if mapping is None:
            mapping = {}
        
        try:
            template = {
                "expected_columns": self.expected_columns,
                "synonyms": self.synonyms,
                "patterns": self.patterns,
                "current_mapping": mapping
            }
            
            with open(file_path, 'w') as f:
                yaml.dump(template, f, default_flow_style=False)
            
            logger.info(f"Mapping template saved to {file_path}")
            
        except Exception as e:
            logger.error(f"Error saving mapping template: {str(e)}")
            raise
    
    def load_mapping_template(self, file_path):
        """
        Load a mapping template from file
        
        Args:
            file_path (str): Path to the template file
            
        Returns:
            dict: Loaded mapping
        """
        try:
            with open(file_path, 'r') as f:
                template = yaml.safe_load(f)
            
            # Update instance variables
            if 'expected_columns' in template:
                self.expected_columns = template['expected_columns']
            
            if 'synonyms' in template:
                self.synonyms = template['synonyms']
            
            if 'patterns' in template:
                self.patterns = template['patterns']
            
            logger.info(f"Mapping template loaded from {file_path}")
            
            return template.get('current_mapping', {})
            
        except Exception as e:
            logger.error(f"Error loading mapping template: {str(e)}")
            raise
    
    def get_mapping_suggestions(self, user_columns, expected_columns=None):
        """
        Get mapping suggestions with confidence scores
        
        Args:
            user_columns (list): List of user column names
            expected_columns (list, optional): List of expected column names
            
        Returns:
            dict: Mapping suggestions with confidence scores
        """
        if expected_columns is None:
            expected_columns = list(self.expected_columns.keys())
        
        suggestions = {}
        
        for user_col in user_columns:
            user_col_clean = self._clean_column_name(user_col)
            
            col_suggestions = []
            
            # Check for exact matches
            if user_col_clean in expected_columns:
                col_suggestions.append({
                    "column": user_col_clean,
                    "confidence": 1.0,
                    "method": "exact_match"
                })
            
            # Check for synonyms
            for expected_col in expected_columns:
                if user_col_clean in self.synonyms.get(expected_col, []):
                    col_suggestions.append({
                        "column": expected_col,
                        "confidence": 0.9,
                        "method": "synonym_match"
                    })
            
            # Check for pattern matches
            for expected_col in expected_columns:
                for pattern in self.patterns.get(expected_col, []):
                    if re.search(pattern, user_col_clean, re.IGNORECASE):
                        col_suggestions.append({
                            "column": expected_col,
                            "confidence": 0.8,
                            "method": "pattern_match"
                        })
            
            # Check for fuzzy matches
            for expected_col in expected_columns:
                similarity = self._calculate_similarity(user_col_clean, expected_col)
                if similarity > 0.6:
                    col_suggestions.append({
                        "column": expected_col,
                        "confidence": similarity,
                        "method": "fuzzy_match"
                    })
            
            # Sort by confidence
            col_suggestions.sort(key=lambda x: x["confidence"], reverse=True)
            
            suggestions[user_col] = col_suggestions
        
        return suggestions

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\ingestion\.ipynb_checkpoints\data_loader-checkpoint.py ===
"""
Data Loader Module
Handles loading and preprocessing of transaction data
"""

import pandas as pd
import numpy as np
import dask.dataframe as dd
import os
import logging
from datetime import datetime
import chardet
import warnings
warnings.filterwarnings('ignore')

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class DataLoader:
    """
    Class for loading and preprocessing transaction data
    Supports CSV, Excel, and Parquet files
    Uses Dask for handling large files
    """
    
    def __init__(self, use_dask=True, chunk_size=100000):
        """
        Initialize DataLoader
        
        Args:
            use_dask (bool): Whether to use Dask for large files
            chunk_size (int): Chunk size for Dask processing
        """
        self.use_dask = use_dask
        self.chunk_size = chunk_size
        self.data = None
        self.original_data = None
        
    def load_data(self, file_path, file_type=None, **kwargs):
        """
        Load data from file
        
        Args:
            file_path (str): Path to the data file
            file_type (str, optional): Type of file (csv, excel, parquet)
            **kwargs: Additional parameters for pandas/dask read functions
            
        Returns:
            DataFrame: Loaded data
        """
        try:
            # Determine file type if not provided
            if file_type is None:
                file_type = os.path.splitext(file_path)[1][1:].lower()
            
            # Get file size to decide whether to use Dask
            file_size = os.path.getsize(file_path) / (1024 * 1024)  # Size in MB
            
            # Use Dask for large files
            if self.use_dask and file_size > 100:  # Use Dask for files > 100MB
                logger.info(f"Loading large file ({file_size:.2f} MB) using Dask")
                return self._load_with_dask(file_path, file_type, **kwargs)
            else:
                logger.info(f"Loading file ({file_size:.2f} MB) using pandas")
                return self._load_with_pandas(file_path, file_type, **kwargs)
                
        except Exception as e:
            logger.error(f"Error loading data: {str(e)}")
            raise
    
    def _load_with_pandas(self, file_path, file_type, **kwargs):
        """
        Load data using pandas
        
        Args:
            file_path (str): Path to the data file
            file_type (str): Type of file
            **kwargs: Additional parameters for pandas read functions
            
        Returns:
            DataFrame: Loaded data
        """
        try:
            # Detect encoding for CSV files
            if file_type == 'csv':
                with open(file_path, 'rb') as f:
                    result = chardet.detect(f.read())
                encoding = result['encoding']
                kwargs.setdefault('encoding', encoding)
            
            # Load data based on file type
            if file_type in ['csv', 'txt']:
                df = pd.read_csv(file_path, **kwargs)
            elif file_type in ['xlsx', 'xls']:
                df = pd.read_excel(file_path, **kwargs)
            elif file_type == 'parquet':
                df = pd.read_parquet(file_path, **kwargs)
            else:
                raise ValueError(f"Unsupported file type: {file_type}")
            
            # Store original data
            self.original_data = df.copy()
            
            # Basic preprocessing
            df = self._preprocess_data(df)
            
            self.data = df
            logger.info(f"Data loaded successfully. Shape: {df.shape}")
            return df
            
        except Exception as e:
            logger.error(f"Error loading data with pandas: {str(e)}")
            raise
    
    def _load_with_dask(self, file_path, file_type, **kwargs):
        """
        Load data using Dask
        
        Args:
            file_path (str): Path to the data file
            file_type (str): Type of file
            **kwargs: Additional parameters for dask read functions
            
        Returns:
            DataFrame: Loaded data
        """
        try:
            # Load data based on file type
            if file_type in ['csv', 'txt']:
                df = dd.read_csv(file_path, blocksize=self.chunk_size, **kwargs)
            elif file_type == 'parquet':
                df = dd.read_parquet(file_path, **kwargs)
            else:
                # For Excel files, we need to use pandas
                logger.warning("Dask does not support Excel files directly. Using pandas instead.")
                return self._load_with_pandas(file_path, file_type, **kwargs)
            
            # Store original data (compute a sample for inspection)
            self.original_data = df.head(1000)
            
            # Basic preprocessing
            df = df.map_partitions(self._preprocess_data)
            
            self.data = df
            logger.info(f"Data loaded successfully with Dask. Partitions: {df.npartitions}")
            return df
            
        except Exception as e:
            logger.error(f"Error loading data with Dask: {str(e)}")
            raise
    
    def _preprocess_data(self, df):
        """
        Basic preprocessing of the data
        
        Args:
            df (DataFrame): Input data
            
        Returns:
            DataFrame: Preprocessed data
        """
        try:
            # Remove duplicate rows
            initial_rows = len(df)
            df = df.drop_duplicates()
            removed_rows = initial_rows - len(df)
            if removed_rows > 0:
                logger.info(f"Removed {removed_rows} duplicate rows")
            
            # Handle missing values
            for col in df.columns:
                # For numeric columns, fill with median
                if pd.api.types.is_numeric_dtype(df[col]):
                    median_val = df[col].median()
                    df[col] = df[col].fillna(median_val)
                # For categorical columns, fill with mode or 'Unknown'
                elif pd.api.types.is_string_dtype(df[col]) or pd.api.types.is_categorical_dtype(df[col]):
                    mode_val = df[col].mode()
                    if len(mode_val) > 0:
                        df[col] = df[col].fillna(mode_val[0])
                    else:
                        df[col] = df[col].fillna('Unknown')
            
            # Convert timestamp columns to datetime
            for col in df.columns:
                if 'time' in col.lower() or 'date' in col.lower():
                    try:
                        df[col] = pd.to_datetime(df[col], errors='coerce')
                    except:
                        logger.warning(f"Could not convert {col} to datetime")
            
            # Convert amount columns to numeric
            for col in df.columns:
                if 'amount' in col.lower() or 'value' in col.lower() or 'sum' in col.lower():
                    try:
                        # Remove currency symbols and commas
                        if pd.api.types.is_string_dtype(df[col]):
                            df[col] = df[col].str.replace('[\$,‚Ç¨,¬£,¬•]', '', regex=True)
                            df[col] = df[col].str.replace(',', '')
                        df[col] = pd.to_numeric(df[col], errors='coerce')
                    except:
                        logger.warning(f"Could not convert {col} to numeric")
            
            # Convert ID columns to string
            for col in df.columns:
                if 'id' in col.lower():
                    df[col] = df[col].astype(str)
            
            # Convert boolean columns
            for col in df.columns:
                if df[col].dtype == 'object':
                    unique_vals = df[col].dropna().unique()
                    if len(unique_vals) <= 2 and all(val.lower() in ['true', 'false', 'yes', 'no', 'y', 'n', '0', '1'] for val in unique_vals):
                        df[col] = df[col].replace({
                            'true': True, 'yes': True, 'y': True, '1': True,
                            'false': False, 'no': False, 'n': False, '0': False
                        })
            
            # Convert categorical columns with many unique values to 'category' dtype
            for col in df.columns:
                if (pd.api.types.is_string_dtype(df[col]) and 
                    df[col].nunique() > 10 and 
                    df[col].nunique() < len(df) * 0.5):
                    df[col] = df[col].astype('category')
            
            logger.info("Data preprocessing completed")
            return df
            
        except Exception as e:
            logger.error(f"Error preprocessing data: {str(e)}")
            raise
    
    def get_data_info(self):
        """
        Get information about the loaded data
        
        Returns:
            dict: Data information
        """
        if self.data is None:
            return {"error": "No data loaded"}
        
        info = {
            "shape": self.data.shape,
            "columns": list(self.data.columns),
            "dtypes": dict(self.data.dtypes),
            "missing_values": dict(self.data.isnull().sum()),
            "memory_usage": dict(self.data.memory_usage(deep=True))
        }
        
        # Add statistics for numeric columns
        numeric_stats = {}
        for col in self.data.select_dtypes(include=[np.number]).columns:
            numeric_stats[col] = {
                "min": self.data[col].min(),
                "max": self.data[col].max(),
                "mean": self.data[col].mean(),
                "median": self.data[col].median(),
                "std": self.data[col].std()
            }
        
        info["numeric_stats"] = numeric_stats
        
        # Add unique counts for categorical columns
        categorical_stats = {}
        for col in self.data.select_dtypes(include=['object', 'category']).columns:
            categorical_stats[col] = {
                "unique_count": self.data[col].nunique(),
                "top_values": self.data[col].value_counts().head(5).to_dict()
            }
        
        info["categorical_stats"] = categorical_stats
        
        return info
    
    def get_sample(self, n=5):
        """
        Get a sample of the data
        
        Args:
            n (int): Number of rows to return
            
        Returns:
            DataFrame: Sample data
        """
        if self.data is None:
            return None
        
        if isinstance(self.data, dd.DataFrame):
            return self.data.head(n)
        else:
            return self.data.sample(n) if len(self.data) > n else self.data
    
    def validate_data(self, required_columns=None):
        """
        Validate the data against required columns
        
        Args:
            required_columns (list, optional): List of required columns
            
        Returns:
            dict: Validation results
        """
        if self.data is None:
            return {"valid": False, "error": "No data loaded"}
        
        if required_columns is None:
            required_columns = ['transaction_id', 'timestamp', 'amount', 'sender_id', 'receiver_id']
        
        validation_result = {
            "valid": True,
            "errors": [],
            "warnings": []
        }
        
        # Check for required columns
        missing_columns = [col for col in required_columns if col not in self.data.columns]
        if missing_columns:
            validation_result["valid"] = False
            validation_result["errors"].append(f"Missing required columns: {', '.join(missing_columns)}")
        
        # Check for duplicate transaction IDs
        if 'transaction_id' in self.data.columns:
            duplicate_ids = self.data['transaction_id'].duplicated().sum()
            if duplicate_ids > 0:
                validation_result["warnings"].append(f"Found {duplicate_ids} duplicate transaction IDs")
        
        # Check for missing values in critical columns
        critical_columns = ['transaction_id', 'timestamp', 'amount']
        for col in critical_columns:
            if col in self.data.columns:
                missing_count = self.data[col].isnull().sum()
                if missing_count > 0:
                    validation_result["warnings"].append(f"Found {missing_count} missing values in {col}")
        
        # Check for negative amounts
        if 'amount' in self.data.columns:
            negative_amounts = (self.data['amount'] < 0).sum()
            if negative_amounts > 0:
                validation_result["warnings"].append(f"Found {negative_amounts} transactions with negative amounts")
        
        # Check for future timestamps
        if 'timestamp' in self.data.columns and pd.api.types.is_datetime64_any_dtype(self.data['timestamp']):
            future_dates = (self.data['timestamp'] > datetime.now()).sum()
            if future_dates > 0:
                validation_result["warnings"].append(f"Found {future_dates} transactions with future timestamps")
        
        return validation_result
    
    def save_data(self, file_path, file_type=None, **kwargs):
        """
        Save the processed data to a file
        
        Args:
            file_path (str): Path to save the file
            file_type (str, optional): Type of file (csv, excel, parquet)
            **kwargs: Additional parameters for pandas save functions
        """
        if self.data is None:
            logger.error("No data to save")
            return
        
        try:
            # Determine file type if not provided
            if file_type is None:
                file_type = os.path.splitext(file_path)[1][1:].lower()
            
            # Save data based on file type
            if file_type == 'csv':
                self.data.to_csv(file_path, index=False, **kwargs)
            elif file_type in ['xlsx', 'xls']:
                self.data.to_excel(file_path, index=False, **kwargs)
            elif file_type == 'parquet':
                self.data.to_parquet(file_path, index=False, **kwargs)
            else:
                raise ValueError(f"Unsupported file type: {file_type}")
            
            logger.info(f"Data saved successfully to {file_path}")
            
        except Exception as e:
            logger.error(f"Error saving data: {str(e)}")
            raise

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\ingestion\.ipynb_checkpoints\__init__-checkpoint.py ===

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\ingestion\__pycache__\column_mapper.cpython-313.pyc ===
Û
    dÄöhü`  „                   Ûº   ï S r SSKrSSKrSSKrSSKrSSKJr  SSK	J
r
  SSKrSSKr\R                  " \R                  S9  \R                  " \5      r " S S5      rg)zU
Column Mapper Module
Handles intelligent mapping of user columns to expected format
È    N)⁄defaultdict)⁄SequenceMatcher)⁄levelc                   ÛÇ   ï \ rS rSrSrSS jrS rS rSS jrSS jr	SS	 jr
S
 rS rS rSS jrSS jrS rSS jrSrg)⁄ColumnMapperÈ   zm
Class for mapping user columns to expected format
Uses AI-powered techniques to intelligently match columns
Nc                 Ûî   ï U R                  5       U l        U R                  U5      U l        U R	                  U5      U l        / U l        g)z\
Initialize ColumnMapper

Args:
    config_path (str, optional): Path to configuration file
N)⁄_get_expected_columns⁄expected_columns⁄_load_synonyms⁄synonyms⁄_load_patterns⁄patterns⁄mapping_history)⁄self⁄config_paths     ⁄mC:\Users\Tranquil Abyss\fraud-detection-platform\app\..\src\fraud_detection_engine\ingestion\column_mapper.py⁄__init__⁄ColumnMapper.__init__   sA   Ä  !%◊ :— :” <à‘ÿ◊+—+®K”8àåÿ◊+—+®K”8àåÿ!à’Û    c                 ÛH   ï [        U R                  R                  5       5      $ )zT
Return the list of expected column names

Returns:
    list: Expected column names
)⁄listr   ⁄keys©r   s    r   ⁄get_expected_columns⁄!ColumnMapper.get_expected_columns#   s   Ä Ù êD◊)—)◊.—.”0”1–1r   c           
      Û6  ï 0 SSSSS._SSSSS._S	S
SSS._SSSSS._SSSSS._SSSSS._SSSSS._SSSSS._SSSSS._SSSSS._SSSSS._SSSSS._SS SSS._S!S"SSS._S#S$SSS._S%S&SSS._S'S(SSS._S)SSS.S*SSS.S+SSS.S,SSS.S-S.SS.S/S.SS.S0.E$ )1zp
Get the expected columns for the fraud detection system

Returns:
    dict: Expected columns with descriptions
⁄transaction_idz%Unique identifier for the transaction⁄stringT)⁄description⁄	data_type⁄required⁄	timestampz Date and time of the transaction⁄datetime⁄amountzTransaction amount⁄float⁄currencyzCurrency code (e.g., USD, EUR)F⁄	sender_idzIdentifier of the sender⁄receiver_idzIdentifier of the receiver⁄sender_account_typez1Type of sender account (e.g., personal, business)⁄receiver_account_typez3Type of receiver account (e.g., personal, business)⁄sender_bankzName of sender bank⁄receiver_bankzName of receiver bank⁄sender_locationz)Location of sender (country, state, city)⁄receiver_locationz+Location of receiver (country, state, city)⁄transaction_typez-Type of transaction (e.g., transfer, payment)⁄transaction_categoryz0Category of transaction (e.g., retail, services)⁄merchant_idzIdentifier of the merchant⁄merchant_categoryzCategory of the merchant⁄
ip_addressz#IP address used for the transactionzIdentifier of the device usedzDescription of the transactionzAdditional notesz/Authorization status (e.g., approved, declined)z(Whether the transaction was charged back⁄booleanz?Whether the transaction is fraudulent (for supervised learning)©⁄	device_idr    ⁄notes⁄authorization_status⁄chargeback_flag⁄
fraud_flag© r   s    r   r
   ⁄"ColumnMapper._get_expected_columns-   s5  Ä t
ÿÿFÿ%ÿ Òt
 ÿAÿ'ÿ Òt
 ÿ3ÿ$ÿ Òt
  ÿ?ÿ%ÿ!Ò!t
* ÿ9ÿ%ÿ Ò+t
4 ÿ;ÿ%ÿ Ò5t
> "ÿRÿ%ÿ!Ò$?t
H $ÿTÿ%ÿ!Ò&It
R ÿ4ÿ%ÿ!ÒSt
\ ÿ6ÿ%ÿ!Ò]t
f ÿJÿ%ÿ!Ò gt
p  ÿLÿ%ÿ!Ò"qt
z ÿNÿ%ÿ!Ò!{t
D #ÿQÿ%ÿ!Ò%Et
N ÿ;ÿ%ÿ!ÒOt
X  ÿ9ÿ%ÿ!Ò"Yt
b ÿDÿ%ÿ!Òct
n  ?ÿ%ÿ!Ò  @ÿ%ÿ!Ò  2ÿ%ÿ!Ò  Qÿ%ÿ!Ò%  Jÿ&ÿ!Ò   aÿ&ÿ!ÒÚ_t
 t	
r   c                 Û¶  ï 0 S/ SQ_S/ SQ_S/ SQ_S/ SQ_S	/ S
Q_S/ SQ_S/ SQ_S/ SQ_S/ SQ_S/ SQ_S/ SQ_S/ SQ_S/ SQ_S/ SQ_S/ SQ_S/ S Q_S!/ S"Q_/ S#Q/ S$Q/ S%Q/ S&Q/ S'Q/ S(QS).EnU(       aí  [         R                  R                  U5      (       an   [        US*5       n[        R
                  " U5      nS+U;   a:  US+   R                  5        H#  u  pVXR;   a  X%   R                  U5        M  XbU'   M%     S,S,S,5        U$ U$ ! , (       d  f       U$ = f! [         a,  n[        R                  S-[        U5       35         S,nAU$ S,nAff = f).zô
Load synonyms for column names

Args:
    config_path (str, optional): Path to configuration file
    
Returns:
    dict: Synonyms for expected columns
r   )⁄idr   ⁄tx_id⁄trans_id⁄	reference⁄ref_no⁄transaction_nor#   )r#   ⁄date⁄timer$   ⁄
trans_date⁄
trans_time⁄transaction_date⁄transaction_timer%   )r%   ⁄value⁄sum⁄total⁄transaction_amount⁄amt⁄	tx_amountr'   )r'   ⁄curr⁄ccy⁄currency_coder(   )r(   ⁄from_id⁄payer_id⁄	source_id⁄originator_idr)   )r)   ⁄to_id⁄payee_id⁄destination_id⁄beneficiary_idr*   )r*   ⁄from_account_type⁄payer_account_type⁄source_account_typer+   )r+   ⁄to_account_type⁄payee_account_type⁄destination_account_typer,   )r,   ⁄	from_bank⁄
payer_bank⁄source_bankr-   )r-   ⁄to_bank⁄
payee_bank⁄destination_bankr.   )r.   ⁄from_location⁄payer_location⁄source_location⁄sender_country⁄from_countryr/   )r/   ⁄to_location⁄payee_location⁄destination_location⁄receiver_country⁄
to_countryr0   )r0   ⁄
trans_type⁄type⁄tx_typer1   )r1   ⁄trans_category⁄category⁄tx_categoryr2   )r2   ⁄merchant⁄retailer_id⁄	vendor_idr3   )r3   ⁄merchant_type⁄retailer_category⁄vendor_categoryr4   )r4   ⁄ip⁄ip_addr)r7   ⁄device⁄device_identifier©r    ⁄desc⁄details⁄	narrative)r8   ⁄note⁄comments⁄remark)r9   ⁄auth_status⁄status⁄approval_status)r:   ⁄
chargeback⁄is_chargeback)r;   ⁄fraud⁄is_fraud⁄
fraudulentr6   ⁄r⁄column_synonymsNz$Error loading synonyms from config: ©⁄os⁄path⁄exists⁄open⁄yaml⁄	safe_load⁄items⁄extend⁄	Exception⁄logger⁄warning⁄str)r   r   r   ⁄f⁄config⁄col⁄syn_list⁄es           r   r   ⁄ColumnMapper._load_synonyms™   s“  Ä 
ÿ“t
‡Ú  G
 “c
 “D	

 “[
 “c
 "“#|
 $Ú  &D
 “T
 “[
 Ú   K
  Ú  "R
 “ U
 #“$i
 “R
   “!o!
" “9#
Ú$ F⁄J⁄<⁄$h⁄Q⁄KÚ/
àˆ6 ú2ü7ô7ü>ô>®+◊6—6P‹ò+†s‘+®q‹!ü^ö^®A”.êFÿ(®F”2‡-3–4E—-F◊-L—-L÷-NôMòCÿ"õÿ (°◊ 4— 4∞X÷ >‡08®£Ò	 .O˜	 , ààxà˜ ,‘+ à˚Ù Û P‹óë–!Eƒc»!√f¿X–N◊O–O‡à˚P˙s7   ¬D ¬%AD√<D ƒ
DƒD ƒD ƒ
Eƒ$!E≈Ec                 Û§  ï 0 S/ SQ_S/ SQ_S/ SQ_S/ SQ_S	/ S
Q_S/ SQ_S/ SQ_S/ SQ_S/ SQ_S/ SQ_S/ SQ_S/ SQ_S/ SQ_S/ SQ_S/ SQ_S/ S Q_S!S"S#/_S$S%// S&Q/ S'Q/ S(QS)/S*S+/S,.EnU(       aí  [         R                  R                  U5      (       an   [        US-5       n[        R
                  " U5      nS.U;   a:  US.   R                  5        H#  u  pVXR;   a  X%   R                  U5        M  XbU'   M%     S/S/S/5        U$ U$ ! , (       d  f       U$ = f! [         a,  n[        R                  S0[        U5       35         S/nAU$ S/nAff = f)1z•
Load regex patterns for column names

Args:
    config_path (str, optional): Path to configuration file
    
Returns:
    dict: Regex patterns for expected columns
r   )ztransaction.?idztx.?idz	trans.?idrB   zref.?(no|num)r#   )r#   z
date.?timeztrans.?dateztrans.?timer%   )r%   rK   rL   rM   r'   )r'   rR   rQ   r(   )z
sender.?idzfrom.?idz	payer.?idz
source.?idzoriginator.?idr)   )zreceiver.?idzto.?idz	payee.?idzdestination.?idzbeneficiary.?idr*   )zsender.?account.?typezfrom.?account.?typezpayer.?account.?typer+   )zreceiver.?account.?typezto.?account.?typezpayee.?account.?typer,   )zsender.?bankz
from.?bankzpayer.?bankzsource.?bankr-   )zreceiver.?bankzto.?bankzpayee.?bankzdestination.?bankr.   )zsender.?locationzfrom.?locationzpayer.?locationzsource.?locationzsender.?countryr/   )zreceiver.?locationzto.?locationzpayee.?locationzdestination.?locationzreceiver.?countryr0   )ztransaction.?typeztrans.?typeztx.?typer1   )ztransaction.?categoryztrans.?categoryztx.?categoryr2   )zmerchant.?idrx   zretailer.?idz
vendor.?idr3   )zmerchant.?categoryzmerchant.?typezretailer.?categoryr4   zip.?addressr~   z
device.?idrÄ   rÇ   )znotes?z	comments?zremarks?)zauthorization.?statuszauth.?statuszapproval.?statuszchargeback.?(flag|is)zfraud.?(flag|is)z	is.?fraudr6   rë   ⁄column_patternsNz$Error loading patterns from config: rì   )r   r   r   r†   r°   r¢   ⁄pat_listr§   s           r   r   ⁄ColumnMapper._load_patterns‡   s‹  Ä 
ÿ“k
‡“V
 “=
 “6	

 “e
 “m
 "“#n
 $“%p
 “\
 “c
 Ú   E
  Ú  "N
 “ S
 #“$c
 “Y
   “!b!
" ò>®5–1#
$ (®–3⁄N⁄;⁄$dÿ 8–9ÿ.∞–=Ú/
àˆ6 ú2ü7ô7ü>ô>®+◊6—6P‹ò+†s‘+®q‹!ü^ö^®A”.êFÿ(®F”2‡-3–4E—-F◊-L—-L÷-NôMòCÿ"õÿ (°◊ 4— 4∞X÷ >‡08®£Ò	 .O˜	 , ààxà˜ ,‘+ à˚Ù Û P‹óë–!Eƒc»!√f¿X–N◊O–O‡à˚P˙s7   ¬D ¬$AD√;D ƒ
DƒD ƒD ƒ
Eƒ#!E
≈
Ec                 ÛH  ï Uc#  [        U R                  R                  5       5      n0 n[        5       nU H7  nU R	                  U5      nXb;   d  M  Xd;  d  M"  XcU'   UR                  U5        M9     U Hb  nXS;   a  M
  U R	                  U5      nU HA  nXt;   a  M
  X`R                  R                  U/ 5      ;   d  M+  XsU'   UR                  U5          M`     Md     U Hí  nXS;   a  M
  U R	                  U5      nU Hq  nXt;   a  M
  U R                  R                  U/ 5       HD  n[        R                  " XÜ[        R                  5      (       d  M/  XsU'   UR                  U5          O   Mp    Mê     Mî     U Hq  nXS;   a  M
  U R	                  U5      nSn	Sn
U H.  nXt;   a  M
  U R                  Xg5      nX∫:î  d  M"  US:î  d  M*  Un
Un	M0     U	(       d  M\  XìU'   UR                  U	5        Ms     U R                  R                  [        R                   R#                  5       UUS.5        U$ )z˘
Automatically map user columns to expected columns

Args:
    user_columns (list): List of user column names
    expected_columns (list, optional): List of expected column names
    
Returns:
    dict: Mapping from user columns to expected columns
Nr   Á333333„?)r#   ⁄user_columns⁄mapping)r   r   r   ⁄set⁄_clean_column_name⁄addr   ⁄getr   ⁄re⁄search⁄
IGNORECASE⁄_calculate_similarityr   ⁄append⁄pd⁄	Timestamp⁄now)r   r¨   r   r≠   ⁄used_columns⁄user_col⁄user_col_clean⁄expected_col⁄pattern⁄
best_match⁄
best_score⁄scores               r   ⁄auto_map_columns⁄ColumnMapper.auto_map_columns  s  Ä  —#‹#†D◊$9—$9◊$>—$>”$@”A–‡à‹ìuàÛ %àHÿ!◊4—4∞X”>àNÿ’1∞n’6Xÿ$2ò—!ÿ◊ — †÷0Ò	 %Û %àHÿ”"Ÿ‡!◊4—4∞X”>àN„ 0êÿ”/Ÿ‡!ß]°]◊%6—%6∞|¿R”%H’Hÿ(4òH—%ÿ ◊$—$†\‘2⁄Û !1Ò %Û  %àHÿ”"Ÿ‡!◊4—4∞X”>àN„ 0êÿ”/Ÿ‡#ü}ô}◊0—0∞∏r÷BêG‹óyíy†º"ø-π-◊H”Hÿ,8†—)ÿ$◊(—(®‘6ŸÒ	  CÒ ⁄Û !1Ò %Û( %àHÿ”"Ÿ‡!◊4—4∞X”>àN‡àJÿàJ„ 0êÿ”/Ÿ ◊2—2∞>”Pê‡’%®%∞#≠+ÿ!&êJÿ!-íJÒ !1˜ àzÿ$.ò—!ÿ◊ — †÷,Ò- %2 	◊—◊#—#‹üô◊)—)”+ÿ(ÿÒ%
Ù 	 àr   c                 Û®   ï UR                  5       n[        R                  " SSU5      n[        R                  " SSU5      nUR                  S5      nU$ )z~
Clean column name for matching

Args:
    column_name (str): Column name to clean
    
Returns:
    str: Cleaned column name
z	[^a-z0-9]⁄_z_+)⁄lowerr≤   ⁄sub⁄strip)r   ⁄column_name⁄cleaneds      r   rØ   ⁄ColumnMapper._clean_column_namet  sO   Ä  ◊#—#”%àÙ ó&í&ò†s®G”4àÙ ó&í&ò††W”-à ó-ë-†”$à‡àr   c                 Û6   ï [        SX5      R                  5       $ )zú
Calculate similarity between two strings

Args:
    str1 (str): First string
    str2 (str): Second string
    
Returns:
    float: Similarity score (0-1)
N)r   ⁄ratio)r   ⁄str1⁄str2s      r   rµ   ⁄"ColumnMapper._calculate_similarityå  s   Ä Ù òt†T”0◊6—6”8–8r   c                 Ûû  ï  UR                  5       n[        R                  " 5       nUR                  5        H  u  pVXQR                  ;   d  M  X   XF'   M     UR                   H  nXr;  d  M
  X   XG'   M     [
        R                  S[        U5       S35        U$ ! [         a'  n[
        R                  S[        U5       35        e SnAff = f)zÆ
Apply column mapping to a DataFrame

Args:
    df (DataFrame): Input DataFrame
    mapping (dict): Column mapping
    
Returns:
    DataFrame: DataFrame with mapped columns
z,Column mapping applied successfully. Mapped z	 columns.zError applying column mapping: N)⁄copyr∑   ⁄	DataFramerö   ⁄columnsrù   ⁄info⁄lenrú   ⁄errorrü   )	r   ⁄dfr≠   ⁄	result_df⁄	mapped_dfrª   rΩ   r¢   r§   s	            r   ⁄apply_mapping⁄ColumnMapper.apply_mappingö  sµ   Ä 	‡üôõ	àIÙ üöõàI +2Ø-©-Æ/—&êÿüzôz’)ÿ.0©lêI”+Ò +:
 ózîzêÿ’%ÿ%'°WêIìNÒ "Ù èKâK–Fƒs»7√|¿n–T]–^‘_ÿ–¯‰Û 	‹èLâL–:º3∏qª6∏(–C‘Dÿ˚	˙s$   ÇAB ¡B ¡--B ¬
C¬%"C√Cc                 ÛÑ  ï Uc:  U R                   R                  5        VVs/ s H  u  p4US   (       d  M  UPM     nnnS/ / / / S.n[        UR                  5       5      nU H#  nX6;  d  M
  SUS'   US   R	                  U5        M%     U R                    Vs/ s H  o3U;  d  M
  UPM     nnU H  nX6;  d  M
  US   R	                  U5        M      US   (       a)  US   R	                  S	S
R                  US   5       35        US   (       a)  US   R	                  SS
R                  US   5       35        U$ s  snnf s  snf )z∞
Validate a column mapping

Args:
    mapping (dict): Column mapping
    required_columns (list, optional): List of required columns
    
Returns:
    dict: Validation results
r"   T)⁄valid⁄errors⁄warnings⁄missing_required⁄missing_optionalFrﬁ   r·   r‚   rﬂ   zMissing required columns: z, r‡   zMissing optional columns: )r   rö   rÆ   ⁄valuesr∂   ⁄join)r   r≠   ⁄required_columnsr¢   r’   ⁄validation_result⁄mapped_columns⁄optional_columnss           r   ⁄validate_mapping⁄ColumnMapper.validate_mappingΩ  sf  Ä  —#ÿ59◊5J—5J◊5P—5P‘5R‘g“5R©	®–VZ–[e’Vfß—5R–—g ÿÿÿ "ÿ "Ò
–Ù òWü^ô^”-”.à„#àCÿ’(ÿ-2–!†'—*ÿ!–"4—5◊<—<∏S÷AÒ $ ,0◊+@“+@”`“+@†C–O_—D_üC—+@––`„#àCÿ’(ÿ!–"4—5◊<—<∏S÷AÒ $
 –/◊0ÿòh—'◊.—.ÿ,®TØY©Y–7H–I[—7\”-]–,^–_Ù
 –/◊0ÿòj—)◊0—0ÿ,®TØY©Y–7H–I[—7\”-]–,^–_Ù !– ˘ÛK  h˘Ú& as   °D7µD7¬	D=¬#D=c                 Ûh  ï Uc  0 n U R                   U R                  U R                  US.n[        US5       n[        R
                  " X4SS9  SSS5        [        R                  SU 35        g! , (       d  f       N'= f! [         a'  n[        R                  S[        U5       35        e SnAff = f)zÖ
Save a mapping template to file

Args:
    file_path (str): Path to save the template
    mapping (dict, optional): Mapping to save
N)r   r   r   ⁄current_mapping⁄wF)⁄default_flow_stylezMapping template saved to zError saving mapping template: )r   r   r   ró   rò   ⁄dumprù   r’   rú   r◊   rü   )r   ⁄	file_pathr≠   ⁄templater†   r§   s         r   ⁄save_mapping_template⁄"ColumnMapper.save_mapping_template  s†   Ä  â?ÿàG	‡$(◊$9—$9ÿ üMôMÿ üMôMÿ#*Ò	àHÙ êi†‘%®‹ó	í	ò(∏%“@˜ &Ù èKâK–4∞Y∞K–@’A˜ &’%˚Ù
 Û 	‹èLâL–:º3∏qª6∏(–C‘Dÿ˚	˙s.   á1B  ∏A/¡ B  ¡/
A=¡9B  ¬ 
B1¬
"B,¬,B1c                 Ûò  ï  [        US5       n[        R                  " U5      nSSS5        SW;   a
  US   U l        SU;   a
  US   U l        SU;   a
  US   U l        [        R                  SU 35        UR                  S0 5      $ ! , (       d  f       Nh= f! [         a'  n[        R                  S[        U5       35        e SnAff = f)	zÄ
Load a mapping template from file

Args:
    file_path (str): Path to the template file
    
Returns:
    dict: Loaded mapping
rë   Nr   r   r   zMapping template loaded from rÏ   z Error loading mapping template: )ró   rò   rô   r   r   r   rù   r’   r±   rú   r◊   rü   )r   r   r†   rÒ   r§   s        r   ⁄load_mapping_template⁄"ColumnMapper.load_mapping_template  s¡   Ä 	‹êi†‘%®‹ü>ö>®!”,ê˜ & "†X”-ÿ(0–1C—(Dê‘%‡òX”%ÿ (®— 4êî‡òX”%ÿ (®— 4êî‰èKâK–7∏	∞{–C‘D‡ó<ë<– 1∞2”6–6˜ &’%˚Ù" Û 	‹èLâL–;ºC¿ªF∏8–D‘Eÿ˚	˙s.   ÇB éB•A!B ¬
B¬B ¬
C	¬""C√C	c           	      Û∂  ï Uc#  [        U R                  R                  5       5      n0 nU GH)  nU R                  U5      n/ nXR;   a  UR	                  USSS.5        U H9  nXPR
                  R                  U/ 5      ;   d  M$  UR	                  USSS.5        M;     U Hg  nU R                  R                  U/ 5       HD  n[        R                  " XÖ[        R                  5      (       d  M/  UR	                  USSS.5        MF     Mi     U H1  nU R                  XW5      n	U	S:î  d  M  UR	                  UU	S	S.5        M3     UR                  S
 SS9  XcU'   GM,     U$ )zÚ
Get mapping suggestions with confidence scores

Args:
    user_columns (list): List of user column names
    expected_columns (list, optional): List of expected column names
    
Returns:
    dict: Mapping suggestions with confidence scores
g      ?⁄exact_match)⁄column⁄
confidence⁄methodgÕÃÃÃÃÃÏ?⁄synonym_matchgöôôôôôÈ?⁄pattern_matchr´   ⁄fuzzy_matchc                 Û   ï U S   $ )Nr˙   r<   )⁄xs    r   ⁄<lambda>⁄6ColumnMapper.get_mapping_suggestions.<locals>.<lambda>g  s   Ä ®q∞™r   T)⁄key⁄reverse)r   r   r   rØ   r∂   r   r±   r   r≤   r≥   r¥   rµ   ⁄sort)
r   r¨   r   ⁄suggestionsrª   rº   ⁄col_suggestionsrΩ   ræ   ⁄
similaritys
             r   ⁄get_mapping_suggestions⁄$ColumnMapper.get_mapping_suggestions,  sl  Ä  —#‹#†D◊$9—$9◊$>—$>”$@”A–‡à‰$àHÿ!◊4—4∞X”>àN‡ àO ”1ÿ◊&—&ÿ,ÿ"%ÿ+Ò(Ù Û !1êÿ!ß]°]◊%6—%6∞|¿R”%H’Hÿ#◊*—*ÿ".ÿ&)ÿ"1Ò,ˆ Ò !1Û !1êÿ#ü}ô}◊0—0∞∏r÷BêG‹óyíy†º"ø-π-◊H”Hÿ'◊.—.ÿ&2ÿ*-ÿ&5Ò0ˆ Û  CÒ !1Û !1êÿ!◊7—7∏”Uê
ÿ†’#ÿ#◊*—*ÿ".ÿ&0ÿ"/Ò,ˆ Ò !1 ◊ — —%>»– —M‡$3ò‘!Ò[ %^ –r   )r   r   r   r   )N)⁄__name__⁄
__module__⁄__qualname__⁄__firstlineno__⁄__doc__r   r   r
   r   r   r¬   rØ   rµ   r€   rÈ   rÚ   rı   r	  ⁄__static_attributes__r<   r   r   r   r      sQ   Ü ÒÙ

"Ú2Ú{
Ùz4Ùl4Ùl\Ú|Ú09Ú!ÙF1!ÙfÚ8˜@?r   r   )r  ⁄pandasr∑   ⁄numpy⁄npr≤   ⁄logging⁄collectionsr   ⁄difflibr   rò   rî   ⁄basicConfig⁄INFO⁄	getLoggerr  rù   r   r<   r   r   ⁄<module>r     sR   ÒÛ € € 	€ › #› #€ € 	‡ ◊ “ ò'ü,ô,“ 'ÿ	◊	“	ò8”	$Ä˜Z	Ú Z	r   

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\ingestion\__pycache__\column_mapper.cpython-39.pyc ===
a
    èöhü`  „                   @   sv   d Z ddlZddlZddlZddlZddlmZ ddl	m
Z
 ddlZddlZejejdç e†e°ZG ddÑ dÉZdS )zU
Column Mapper Module
Handles intelligent mapping of user columns to expected format
È    N)⁄defaultdict)⁄SequenceMatcher)⁄levelc                   @   sÜ   e Zd ZdZdddÑZddÑ ZddÑ Zdd	d
ÑZdddÑZd ddÑZ	ddÑ Z
ddÑ ZddÑ Zd!ddÑZd"ddÑZddÑ Zd#ddÑZdS )$⁄ColumnMapperzy
    Class for mapping user columns to expected format
    Uses AI-powered techniques to intelligently match columns
    Nc                 C   s,   | † ° | _| †|°| _| †|°| _g | _dS )zÑ
        Initialize ColumnMapper
        
        Args:
            config_path (str, optional): Path to configuration file
        N)⁄_get_expected_columns⁄expected_columns⁄_load_synonyms⁄synonyms⁄_load_patterns⁄patterns⁄mapping_history)⁄self⁄config_path© r   ˙mC:\Users\Tranquil Abyss\fraud-detection-platform\app\..\src\fraud_detection_engine\ingestion\column_mapper.py⁄__init__   s    
zColumnMapper.__init__c                 C   s   t | j†° ÉS )z|
        Return the list of expected column names
        
        Returns:
            list: Expected column names
        )⁄listr   ⁄keys©r   r   r   r   ⁄get_expected_columns#   s    z!ColumnMapper.get_expected_columnsc                 C   sÏ   ddddúddddúddddúd	dd
dúddddúddddúddd
dúddd
dúddd
dúddd
dúddd
dúddd
dúddd
dúddd
dúddd
dúddd
dúddd
dúddd
dúddd
dúddd
dúddd
dúddd
dúddd
dúdúS ) zò
        Get the expected columns for the fraud detection system
        
        Returns:
            dict: Expected columns with descriptions
        z%Unique identifier for the transaction⁄stringT)⁄description⁄	data_type⁄requiredz Date and time of the transaction⁄datetimezTransaction amount⁄floatzCurrency code (e.g., USD, EUR)FzIdentifier of the senderzIdentifier of the receiverz1Type of sender account (e.g., personal, business)z3Type of receiver account (e.g., personal, business)zName of sender bankzName of receiver bankz)Location of sender (country, state, city)z+Location of receiver (country, state, city)z-Type of transaction (e.g., transfer, payment)z0Category of transaction (e.g., retail, services)zIdentifier of the merchantzCategory of the merchantz#IP address used for the transactionzIdentifier of the device usedzDescription of the transactionzAdditional notesz/Authorization status (e.g., approved, declined)z(Whether the transaction was charged back⁄booleanz?Whether the transaction is fraudulent (for supervised learning)©⁄transaction_id⁄	timestamp⁄amount⁄currency⁄	sender_id⁄receiver_id⁄sender_account_type⁄receiver_account_type⁄sender_bank⁄receiver_bank⁄sender_location⁄receiver_location⁄transaction_type⁄transaction_category⁄merchant_id⁄merchant_category⁄
ip_address⁄	device_idr   ⁄notes⁄authorization_status⁄chargeback_flag⁄
fraud_flagr   r   r   r   r   r   -   s∫    	˝˝˝˝˝˝˝˝˝˝˝˝˝˝˝˝˝˝˝˝˝˝˝ëz"ColumnMapper._get_expected_columnsc                 C   s\  g d¢g d¢g d¢g d¢g d¢g d¢g d¢g d¢g d	¢g d
¢g d¢g d¢g d¢g d¢g d¢g d¢g d¢g d¢g d¢g d¢g d¢g d¢g d¢dú}|êrXt j†|°êrXzxt|dÉèX}t†|°}d|v r˙|d †° D ](\}}||v r|| †|° q–|||< q–W d  É n1 ês0    Y  W n: têyV } z t	†
dt|Éõ ù° W Y d}~n
d}~0 0 |S )zŸ
        Load synonyms for column names
        
        Args:
            config_path (str, optional): Path to configuration file
            
        Returns:
            dict: Synonyms for expected columns
        )⁄idr   Ztx_idZtrans_id⁄	referenceZref_noZtransaction_no)r   ⁄date⁄timer   Z
trans_dateZ
trans_timeZtransaction_dateZtransaction_time)r    ⁄value⁄sum⁄totalZtransaction_amount⁄amtZ	tx_amount)r!   ⁄curr⁄ccyZcurrency_code)r"   Zfrom_idZpayer_idZ	source_idZoriginator_id)r#   Zto_idZpayee_idZdestination_idZbeneficiary_id)r$   Zfrom_account_typeZpayer_account_typeZsource_account_type)r%   Zto_account_typeZpayee_account_typeZdestination_account_type)r&   Z	from_bankZ
payer_bankZsource_bank)r'   Zto_bankZ
payee_bankZdestination_bank)r(   Zfrom_locationZpayer_locationZsource_locationZsender_countryZfrom_country)r)   Zto_locationZpayee_locationZdestination_locationZreceiver_countryZ
to_country)r*   Z
trans_type⁄typeZtx_type)r+   Ztrans_category⁄categoryZtx_category)r,   ⁄merchantZretailer_idZ	vendor_id)r-   Zmerchant_typeZretailer_categoryZvendor_category)r.   ⁄ip⁄ip_addr)r/   ⁄deviceZdevice_identifier©r   ⁄desc⁄detailsZ	narrative)r0   ⁄note⁄commentsZremark)r1   Zauth_status⁄statusZapproval_status)r2   Z
chargebackZis_chargeback)r3   Zfraud⁄is_fraudZ
fraudulentr   ⁄rZcolumn_synonymsNz$Error loading synonyms from config: ©⁄os⁄path⁄exists⁄open⁄yaml⁄	safe_load⁄items⁄extend⁄	Exception⁄logger⁄warning⁄str)r   r   r	   ⁄f⁄config⁄colZsyn_list⁄er   r   r   r   ™   sH    È
.*zColumnMapper._load_synonymsc                 C   sZ  g d¢g d¢g d¢g d¢g d¢g d¢g d¢g d¢g d	¢g d
¢g d¢g d¢g d¢g d¢g d¢g d¢ddgddgg d¢g d¢g d¢dgddgdú}|êrVt j†|°êrVzxt|dÉèX}t†|°}d|v r¯|d †° D ](\}}||v rÓ|| †|° qŒ|||< qŒW d  É n1 ês0    Y  W n: têyT } z t	†
dt|Éõ ù° W Y d}~n
d}~0 0 |S ) zÂ
        Load regex patterns for column names
        
        Args:
            config_path (str, optional): Path to configuration file
            
        Returns:
            dict: Regex patterns for expected columns
        )ztransaction.?idztx.?idz	trans.?idr5   zref.?(no|num))r   z
date.?timeztrans.?dateztrans.?time)r    r8   r9   r:   )r!   r=   r<   )z
sender.?idzfrom.?idz	payer.?idz
source.?idzoriginator.?id)zreceiver.?idzto.?idz	payee.?idzdestination.?idzbeneficiary.?id)zsender.?account.?typezfrom.?account.?typezpayer.?account.?type)zreceiver.?account.?typezto.?account.?typezpayee.?account.?type)zsender.?bankz
from.?bankzpayer.?bankzsource.?bank)zreceiver.?bankzto.?bankzpayee.?bankzdestination.?bank)zsender.?locationzfrom.?locationzpayer.?locationzsource.?locationzsender.?country)zreceiver.?locationzto.?locationzpayee.?locationzdestination.?locationzreceiver.?country)ztransaction.?typeztrans.?typeztx.?type)ztransaction.?categoryztrans.?categoryztx.?category)zmerchant.?idr@   zretailer.?idz
vendor.?id)zmerchant.?categoryzmerchant.?typezretailer.?categoryzip.?addressrA   z
device.?idrC   rD   )znotes?z	comments?zremarks?)zauthorization.?statuszauth.?statuszapproval.?statuszchargeback.?(flag|is)zfraud.?(flag|is)z	is.?fraudr   rK   Zcolumn_patternsNz$Error loading patterns from config: rL   )r   r   r   rY   rZ   r[   Zpat_listr\   r   r   r   r
   ‡   sH    È
.*zColumnMapper._load_patternsc                 C   s»  |du rt | j†° É}i }tÉ }|D ]0}| †|°}||v r$||vr$|||< |†|° q$|D ]T}||v rhqZ| †|°}|D ]6}||v rÑqv|| j†|g °v rv|||< |†|°  qZqvqZ|D ]n}||v r¬q¥| †|°}|D ]P}||v rﬁq–| j†|g °D ],}t	†
||t	j°rÏ|||< |†|°  êqqÏq– q¥q–q¥|D ]Ä}||v êr:êq(| †|°}d}	d}
|D ]<}||v êrbêqP| †||°}||
kêrP|dkêrP|}
|}	êqP|	êr(|	||< |†|	° êq(| j†tj†° ||dú° |S )aA  
        Automatically map user columns to expected columns
        
        Args:
            user_columns (list): List of user column names
            expected_columns (list, optional): List of expected column names
            
        Returns:
            dict: Mapping from user columns to expected columns
        Nr   Á333333„?)r   ⁄user_columns⁄mapping)r   r   r   ⁄set⁄_clean_column_name⁄addr	   ⁄getr   ⁄re⁄search⁄
IGNORECASE⁄_calculate_similarityr   ⁄append⁄pd⁄	Timestamp⁄now)r   r^   r   r_   ⁄used_columns⁄user_col⁄user_col_clean⁄expected_col⁄patternZ
best_matchZ
best_score⁄scorer   r   r   ⁄auto_map_columns  sp    







˝zColumnMapper.auto_map_columnsc                 C   s2   |† ° }t†dd|°}t†dd|°}|†d°}|S )zæ
        Clean column name for matching
        
        Args:
            column_name (str): Column name to clean
            
        Returns:
            str: Cleaned column name
        z	[^a-z0-9]⁄_z_+)⁄lowerrd   ⁄sub⁄strip)r   ⁄column_name⁄cleanedr   r   r   ra   t  s
    
zColumnMapper._clean_column_namec                 C   s   t d||É†° S )z‰
        Calculate similarity between two strings
        
        Args:
            str1 (str): First string
            str2 (str): Second string
            
        Returns:
            float: Similarity score (0-1)
        N)r   ⁄ratio)r   ⁄str1⁄str2r   r   r   rg   å  s    z"ColumnMapper._calculate_similarityc           	   
   C   s¥   zt|† ° }t†° }|†° D ]\}}||jv r|| ||< q|jD ]}||vr@|| ||< q@t†dt|Éõ dù° |W S  tyÆ } z"t†	dt
|Éõ ù° Ç W Y d}~n
d}~0 0 dS )zˆ
        Apply column mapping to a DataFrame
        
        Args:
            df (DataFrame): Input DataFrame
            mapping (dict): Column mapping
            
        Returns:
            DataFrame: DataFrame with mapped columns
        z,Column mapping applied successfully. Mapped z	 columns.zError applying column mapping: N)⁄copyri   ⁄	DataFramerS   ⁄columnsrV   ⁄info⁄lenrU   ⁄errorrX   )	r   ⁄dfr_   Z	result_dfZ	mapped_dfrm   ro   r[   r\   r   r   r   ⁄apply_mappingö  s    

zColumnMapper.apply_mappingc                    s‰   à du rddÑ | j †° D Éâ dg g g g dú}t|†° É}à D ]"}||vr<d|d< |d †|° q<á fd	dÑ| j D É}|D ]}||vrx|d
 †|° qx|d r∫|d †dd†|d °õ ù° |d
 r‡|d †dd†|d
 °õ ù° |S )z¯
        Validate a column mapping
        
        Args:
            mapping (dict): Column mapping
            required_columns (list, optional): List of required columns
            
        Returns:
            dict: Validation results
        Nc                 S   s   g | ]\}}|d  r|ëqS )r   r   )⁄.0r[   r   r   r   r   ⁄
<listcomp>…  Û    z1ColumnMapper.validate_mapping.<locals>.<listcomp>T)⁄valid⁄errors⁄warnings⁄missing_required⁄missing_optionalFrá   rä   c                    s   g | ]}|à vr|ëqS r   r   )rÑ   r[   ©⁄required_columnsr   r   rÖ   ‹  rÜ   rã   rà   zMissing required columns: z, râ   zMissing optional columns: )r   rS   r`   ⁄valuesrh   ⁄join)r   r_   rç   ⁄validation_resultZmapped_columnsr[   Zoptional_columnsr   rå   r   ⁄validate_mappingΩ  s4    ˚	ˇˇzColumnMapper.validate_mappingc              
   C   sÆ   |du ri }zb| j | j| j|dú}t|dÉè }tj||ddç W d  É n1 sR0    Y  t†d|õ ù° W n: ty® } z"t†	dt
|Éõ ù° Ç W Y d}~n
d}~0 0 dS )zµ
        Save a mapping template to file
        
        Args:
            file_path (str): Path to save the template
            mapping (dict, optional): Mapping to save
        N)r   r	   r   ⁄current_mapping⁄wF)⁄default_flow_stylezMapping template saved to zError saving mapping template: )r   r	   r   rP   rQ   ⁄dumprV   r   rU   rÅ   rX   )r   ⁄	file_pathr_   ⁄templaterY   r\   r   r   r   ⁄save_mapping_template  s    ¸.z"ColumnMapper.save_mapping_templatec              
   C   s»   zàt |dÉè}t†|°}W d  É n1 s,0    Y  d|v rH|d | _d|v rZ|d | _d|v rl|d | _t†d|õ ù° |†di °W S  t	y¬ } z"t†
dt|Éõ ù° Ç W Y d}~n
d}~0 0 dS )	z¿
        Load a mapping template from file
        
        Args:
            file_path (str): Path to the template file
            
        Returns:
            dict: Loaded mapping
        rK   Nr   r	   r   zMapping template loaded from rí   z Error loading mapping template: )rP   rQ   rR   r   r	   r   rV   r   rc   rU   rÅ   rX   )r   rñ   rY   ró   r\   r   r   r   ⁄load_mapping_template  s    
(


z"ColumnMapper.load_mapping_templatec           
   	   C   s  |du rt | j†° É}i }|D ]‰}| †|°}g }||v rJ|†|dddú° |D ](}|| j†|g °v rN|†|dddú° qN|D ]:}| j†|g °D ]&}t†	||tj
°ré|†|dddú° qéq||D ]*}| †||°}	|	d	krº|†||	d
dú° qº|jddÑ ddç |||< q|S )a:  
        Get mapping suggestions with confidence scores
        
        Args:
            user_columns (list): List of user column names
            expected_columns (list, optional): List of expected column names
            
        Returns:
            dict: Mapping suggestions with confidence scores
        Ng      ?Zexact_match)⁄column⁄
confidence⁄methodgÕÃÃÃÃÃÏ?Zsynonym_matchgöôôôôôÈ?Zpattern_matchr]   Zfuzzy_matchc                 S   s   | d S )Nrõ   r   )⁄xr   r   r   ⁄<lambda>g  rÜ   z6ColumnMapper.get_mapping_suggestions.<locals>.<lambda>T)⁄key⁄reverse)r   r   r   ra   rh   r	   rc   r   rd   re   rf   rg   ⁄sort)
r   r^   r   Zsuggestionsrm   rn   Zcol_suggestionsro   rp   Z
similarityr   r   r   ⁄get_mapping_suggestions,  sL    
˝˝
˝˝

z$ColumnMapper.get_mapping_suggestions)N)N)N)N)N)N)N)⁄__name__⁄
__module__⁄__qualname__⁄__doc__r   r   r   r   r
   rr   ra   rg   rÉ   rë   rò   rô   r¢   r   r   r   r   r      s   

}
6
6
^#
3
 r   )r¶   ⁄pandasri   ⁄numpy⁄nprd   ⁄logging⁄collectionsr   ⁄difflibr   rQ   rM   ⁄basicConfig⁄INFO⁄	getLoggerr£   rV   r   r   r   r   r   ⁄<module>   s   


=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\ingestion\__pycache__\data_loader.cpython-313.pyc ===
Û
    dÄöhQ:  „                   Û‡   ï S r SSKrSSKrSSKJr  SSKrSSK	r	SSK
J
r
  SSKrSSKr\R                  " S5        \	R                  " \	R                  S9  \	R                   " \5      r " S S5      rg)zJ
Data Loader Module
Handles loading and preprocessing of transaction data
È    N)⁄datetime⁄ignore)⁄levelc                   Ûb   ï \ rS rSrSrSS jrSS jrS rS rS r	S	 r
SS
 jrSS jrSS jrSrg)⁄
DataLoaderÈ   zÄ
Class for loading and preprocessing transaction data
Supports CSV, Excel, and Parquet files
Uses Dask for handling large files
c                 Û8   ï Xl         X l        SU l        SU l        g)zå
Initialize DataLoader

Args:
    use_dask (bool): Whether to use Dask for large files
    chunk_size (int): Chunk size for Dask processing
N)⁄use_dask⁄
chunk_size⁄data⁄original_data)⁄selfr
   r   s      ⁄kC:\Users\Tranquil Abyss\fraud-detection-platform\app\..\src\fraud_detection_engine\ingestion\data_loader.py⁄__init__⁄DataLoader.__init__   s   Ä  !åÿ$åÿàå	ÿ!à’Û    Nc                 Û˛  ï  Uc3  [         R                  R                  U5      S   SS R                  5       n[         R                  R	                  U5      S-  nU R
                  (       a3  US:î  a-  [        R                  SUS S35        U R                  " X40 UD6$ [        R                  SUS S	35        U R                  " X40 UD6$ ! [         a'  n[        R                  S
[        U5       35        e SnAff = f)zı
Load data from file

Args:
    file_path (str): Path to the data file
    file_type (str, optional): Type of file (csv, excel, parquet)
    **kwargs: Additional parameters for pandas/dask read functions
    
Returns:
    DataFrame: Loaded data
NÈ   i   Èd   zLoading large file (z.2fz MB) using DaskzLoading file (z MB) using pandaszError loading data: )⁄os⁄path⁄splitext⁄lower⁄getsizer
   ⁄logger⁄info⁄_load_with_dask⁄_load_with_pandas⁄	Exception⁄error⁄str)r   ⁄	file_path⁄	file_type⁄kwargs⁄	file_size⁄es         r   ⁄	load_data⁄DataLoader.load_data(   sÂ   Ä 	‡— ‹üGôG◊,—,®Y”7∏—:∏1∏2–>◊D—D”Fê	Ù üôüô®	”2∞k—BàI è}è}†®S£‹óë–2∞9∏S∞/¿–Q‘Rÿ◊+“+®I—K¿F—K–K‰óëòn®Y∞s®O–;L–M‘Nÿ◊-“-®i—M¿f—M–M¯‰Û 	‹èLâL–/¥∞A≥®x–8‘9ÿ˚	˙s   ÇBC ¬,C √
C<√"C7√7C<c                 Û™  ï  US:X  aP  [        US5       n[        R                  " UR                  5       5      nSSS5        WS   nUR	                  SU5        US;   a  [
        R                  " U40 UD6nOJUS;   a  [
        R                  " U40 UD6nO,US:X  a  [
        R                  " U40 UD6nO[        SU 35      eUR                  5       U l        U R                  U5      nXpl        [        R                  S	UR                    35        U$ ! , (       d  f       N›= f! ["         a'  n[        R%                  S
['        U5       35        e SnAff = f)z”
Load data using pandas

Args:
    file_path (str): Path to the data file
    file_type (str): Type of file
    **kwargs: Additional parameters for pandas read functions
    
Returns:
    DataFrame: Loaded data
⁄csv⁄rbN⁄encoding©r*   ⁄txt©⁄xlsx⁄xls⁄parquet˙Unsupported file type: z!Data loaded successfully. Shape: z Error loading data with pandas: )⁄open⁄chardet⁄detect⁄read⁄
setdefault⁄pd⁄read_csv⁄
read_excel⁄read_parquet⁄
ValueError⁄copyr   ⁄_preprocess_datar   r   r   ⁄shaper   r    r!   )	r   r"   r#   r$   ⁄f⁄resultr,   ⁄dfr&   s	            r   r   ⁄DataLoader._load_with_pandasH   s-  Ä 	‡òE”!‹ò)†T‘*®a‹$ü^ö^®AØF©F´H”5êF˜ +‡!†*—-êÿ◊!—!†*®h‘7 òN”*‹ó[í[†—5®f—5ëÿòo”-‹ó]í]†9—7∞—7ëÿòi”'‹ó_í_†Y—9∞&—9ë‰ –#:∏9∏+–!F”G–G "$ß°£àD‘ ◊&—&†r”*àB‡åI‹èKâK–;∏BøHπH∏:–F‘GÿàI˜/ +’*˚Ù2 Û 	‹èLâL–;ºC¿ªF∏8–D‘Eÿ˚	˙s.   ÇD! î%DπCD! ƒ
DƒD! ƒ!
Eƒ+"E≈Ec                 Û  ï  US;   a$  [         R                  " U4SU R                  0UD6nOFUS:X  a  [         R                  " U40 UD6nO([        R                  S5        U R                  " X40 UD6$ UR                  S5      U l        UR                  U R                  5      nX@l        [        R                  SUR                   35        U$ ! [         a'  n[        R                  S[!        U5       35        e SnAff = f)	zœ
Load data using Dask

Args:
    file_path (str): Path to the data file
    file_type (str): Type of file
    **kwargs: Additional parameters for dask read functions
    
Returns:
    DataFrame: Loaded data
r-   ⁄	blocksizer2   zADask does not support Excel files directly. Using pandas instead.iË  z0Data loaded successfully with Dask. Partitions: zError loading data with Dask: N)⁄ddr:   r   r<   r   ⁄warningr   ⁄headr   ⁄map_partitionsr?   r   r   ⁄npartitionsr   r    r!   )r   r"   r#   r$   rC   r&   s         r   r   ⁄DataLoader._load_with_daskt   s„   Ä 	‡òN”*‹ó[í[†—P∞d∑o±o–P»—Pëÿòi”'‹ó_í_†Y—9∞&—9ëÙ óë–b‘cÿ◊-“-®i—M¿f—M–M "$ß°®£àD‘ ◊"—"†4◊#8—#8”9àB‡åI‹èKâK–J»2œ>…>–JZ–[‘\ÿàI¯‰Û 	‹èLâL–9º#∏aª&∏–B‘Cÿ˚	˙s   ÇA/C ¡2AC √
C>√"C9√9C>c                 Û"	  ï  [        U5      nUR                  5       nU[        U5      -
  nUS:î  a  [        R                  SU S35        UR                   GH  n[
        R                  R                  R                  X   5      (       a)  X   R                  5       nX   R                  U5      X'   M]  [
        R                  R                  R                  X   5      (       d2  [
        R                  R                  R                  X   5      (       d  Mø  X   R                  5       n[        U5      S:î  a  X   R                  US   5      X'   M˙  X   R                  S5      X'   GM     UR                   HG  nSUR                  5       ;   d  SUR                  5       ;   d  M-   [
        R                  " X   SS9X'   MI     UR                   H   nSUR                  5       ;   d*  SUR                  5       ;   d  SUR                  5       ;   d  MA   [
        R                  R                  R                  X   5      (       a?  X   R"                  R%                  SSSS9X'   X   R"                  R%                  SS5      X'   [
        R&                  " X   SS9X'   MÃ     UR                   H2  nSUR                  5       ;   d  M  X   R)                  ["        5      X'   M4     UR                   H  nX   R*                  S:X  d  M  X   R-                  5       R/                  5       n[        U5      S::  d  MH  [1        S U 5       5      (       d  Ma  X   R%                  SSSSSSSSS.5      X'   MÅ     UR                   HÜ  n[
        R                  R                  R                  X   5      (       d  M5  X   R3                  5       S:î  d  MM  X   R3                  5       [        U5      S-  :  d  Mq  X   R)                  S5      X'   Mà     [        R                  S5        U$ !   [        R!                  S	U S
35         GM¬  = f!   [        R!                  S	U S35         GMç  = f! [4         a'  n[        R7                  S[#        U5       35        e SnAff = f) zv
Basic preprocessing of the data

Args:
    df (DataFrame): Input data
    
Returns:
    DataFrame: Preprocessed data
r   zRemoved z duplicate rows⁄Unknown⁄time⁄date⁄coerce)⁄errorszCould not convert z to datetime⁄amount⁄value⁄sumu   [\$,‚Ç¨,¬£,¬•]⁄ T)⁄regex⁄,z to numeric⁄id⁄objectÈ   c              3   ÛF   #   ï U  H  oR                  5       S ;   v ï  M     g7f))⁄true⁄false⁄yes⁄no⁄y⁄n⁄0⁄1N)r   )⁄.0⁄vals     r   ⁄	<genexpr>⁄.DataLoader._preprocess_data.<locals>.<genexpr>÷   s(   È Ä   5OÚ  CN–{~∑Y±Y≥[–Dv÷5vÚ  CN˘s   Ç!F)r]   r_   ra   rd   r^   r`   rb   rc   È
   g      ‡?⁄categoryzData preprocessing completedzError preprocessing data: N)⁄len⁄drop_duplicatesr   r   ⁄columnsr9   ⁄api⁄types⁄is_numeric_dtype⁄median⁄fillna⁄is_string_dtype⁄is_categorical_dtype⁄moder   ⁄to_datetimerH   r!   ⁄replace⁄
to_numeric⁄astype⁄dtype⁄dropna⁄unique⁄all⁄nuniquer   r    )	r   rC   ⁄initial_rows⁄removed_rows⁄col⁄
median_val⁄mode_val⁄unique_valsr&   s	            r   r?   ⁄DataLoader._preprocess_dataô   s£  Ä E	‰òrõ7àLÿ◊#—#”%àBÿ'¨#®b´'—1àLÿòa”‹óëòh†|†n∞O–D‘E ózïzê‰ó6ë6ó<ë<◊0—0∞±◊9—9ÿ!#°ß°”!1êJÿ ôgünôn®Z”8êBìG‰óVëVó\ë\◊1—1∞"±'◊:—:ºbøfπfølπl◊>_—>_–`b—`g◊>h”>hÿ!ôwü|ô|õ~êH‹ò8ì}†q”(ÿ"$°'ß.°.∞∏!±”"=òõ‡"$°'ß.°.∞”";òúÒ " ózîzêÿòSüYôYõ[”(®F∞c∑i±i≥k’,AO‹"$ß.¢.∞±¿—"JòõÒ " ózîzêÿòsüyôyõ{”*®g∏øπª”.D»–QT◊QZ—QZ”Q\’H\N‰ü6ô6ü<ô<◊7—7∏π◊@—@ÿ&(°gßk°k◊&9—&9–:J»B–VZ–&9–&[òBôGÿ&(°gßk°k◊&9—&9∏#∏r”&BòBôG‹"$ß-¢-∞±¿—"IòõÒ " ózîzêÿò3ü9ô9õ;’&ÿ ôgünôn¨S”1êBìGÒ "
 ózîzêÿë7ó=ë=†H’,ÿ"$°'ß.°.”"2◊"9—"9”";êK‹ò;”'®1’,¥Ò  5OÒ  CNÛ  5O˜  2OÛ  2Oÿ"$°'ß/°/ÿ$(∞∏D¿tÿ%*∞%∏e»%Ò3Û #òõÒ	 " ózîzê‹óFëFóLëL◊0—0∞±◊9”9ÿëGóOëO”%®’*ÿëGóOëO”%¨®B´∞#©’5ÿ ôgünôn®Z”8êBìGÒ	 "Ù èKâK–6‘7ÿàI¯MO‹üô–);∏C∏5¿–'M◊N–N˚N‹üô–);∏C∏5¿–'L◊M–M˚Ù6 Û 	‹èLâL–5¥c∏!≥f∞X–>‘?ÿ˚	˙s|   ÇDQ ƒB
Q ∆$P∆=AQ »BP; %Q À <Q Ã -Q Ã1Q Õ
AQ Œ-Q œ Q œ)/Q –P8–4Q –;Q—Q —
R—'"R	“	Rc           
      Û*  ï U R                   c  SS0$ U R                   R                  [        U R                   R                  5      [	        U R                   R
                  5      [	        U R                   R                  5       R                  5       5      [	        U R                   R                  SS95      S.n0 nU R                   R                  [        R                  /S9R                   Hî  nU R                   U   R                  5       U R                   U   R                  5       U R                   U   R                  5       U R                   U   R                  5       U R                   U   R!                  5       S.X#'   Mñ     X!S'   0 nU R                   R                  S	S
/S9R                   H]  nU R                   U   R#                  5       U R                   U   R%                  5       R'                  S5      R)                  5       S.XC'   M_     XAS'   U$ )zL
Get information about the loaded data

Returns:
    dict: Data information
r    ˙No data loadedT)⁄deep)r@   rm   ⁄dtypes⁄missing_values⁄memory_usage)⁄include)⁄min⁄max⁄meanrq   ⁄std⁄numeric_statsrZ   rj   È   )⁄unique_count⁄
top_values⁄categorical_stats)r   r@   ⁄listrm   ⁄dictrâ   ⁄isnullrU   rã   ⁄select_dtypes⁄np⁄numberrç   ré   rè   rq   rê   r~   ⁄value_countsrI   ⁄to_dict)r   r   rë   rÅ   rï   s        r   ⁄get_data_info⁄DataLoader.get_data_infoÍ   sπ  Ä  è9â9—ÿ–-–.–. óYëYó_ë_‹òDüIôI◊-—-”.‹ò4ü9ô9◊+—+”,‹"†4ß9°9◊#3—#3”#5◊#9—#9”#;”<‹ †ß°◊!7—!7∏T–!7–!B”CÒ
à àÿó9ë9◊*—*¥B∑I±I∞;–*–?◊G‘GàC‡óyëy†ë~◊)—)”+ÿóyëy†ë~◊)—)”+ÿü	ô	†#ô◊+—+”-ÿü)ô)†Cô.◊/—/”1ÿóyëy†ë~◊)—)”+Ò"àM”Ò H !.à_— –ÿó9ë9◊*—*∞H∏j–3I–*–J◊R‘RàC‡ $ß	°	®#°◊ 6— 6” 8ÿ"üiôi®ôn◊9—9”;◊@—@¿”C◊K—K”MÒ&–”"Ò S %6– —!‡àr   c                 Û&  ï U R                   c  g[        U R                   [        R                  5      (       a  U R                   R	                  U5      $ [        U R                   5      U:î  a  U R                   R                  U5      $ U R                   $ )zp
Get a sample of the data

Args:
    n (int): Number of rows to return
    
Returns:
    DataFrame: Sample data
N)r   ⁄
isinstancerG   ⁄	DataFramerI   rk   ⁄sample)r   rb   s     r   ⁄
get_sample⁄DataLoader.get_sample  sf   Ä  è9â9—ÿ‰êdóiëi§ß°◊.—.ÿó9ë9ó>ë>†!”$–$‰*-®dØi©i´.∏1”*<ê4ó9ë9◊#—#†A”&–K¿$«)¡)–Kr   c                 Û∏  ï U R                   c  SSS.$ Uc  / SQnS/ / S.nU Vs/ s H   o3U R                   R                  ;  d  M  UPM"     nnU(       a+  SUS'   US   R                  S	S
R                  U5       35        SU R                   R                  ;   aI  U R                   S   R	                  5       R                  5       nUS:î  a  US   R                  SU S35        / SQnU Hk  nX0R                   R                  ;   d  M  U R                   U   R                  5       R                  5       nUS:î  d  MQ  US   R                  SU SU 35        Mm     SU R                   R                  ;   a>  U R                   S   S:  R                  5       nUS:î  a  US   R                  SU S35        SU R                   R                  ;   aå  [        R                  R                  R                  U R                   S   5      (       aQ  U R                   S   [        R                  " 5       :Ñ  R                  5       n	U	S:î  a  US   R                  SU	 S35        U$ s  snf )zû
Validate the data against required columns

Args:
    required_columns (list, optional): List of required columns
    
Returns:
    dict: Validation results
Frá   )⁄validr    )⁄transaction_id⁄	timestamprS   ⁄	sender_id⁄receiver_idT)rß   rR   ⁄warningsrß   rR   zMissing required columns: z, r®   r   r¨   zFound z duplicate transaction IDs)r®   r©   rS   z missing values in rS   z# transactions with negative amountsr©   z$ transactions with future timestamps)r   rm   ⁄append⁄join⁄
duplicatedrU   rò   r9   rn   ro   ⁄is_datetime64_any_dtyper   ⁄now)
r   ⁄required_columns⁄validation_resultrÅ   ⁄missing_columns⁄duplicate_ids⁄critical_columns⁄missing_count⁄negative_amounts⁄future_datess
             r   ⁄validate_data⁄DataLoader.validate_data'  s?  Ä  è9â9—ÿ"–-=—>–>‡—#⁄d– ÿÿÒ
–Ò +;”[“*:†3»œ…◊IZ—IZ—>Zü3—*:à–[ﬁÿ).–òg—&ÿòh—'◊.—.–1K»DœI…I–Ve”Lf–Kg–/h‘i òtüyôy◊0—0”0ÿ üIôI–&6—7◊B—B”D◊H—H”JàMÿòq” ÿ!†*—-◊4—4∞v∏m∏_–Lf–5g‘hÚ E–€#àCÿóiëi◊'—'’'ÿ $ß	°	®#°◊ 5— 5” 7◊ ;— ;” =êÿ †1’$ÿ%†j—1◊8—8∏6¿-¿–Pc–dg–ch–9i÷jÒ	 $ êtóyëy◊(—(”(ÿ $ß	°	®(— 3∞a— 7◊<—<”>–ÿ†!”#ÿ!†*—-◊4—4∞v–>N–=O–Or–5s‘t ò$ü)ô)◊+—+”+¥∑±∑±◊0T—0T–UY◊U^—U^–_j—Uk◊0l—0lÿ üIôI†k—2¥X∑\≤\≥^—C◊H—H”JàLÿòa”ÿ!†*—-◊4—4∞v∏l∏^–Ko–5p‘q‡ – ˘Ú? \s   §I¡Ic                 ÛP  ï U R                   c  [        R                  S5        g Uc3  [        R                  R                  U5      S   SS R                  5       nUS:X  a   U R                   R                  " U4SS0UD6  OZUS;   a   U R                   R                  " U4SS0UD6  O4US:X  a   U R                   R                  " U4SS0UD6  O[        S	U 35      e[        R                  S
U 35        g! [         a'  n[        R                  S[        U5       35        e SnAff = f)z’
Save the processed data to a file

Args:
    file_path (str): Path to save the file
    file_type (str, optional): Type of file (csv, excel, parquet)
    **kwargs: Additional parameters for pandas save functions
NzNo data to saver   r*   ⁄indexFr/   r2   r3   zData saved successfully to zError saving data: )r   r   r    r   r   r   r   ⁄to_csv⁄to_excel⁄
to_parquetr=   r   r   r!   )r   r"   r#   r$   r&   s        r   ⁄	save_data⁄DataLoader.save_data_  s  Ä  è9â9—‹èLâL–*‘+ÿ	‡— ‹üGôG◊,—,®Y”7∏—:∏1∏2–>◊D—D”Fê	 òE”!ÿó	ë	◊ “ †—B∞%–B∏6”Bÿòo”-ÿó	ë	◊"“"†9—D∞E–D∏V”Dÿòi”'ÿó	ë	◊$“$†Y—F∞e–F∏v”F‰ –#:∏9∏+–!F”G–G‰èKâK–5∞i∞[–A’B¯‰Û 	‹èLâL–.¨s∞1´v®h–7‘8ÿ˚	˙s   •CC4 √4
D%√>"D ƒ D%)r   r   r   r
   )Ti†Ü )N)rí   )⁄__name__⁄
__module__⁄__qualname__⁄__firstlineno__⁄__doc__r   r'   r   r   r?   rû   r§   r∫   r¡   ⁄__static_attributes__© r   r   r   r      s<   Ü ÒÙ"ÙÚ@*ÚX#ÚJOÚb)ÙVLÙ$6!˜p r   r   )r«   ⁄pandasr9   ⁄numpyrö   ⁄dask.dataframe⁄	dataframerG   r   ⁄loggingr   r5   r¨   ⁄filterwarnings⁄basicConfig⁄INFO⁄	getLoggerr√   r   r   r…   r   r   ⁄<module>r”      sb   ÒÛ
 € › € 	€ › € € ÿ ◊ “ ò‘ ! ◊ “ ò'ü,ô,“ 'ÿ	◊	“	ò8”	$Ä˜kÚ kr   

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\ingestion\__pycache__\data_loader.cpython-39.pyc ===
a
    èöhQ:  „                   @   sÄ   d Z ddlZddlZddlmZ ddlZddl	Z	ddl
m
Z
 ddlZddlZe†d° e	je	jdç e	†e°ZG ddÑ dÉZdS )zJ
Data Loader Module
Handles loading and preprocessing of transaction data
È    N)⁄datetime⁄ignore)⁄levelc                   @   sb   e Zd ZdZdddÑZdddÑZd	d
Ñ ZddÑ ZddÑ ZddÑ Z	dddÑZ
dddÑZdddÑZdS )⁄
DataLoaderzê
    Class for loading and preprocessing transaction data
    Supports CSV, Excel, and Parquet files
    Uses Dask for handling large files
    TÈ†Ü c                 C   s   || _ || _d| _d| _dS )zº
        Initialize DataLoader
        
        Args:
            use_dask (bool): Whether to use Dask for large files
            chunk_size (int): Chunk size for Dask processing
        N)⁄use_dask⁄
chunk_size⁄data⁄original_data)⁄selfr   r   © r   ˙kC:\Users\Tranquil Abyss\fraud-detection-platform\app\..\src\fraud_detection_engine\ingestion\data_loader.py⁄__init__   s    zDataLoader.__init__Nc              
   K   s⁄   zö|du r&t j†|°d ddÖ †° }t j†|°d }| jrn|dkrnt†d|dõdù° | j||fi |§éW S t†d|dõd	ù° | j	||fi |§éW S W n: t
y‘ } z"t†d
t|Éõ ù° Ç W Y d}~n
d}~0 0 dS )aE  
        Load data from file
        
        Args:
            file_path (str): Path to the data file
            file_type (str, optional): Type of file (csv, excel, parquet)
            **kwargs: Additional parameters for pandas/dask read functions
            
        Returns:
            DataFrame: Loaded data
        NÈ   i   Èd   zLoading large file (z.2fz MB) using DaskzLoading file (z MB) using pandaszError loading data: )⁄os⁄path⁄splitext⁄lower⁄getsizer   ⁄logger⁄info⁄_load_with_dask⁄_load_with_pandas⁄	Exception⁄error⁄str)r   ⁄	file_path⁄	file_type⁄kwargs⁄	file_size⁄er   r   r   ⁄	load_data(   s    zDataLoader.load_datac           	   
   K   s*  zË|dkrVt |dÉè}t†|†° °}W d  É n1 s80    Y  |d }|†d|° |dv rrtj|fi |§é}nF|dv rétj|fi |§é}n*|dkr™tj|fi |§é}nt	d|õ ùÉÇ|†
° | _| †|°}|| _t†d	|jõ ù° |W S  têy$ } z"t†d
t|Éõ ù° Ç W Y d}~n
d}~0 0 dS )a#  
        Load data using pandas
        
        Args:
            file_path (str): Path to the data file
            file_type (str): Type of file
            **kwargs: Additional parameters for pandas read functions
            
        Returns:
            DataFrame: Loaded data
        ⁄csv⁄rbN⁄encoding©r#   ⁄txt©⁄xlsx⁄xls⁄parquet˙Unsupported file type: z!Data loaded successfully. Shape: z Error loading data with pandas: )⁄open⁄chardet⁄detect⁄read⁄
setdefault⁄pd⁄read_csv⁄
read_excel⁄read_parquet⁄
ValueError⁄copyr
   ⁄_preprocess_datar	   r   r   ⁄shaper   r   r   )	r   r   r   r   ⁄f⁄resultr%   ⁄dfr!   r   r   r   r   H   s*    ,

zDataLoader._load_with_pandasc              
   K   s‘   zî|dv r$t j|fd| ji|§é}n<|dkr@t j|fi |§é}n t†d° | j||fi |§éW S |†d°| _|†	| j
°}|| _t†d|jõ ù° |W S  tyŒ } z"t†dt|Éõ ù° Ç W Y d}~n
d}~0 0 dS )	a  
        Load data using Dask
        
        Args:
            file_path (str): Path to the data file
            file_type (str): Type of file
            **kwargs: Additional parameters for dask read functions
            
        Returns:
            DataFrame: Loaded data
        r&   ⁄	blocksizer+   zADask does not support Excel files directly. Using pandas instead.iË  z0Data loaded successfully with Dask. Partitions: zError loading data with Dask: N)⁄ddr3   r   r5   r   ⁄warningr   ⁄headr
   Zmap_partitionsr8   r	   r   Znpartitionsr   r   r   )r   r   r   r   r<   r!   r   r   r   r   t   s    
zDataLoader._load_with_daskc           	      C   sF  êzt |É}|†° }|t |É }|dkr:t†d|õ dù° |jD ]ú}tjj†|| °rv|| †	° }|| †
|°||< q@tjj†|| °sötjj†|| °r@|| †° }t |Édkr || †
|d °||< q@|| †
d°||< q@|jD ]X}d|†° v êsd|†° v r‰ztj|| ddç||< W q‰   t†d	|õ d
ù° Y q‰0 q‰|jD ]Æ}d|†° v êsrd|†° v êsrd|†° v êrDz^tjj†|| °êr∏|| jjddddç||< || j†dd°||< tj|| ddç||< W n   t†d	|õ dù° Y n0 êqD|jD ]&}d|†° v êr˙|| †t°||< êq˙|jD ]l}|| jdkêr(|| †° †° }t |Édkêr(tddÑ |D ÉÉêr(|| †dddddddddú°||< êq(|jD ]X}tjj†|| °êrú|| †° dkêrú|| †° t |Éd k êrú|| †d°||< êqút†d° |W S  têy@ } z"t†dt|Éõ ù° Ç W Y d }~n
d }~0 0 d S )!z∂
        Basic preprocessing of the data
        
        Args:
            df (DataFrame): Input data
            
        Returns:
            DataFrame: Preprocessed data
        r   zRemoved z duplicate rows⁄Unknown⁄time⁄date⁄coerce)⁄errorszCould not convert z to datetime⁄amount⁄value⁄sumu   [\$,‚Ç¨,¬£,¬•]⁄ T)⁄regex˙,z to numeric⁄id⁄objectÈ   c                 s   s   | ]}|† ° d v V  qdS ))⁄true⁄false⁄yes⁄no⁄y⁄n⁄0⁄1N)r   )⁄.0⁄valr   r   r   ⁄	<genexpr>÷   Û    z.DataLoader._preprocess_data.<locals>.<genexpr>F)rO   rQ   rS   rV   rP   rR   rT   rU   È
   g      ‡?⁄categoryzData preprocessing completedzError preprocessing data: N)⁄len⁄drop_duplicatesr   r   ⁄columnsr2   ⁄api⁄types⁄is_numeric_dtype⁄median⁄fillna⁄is_string_dtype⁄is_categorical_dtype⁄moder   ⁄to_datetimer?   r   ⁄replace⁄
to_numeric⁄astype⁄dtype⁄dropna⁄unique⁄all⁄nuniquer   r   )	r   r<   Zinitial_rowsZremoved_rows⁄col⁄
median_val⁄mode_val⁄unique_valsr!   r   r   r   r8   ô   sj    

$

*

"˛
ˇ˛
zDataLoader._preprocess_datac                 C   s  | j du rddiS | j jt| j jÉt| j jÉt| j †° †° Ét| j jddçÉdú}i }| j j	t
jgdçjD ]J}| j | †° | j | †° | j | †° | j | †° | j | †° dú||< qj||d	< i }| j j	d
dgdçjD ]0}| j | †° | j | †° †d°†° dú||< q÷||d< |S )zt
        Get information about the loaded data
        
        Returns:
            dict: Data information
        Nr   ˙No data loadedT)⁄deep)r9   r_   ⁄dtypes⁄missing_values⁄memory_usage)⁄include)⁄min⁄max⁄meanrc   ⁄std⁄numeric_statsrM   r\   È   )Zunique_countZ
top_values⁄categorical_stats)r	   r9   ⁄listr_   ⁄dictrw   ⁄isnullrH   ry   ⁄select_dtypes⁄np⁄numberr{   r|   r}   rc   r~   rp   ⁄value_countsr@   ⁄to_dict)r   r   r   rq   rÅ   r   r   r   ⁄get_data_infoÍ   s0    


˚	˚˛zDataLoader.get_data_inforÄ   c                 C   sL   | j du rdS t| j tjÉr(| j †|°S t| j É|krB| j †|°S | j S dS )z∞
        Get a sample of the data
        
        Args:
            n (int): Number of rows to return
            
        Returns:
            DataFrame: Sample data
        N)r	   ⁄
isinstancer>   ⁄	DataFramer@   r]   ⁄sample)r   rT   r   r   r   ⁄
get_sample  s
    

zDataLoader.get_samplec           
         sî  à j du rdddúS |du r$g d¢}dg g dú}á fdd	Ñ|D É}|rhd|d
< |d †dd†|°õ ù° dà j jv r§à j d †° †° }|dkr§|d †d|õ dù° g d¢}|D ]D}|à j jv r∞à j | †° †° }|dkr∞|d †d|õ d|õ ù° q∞dà j jv êr6à j d dk †° }|dkêr6|d †d|õ dù° dà j jv êrêtjj	†
à j d °êrêà j d t†° k†° }	|	dkêrê|d †d|	õ dù° |S )zﬁ
        Validate the data against required columns
        
        Args:
            required_columns (list, optional): List of required columns
            
        Returns:
            dict: Validation results
        NFru   )⁄validr   )⁄transaction_id⁄	timestamprF   ⁄	sender_id⁄receiver_idT)rè   rE   ⁄warningsc                    s   g | ]}|à j jvr|ëqS r   )r	   r_   )rW   rq   ©r   r   r   ⁄
<listcomp>>  rZ   z,DataLoader.validate_data.<locals>.<listcomp>rè   rE   zMissing required columns: z, rê   r   rî   zFound z duplicate transaction IDs)rê   rë   rF   z missing values in rF   z# transactions with negative amountsrë   z$ transactions with future timestamps)r	   ⁄append⁄joinr_   ⁄
duplicatedrH   rÑ   r2   r`   ra   ⁄is_datetime64_any_dtyper   ⁄now)
r   Zrequired_columnsZvalidation_result⁄missing_columnsZduplicate_idsZcritical_columnsrq   Zmissing_countZnegative_amountsZfuture_datesr   rï   r   ⁄validate_data'  s>    


˝
$
zDataLoader.validate_datac              
   K   s  | j du rt†d° dS z¨|du r>tj†|°d ddÖ †° }|dkr`| j j|fddi|§é nR|dv rÇ| j j|fddi|§é n0|dkr§| j j	|fddi|§é nt
d	|õ ùÉÇt†d
|õ ù° W n: ty˛ } z"t†dt|Éõ ù° Ç W Y d}~n
d}~0 0 dS )a  
        Save the processed data to a file
        
        Args:
            file_path (str): Path to save the file
            file_type (str, optional): Type of file (csv, excel, parquet)
            **kwargs: Additional parameters for pandas save functions
        NzNo data to saver   r#   ⁄indexFr(   r+   r,   zData saved successfully to zError saving data: )r	   r   r   r   r   r   r   ⁄to_csv⁄to_excel⁄
to_parquetr6   r   r   r   )r   r   r   r   r!   r   r   r   ⁄	save_data_  s"    	

zDataLoader.save_data)Tr   )N)rÄ   )N)N)⁄__name__⁄
__module__⁄__qualname__⁄__doc__r   r"   r   r   r8   rä   ré   rù   r¢   r   r   r   r   r      s   

 ,%Q+

8r   )r¶   ⁄pandasr2   ⁄numpyrÜ   Zdask.dataframe⁄	dataframer>   r   ⁄loggingr   r.   rî   ⁄filterwarnings⁄basicConfig⁄INFO⁄	getLoggerr£   r   r   r   r   r   r   ⁄<module>   s   



=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\ingestion\__pycache__\__init__.cpython-313.pyc ===
Û
    dÄöh    „                   Û   ï g )N© r   Û    ⁄hC:\Users\Tranquil Abyss\fraud-detection-platform\app\..\src\fraud_detection_engine\ingestion\__init__.py⁄<module>r      s   Òr   

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\ingestion\__pycache__\__init__.cpython-39.pyc ===
a
    èöh    „                   @   s   d S )N© r   r   r   ˙hC:\Users\Tranquil Abyss\fraud-detection-platform\app\..\src\fraud_detection_engine\ingestion\__init__.py⁄<module>   Û    

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\models\rule_based.py ===
"""
Rule-based Models Module
Implements rule-based fraud detection
"""

import pandas as pd
import numpy as np
import re
from datetime import datetime, timedelta
import yaml
import os
import warnings
import logging
from typing import Dict, List, Tuple, Union, Callable

from fraud_detection_engine.utils.api_utils import is_api_available, get_demo_sanctions_data, get_demo_tax_compliance_data, get_demo_bank_verification_data, get_demo_identity_verification_data

warnings.filterwarnings('ignore')
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class RuleEngine:
    """
    Class for rule-based fraud detection
    Implements configurable rules with weights
    """
    
    def __init__(self, config_path=None, threshold=0.7):
        """
        Initialize RuleEngine
        
        Args:
            config_path (str, optional): Path to configuration file
            threshold (float): Threshold for rule violation
        """
        self.threshold = threshold
        self.rules = {}
        self.rule_weights = {}
        self.rule_descriptions = {}
        self.fitted = False
        self.api_available = {}
        
        # Load configuration
        if config_path and os.path.exists(config_path):
            self._load_config(config_path)
        else:
            self._load_default_rules()
    
    def _load_config(self, config_path):
        """
        Load configuration from YAML file
        
        Args:
            config_path (str): Path to configuration file
        """
        try:
            with open(config_path, 'r') as f:
                config = yaml.safe_load(f)
            
            # Load rules
            if 'rules' in config:
                for rule_name, rule_config in config['rules'].items():
                    if rule_config.get('enabled', True):
                        self.rules[rule_name] = self._create_rule_function(rule_config)
                        self.rule_weights[rule_name] = rule_config.get('weight', 1.0)
                        self.rule_descriptions[rule_name] = rule_config.get('description', '')
            
            # Check API availability for rules that depend on external services
            self.api_available = {
                'sanctions': is_api_available('sanctions'),
                'tax_compliance': is_api_available('tax_compliance'),
                'bank_verification': is_api_available('bank_verification'),
                'identity_verification': is_api_available('identity_verification'),
                'geolocation': is_api_available('geolocation')
            }
            
            logger.info(f"API availability: {self.api_available}")
            
        except Exception as e:
            logger.error(f"Error loading configuration: {str(e)}")
            self._load_default_rules()
    
    def _load_default_rules(self):
        """
        Load default rules
        """
        try:
            # Amount-based rules
            self.rules['high_amount'] = self._high_amount_rule
            self.rule_weights['high_amount'] = 0.3
            self.rule_descriptions['high_amount'] = "Transaction amount exceeds threshold"
            
            self.rules['unusual_amount_for_sender'] = self._unusual_amount_for_sender_rule
            self.rule_weights['unusual_amount_for_sender'] = 0.2
            self.rule_descriptions['unusual_amount_for_sender'] = "Amount is unusual for the sender"
            
            self.rules['unusual_amount_for_receiver'] = self._unusual_amount_for_receiver_rule
            self.rule_weights['unusual_amount_for_receiver'] = 0.2
            self.rule_descriptions['unusual_amount_for_receiver'] = "Amount is unusual for the receiver"
            
            self.rules['round_amount'] = self._round_amount_rule
            self.rule_weights['round_amount'] = 0.1
            self.rule_descriptions['round_amount'] = "Transaction amount is suspiciously round"
            
            # Frequency-based rules
            self.rules['high_frequency_sender'] = self._high_frequency_sender_rule
            self.rule_weights['high_frequency_sender'] = 0.3
            self.rule_descriptions['high_frequency_sender'] = "High transaction frequency from sender"
            
            self.rules['high_frequency_receiver'] = self._high_frequency_receiver_rule
            self.rule_weights['high_frequency_receiver'] = 0.3
            self.rule_descriptions['high_frequency_receiver'] = "High transaction frequency to receiver"
            
            self.rules['rapid_succession'] = self._rapid_succession_rule
            self.rule_weights['rapid_succession'] = 0.4
            self.rule_descriptions['rapid_succession'] = "Multiple transactions in rapid succession"
            
            # Location-based rules
            self.rules['cross_border'] = self._cross_border_rule
            self.rule_weights['cross_border'] = 0.3
            self.rule_descriptions['cross_border'] = "Transaction crosses international borders"
            
            self.rules['high_risk_country'] = self._high_risk_country_rule
            self.rule_weights['high_risk_country'] = 0.5
            self.rule_descriptions['high_risk_country'] = "Transaction involves high-risk country"
            
            self.rules['unusual_location_for_sender'] = self._unusual_location_for_sender_rule
            self.rule_weights['unusual_location_for_sender'] = 0.3
            self.rule_descriptions['unusual_location_for_sender'] = "Transaction from unusual location for sender"
            
            # Time-based rules
            self.rules['unusual_hour'] = self._unusual_hour_rule
            self.rule_weights['unusual_hour'] = 0.2
            self.rule_descriptions['unusual_hour'] = "Transaction during unusual hours"
            
            self.rules['weekend'] = self._weekend_rule
            self.rule_weights['weekend'] = 0.1
            self.rule_descriptions['weekend'] = "Transaction on weekend"
            
            # Identity-based rules
            self.rules['new_sender'] = self._new_sender_rule
            self.rule_weights['new_sender'] = 0.3
            self.rule_descriptions['new_sender'] = "First transaction from new sender"
            
            self.rules['new_receiver'] = self._new_receiver_rule
            self.rule_weights['new_receiver'] = 0.3
            self.rule_descriptions['new_receiver'] = "First transaction to new receiver"
            
            # External API-based rules
            self.rules['sanctions_check'] = self._sanctions_check_rule
            self.rule_weights['sanctions_check'] = 0.5
            self.rule_descriptions['sanctions_check'] = "Transaction involves sanctioned entities"
            
            self.rules['tax_compliance'] = self._tax_compliance_rule
            self.rule_weights['tax_compliance'] = 0.3
            self.rule_descriptions['tax_compliance'] = "Transaction involves non-compliant entities"
            
            self.rules['bank_verification'] = self._bank_verification_rule
            self.rule_weights['bank_verification'] = 0.3
            self.rule_descriptions['bank_verification'] = "Transaction involves unverified bank accounts"
            
            self.rules['identity_verification'] = self._identity_verification_rule
            self.rule_weights['identity_verification'] = 0.3
            self.rule_descriptions['identity_verification'] = "Transaction involves unverified identities"
            
            # Check API availability
            self.api_available = {
                'sanctions': is_api_available('sanctions'),
                'tax_compliance': is_api_available('tax_compliance'),
                'bank_verification': is_api_available('bank_verification'),
                'identity_verification': is_api_available('identity_verification'),
                'geolocation': is_api_available('geolocation')
            }
            
            logger.info(f"API availability: {self.api_available}")
            
        except Exception as e:
            logger.error(f"Error loading default rules: {str(e)}")
            raise
    
    def _create_rule_function(self, rule_config):
        """
        Create a rule function from configuration
        
        Args:
            rule_config (dict): Rule configuration
            
        Returns:
            function: Rule function
        """
        rule_type = rule_config.get('type')
        
        if rule_type == 'amount_threshold':
            threshold = rule_config.get('threshold', 10000)
            return lambda row: row.get('amount', 0) > threshold
        
        elif rule_type == 'sender_amount_outlier':
            std_multiplier = rule_config.get('std_multiplier', 3)
            min_transactions = rule_config.get('min_transactions', 5)
            return lambda row: self._is_sender_amount_outlier(row, std_multiplier, min_transactions)
        
        elif rule_type == 'receiver_amount_outlier':
            std_multiplier = rule_config.get('std_multiplier', 3)
            min_transactions = rule_config.get('min_transactions', 5)
            return lambda row: self._is_receiver_amount_outlier(row, std_multiplier, min_transactions)
        
        elif rule_type == 'round_amount':
            threshold = rule_config.get('threshold', 1000)
            return lambda row: row.get('amount', 0) > threshold and row.get('amount', 0) % 1000 == 0
        
        elif rule_type == 'high_frequency_sender':
            time_window = rule_config.get('time_window', '1H')
            max_transactions = rule_config.get('max_transactions', 10)
            return lambda row: self._is_high_frequency_sender(row, time_window, max_transactions)
        
        elif rule_type == 'high_frequency_receiver':
            time_window = rule_config.get('time_window', '1H')
            max_transactions = rule_config.get('max_transactions', 10)
            return lambda row: self._is_high_frequency_receiver(row, time_window, max_transactions)
        
        elif rule_type == 'rapid_succession':
            time_window = rule_config.get('time_window', '5M')
            min_transactions = rule_config.get('min_transactions', 3)
            return lambda row: self._is_rapid_succession(row, time_window, min_transactions)
        
        elif rule_type == 'cross_border':
            return lambda row: self._is_cross_border(row)
        
        elif rule_type == 'high_risk_country':
            countries = rule_config.get('countries', ['North Korea', 'Iran', 'Syria', 'Cuba'])
            return lambda row: self._is_high_risk_country(row, countries)
        
        elif rule_type == 'unusual_location_for_sender':
            return lambda row: self._is_unusual_location_for_sender(row)
        
        elif rule_type == 'unusual_hour':
            start_hour = rule_config.get('start_hour', 23)
            end_hour = rule_config.get('end_hour', 5)
            return lambda row: self._is_unusual_hour(row, start_hour, end_hour)
        
        elif rule_type == 'weekend':
            return lambda row: self._is_weekend(row)
        
        elif rule_type == 'new_sender':
            time_window = rule_config.get('time_window', '7D')
            return lambda row: self._is_new_sender(row, time_window)
        
        elif rule_type == 'new_receiver':
            time_window = rule_config.get('time_window', '7D')
            return lambda row: self._is_new_receiver(row, time_window)
        
        else:
            logger.warning(f"Unknown rule type: {rule_type}")
            return lambda row: False
    
    def apply_rules(self, df):
        """
        Apply all rules to the dataframe
        
        Args:
            df (DataFrame): Input data
            
        Returns:
            dict: Rule results
        """
        try:
            # Sort by timestamp if available
            if 'timestamp' in df.columns and not pd.api.types.is_datetime64_any_dtype(df['timestamp']):
                df = df.copy()
                df['timestamp'] = pd.to_datetime(df['timestamp'])
            
            if 'timestamp' in df.columns:
                df = df.sort_values('timestamp')
            
            # Initialize results
            rule_results = {}
            rule_scores = {}
            violated_rules = {}
            
            # Apply each rule
            for rule_name, rule_func in self.rules.items():
                try:
                    # Apply rule to each row
                    results = []
                    for _, row in df.iterrows():
                        try:
                            result = rule_func(row)
                            results.append(result)
                        except Exception as e:
                            logger.warning(f"Error applying rule {rule_name} to row: {str(e)}")
                            results.append(False)
                    
                    rule_results[rule_name] = results
                    
                    # Calculate weighted score
                    weight = self.rule_weights.get(rule_name, 1.0)
                    rule_scores[rule_name] = [weight if result else 0 for result in results]
                    
                    # Track violated rules
                    violated_rules[rule_name] = [i for i, result in enumerate(results) if result]
                    
                except Exception as e:
                    logger.error(f"Error applying rule {rule_name}: {str(e)}")
                    rule_results[rule_name] = [False] * len(df)
                    rule_scores[rule_name] = [0] * len(df)
                    violated_rules[rule_name] = []
            
            # Calculate total rule score for each transaction
            total_scores = []
            for i in range(len(df)):
                score = sum(rule_scores[rule_name][i] for rule_name in rule_scores)
                total_scores.append(score)
            
            # Normalize scores
            max_possible_score = sum(self.rule_weights.values())
            normalized_scores = [score / max_possible_score for score in total_scores]
            
            # Determine if transaction violates rules based on threshold
            rule_violations = [score >= self.threshold for score in normalized_scores]
            
            # Get violated rule names for each transaction
            violated_rule_names = []
            for i in range(len(df)):
                names = []
                for rule_name in violated_rules:
                    if i in violated_rules[rule_name]:
                        names.append(rule_name)
                violated_rule_names.append(names)
            
            self.fitted = True
            
            return {
                'rule_results': rule_results,
                'rule_scores': rule_scores,
                'total_scores': total_scores,
                'normalized_scores': normalized_scores,
                'rule_violations': rule_violations,
                'violated_rule_names': violated_rule_names,
                'rules': self.rules,
                'rule_weights': self.rule_weights,
                'rule_descriptions': self.rule_descriptions
            }
            
        except Exception as e:
            logger.error(f"Error applying rules: {str(e)}")
            raise
    
    # Rule functions
    def _high_amount_rule(self, row):
        """Check if transaction amount is high"""
        return row.get('amount', 0) > 10000
    
    def _unusual_amount_for_sender_rule(self, row, std_multiplier=3, min_transactions=5):
        """Check if amount is unusual for the sender"""
        # This would require access to sender's transaction history
        # For simplicity, we'll use a placeholder
        return False
    
    def _unusual_amount_for_receiver_rule(self, row, std_multiplier=3, min_transactions=5):
        """Check if amount is unusual for the receiver"""
        # This would require access to receiver's transaction history
        # For simplicity, we'll use a placeholder
        return False
    
    def _round_amount_rule(self, row):
        """Check if transaction amount is suspiciously round"""
        amount = row.get('amount', 0)
        return amount > 1000 and amount % 1000 == 0
    
    def _high_frequency_sender_rule(self, row, time_window='1H', max_transactions=10):
        """Check if sender has high transaction frequency"""
        # This would require access to sender's transaction history
        # For simplicity, we'll use a placeholder
        return False
    
    def _high_frequency_receiver_rule(self, row, time_window='1H', max_transactions=10):
        """Check if receiver has high transaction frequency"""
        # This would require access to receiver's transaction history
        # For simplicity, we'll use a placeholder
        return False
    
    def _rapid_succession_rule(self, row, time_window='5M', min_transactions=3):
        """Check if there are multiple transactions in rapid succession"""
        # This would require access to transaction history
        # For simplicity, we'll use a placeholder
        return False
    
    def _cross_border_rule(self, row):
        """Check if transaction crosses international borders"""
        sender_location = row.get('sender_location', '')
        receiver_location = row.get('receiver_location', '')
        
        if not sender_location or not receiver_location:
            return False
        
        # Extract countries from locations
        sender_country = sender_location.split(',')[0].strip()
        receiver_country = receiver_location.split(',')[0].strip()
        
        return sender_country != receiver_country
    
    def _high_risk_country_rule(self, row, countries=None):
        """Check if transaction involves high-risk country"""
        if countries is None:
            countries = ['North Korea', 'Iran', 'Syria', 'Cuba']
        
        sender_location = row.get('sender_location', '')
        receiver_location = row.get('receiver_location', '')
        
        # Check if any high-risk country is involved
        for country in countries:
            if country in sender_location or country in receiver_location:
                return True
        
        return False
    
    def _unusual_location_for_sender_rule(self, row):
        """Check if transaction is from unusual location for sender"""
        # This would require access to sender's transaction history
        # For simplicity, we'll use a placeholder
        return False
    
    def _unusual_hour_rule(self, row, start_hour=23, end_hour=5):
        """Check if transaction is during unusual hours"""
        timestamp = row.get('timestamp')
        if timestamp is None:
            return False
        
        if not pd.api.types.is_datetime64_any_dtype(timestamp):
            timestamp = pd.to_datetime(timestamp)
        
        hour = timestamp.hour
        
        # Handle overnight range
        if start_hour > end_hour:
            return hour >= start_hour or hour < end_hour
        else:
            return start_hour <= hour < end_hour
    
    def _weekend_rule(self, row):
        """Check if transaction is on weekend"""
        timestamp = row.get('timestamp')
        if timestamp is None:
            return False
        
        if not pd.api.types.is_datetime64_any_dtype(timestamp):
            timestamp = pd.to_datetime(timestamp)
        
        # Saturday=5, Sunday=6
        return timestamp.dayofweek >= 5
    
    def _new_sender_rule(self, row, time_window='7D'):
        """Check if this is the first transaction from sender"""
        # This would require access to sender's transaction history
        # For simplicity, we'll use a placeholder
        return False
    
    def _new_receiver_rule(self, row, time_window='7D'):
        """Check if this is the first transaction to receiver"""
        # This would require access to receiver's transaction history
        # For simplicity, we'll use a placeholder
        return False
    
    def _sanctions_check_rule(self, row):
        """Check if transaction involves sanctioned entities"""
        try:
            if not self.api_available.get('sanctions', False):
                # Use demo data
                sanctions_data = get_demo_sanctions_data()
                
                # Simple demo check
                sender_id = row.get('sender_id', '')
                receiver_id = row.get('receiver_id', '')
                
                # Check if sender or receiver is in demo sanctions list
                is_sanctioned = (
                    sanctions_data['entity_id'].str.contains(sender_id, na=False).any() or
                    sanctions_data['entity_id'].str.contains(receiver_id, na=False).any()
                )
                
                return is_sanctioned
            else:
                # Here you would implement the actual API call
                # For now, return False
                return False
        except Exception as e:
            logger.warning(f"Error in sanctions check: {str(e)}")
            return False
    
    def _tax_compliance_rule(self, row):
        """Check if entities are tax compliant"""
        try:
            if not self.api_available.get('tax_compliance', False):
                # Use demo data
                tax_data = get_demo_tax_compliance_data()
                
                # Simple demo check
                sender_id = row.get('sender_id', '')
                receiver_id = row.get('receiver_id', '')
                
                # Check if sender or receiver is non-compliant
                is_non_compliant = False
                
                for _, entity in tax_data.iterrows():
                    if entity['entity_id'] in [sender_id, receiver_id] and entity['compliance_status'] == 'Non-compliant':
                        is_non_compliant = True
                        break
                
                return is_non_compliant
            else:
                # Here you would implement the actual API call
                # For now, return False
                return False
        except Exception as e:
            logger.warning(f"Error in tax compliance check: {str(e)}")
            return False
    
    def _bank_verification_rule(self, row):
        """Check if bank accounts are verified"""
        try:
            if not self.api_available.get('bank_verification', False):
                # Use demo data
                bank_data = get_demo_bank_verification_data()
                
                # Simple demo check
                sender_id = row.get('sender_id', '')
                receiver_id = row.get('receiver_id', '')
                
                # Check if sender or receiver account is not verified
                is_unverified = False
                
                for _, account in bank_data.iterrows():
                    if account['account_number'] in [sender_id, receiver_id] and account['verification_status'] == 'Not Verified':
                        is_unverified = True
                        break
                
                return is_unverified
            else:
                # Here you would implement the actual API call
                # For now, return False
                return False
        except Exception as e:
            logger.warning(f"Error in bank verification check: {str(e)}")
            return False
    
    def _identity_verification_rule(self, row):
        """Check if identities are verified"""
        try:
            if not self.api_available.get('identity_verification', False):
                # Use demo data
                identity_data = get_demo_identity_verification_data()
                
                # Simple demo check
                sender_id = row.get('sender_id', '')
                receiver_id = row.get('receiver_id', '')
                
                # Check if sender or receiver identity is not verified
                is_unverified = False
                
                for _, identity in identity_data.iterrows():
                    if identity['id_number'] in [sender_id, receiver_id] and identity['verification_status'] == 'Not Verified':
                        is_unverified = True
                        break
                
                return is_unverified
            else:
                # Here you would implement the actual API call
                # For now, return False
                return False
        except Exception as e:
            logger.warning(f"Error in identity verification check: {str(e)}")
            return False
    
    def add_rule(self, rule_name, rule_func, weight=1.0, description=''):
        """
        Add a custom rule
        
        Args:
            rule_name (str): Name of the rule
            rule_func (function): Rule function
            weight (float): Weight of the rule
            description (str): Description of the rule
        """
        self.rules[rule_name] = rule_func
        self.rule_weights[rule_name] = weight
        self.rule_descriptions[rule_name] = description
        logger.info(f"Added rule: {rule_name}")
    
    def remove_rule(self, rule_name):
        """
        Remove a rule
        
        Args:
            rule_name (str): Name of the rule to remove
        """
        if rule_name in self.rules:
            del self.rules[rule_name]
            del self.rule_weights[rule_name]
            del self.rule_descriptions[rule_name]
            logger.info(f"Removed rule: {rule_name}")
        else:
            logger.warning(f"Rule not found: {rule_name}")
    
    def update_rule_weight(self, rule_name, weight):
        """
        Update the weight of a rule
        
        Args:
            rule_name (str): Name of the rule
            weight (float): New weight
        """
        if rule_name in self.rule_weights:
            self.rule_weights[rule_name] = weight
            logger.info(f"Updated weight for rule {rule_name}: {weight}")
        else:
            logger.warning(f"Rule not found: {rule_name}")
    
    def get_rules(self):
        """
        Get all rules
        
        Returns:
            dict: Rules information
        """
        return {
            'rules': list(self.rules.keys()),
            'weights': self.rule_weights,
            'descriptions': self.rule_descriptions
        }
    
    def save_config(self, config_path):
        """
        Save current configuration to file
        
        Args:
            config_path (str): Path to save configuration
        """
        try:
            config = {
                'rules': {}
            }
            
            for rule_name in self.rules:
                config['rules'][rule_name] = {
                    'enabled': True,
                    'weight': self.rule_weights[rule_name],
                    'description': self.rule_descriptions[rule_name]
                }
            
            with open(config_path, 'w') as f:
                yaml.dump(config, f, default_flow_style=False)
            
            logger.info(f"Configuration saved to {config_path}")
            
        except Exception as e:
            logger.error(f"Error saving configuration: {str(e)}")
            raise

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\models\supervised.py ===
"""
Supervised Models Module
Implements supervised learning models for fraud detection
"""
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score, 
    roc_auc_score, confusion_matrix, classification_report, 
    precision_recall_curve, average_precision_score, roc_curve
)
from sklearn.preprocessing import StandardScaler, LabelEncoder, RobustScaler
from sklearn.feature_selection import SelectKBest, f_classif, RFE
from imblearn.over_sampling import SMOTE, ADASYN, BorderlineSMOTE
from imblearn.under_sampling import RandomUnderSampler, TomekLinks
from imblearn.combine import SMOTEENN, SMOTETomek
import xgboost as xgb
import lightgbm as lgb
import shap
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
import logging
from typing import Dict, List, Tuple, Union
warnings.filterwarnings('ignore')
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class SupervisedModels:
    """
    Class for supervised fraud detection models
    Implements Random Forest, XGBoost, Logistic Regression, etc.
    """
    
    def __init__(self, test_size=0.2, random_state=42, handle_imbalance=True):
        """
        Initialize SupervisedModels
        
        Args:
            test_size (float): Proportion of data for testing
            random_state (int): Random seed
            handle_imbalance (bool): Whether to handle class imbalance
        """
        self.test_size = test_size
        self.random_state = random_state
        self.handle_imbalance = handle_imbalance
        self.models = {}
        self.feature_names = {}
        self.scalers = {}
        self.label_encoders = {}
        self.feature_selectors = {}
        self.resamplers = {}
        self.performance = {}
        self.feature_importance = {}
        self.shap_values = {}
        self.fitted = False
        
    def run_models(self, df):
        """
        Run all supervised models
        
        Args:
            df (DataFrame): Input data
            
        Returns:
            dict: Model results
        """
        try:
            # Check if fraud_flag column exists
            if 'fraud_flag' not in df.columns:
                logger.warning("No fraud_flag column found. Using synthetic labels for demonstration.")
                # Create synthetic labels based on statistical outliers
                numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
                if len(numeric_cols) > 0:
                    # Use Z-score to identify outliers as potential fraud
                    X = df[numeric_cols].fillna(0)
                    scaler = StandardScaler()
                    X_scaled = scaler.fit_transform(X)
                    
                    # Calculate Z-scores
                    z_scores = np.abs(X_scaled).max(axis=1)
                    
                    # Label top 5% as fraud
                    threshold = np.percentile(z_scores, 95)
                    y = (z_scores > threshold).astype(int)
                    
                    # Add synthetic labels to dataframe
                    df = df.copy()
                    df['fraud_flag'] = y
                else:
                    raise ValueError("No numeric columns found for creating synthetic labels")
            else:
                y = df['fraud_flag'].astype(int)
            
            # Get feature columns (exclude target and ID columns)
            exclude_cols = ['fraud_flag', 'transaction_id', 'sender_id', 'receiver_id']
            feature_cols = [col for col in df.columns if col not in exclude_cols]
            
            # Get numeric and categorical features
            numeric_features = df[feature_cols].select_dtypes(include=[np.number]).columns.tolist()
            categorical_features = df[feature_cols].select_dtypes(include=['object', 'category']).columns.tolist()
            
            # Prepare data
            X = df[feature_cols].copy()
            
            # Clean data before processing
            X = self._clean_data(X)
            
            # Handle categorical features
            for col in categorical_features:
                if col in X.columns:
                    # Label encode categorical features
                    le = LabelEncoder()
                    X[col] = le.fit_transform(X[col].astype(str))
                    self.label_encoders[col] = le
            
            # Split data
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=self.test_size, random_state=self.random_state, stratify=y
            )
            
            # Scale numeric features using RobustScaler
            if numeric_features:
                scaler = RobustScaler()
                X_train[numeric_features] = scaler.fit_transform(X_train[numeric_features])
                X_test[numeric_features] = scaler.transform(X_test[numeric_features])
                self.scalers['numeric'] = scaler
            
            # Handle class imbalance
            if self.handle_imbalance and y_train.sum() / len(y_train) < 0.1:  # If minority class < 10%
                # Try different resampling techniques
                resampling_methods = {
                    'smote': SMOTE(random_state=self.random_state),
                    'adasyn': ADASYN(random_state=self.random_state),
                    'borderline_smote': BorderlineSMOTE(random_state=self.random_state),
                    'smote_enn': SMOTEENN(random_state=self.random_state),
                    'smote_tomek': SMOTETomek(random_state=self.random_state)
                }
                
                # Evaluate each method using cross-validation
                best_method = None
                best_score = 0
                
                for method_name, resampler in resampling_methods.items():
                    try:
                        # Apply resampling
                        X_resampled, y_resampled = resampler.fit_resample(X_train, y_train)
                        
                        # Evaluate with a simple model
                        rf = RandomForestClassifier(n_estimators=50, random_state=self.random_state, n_jobs=-1)
                        scores = cross_val_score(rf, X_resampled, y_resampled, cv=3, scoring='f1')
                        avg_score = scores.mean()
                        
                        if avg_score > best_score:
                            best_score = avg_score
                            best_method = method_name
                            self.resamplers['best'] = resampler
                            X_train, y_train = X_resampled, y_resampled
                    except Exception as e:
                        logger.warning(f"Error with {method_name}: {str(e)}")
                
                if best_method:
                    logger.info(f"Using {best_method} for handling class imbalance")
                else:
                    logger.warning("No suitable resampling method found")
            
            # Store feature names
            all_feature_names = feature_cols
            
            # Run different models
            results = {}
            
            # Random Forest
            try:
                rf_model = RandomForestClassifier(
                    n_estimators=100,
                    max_depth=10,
                    min_samples_split=5,
                    min_samples_leaf=2,
                    class_weight='balanced',
                    random_state=self.random_state,
                    n_jobs=-1
                )
                
                rf_model.fit(X_train, y_train)
                rf_pred = rf_model.predict(X_test)
                rf_proba = rf_model.predict_proba(X_test)[:, 1]
                
                # Calculate performance metrics
                rf_performance = self._calculate_performance_metrics(y_test, rf_pred, rf_proba)
                
                # Get feature importance
                rf_importance = pd.DataFrame({
                    'feature': all_feature_names,
                    'importance': rf_model.feature_importances_
                }).sort_values('importance', ascending=False)
                
                # Calculate SHAP values
                explainer = shap.TreeExplainer(rf_model)
                rf_shap_values = explainer.shap_values(X_test)
                
                results['random_forest'] = {
                    'model': rf_model,
                    'predictions': rf_pred,
                    'probabilities': rf_proba,
                    'performance': rf_performance,
                    'feature_importance': rf_importance,
                    'shap_values': rf_shap_values,
                    'feature_names': all_feature_names
                }
                
                self.models['random_forest'] = rf_model
                self.feature_names['random_forest'] = all_feature_names
                self.performance['random_forest'] = rf_performance
                self.feature_importance['random_forest'] = rf_importance
                self.shap_values['random_forest'] = rf_shap_values
                
                logger.info("Random Forest model completed")
            except Exception as e:
                logger.error(f"Error running Random Forest: {str(e)}")
            
            # XGBoost
            try:
                xgb_model = xgb.XGBClassifier(
                    n_estimators=100,
                    max_depth=6,
                    learning_rate=0.1,
                    subsample=0.8,
                    colsample_bytree=0.8,
                    scale_pos_weight=len(y_train[y_train==0]) / len(y_train[y_train==1]),
                    random_state=self.random_state,
                    use_label_encoder=False,
                    eval_metric='logloss'
                )
                
                xgb_model.fit(X_train, y_train)
                xgb_pred = xgb_model.predict(X_test)
                xgb_proba = xgb_model.predict_proba(X_test)[:, 1]
                
                # Calculate performance metrics
                xgb_performance = self._calculate_performance_metrics(y_test, xgb_pred, xgb_proba)
                
                # Get feature importance
                xgb_importance = pd.DataFrame({
                    'feature': all_feature_names,
                    'importance': xgb_model.feature_importances_
                }).sort_values('importance', ascending=False)
                
                # Calculate SHAP values
                explainer = shap.TreeExplainer(xgb_model)
                xgb_shap_values = explainer.shap_values(X_test)
                
                results['xgboost'] = {
                    'model': xgb_model,
                    'predictions': xgb_pred,
                    'probabilities': xgb_proba,
                    'performance': xgb_performance,
                    'feature_importance': xgb_importance,
                    'shap_values': xgb_shap_values,
                    'feature_names': all_feature_names
                }
                
                self.models['xgboost'] = xgb_model
                self.feature_names['xgboost'] = all_feature_names
                self.performance['xgboost'] = xgb_performance
                self.feature_importance['xgboost'] = xgb_importance
                self.shap_values['xgboost'] = xgb_shap_values
                
                logger.info("XGBoost model completed")
            except Exception as e:
                logger.error(f"Error running XGBoost: {str(e)}")
            
            # LightGBM
            try:
                lgb_model = lgb.LGBMClassifier(
                    n_estimators=100,
                    max_depth=6,
                    learning_rate=0.1,
                    subsample=0.8,
                    colsample_bytree=0.8,
                    scale_pos_weight=len(y_train[y_train==0]) / len(y_train[y_train==1]),
                    random_state=self.random_state
                )
                
                lgb_model.fit(X_train, y_train)
                lgb_pred = lgb_model.predict(X_test)
                lgb_proba = lgb_model.predict_proba(X_test)[:, 1]
                
                # Calculate performance metrics
                lgb_performance = self._calculate_performance_metrics(y_test, lgb_pred, lgb_proba)
                
                # Get feature importance
                lgb_importance = pd.DataFrame({
                    'feature': all_feature_names,
                    'importance': lgb_model.feature_importances_
                }).sort_values('importance', ascending=False)
                
                # Calculate SHAP values
                explainer = shap.TreeExplainer(lgb_model)
                lgb_shap_values = explainer.shap_values(X_test)
                
                results['lightgbm'] = {
                    'model': lgb_model,
                    'predictions': lgb_pred,
                    'probabilities': lgb_proba,
                    'performance': lgb_performance,
                    'feature_importance': lgb_importance,
                    'shap_values': lgb_shap_values,
                    'feature_names': all_feature_names
                }
                
                self.models['lightgbm'] = lgb_model
                self.feature_names['lightgbm'] = all_feature_names
                self.performance['lightgbm'] = lgb_performance
                self.feature_importance['lightgbm'] = lgb_importance
                self.shap_values['lightgbm'] = lgb_shap_values
                
                logger.info("LightGBM model completed")
            except Exception as e:
                logger.error(f"Error running LightGBM: {str(e)}")
            
            # Logistic Regression
            try:
                lr_model = LogisticRegression(
                    C=1.0,
                    class_weight='balanced',
                    random_state=self.random_state,
                    max_iter=1000
                )
                
                lr_model.fit(X_train, y_train)
                lr_pred = lr_model.predict(X_test)
                lr_proba = lr_model.predict_proba(X_test)[:, 1]
                
                # Calculate performance metrics
                lr_performance = self._calculate_performance_metrics(y_test, lr_pred, lr_proba)
                
                # Get feature importance (coefficients)
                lr_importance = pd.DataFrame({
                    'feature': all_feature_names,
                    'importance': np.abs(lr_model.coef_[0])
                }).sort_values('importance', ascending=False)
                
                # Calculate SHAP values
                explainer = shap.LinearExplainer(lr_model, X_train)
                lr_shap_values = explainer.shap_values(X_test)
                
                results['logistic_regression'] = {
                    'model': lr_model,
                    'predictions': lr_pred,
                    'probabilities': lr_proba,
                    'performance': lr_performance,
                    'feature_importance': lr_importance,
                    'shap_values': lr_shap_values,
                    'feature_names': all_feature_names
                }
                
                self.models['logistic_regression'] = lr_model
                self.feature_names['logistic_regression'] = all_feature_names
                self.performance['logistic_regression'] = lr_performance
                self.feature_importance['logistic_regression'] = lr_importance
                self.shap_values['logistic_regression'] = lr_shap_values
                
                logger.info("Logistic Regression model completed")
            except Exception as e:
                logger.error(f"Error running Logistic Regression: {str(e)}")
            
            # Gradient Boosting
            try:
                gb_model = GradientBoostingClassifier(
                    n_estimators=100,
                    max_depth=6,
                    learning_rate=0.1,
                    subsample=0.8,
                    random_state=self.random_state
                )
                
                gb_model.fit(X_train, y_train)
                gb_pred = gb_model.predict(X_test)
                gb_proba = gb_model.predict_proba(X_test)[:, 1]
                
                # Calculate performance metrics
                gb_performance = self._calculate_performance_metrics(y_test, gb_pred, gb_proba)
                
                # Get feature importance
                gb_importance = pd.DataFrame({
                    'feature': all_feature_names,
                    'importance': gb_model.feature_importances_
                }).sort_values('importance', ascending=False)
                
                # Calculate SHAP values
                explainer = shap.TreeExplainer(gb_model)
                gb_shap_values = explainer.shap_values(X_test)
                
                results['gradient_boosting'] = {
                    'model': gb_model,
                    'predictions': gb_pred,
                    'probabilities': gb_proba,
                    'performance': gb_performance,
                    'feature_importance': gb_importance,
                    'shap_values': gb_shap_values,
                    'feature_names': all_feature_names
                }
                
                self.models['gradient_boosting'] = gb_model
                self.feature_names['gradient_boosting'] = all_feature_names
                self.performance['gradient_boosting'] = gb_performance
                self.feature_importance['gradient_boosting'] = gb_importance
                self.shap_values['gradient_boosting'] = gb_shap_values
                
                logger.info("Gradient Boosting model completed")
            except Exception as e:
                logger.error(f"Error running Gradient Boosting: {str(e)}")
            
            # Neural Network
            try:
                nn_model = MLPClassifier(
                    hidden_layer_sizes=(100, 50),
                    activation='relu',
                    solver='adam',
                    alpha=0.0001,
                    batch_size=32,
                    learning_rate='adaptive',
                    max_iter=200,
                    random_state=self.random_state
                )
                
                nn_model.fit(X_train, y_train)
                nn_pred = nn_model.predict(X_test)
                nn_proba = nn_model.predict_proba(X_test)[:, 1]
                
                # Calculate performance metrics
                nn_performance = self._calculate_performance_metrics(y_test, nn_pred, nn_proba)
                
                # For neural networks, we don't have direct feature importance
                # Use permutation importance instead
                from sklearn.inspection import permutation_importance
                
                perm_importance = permutation_importance(
                    nn_model, X_test, y_test, n_repeats=5, random_state=self.random_state
                )
                
                nn_importance = pd.DataFrame({
                    'feature': all_feature_names,
                    'importance': perm_importance.importances_mean
                }).sort_values('importance', ascending=False)
                
                # Calculate SHAP values (using KernelExplainer for black-box models)
                explainer = shap.KernelExplainer(nn_model.predict_proba, X_train[:100])  # Use subset for speed
                nn_shap_values = explainer.shap_values(X_test[:100])  # Use subset for speed
                
                results['neural_network'] = {
                    'model': nn_model,
                    'predictions': nn_pred,
                    'probabilities': nn_proba,
                    'performance': nn_performance,
                    'feature_importance': nn_importance,
                    'shap_values': nn_shap_values,
                    'feature_names': all_feature_names
                }
                
                self.models['neural_network'] = nn_model
                self.feature_names['neural_network'] = all_feature_names
                self.performance['neural_network'] = nn_performance
                self.feature_importance['neural_network'] = nn_importance
                self.shap_values['neural_network'] = nn_shap_values
                
                logger.info("Neural Network model completed")
            except Exception as e:
                logger.error(f"Error running Neural Network: {str(e)}")
            
            # Ensemble model (combine predictions from all models)
            try:
                # Get predictions from all models
                all_predictions = []
                all_probabilities = []
                
                for model_name in results:
                    all_predictions.append(results[model_name]['predictions'])
                    all_probabilities.append(results[model_name]['probabilities'])
                
                # Majority voting for predictions
                ensemble_pred = np.array(all_predictions).mean(axis=0) > 0.5
                
                # Average probabilities
                ensemble_proba = np.array(all_probabilities).mean(axis=0)
                
                # Calculate performance metrics
                ensemble_performance = self._calculate_performance_metrics(y_test, ensemble_pred, ensemble_proba)
                
                # For ensemble, average feature importances
                ensemble_importance = pd.DataFrame({
                    'feature': all_feature_names,
                    'importance': np.zeros(len(all_feature_names))
                })
                
                for model_name in results:
                    if model_name in self.feature_importance:
                        model_importance = self.feature_importance[model_name]
                        for i, feature in enumerate(all_feature_names):
                            if feature in model_importance['feature'].values:
                                idx = model_importance[model_importance['feature'] == feature].index[0]
                                ensemble_importance.loc[i, 'importance'] += model_importance.loc[idx, 'importance']
                
                # Normalize importance
                ensemble_importance['importance'] = ensemble_importance['importance'] / len(results)
                ensemble_importance = ensemble_importance.sort_values('importance', ascending=False)
                
                results['ensemble'] = {
                    'predictions': ensemble_pred,
                    'probabilities': ensemble_proba,
                    'performance': ensemble_performance,
                    'feature_importance': ensemble_importance,
                    'feature_names': all_feature_names
                }
                
                self.performance['ensemble'] = ensemble_performance
                self.feature_importance['ensemble'] = ensemble_importance
                
                logger.info("Ensemble model completed")
            except Exception as e:
                logger.error(f"Error running Ensemble model: {str(e)}")
            
            self.fitted = True
            return results
            
        except Exception as e:
            logger.error(f"Error running supervised models: {str(e)}")
            raise
    
    def _clean_data(self, X):
        """
        Clean data by handling infinity, NaN, and extreme values
        
        Args:
            X (DataFrame): Input data
            
        Returns:
            DataFrame: Cleaned data
        """
        try:
            # Replace infinity with NaN
            X = X.replace([np.inf, -np.inf], np.nan)
            
            # Replace extremely large values with a more reasonable maximum
            for col in X.columns:
                if X[col].dtype in ['float64', 'int64']:
                    # Calculate 99th percentile as a reasonable maximum
                    percentile_99 = np.nanpercentile(X[col], 99)
                    if not np.isnan(percentile_99):
                        # Cap values at 10 times the 99th percentile
                        max_val = percentile_99 * 10
                        X[col] = np.where(X[col] > max_val, max_val, X[col])
                        
                        # Similarly, handle extremely negative values
                        percentile_1 = np.nanpercentile(X[col], 1)
                        if not np.isnan(percentile_1):
                            min_val = percentile_1 * 10
                            X[col] = np.where(X[col] < min_val, min_val, X[col])
            
            return X
            
        except Exception as e:
            logger.error(f"Error cleaning data: {str(e)}")
            return X
    
    def _calculate_performance_metrics(self, y_true, y_pred, y_proba):
        """
        Calculate performance metrics for a model
        
        Args:
            y_true (array): True labels
            y_pred (array): Predicted labels
            y_proba (array): Predicted probabilities
            
        Returns:
            dict: Performance metrics
        """
        try:
            # Basic metrics
            accuracy = accuracy_score(y_true, y_pred)
            precision = precision_score(y_true, y_pred, zero_division=0)
            recall = recall_score(y_true, y_pred, zero_division=0)
            f1 = f1_score(y_true, y_pred, zero_division=0)
            
            # ROC AUC
            roc_auc = roc_auc_score(y_true, y_proba)
            
            # Average precision
            avg_precision = average_precision_score(y_true, y_proba)
            
            # Confusion matrix
            cm = confusion_matrix(y_true, y_pred)
            
            # Classification report
            class_report = classification_report(y_true, y_pred, output_dict=True)
            
            # Precision-Recall curve
            precision_curve, recall_curve, _ = precision_recall_curve(y_true, y_proba)
            
            # ROC curve
            fpr, tpr, _ = roc_curve(y_true, y_proba)
            
            return {
                'accuracy': accuracy,
                'precision': precision,
                'recall': recall,
                'f1': f1,
                'roc_auc': roc_auc,
                'avg_precision': avg_precision,
                'confusion_matrix': cm,
                'classification_report': class_report,
                'precision_curve': precision_curve,
                'recall_curve': recall_curve,
                'fpr': fpr,
                'tpr': tpr
            }
            
        except Exception as e:
            logger.error(f"Error calculating performance metrics: {str(e)}")
            raise
    
    def predict(self, df, model_name):
        """
        Make predictions using a fitted model
        
        Args:
            df (DataFrame): Input data
            model_name (str): Name of the model to use
            
        Returns:
            dict: Predictions and probabilities
        """
        if not self.fitted:
            raise ValueError("Models not fitted. Call run_models first.")
        
        if model_name not in self.models and model_name != 'ensemble':
            raise ValueError(f"Model {model_name} not found.")
        
        try:
            # Get feature names for this model
            if model_name == 'ensemble':
                # For ensemble, use all feature names
                feature_names = []
                for name in self.feature_names:
                    feature_names.extend(self.feature_names[name])
                feature_names = list(set(feature_names))  # Remove duplicates
            else:
                feature_names = self.feature_names[model_name]
            
            # Prepare data
            X = df[feature_names].copy()
            
            # Clean the data
            X = self._clean_data(X)
            
            # Handle categorical features
            for col in X.columns:
                if col in self.label_encoders:
                    le = self.label_encoders[col]
                    # Handle unseen categories
                    X[col] = X[col].astype(str).map(
                        lambda x: le.transform([x])[0] if x in le.classes_ else -1
                    )
            
            # Scale numeric features if scaler exists
            if 'numeric' in self.scalers:
                numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()
                if numeric_features:
                    X[numeric_features] = self.scalers['numeric'].transform(X[numeric_features])
            
            # Make predictions
            if model_name == 'ensemble':
                # Get predictions from all models
                all_predictions = []
                all_probabilities = []
                
                for name in self.models:
                    model = self.models[name]
                    pred = model.predict(X)
                    proba = model.predict_proba(X)[:, 1]
                    all_predictions.append(pred)
                    all_probabilities.append(proba)
                
                # Majority voting for predictions
                predictions = np.array(all_predictions).mean(axis=0) > 0.5
                
                # Average probabilities
                probabilities = np.array(all_probabilities).mean(axis=0)
                
            else:
                # Get model
                model = self.models[model_name]
                
                # Make predictions
                predictions = model.predict(X)
                probabilities = model.predict_proba(X)[:, 1]
            
            return {
                'predictions': predictions,
                'probabilities': probabilities
            }
            
        except Exception as e:
            logger.error(f"Error making predictions with {model_name}: {str(e)}")
            raise
    
    def get_feature_importance(self, model_name):
        """
        Get feature importance for a model
        
        Args:
            model_name (str): Name of the model
            
        Returns:
            DataFrame: Feature importance
        """
        if not self.fitted:
            raise ValueError("Models not fitted. Call run_models first.")
        
        if model_name not in self.feature_importance:
            raise ValueError(f"Feature importance for {model_name} not found.")
        
        return self.feature_importance[model_name]
    
    def get_performance(self, model_name):
        """
        Get performance metrics for a model
        
        Args:
            model_name (str): Name of the model
            
        Returns:
            dict: Performance metrics
        """
        if not self.fitted:
            raise ValueError("Models not fitted. Call run_models first.")
        
        if model_name not in self.performance:
            raise ValueError(f"Performance for {model_name} not found.")
        
        return self.performance[model_name]
    
    def get_shap_values(self, model_name):
        """
        Get SHAP values for a model
        
        Args:
            model_name (str): Name of the model
            
        Returns:
            array: SHAP values
        """
        if not self.fitted:
            raise ValueError("Models not fitted. Call run_models first.")
        
        if model_name not in self.shap_values:
            raise ValueError(f"SHAP values for {model_name} not found.")
        
        return self.shap_values[model_name]
    
    def plot_feature_importance(self, model_name, top_n=20):
        """
        Plot feature importance for a model
        
        Args:
            model_name (str): Name of the model
            top_n (int): Number of top features to show
        """
        try:
            # Get feature importance
            importance_df = self.get_feature_importance(model_name)
            
            # Get top features
            top_features = importance_df.head(top_n)
            
            # Create plot
            plt.figure(figsize=(10, 8))
            sns.barplot(x='importance', y='feature', data=top_features)
            plt.title(f'Top {top_n} Feature Importance - {model_name}')
            plt.tight_layout()
            
            return plt.gcf()
            
        except Exception as e:
            logger.error(f"Error plotting feature importance for {model_name}: {str(e)}")
            raise
    
    def plot_confusion_matrix(self, model_name):
        """
        Plot confusion matrix for a model
        
        Args:
            model_name (str): Name of the model
        """
        try:
            # Get performance metrics
            performance = self.get_performance(model_name)
            cm = performance['confusion_matrix']
            
            # Create plot
            plt.figure(figsize=(8, 6))
            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                       xticklabels=['Not Fraud', 'Fraud'], 
                       yticklabels=['Not Fraud', 'Fraud'])
            plt.title(f'Confusion Matrix - {model_name}')
            plt.ylabel('Actual')
            plt.xlabel('Predicted')
            plt.tight_layout()
            
            return plt.gcf()
            
        except Exception as e:
            logger.error(f"Error plotting confusion matrix for {model_name}: {str(e)}")
            raise
    
    def plot_roc_curve(self, model_name):
        """
        Plot ROC curve for a model
        
        Args:
            model_name (str): Name of the model
        """
        try:
            # Get performance metrics
            performance = self.get_performance(model_name)
            fpr = performance['fpr']
            tpr = performance['tpr']
            roc_auc = performance['roc_auc']
            
            # Create plot
            plt.figure(figsize=(8, 6))
            plt.plot(fpr, tpr, label=f'{model_name} (AUC = {roc_auc:.3f})')
            plt.plot([0, 1], [0, 1], 'k--')
            plt.xlim([0.0, 1.0])
            plt.ylim([0.0, 1.05])
            plt.xlabel('False Positive Rate')
            plt.ylabel('True Positive Rate')
            plt.title(f'ROC Curve - {model_name}')
            plt.legend(loc="lower right")
            plt.tight_layout()
            
            return plt.gcf()
            
        except Exception as e:
            logger.error(f"Error plotting ROC curve for {model_name}: {str(e)}")
            raise
    
    def plot_precision_recall_curve(self, model_name):
        """
        Plot precision-recall curve for a model
        
        Args:
            model_name (str): Name of the model
        """
        try:
            # Get performance metrics
            performance = self.get_performance(model_name)
            precision_curve = performance['precision_curve']
            recall_curve = performance['recall_curve']
            avg_precision = performance['avg_precision']
            
            # Create plot
            plt.figure(figsize=(8, 6))
            plt.plot(recall_curve, precision_curve, label=f'{model_name} (AP = {avg_precision:.3f})')
            plt.xlim([0.0, 1.0])
            plt.ylim([0.0, 1.05])
            plt.xlabel('Recall')
            plt.ylabel('Precision')
            plt.title(f'Precision-Recall Curve - {model_name}')
            plt.legend(loc="lower left")
            plt.tight_layout()
            
            return plt.gcf()
            
        except Exception as e:
            logger.error(f"Error plotting precision-recall curve for {model_name}: {str(e)}")
            raise

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\models\unsupervised.py ===
"""
Unsupervised Models Module
Implements unsupervised learning models for fraud detection
"""
import pandas as pd
import numpy as np
from sklearn.ensemble import IsolationForest
from sklearn.neighbors import LocalOutlierFactor
from sklearn.svm import OneClassSVM
from sklearn.cluster import DBSCAN, KMeans
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.impute import SimpleImputer
from sklearn.metrics import silhouette_score
import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.callbacks import EarlyStopping
import warnings
import logging
from typing import Dict, List, Tuple, Union
warnings.filterwarnings('ignore')
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class UnsupervisedModels:
    """
    Class for unsupervised fraud detection models
    Implements Isolation Forest, Local Outlier Factor, Autoencoders, etc.
    """
    
    def __init__(self, contamination=0.01, random_state=42):
        """
        Initialize UnsupervisedModels
        
        Args:
            contamination (float): Expected proportion of outliers
            random_state (int): Random seed
        """
        self.contamination = contamination
        self.random_state = random_state
        self.models = {}
        self.feature_names = {}
        self.scalers = {}
        self.imputers = {}
        self.fitted = False
        
    def run_models(self, df):
        """
        Run all unsupervised models
        
        Args:
            df (DataFrame): Input data
            
        Returns:
            dict: Model results
        """
        try:
            # Get numeric columns
            numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
            
            if len(numeric_cols) < 2:
                logger.warning("Not enough numeric columns for unsupervised models")
                return {}
            
            # Prepare data
            X = df[numeric_cols].copy()
            
            # Clean the data - handle infinity and very large values
            X = self._clean_data(X)
            
            # Scale data
            X_scaled, scaler, imputer = self._scale_data(X)
            
            # Store scaler and imputer
            self.scalers['global'] = scaler
            self.imputers['global'] = imputer
            
            # Run different models
            results = {}
            
            # Isolation Forest
            try:
                if_model = IsolationForest(
                    contamination=self.contamination,
                    random_state=self.random_state,
                    n_jobs=-1,
                    max_samples='auto'
                )
                if_predictions = if_model.fit_predict(X_scaled)
                if_scores = if_model.decision_function(X_scaled)
                
                # Normalize scores to 0-1 range
                if_scores_normalized = self._normalize_scores(if_scores)
                
                results['isolation_forest'] = {
                    'predictions': if_predictions,
                    'scores': if_scores_normalized,
                    'model': if_model,
                    'feature_names': numeric_cols
                }
                
                self.models['isolation_forest'] = if_model
                self.feature_names['isolation_forest'] = numeric_cols
                
                logger.info("Isolation Forest model completed")
            except Exception as e:
                logger.error(f"Error running Isolation Forest: {str(e)}")
            
            # Local Outlier Factor
            try:
                lof_model = LocalOutlierFactor(
                    contamination=self.contamination,
                    n_neighbors=min(20, len(X_scaled) - 1),  # Ensure n_neighbors < n_samples
                    novelty=True,
                    n_jobs=-1
                )
                lof_predictions = lof_model.fit_predict(X_scaled)
                lof_scores = lof_model.decision_function(X_scaled)
                
                # Normalize scores to 0-1 range
                lof_scores_normalized = self._normalize_scores(lof_scores)
                
                results['local_outlier_factor'] = {
                    'predictions': lof_predictions,
                    'scores': lof_scores_normalized,
                    'model': lof_model,
                    'feature_names': numeric_cols
                }
                
                self.models['local_outlier_factor'] = lof_model
                self.feature_names['local_outlier_factor'] = numeric_cols
                
                logger.info("Local Outlier Factor model completed")
            except Exception as e:
                logger.error(f"Error running Local Outlier Factor: {str(e)}")
            
            # One-Class SVM
            try:
                ocsvm_model = OneClassSVM(
                    nu=self.contamination,
                    kernel='rbf',
                    gamma='scale'
                )
                ocsvm_predictions = ocsvm_model.fit_predict(X_scaled)
                ocsvm_scores = ocsvm_model.decision_function(X_scaled)
                
                # Normalize scores to 0-1 range
                ocsvm_scores_normalized = self._normalize_scores(ocsvm_scores)
                
                results['one_class_svm'] = {
                    'predictions': ocsvm_predictions,
                    'scores': ocsvm_scores_normalized,
                    'model': ocsvm_model,
                    'feature_names': numeric_cols
                }
                
                self.models['one_class_svm'] = ocsvm_model
                self.feature_names['one_class_svm'] = numeric_cols
                
                logger.info("One-Class SVM model completed")
            except Exception as e:
                logger.error(f"Error running One-Class SVM: {str(e)}")
            
            # DBSCAN Clustering
            try:
                # Use a subset of data for DBSCAN if it's too large
                if len(X_scaled) > 10000:
                    X_subset = X_scaled[np.random.choice(len(X_scaled), 10000, replace=False)]
                else:
                    X_subset = X_scaled
                
                dbscan_model = DBSCAN(eps=0.5, min_samples=5)
                dbscan_labels = dbscan_model.fit_predict(X_subset)
                
                # Convert to outlier predictions (-1 is outlier in DBSCAN)
                dbscan_predictions = np.where(dbscan_labels == -1, -1, 1)
                
                # Calculate distance to nearest core point as anomaly score
                dbscan_scores = np.zeros(len(X_subset))
                for i in range(len(X_subset)):
                    if dbscan_labels[i] == -1:  # Outlier
                        # Find distance to nearest core point
                        core_points = np.where(dbscan_labels != -1)[0]
                        if len(core_points) > 0:
                            distances = np.linalg.norm(X_subset[i] - X_subset[core_points], axis=1)
                            dbscan_scores[i] = distances.min()
                        else:
                            dbscan_scores[i] = np.max(np.linalg.norm(X_subset - X_subset.mean(axis=0), axis=1))
                
                # Normalize scores to 0-1 range
                dbscan_scores_normalized = self._normalize_scores(dbscan_scores)
                
                # For the full dataset, use the model to predict
                if len(X_scaled) > 10000:
                    full_predictions = dbscan_model.fit_predict(X_scaled)
                    full_predictions = np.where(full_predictions == -1, -1, 1)
                    
                    # Calculate scores for full dataset
                    full_scores = np.zeros(len(X_scaled))
                    for i in range(len(X_scaled)):
                        if full_predictions[i] == -1:  # Outlier
                            # Find distance to nearest core point
                            core_points = np.where(full_predictions != -1)[0]
                            if len(core_points) > 0:
                                distances = np.linalg.norm(X_scaled[i] - X_scaled[core_points], axis=1)
                                full_scores[i] = distances.min()
                            else:
                                full_scores[i] = np.max(np.linalg.norm(X_scaled - X_scaled.mean(axis=0), axis=1))
                    
                    # Normalize scores
                    full_scores_normalized = self._normalize_scores(full_scores)
                else:
                    full_predictions = dbscan_predictions
                    full_scores_normalized = dbscan_scores_normalized
                
                results['dbscan'] = {
                    'predictions': full_predictions,
                    'scores': full_scores_normalized,
                    'model': dbscan_model,
                    'feature_names': numeric_cols
                }
                
                self.models['dbscan'] = dbscan_model
                self.feature_names['dbscan'] = numeric_cols
                
                logger.info("DBSCAN model completed")
            except Exception as e:
                logger.error(f"Error running DBSCAN: {str(e)}")
            
            # K-Means Clustering
            try:
                # Determine optimal number of clusters
                silhouette_scores = []
                k_range = range(2, min(11, len(X_scaled) // 2))
                
                for k in k_range:
                    kmeans = KMeans(n_clusters=k, random_state=self.random_state, n_init=10)
                    labels = kmeans.fit_predict(X_scaled)
                    silhouette_scores.append(silhouette_score(X_scaled, labels))
                
                # Find optimal k
                optimal_k = k_range[np.argmax(silhouette_scores)]
                
                # Fit K-Means with optimal k
                kmeans_model = KMeans(n_clusters=optimal_k, random_state=self.random_state, n_init=10)
                kmeans_labels = kmeans_model.fit_predict(X_scaled)
                
                # Calculate distance to nearest cluster center as anomaly score
                distances = kmeans_model.transform(X_scaled)
                min_distances = distances.min(axis=1)
                
                # Normalize scores to 0-1 range
                kmeans_scores_normalized = self._normalize_scores(min_distances)
                
                # Convert to outlier predictions (top contamination% are outliers)
                threshold = np.percentile(kmeans_scores_normalized, (1 - self.contamination) * 100)
                kmeans_predictions = np.where(kmeans_scores_normalized > threshold, -1, 1)
                
                results['kmeans'] = {
                    'predictions': kmeans_predictions,
                    'scores': kmeans_scores_normalized,
                    'model': kmeans_model,
                    'feature_names': numeric_cols,
                    'optimal_k': optimal_k,
                    'silhouette_scores': silhouette_scores
                }
                
                self.models['kmeans'] = kmeans_model
                self.feature_names['kmeans'] = numeric_cols
                
                logger.info(f"K-Means model completed with {optimal_k} clusters")
            except Exception as e:
                logger.error(f"Error running K-Means: {str(e)}")
            
            # Autoencoder
            try:
                # Build autoencoder model
                input_dim = X_scaled.shape[1]
                encoding_dim = max(2, input_dim // 2)  # At least 2 dimensions
                
                autoencoder = self._build_autoencoder(input_dim, encoding_dim)
                
                # Train autoencoder
                early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
                
                history = autoencoder.fit(
                    X_scaled, X_scaled,
                    epochs=50,
                    batch_size=32,
                    validation_split=0.2,
                    callbacks=[early_stopping],
                    verbose=0
                )
                
                # Get reconstruction errors
                reconstructions = autoencoder.predict(X_scaled)
                mse = np.mean(np.power(X_scaled - reconstructions, 2), axis=1)
                
                # Handle any infinity or NaN values in MSE
                mse = np.nan_to_num(mse)
                mse = np.clip(mse, 0, np.finfo(np.float64).max)  # Clip to valid range
                
                # Normalize scores to 0-1 range
                mse_normalized = self._normalize_scores(mse)
                
                # Convert to outlier predictions (top contamination% are outliers)
                threshold = np.percentile(mse_normalized, (1 - self.contamination) * 100)
                autoencoder_predictions = np.where(mse_normalized > threshold, -1, 1)
                
                results['autoencoder'] = {
                    'predictions': autoencoder_predictions,
                    'scores': mse_normalized,
                    'model': autoencoder,
                    'feature_names': numeric_cols,
                    'history': history.history
                }
                
                self.models['autoencoder'] = autoencoder
                self.feature_names['autoencoder'] = numeric_cols
                
                logger.info("Autoencoder model completed")
            except Exception as e:
                logger.error(f"Error running Autoencoder: {str(e)}")
            
            # PCA-based anomaly detection
            try:
                # Fit PCA
                pca = PCA(n_components=min(10, X_scaled.shape[1] - 1), random_state=self.random_state)
                pca_transformed = pca.fit_transform(X_scaled)
                
                # Calculate reconstruction error
                X_reconstructed = pca.inverse_transform(pca_transformed)
                pca_errors = np.mean(np.power(X_scaled - X_reconstructed, 2), axis=1)
                
                # Handle any infinity or NaN values in errors
                pca_errors = np.nan_to_num(pca_errors)
                pca_errors = np.clip(pca_errors, 0, np.finfo(np.float64).max)  # Clip to valid range
                
                # Normalize scores to 0-1 range
                pca_errors_normalized = self._normalize_scores(pca_errors)
                
                # Convert to outlier predictions (top contamination% are outliers)
                threshold = np.percentile(pca_errors_normalized, (1 - self.contamination) * 100)
                pca_predictions = np.where(pca_errors_normalized > threshold, -1, 1)
                
                results['pca'] = {
                    'predictions': pca_predictions,
                    'scores': pca_errors_normalized,
                    'model': pca,
                    'feature_names': numeric_cols,
                    'explained_variance': pca.explained_variance_ratio_
                }
                
                self.models['pca'] = pca
                self.feature_names['pca'] = numeric_cols
                
                logger.info("PCA-based anomaly detection completed")
            except Exception as e:
                logger.error(f"Error running PCA-based anomaly detection: {str(e)}")
            
            self.fitted = True
            return results
            
        except Exception as e:
            logger.error(f"Error running unsupervised models: {str(e)}")
            raise
    
    def _clean_data(self, X):
        """
        Clean data by handling infinity, NaN, and extreme values
        
        Args:
            X (DataFrame): Input data
            
        Returns:
            DataFrame: Cleaned data
        """
        try:
            # Replace infinity with NaN
            X = X.replace([np.inf, -np.inf], np.nan)
            
            # Replace extremely large values with a more reasonable maximum
            for col in X.columns:
                if X[col].dtype in ['float64', 'int64']:
                    # Calculate 99th percentile as a reasonable maximum
                    percentile_99 = np.nanpercentile(X[col], 99)
                    if not np.isnan(percentile_99):
                        # Cap values at 10 times the 99th percentile
                        max_val = percentile_99 * 10
                        X[col] = np.where(X[col] > max_val, max_val, X[col])
                        
                        # Similarly, handle extremely negative values
                        percentile_1 = np.nanpercentile(X[col], 1)
                        if not np.isnan(percentile_1):
                            min_val = percentile_1 * 10
                            X[col] = np.where(X[col] < min_val, min_val, X[col])
            
            return X
            
        except Exception as e:
            logger.error(f"Error cleaning data: {str(e)}")
            return X
    
    def _scale_data(self, X):
        """
        Scale data using RobustScaler (more resistant to outliers)
        
        Args:
            X (DataFrame): Input data
            
        Returns:
            tuple: (scaled data, scaler, imputer)
        """
        try:
            # First impute missing values
            imputer = SimpleImputer(strategy='median')
            X_imputed = imputer.fit_transform(X)
            
            # Use RobustScaler instead of StandardScaler to handle outliers better
            scaler = RobustScaler()
            X_scaled = scaler.fit_transform(X_imputed)
            
            # Check for any remaining infinity or NaN values
            X_scaled = np.nan_to_num(X_scaled)
            X_scaled = np.clip(X_scaled, -1e10, 1e10)  # Clip to reasonable range
            
            return X_scaled, scaler, imputer
            
        except Exception as e:
            logger.error(f"Error scaling data: {str(e)}")
            # Fallback to simple normalization
            X_normalized = (X - X.min()) / (X.max() - X.min())
            return X_normalized.values, None, None
    
    def _normalize_scores(self, scores):
        """
        Normalize scores to 0-1 range
        
        Args:
            scores (array): Input scores
            
        Returns:
            array: Normalized scores
        """
        try:
            # Handle NaN and infinity
            scores = np.nan_to_num(scores)
            scores = np.clip(scores, -1e10, 1e10)  # Clip to reasonable range
            
            # Min-max normalization
            min_score = scores.min()
            max_score = scores.max()
            
            if max_score > min_score:
                normalized_scores = (scores - min_score) / (max_score - min_score)
            else:
                normalized_scores = np.zeros_like(scores)
            
            return normalized_scores
            
        except Exception as e:
            logger.error(f"Error normalizing scores: {str(e)}")
            return np.zeros_like(scores)
    
    def _build_autoencoder(self, input_dim, encoding_dim):
        """
        Build autoencoder model
        
        Args:
            input_dim (int): Input dimension
            encoding_dim (int): Encoding dimension
            
        Returns:
            Model: Autoencoder model
        """
        try:
            # Encoder
            input_layer = layers.Input(shape=(input_dim,))
            encoded = layers.Dense(encoding_dim * 2, activation='relu')(input_layer)
            encoded = layers.Dense(encoding_dim, activation='relu')(encoded)
            
            # Decoder
            decoded = layers.Dense(encoding_dim * 2, activation='relu')(encoded)
            decoded = layers.Dense(input_dim, activation='linear')(decoded)
            
            # Autoencoder model
            autoencoder = models.Model(input_layer, decoded)
            
            # Compile model
            autoencoder.compile(optimizer='adam', loss='mean_squared_error')
            
            return autoencoder
            
        except Exception as e:
            logger.error(f"Error building autoencoder: {str(e)}")
            raise
    
    def predict(self, df, model_name):
        """
        Make predictions using a fitted model
        
        Args:
            df (DataFrame): Input data
            model_name (str): Name of the model to use
            
        Returns:
            dict: Predictions and scores
        """
        if not self.fitted:
            raise ValueError("Models not fitted. Call run_models first.")
        
        if model_name not in self.models:
            raise ValueError(f"Model {model_name} not found.")
        
        try:
            # Get feature names for this model
            feature_names = self.feature_names[model_name]
            
            # Prepare data
            X = df[feature_names].copy()
            
            # Clean the data
            X = self._clean_data(X)
            
            # Scale data using fitted scaler and imputer
            if 'global' in self.scalers and 'global' in self.imputers:
                X_imputed = self.imputers['global'].transform(X)
                X_scaled = self.scalers['global'].transform(X_imputed)
            else:
                # Fallback to simple normalization
                X_scaled = (X - X.min()) / (X.max() - X.min())
                X_scaled = np.nan_to_num(X_scaled)
                X_scaled = np.clip(X_scaled, -1e10, 1e10)
            
            # Get model
            model = self.models[model_name]
            
            # Make predictions
            if model_name == 'isolation_forest':
                predictions = model.predict(X_scaled)
                scores = model.decision_function(X_scaled)
                # Normalize scores to 0-1 range
                scores_normalized = self._normalize_scores(scores)
                
            elif model_name == 'local_outlier_factor':
                predictions = model.predict(X_scaled)
                scores = model.decision_function(X_scaled)
                # Normalize scores to 0-1 range
                scores_normalized = self._normalize_scores(scores)
                
            elif model_name == 'one_class_svm':
                predictions = model.predict(X_scaled)
                scores = model.decision_function(X_scaled)
                # Normalize scores to 0-1 range
                scores_normalized = self._normalize_scores(scores)
                
            elif model_name == 'dbscan':
                predictions = model.fit_predict(X_scaled)
                # Convert to outlier predictions (-1 is outlier in DBSCAN)
                predictions = np.where(predictions == -1, -1, 1)
                
                # Calculate distance to nearest core point as anomaly score
                scores = np.zeros(len(X_scaled))
                for i in range(len(X_scaled)):
                    if predictions[i] == -1:  # Outlier
                        # Find distance to nearest core point
                        core_points = np.where(predictions != -1)[0]
                        if len(core_points) > 0:
                            distances = np.linalg.norm(X_scaled[i] - X_scaled[core_points], axis=1)
                            scores[i] = distances.min()
                        else:
                            scores[i] = np.max(np.linalg.norm(X_scaled - X_scaled.mean(axis=0), axis=1))
                
                # Normalize scores to 0-1 range
                scores_normalized = self._normalize_scores(scores)
                
            elif model_name == 'kmeans':
                # Transform data
                distances = model.transform(X_scaled)
                min_distances = distances.min(axis=1)
                
                # Normalize scores to 0-1 range
                scores_normalized = self._normalize_scores(min_distances)
                
                # Convert to outlier predictions (top contamination% are outliers)
                threshold = np.percentile(scores_normalized, (1 - self.contamination) * 100)
                predictions = np.where(scores_normalized > threshold, -1, 1)
                
            elif model_name == 'autoencoder':
                # Get reconstructions
                reconstructions = model.predict(X_scaled)
                mse = np.mean(np.power(X_scaled - reconstructions, 2), axis=1)
                
                # Handle any infinity or NaN values in MSE
                mse = np.nan_to_num(mse)
                mse = np.clip(mse, 0, np.finfo(np.float64).max)
                
                # Normalize scores to 0-1 range
                scores_normalized = self._normalize_scores(mse)
                
                # Convert to outlier predictions (top contamination% are outliers)
                threshold = np.percentile(scores_normalized, (1 - self.contamination) * 100)
                predictions = np.where(scores_normalized > threshold, -1, 1)
                
            elif model_name == 'pca':
                # Transform data
                pca_transformed = model.transform(X_scaled)
                
                # Calculate reconstruction error
                X_reconstructed = model.inverse_transform(pca_transformed)
                mse = np.mean(np.power(X_scaled - X_reconstructed, 2), axis=1)
                
                # Handle any infinity or NaN values in errors
                mse = np.nan_to_num(mse)
                mse = np.clip(mse, 0, np.finfo(np.float64).max)
                
                # Normalize scores to 0-1 range
                scores_normalized = self._normalize_scores(mse)
                
                # Convert to outlier predictions (top contamination% are outliers)
                threshold = np.percentile(scores_normalized, (1 - self.contamination) * 100)
                predictions = np.where(scores_normalized > threshold, -1, 1)
                
            else:
                raise ValueError(f"Unknown model: {model_name}")
            
            return {
                'predictions': predictions,
                'scores': scores_normalized
            }
            
        except Exception as e:
            logger.error(f"Error making predictions with {model_name}: {str(e)}")
            raise
    
    def get_feature_importance(self, model_name):
        """
        Get feature importance for a model
        
        Args:
            model_name (str): Name of the model
            
        Returns:
            DataFrame: Feature importance
        """
        if not self.fitted:
            raise ValueError("Models not fitted. Call run_models first.")
        
        if model_name not in self.models:
            raise ValueError(f"Model {model_name} not found.")
        
        try:
            # Get feature names
            feature_names = self.feature_names[model_name]
            
            if model_name == 'isolation_forest':
                # Get feature importance from the model
                importance = self.models[model_name].feature_importances_
                
                # Create DataFrame
                importance_df = pd.DataFrame({
                    'feature': feature_names,
                    'importance': importance
                }).sort_values('importance', ascending=False)
                
                return importance_df
                
            elif model_name == 'pca':
                # Get explained variance ratio
                explained_variance = self.models[model_name].explained_variance_ratio_
                
                # Create DataFrame
                importance_df = pd.DataFrame({
                    'feature': [f'PC{i+1}' for i in range(len(explained_variance))],
                    'importance': explained_variance
                }).sort_values('importance', ascending=False)
                
                return importance_df
                
            else:
                # For other models, return equal importance
                importance = np.ones(len(feature_names)) / len(feature_names)
                
                # Create DataFrame
                importance_df = pd.DataFrame({
                    'feature': feature_names,
                    'importance': importance
                }).sort_values('importance', ascending=False)
                
                return importance_df
                
        except Exception as e:
            logger.error(f"Error getting feature importance for {model_name}: {str(e)}")
            raise

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\models\__init__.py ===

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\models\.ipynb_checkpoints\rule_based-checkpoint.py ===
"""
Rule-based Models Module
Implements rule-based fraud detection
"""

import pandas as pd
import numpy as np
import re
from datetime import datetime, timedelta
import yaml
import os
import warnings
import logging
from typing import Dict, List, Tuple, Union, Callable

from fraud_detection_engine.utils.api_utils import is_api_available, get_demo_sanctions_data, get_demo_tax_compliance_data, get_demo_bank_verification_data, get_demo_identity_verification_data

warnings.filterwarnings('ignore')
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class RuleEngine:
    """
    Class for rule-based fraud detection
    Implements configurable rules with weights
    """
    
    def __init__(self, config_path=None, threshold=0.7):
        """
        Initialize RuleEngine
        
        Args:
            config_path (str, optional): Path to configuration file
            threshold (float): Threshold for rule violation
        """
        self.threshold = threshold
        self.rules = {}
        self.rule_weights = {}
        self.rule_descriptions = {}
        self.fitted = False
        self.api_available = {}
        
        # Load configuration
        if config_path and os.path.exists(config_path):
            self._load_config(config_path)
        else:
            self._load_default_rules()
    
    def _load_config(self, config_path):
        """
        Load configuration from YAML file
        
        Args:
            config_path (str): Path to configuration file
        """
        try:
            with open(config_path, 'r') as f:
                config = yaml.safe_load(f)
            
            # Load rules
            if 'rules' in config:
                for rule_name, rule_config in config['rules'].items():
                    if rule_config.get('enabled', True):
                        self.rules[rule_name] = self._create_rule_function(rule_config)
                        self.rule_weights[rule_name] = rule_config.get('weight', 1.0)
                        self.rule_descriptions[rule_name] = rule_config.get('description', '')
            
            # Check API availability for rules that depend on external services
            self.api_available = {
                'sanctions': is_api_available('sanctions'),
                'tax_compliance': is_api_available('tax_compliance'),
                'bank_verification': is_api_available('bank_verification'),
                'identity_verification': is_api_available('identity_verification'),
                'geolocation': is_api_available('geolocation')
            }
            
            logger.info(f"API availability: {self.api_available}")
            
        except Exception as e:
            logger.error(f"Error loading configuration: {str(e)}")
            self._load_default_rules()
    
    def _load_default_rules(self):
        """
        Load default rules
        """
        try:
            # Amount-based rules
            self.rules['high_amount'] = self._high_amount_rule
            self.rule_weights['high_amount'] = 0.3
            self.rule_descriptions['high_amount'] = "Transaction amount exceeds threshold"
            
            self.rules['unusual_amount_for_sender'] = self._unusual_amount_for_sender_rule
            self.rule_weights['unusual_amount_for_sender'] = 0.2
            self.rule_descriptions['unusual_amount_for_sender'] = "Amount is unusual for the sender"
            
            self.rules['unusual_amount_for_receiver'] = self._unusual_amount_for_receiver_rule
            self.rule_weights['unusual_amount_for_receiver'] = 0.2
            self.rule_descriptions['unusual_amount_for_receiver'] = "Amount is unusual for the receiver"
            
            self.rules['round_amount'] = self._round_amount_rule
            self.rule_weights['round_amount'] = 0.1
            self.rule_descriptions['round_amount'] = "Transaction amount is suspiciously round"
            
            # Frequency-based rules
            self.rules['high_frequency_sender'] = self._high_frequency_sender_rule
            self.rule_weights['high_frequency_sender'] = 0.3
            self.rule_descriptions['high_frequency_sender'] = "High transaction frequency from sender"
            
            self.rules['high_frequency_receiver'] = self._high_frequency_receiver_rule
            self.rule_weights['high_frequency_receiver'] = 0.3
            self.rule_descriptions['high_frequency_receiver'] = "High transaction frequency to receiver"
            
            self.rules['rapid_succession'] = self._rapid_succession_rule
            self.rule_weights['rapid_succession'] = 0.4
            self.rule_descriptions['rapid_succession'] = "Multiple transactions in rapid succession"
            
            # Location-based rules
            self.rules['cross_border'] = self._cross_border_rule
            self.rule_weights['cross_border'] = 0.3
            self.rule_descriptions['cross_border'] = "Transaction crosses international borders"
            
            self.rules['high_risk_country'] = self._high_risk_country_rule
            self.rule_weights['high_risk_country'] = 0.5
            self.rule_descriptions['high_risk_country'] = "Transaction involves high-risk country"
            
            self.rules['unusual_location_for_sender'] = self._unusual_location_for_sender_rule
            self.rule_weights['unusual_location_for_sender'] = 0.3
            self.rule_descriptions['unusual_location_for_sender'] = "Transaction from unusual location for sender"
            
            # Time-based rules
            self.rules['unusual_hour'] = self._unusual_hour_rule
            self.rule_weights['unusual_hour'] = 0.2
            self.rule_descriptions['unusual_hour'] = "Transaction during unusual hours"
            
            self.rules['weekend'] = self._weekend_rule
            self.rule_weights['weekend'] = 0.1
            self.rule_descriptions['weekend'] = "Transaction on weekend"
            
            # Identity-based rules
            self.rules['new_sender'] = self._new_sender_rule
            self.rule_weights['new_sender'] = 0.3
            self.rule_descriptions['new_sender'] = "First transaction from new sender"
            
            self.rules['new_receiver'] = self._new_receiver_rule
            self.rule_weights['new_receiver'] = 0.3
            self.rule_descriptions['new_receiver'] = "First transaction to new receiver"
            
            # External API-based rules
            self.rules['sanctions_check'] = self._sanctions_check_rule
            self.rule_weights['sanctions_check'] = 0.5
            self.rule_descriptions['sanctions_check'] = "Transaction involves sanctioned entities"
            
            self.rules['tax_compliance'] = self._tax_compliance_rule
            self.rule_weights['tax_compliance'] = 0.3
            self.rule_descriptions['tax_compliance'] = "Transaction involves non-compliant entities"
            
            self.rules['bank_verification'] = self._bank_verification_rule
            self.rule_weights['bank_verification'] = 0.3
            self.rule_descriptions['bank_verification'] = "Transaction involves unverified bank accounts"
            
            self.rules['identity_verification'] = self._identity_verification_rule
            self.rule_weights['identity_verification'] = 0.3
            self.rule_descriptions['identity_verification'] = "Transaction involves unverified identities"
            
            # Check API availability
            self.api_available = {
                'sanctions': is_api_available('sanctions'),
                'tax_compliance': is_api_available('tax_compliance'),
                'bank_verification': is_api_available('bank_verification'),
                'identity_verification': is_api_available('identity_verification'),
                'geolocation': is_api_available('geolocation')
            }
            
            logger.info(f"API availability: {self.api_available}")
            
        except Exception as e:
            logger.error(f"Error loading default rules: {str(e)}")
            raise
    
    def _create_rule_function(self, rule_config):
        """
        Create a rule function from configuration
        
        Args:
            rule_config (dict): Rule configuration
            
        Returns:
            function: Rule function
        """
        rule_type = rule_config.get('type')
        
        if rule_type == 'amount_threshold':
            threshold = rule_config.get('threshold', 10000)
            return lambda row: row.get('amount', 0) > threshold
        
        elif rule_type == 'sender_amount_outlier':
            std_multiplier = rule_config.get('std_multiplier', 3)
            min_transactions = rule_config.get('min_transactions', 5)
            return lambda row: self._is_sender_amount_outlier(row, std_multiplier, min_transactions)
        
        elif rule_type == 'receiver_amount_outlier':
            std_multiplier = rule_config.get('std_multiplier', 3)
            min_transactions = rule_config.get('min_transactions', 5)
            return lambda row: self._is_receiver_amount_outlier(row, std_multiplier, min_transactions)
        
        elif rule_type == 'round_amount':
            threshold = rule_config.get('threshold', 1000)
            return lambda row: row.get('amount', 0) > threshold and row.get('amount', 0) % 1000 == 0
        
        elif rule_type == 'high_frequency_sender':
            time_window = rule_config.get('time_window', '1H')
            max_transactions = rule_config.get('max_transactions', 10)
            return lambda row: self._is_high_frequency_sender(row, time_window, max_transactions)
        
        elif rule_type == 'high_frequency_receiver':
            time_window = rule_config.get('time_window', '1H')
            max_transactions = rule_config.get('max_transactions', 10)
            return lambda row: self._is_high_frequency_receiver(row, time_window, max_transactions)
        
        elif rule_type == 'rapid_succession':
            time_window = rule_config.get('time_window', '5M')
            min_transactions = rule_config.get('min_transactions', 3)
            return lambda row: self._is_rapid_succession(row, time_window, min_transactions)
        
        elif rule_type == 'cross_border':
            return lambda row: self._is_cross_border(row)
        
        elif rule_type == 'high_risk_country':
            countries = rule_config.get('countries', ['North Korea', 'Iran', 'Syria', 'Cuba'])
            return lambda row: self._is_high_risk_country(row, countries)
        
        elif rule_type == 'unusual_location_for_sender':
            return lambda row: self._is_unusual_location_for_sender(row)
        
        elif rule_type == 'unusual_hour':
            start_hour = rule_config.get('start_hour', 23)
            end_hour = rule_config.get('end_hour', 5)
            return lambda row: self._is_unusual_hour(row, start_hour, end_hour)
        
        elif rule_type == 'weekend':
            return lambda row: self._is_weekend(row)
        
        elif rule_type == 'new_sender':
            time_window = rule_config.get('time_window', '7D')
            return lambda row: self._is_new_sender(row, time_window)
        
        elif rule_type == 'new_receiver':
            time_window = rule_config.get('time_window', '7D')
            return lambda row: self._is_new_receiver(row, time_window)
        
        else:
            logger.warning(f"Unknown rule type: {rule_type}")
            return lambda row: False
    
    def apply_rules(self, df):
        """
        Apply all rules to the dataframe
        
        Args:
            df (DataFrame): Input data
            
        Returns:
            dict: Rule results
        """
        try:
            # Sort by timestamp if available
            if 'timestamp' in df.columns and not pd.api.types.is_datetime64_any_dtype(df['timestamp']):
                df = df.copy()
                df['timestamp'] = pd.to_datetime(df['timestamp'])
            
            if 'timestamp' in df.columns:
                df = df.sort_values('timestamp')
            
            # Initialize results
            rule_results = {}
            rule_scores = {}
            violated_rules = {}
            
            # Apply each rule
            for rule_name, rule_func in self.rules.items():
                try:
                    # Apply rule to each row
                    results = []
                    for _, row in df.iterrows():
                        try:
                            result = rule_func(row)
                            results.append(result)
                        except Exception as e:
                            logger.warning(f"Error applying rule {rule_name} to row: {str(e)}")
                            results.append(False)
                    
                    rule_results[rule_name] = results
                    
                    # Calculate weighted score
                    weight = self.rule_weights.get(rule_name, 1.0)
                    rule_scores[rule_name] = [weight if result else 0 for result in results]
                    
                    # Track violated rules
                    violated_rules[rule_name] = [i for i, result in enumerate(results) if result]
                    
                except Exception as e:
                    logger.error(f"Error applying rule {rule_name}: {str(e)}")
                    rule_results[rule_name] = [False] * len(df)
                    rule_scores[rule_name] = [0] * len(df)
                    violated_rules[rule_name] = []
            
            # Calculate total rule score for each transaction
            total_scores = []
            for i in range(len(df)):
                score = sum(rule_scores[rule_name][i] for rule_name in rule_scores)
                total_scores.append(score)
            
            # Normalize scores
            max_possible_score = sum(self.rule_weights.values())
            normalized_scores = [score / max_possible_score for score in total_scores]
            
            # Determine if transaction violates rules based on threshold
            rule_violations = [score >= self.threshold for score in normalized_scores]
            
            # Get violated rule names for each transaction
            violated_rule_names = []
            for i in range(len(df)):
                names = []
                for rule_name in violated_rules:
                    if i in violated_rules[rule_name]:
                        names.append(rule_name)
                violated_rule_names.append(names)
            
            self.fitted = True
            
            return {
                'rule_results': rule_results,
                'rule_scores': rule_scores,
                'total_scores': total_scores,
                'normalized_scores': normalized_scores,
                'rule_violations': rule_violations,
                'violated_rule_names': violated_rule_names,
                'rules': self.rules,
                'rule_weights': self.rule_weights,
                'rule_descriptions': self.rule_descriptions
            }
            
        except Exception as e:
            logger.error(f"Error applying rules: {str(e)}")
            raise
    
    # Rule functions
    def _high_amount_rule(self, row):
        """Check if transaction amount is high"""
        return row.get('amount', 0) > 10000
    
    def _unusual_amount_for_sender_rule(self, row, std_multiplier=3, min_transactions=5):
        """Check if amount is unusual for the sender"""
        # This would require access to sender's transaction history
        # For simplicity, we'll use a placeholder
        return False
    
    def _unusual_amount_for_receiver_rule(self, row, std_multiplier=3, min_transactions=5):
        """Check if amount is unusual for the receiver"""
        # This would require access to receiver's transaction history
        # For simplicity, we'll use a placeholder
        return False
    
    def _round_amount_rule(self, row):
        """Check if transaction amount is suspiciously round"""
        amount = row.get('amount', 0)
        return amount > 1000 and amount % 1000 == 0
    
    def _high_frequency_sender_rule(self, row, time_window='1H', max_transactions=10):
        """Check if sender has high transaction frequency"""
        # This would require access to sender's transaction history
        # For simplicity, we'll use a placeholder
        return False
    
    def _high_frequency_receiver_rule(self, row, time_window='1H', max_transactions=10):
        """Check if receiver has high transaction frequency"""
        # This would require access to receiver's transaction history
        # For simplicity, we'll use a placeholder
        return False
    
    def _rapid_succession_rule(self, row, time_window='5M', min_transactions=3):
        """Check if there are multiple transactions in rapid succession"""
        # This would require access to transaction history
        # For simplicity, we'll use a placeholder
        return False
    
    def _cross_border_rule(self, row):
        """Check if transaction crosses international borders"""
        sender_location = row.get('sender_location', '')
        receiver_location = row.get('receiver_location', '')
        
        if not sender_location or not receiver_location:
            return False
        
        # Extract countries from locations
        sender_country = sender_location.split(',')[0].strip()
        receiver_country = receiver_location.split(',')[0].strip()
        
        return sender_country != receiver_country
    
    def _high_risk_country_rule(self, row, countries=None):
        """Check if transaction involves high-risk country"""
        if countries is None:
            countries = ['North Korea', 'Iran', 'Syria', 'Cuba']
        
        sender_location = row.get('sender_location', '')
        receiver_location = row.get('receiver_location', '')
        
        # Check if any high-risk country is involved
        for country in countries:
            if country in sender_location or country in receiver_location:
                return True
        
        return False
    
    def _unusual_location_for_sender_rule(self, row):
        """Check if transaction is from unusual location for sender"""
        # This would require access to sender's transaction history
        # For simplicity, we'll use a placeholder
        return False
    
    def _unusual_hour_rule(self, row, start_hour=23, end_hour=5):
        """Check if transaction is during unusual hours"""
        timestamp = row.get('timestamp')
        if timestamp is None:
            return False
        
        if not pd.api.types.is_datetime64_any_dtype(timestamp):
            timestamp = pd.to_datetime(timestamp)
        
        hour = timestamp.hour
        
        # Handle overnight range
        if start_hour > end_hour:
            return hour >= start_hour or hour < end_hour
        else:
            return start_hour <= hour < end_hour
    
    def _weekend_rule(self, row):
        """Check if transaction is on weekend"""
        timestamp = row.get('timestamp')
        if timestamp is None:
            return False
        
        if not pd.api.types.is_datetime64_any_dtype(timestamp):
            timestamp = pd.to_datetime(timestamp)
        
        # Saturday=5, Sunday=6
        return timestamp.dayofweek >= 5
    
    def _new_sender_rule(self, row, time_window='7D'):
        """Check if this is the first transaction from sender"""
        # This would require access to sender's transaction history
        # For simplicity, we'll use a placeholder
        return False
    
    def _new_receiver_rule(self, row, time_window='7D'):
        """Check if this is the first transaction to receiver"""
        # This would require access to receiver's transaction history
        # For simplicity, we'll use a placeholder
        return False
    
    def _sanctions_check_rule(self, row):
        """Check if transaction involves sanctioned entities"""
        try:
            if not self.api_available.get('sanctions', False):
                # Use demo data
                sanctions_data = get_demo_sanctions_data()
                
                # Simple demo check
                sender_id = row.get('sender_id', '')
                receiver_id = row.get('receiver_id', '')
                
                # Check if sender or receiver is in demo sanctions list
                is_sanctioned = (
                    sanctions_data['entity_id'].str.contains(sender_id, na=False).any() or
                    sanctions_data['entity_id'].str.contains(receiver_id, na=False).any()
                )
                
                return is_sanctioned
            else:
                # Here you would implement the actual API call
                # For now, return False
                return False
        except Exception as e:
            logger.warning(f"Error in sanctions check: {str(e)}")
            return False
    
    def _tax_compliance_rule(self, row):
        """Check if entities are tax compliant"""
        try:
            if not self.api_available.get('tax_compliance', False):
                # Use demo data
                tax_data = get_demo_tax_compliance_data()
                
                # Simple demo check
                sender_id = row.get('sender_id', '')
                receiver_id = row.get('receiver_id', '')
                
                # Check if sender or receiver is non-compliant
                is_non_compliant = False
                
                for _, entity in tax_data.iterrows():
                    if entity['entity_id'] in [sender_id, receiver_id] and entity['compliance_status'] == 'Non-compliant':
                        is_non_compliant = True
                        break
                
                return is_non_compliant
            else:
                # Here you would implement the actual API call
                # For now, return False
                return False
        except Exception as e:
            logger.warning(f"Error in tax compliance check: {str(e)}")
            return False
    
    def _bank_verification_rule(self, row):
        """Check if bank accounts are verified"""
        try:
            if not self.api_available.get('bank_verification', False):
                # Use demo data
                bank_data = get_demo_bank_verification_data()
                
                # Simple demo check
                sender_id = row.get('sender_id', '')
                receiver_id = row.get('receiver_id', '')
                
                # Check if sender or receiver account is not verified
                is_unverified = False
                
                for _, account in bank_data.iterrows():
                    if account['account_number'] in [sender_id, receiver_id] and account['verification_status'] == 'Not Verified':
                        is_unverified = True
                        break
                
                return is_unverified
            else:
                # Here you would implement the actual API call
                # For now, return False
                return False
        except Exception as e:
            logger.warning(f"Error in bank verification check: {str(e)}")
            return False
    
    def _identity_verification_rule(self, row):
        """Check if identities are verified"""
        try:
            if not self.api_available.get('identity_verification', False):
                # Use demo data
                identity_data = get_demo_identity_verification_data()
                
                # Simple demo check
                sender_id = row.get('sender_id', '')
                receiver_id = row.get('receiver_id', '')
                
                # Check if sender or receiver identity is not verified
                is_unverified = False
                
                for _, identity in identity_data.iterrows():
                    if identity['id_number'] in [sender_id, receiver_id] and identity['verification_status'] == 'Not Verified':
                        is_unverified = True
                        break
                
                return is_unverified
            else:
                # Here you would implement the actual API call
                # For now, return False
                return False
        except Exception as e:
            logger.warning(f"Error in identity verification check: {str(e)}")
            return False
    
    def add_rule(self, rule_name, rule_func, weight=1.0, description=''):
        """
        Add a custom rule
        
        Args:
            rule_name (str): Name of the rule
            rule_func (function): Rule function
            weight (float): Weight of the rule
            description (str): Description of the rule
        """
        self.rules[rule_name] = rule_func
        self.rule_weights[rule_name] = weight
        self.rule_descriptions[rule_name] = description
        logger.info(f"Added rule: {rule_name}")
    
    def remove_rule(self, rule_name):
        """
        Remove a rule
        
        Args:
            rule_name (str): Name of the rule to remove
        """
        if rule_name in self.rules:
            del self.rules[rule_name]
            del self.rule_weights[rule_name]
            del self.rule_descriptions[rule_name]
            logger.info(f"Removed rule: {rule_name}")
        else:
            logger.warning(f"Rule not found: {rule_name}")
    
    def update_rule_weight(self, rule_name, weight):
        """
        Update the weight of a rule
        
        Args:
            rule_name (str): Name of the rule
            weight (float): New weight
        """
        if rule_name in self.rule_weights:
            self.rule_weights[rule_name] = weight
            logger.info(f"Updated weight for rule {rule_name}: {weight}")
        else:
            logger.warning(f"Rule not found: {rule_name}")
    
    def get_rules(self):
        """
        Get all rules
        
        Returns:
            dict: Rules information
        """
        return {
            'rules': list(self.rules.keys()),
            'weights': self.rule_weights,
            'descriptions': self.rule_descriptions
        }
    
    def save_config(self, config_path):
        """
        Save current configuration to file
        
        Args:
            config_path (str): Path to save configuration
        """
        try:
            config = {
                'rules': {}
            }
            
            for rule_name in self.rules:
                config['rules'][rule_name] = {
                    'enabled': True,
                    'weight': self.rule_weights[rule_name],
                    'description': self.rule_descriptions[rule_name]
                }
            
            with open(config_path, 'w') as f:
                yaml.dump(config, f, default_flow_style=False)
            
            logger.info(f"Configuration saved to {config_path}")
            
        except Exception as e:
            logger.error(f"Error saving configuration: {str(e)}")
            raise

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\models\.ipynb_checkpoints\supervised-checkpoint.py ===
"""
Supervised Models Module
Implements supervised learning models for fraud detection
"""
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score, 
    roc_auc_score, confusion_matrix, classification_report, 
    precision_recall_curve, average_precision_score, roc_curve
)
from sklearn.preprocessing import StandardScaler, LabelEncoder, RobustScaler
from sklearn.feature_selection import SelectKBest, f_classif, RFE
from imblearn.over_sampling import SMOTE, ADASYN, BorderlineSMOTE
from imblearn.under_sampling import RandomUnderSampler, TomekLinks
from imblearn.combine import SMOTEENN, SMOTETomek
import xgboost as xgb
import lightgbm as lgb
import shap
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
import logging
from typing import Dict, List, Tuple, Union
warnings.filterwarnings('ignore')
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class SupervisedModels:
    """
    Class for supervised fraud detection models
    Implements Random Forest, XGBoost, Logistic Regression, etc.
    """
    
    def __init__(self, test_size=0.2, random_state=42, handle_imbalance=True):
        """
        Initialize SupervisedModels
        
        Args:
            test_size (float): Proportion of data for testing
            random_state (int): Random seed
            handle_imbalance (bool): Whether to handle class imbalance
        """
        self.test_size = test_size
        self.random_state = random_state
        self.handle_imbalance = handle_imbalance
        self.models = {}
        self.feature_names = {}
        self.scalers = {}
        self.label_encoders = {}
        self.feature_selectors = {}
        self.resamplers = {}
        self.performance = {}
        self.feature_importance = {}
        self.shap_values = {}
        self.fitted = False
        
    def run_models(self, df):
        """
        Run all supervised models
        
        Args:
            df (DataFrame): Input data
            
        Returns:
            dict: Model results
        """
        try:
            # Check if fraud_flag column exists
            if 'fraud_flag' not in df.columns:
                logger.warning("No fraud_flag column found. Using synthetic labels for demonstration.")
                # Create synthetic labels based on statistical outliers
                numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
                if len(numeric_cols) > 0:
                    # Use Z-score to identify outliers as potential fraud
                    X = df[numeric_cols].fillna(0)
                    scaler = StandardScaler()
                    X_scaled = scaler.fit_transform(X)
                    
                    # Calculate Z-scores
                    z_scores = np.abs(X_scaled).max(axis=1)
                    
                    # Label top 5% as fraud
                    threshold = np.percentile(z_scores, 95)
                    y = (z_scores > threshold).astype(int)
                    
                    # Add synthetic labels to dataframe
                    df = df.copy()
                    df['fraud_flag'] = y
                else:
                    raise ValueError("No numeric columns found for creating synthetic labels")
            else:
                y = df['fraud_flag'].astype(int)
            
            # Get feature columns (exclude target and ID columns)
            exclude_cols = ['fraud_flag', 'transaction_id', 'sender_id', 'receiver_id']
            feature_cols = [col for col in df.columns if col not in exclude_cols]
            
            # Get numeric and categorical features
            numeric_features = df[feature_cols].select_dtypes(include=[np.number]).columns.tolist()
            categorical_features = df[feature_cols].select_dtypes(include=['object', 'category']).columns.tolist()
            
            # Prepare data
            X = df[feature_cols].copy()
            
            # Clean data before processing
            X = self._clean_data(X)
            
            # Handle categorical features
            for col in categorical_features:
                if col in X.columns:
                    # Label encode categorical features
                    le = LabelEncoder()
                    X[col] = le.fit_transform(X[col].astype(str))
                    self.label_encoders[col] = le
            
            # Split data
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=self.test_size, random_state=self.random_state, stratify=y
            )
            
            # Scale numeric features using RobustScaler
            if numeric_features:
                scaler = RobustScaler()
                X_train[numeric_features] = scaler.fit_transform(X_train[numeric_features])
                X_test[numeric_features] = scaler.transform(X_test[numeric_features])
                self.scalers['numeric'] = scaler
            
            # Handle class imbalance
            if self.handle_imbalance and y_train.sum() / len(y_train) < 0.1:  # If minority class < 10%
                # Try different resampling techniques
                resampling_methods = {
                    'smote': SMOTE(random_state=self.random_state),
                    'adasyn': ADASYN(random_state=self.random_state),
                    'borderline_smote': BorderlineSMOTE(random_state=self.random_state),
                    'smote_enn': SMOTEENN(random_state=self.random_state),
                    'smote_tomek': SMOTETomek(random_state=self.random_state)
                }
                
                # Evaluate each method using cross-validation
                best_method = None
                best_score = 0
                
                for method_name, resampler in resampling_methods.items():
                    try:
                        # Apply resampling
                        X_resampled, y_resampled = resampler.fit_resample(X_train, y_train)
                        
                        # Evaluate with a simple model
                        rf = RandomForestClassifier(n_estimators=50, random_state=self.random_state, n_jobs=-1)
                        scores = cross_val_score(rf, X_resampled, y_resampled, cv=3, scoring='f1')
                        avg_score = scores.mean()
                        
                        if avg_score > best_score:
                            best_score = avg_score
                            best_method = method_name
                            self.resamplers['best'] = resampler
                            X_train, y_train = X_resampled, y_resampled
                    except Exception as e:
                        logger.warning(f"Error with {method_name}: {str(e)}")
                
                if best_method:
                    logger.info(f"Using {best_method} for handling class imbalance")
                else:
                    logger.warning("No suitable resampling method found")
            
            # Store feature names
            all_feature_names = feature_cols
            
            # Run different models
            results = {}
            
            # Random Forest
            try:
                rf_model = RandomForestClassifier(
                    n_estimators=100,
                    max_depth=10,
                    min_samples_split=5,
                    min_samples_leaf=2,
                    class_weight='balanced',
                    random_state=self.random_state,
                    n_jobs=-1
                )
                
                rf_model.fit(X_train, y_train)
                rf_pred = rf_model.predict(X_test)
                rf_proba = rf_model.predict_proba(X_test)[:, 1]
                
                # Calculate performance metrics
                rf_performance = self._calculate_performance_metrics(y_test, rf_pred, rf_proba)
                
                # Get feature importance
                rf_importance = pd.DataFrame({
                    'feature': all_feature_names,
                    'importance': rf_model.feature_importances_
                }).sort_values('importance', ascending=False)
                
                # Calculate SHAP values
                explainer = shap.TreeExplainer(rf_model)
                rf_shap_values = explainer.shap_values(X_test)
                
                results['random_forest'] = {
                    'model': rf_model,
                    'predictions': rf_pred,
                    'probabilities': rf_proba,
                    'performance': rf_performance,
                    'feature_importance': rf_importance,
                    'shap_values': rf_shap_values,
                    'feature_names': all_feature_names
                }
                
                self.models['random_forest'] = rf_model
                self.feature_names['random_forest'] = all_feature_names
                self.performance['random_forest'] = rf_performance
                self.feature_importance['random_forest'] = rf_importance
                self.shap_values['random_forest'] = rf_shap_values
                
                logger.info("Random Forest model completed")
            except Exception as e:
                logger.error(f"Error running Random Forest: {str(e)}")
            
            # XGBoost
            try:
                xgb_model = xgb.XGBClassifier(
                    n_estimators=100,
                    max_depth=6,
                    learning_rate=0.1,
                    subsample=0.8,
                    colsample_bytree=0.8,
                    scale_pos_weight=len(y_train[y_train==0]) / len(y_train[y_train==1]),
                    random_state=self.random_state,
                    use_label_encoder=False,
                    eval_metric='logloss'
                )
                
                xgb_model.fit(X_train, y_train)
                xgb_pred = xgb_model.predict(X_test)
                xgb_proba = xgb_model.predict_proba(X_test)[:, 1]
                
                # Calculate performance metrics
                xgb_performance = self._calculate_performance_metrics(y_test, xgb_pred, xgb_proba)
                
                # Get feature importance
                xgb_importance = pd.DataFrame({
                    'feature': all_feature_names,
                    'importance': xgb_model.feature_importances_
                }).sort_values('importance', ascending=False)
                
                # Calculate SHAP values
                explainer = shap.TreeExplainer(xgb_model)
                xgb_shap_values = explainer.shap_values(X_test)
                
                results['xgboost'] = {
                    'model': xgb_model,
                    'predictions': xgb_pred,
                    'probabilities': xgb_proba,
                    'performance': xgb_performance,
                    'feature_importance': xgb_importance,
                    'shap_values': xgb_shap_values,
                    'feature_names': all_feature_names
                }
                
                self.models['xgboost'] = xgb_model
                self.feature_names['xgboost'] = all_feature_names
                self.performance['xgboost'] = xgb_performance
                self.feature_importance['xgboost'] = xgb_importance
                self.shap_values['xgboost'] = xgb_shap_values
                
                logger.info("XGBoost model completed")
            except Exception as e:
                logger.error(f"Error running XGBoost: {str(e)}")
            
            # LightGBM
            try:
                lgb_model = lgb.LGBMClassifier(
                    n_estimators=100,
                    max_depth=6,
                    learning_rate=0.1,
                    subsample=0.8,
                    colsample_bytree=0.8,
                    scale_pos_weight=len(y_train[y_train==0]) / len(y_train[y_train==1]),
                    random_state=self.random_state
                )
                
                lgb_model.fit(X_train, y_train)
                lgb_pred = lgb_model.predict(X_test)
                lgb_proba = lgb_model.predict_proba(X_test)[:, 1]
                
                # Calculate performance metrics
                lgb_performance = self._calculate_performance_metrics(y_test, lgb_pred, lgb_proba)
                
                # Get feature importance
                lgb_importance = pd.DataFrame({
                    'feature': all_feature_names,
                    'importance': lgb_model.feature_importances_
                }).sort_values('importance', ascending=False)
                
                # Calculate SHAP values
                explainer = shap.TreeExplainer(lgb_model)
                lgb_shap_values = explainer.shap_values(X_test)
                
                results['lightgbm'] = {
                    'model': lgb_model,
                    'predictions': lgb_pred,
                    'probabilities': lgb_proba,
                    'performance': lgb_performance,
                    'feature_importance': lgb_importance,
                    'shap_values': lgb_shap_values,
                    'feature_names': all_feature_names
                }
                
                self.models['lightgbm'] = lgb_model
                self.feature_names['lightgbm'] = all_feature_names
                self.performance['lightgbm'] = lgb_performance
                self.feature_importance['lightgbm'] = lgb_importance
                self.shap_values['lightgbm'] = lgb_shap_values
                
                logger.info("LightGBM model completed")
            except Exception as e:
                logger.error(f"Error running LightGBM: {str(e)}")
            
            # Logistic Regression
            try:
                lr_model = LogisticRegression(
                    C=1.0,
                    class_weight='balanced',
                    random_state=self.random_state,
                    max_iter=1000
                )
                
                lr_model.fit(X_train, y_train)
                lr_pred = lr_model.predict(X_test)
                lr_proba = lr_model.predict_proba(X_test)[:, 1]
                
                # Calculate performance metrics
                lr_performance = self._calculate_performance_metrics(y_test, lr_pred, lr_proba)
                
                # Get feature importance (coefficients)
                lr_importance = pd.DataFrame({
                    'feature': all_feature_names,
                    'importance': np.abs(lr_model.coef_[0])
                }).sort_values('importance', ascending=False)
                
                # Calculate SHAP values
                explainer = shap.LinearExplainer(lr_model, X_train)
                lr_shap_values = explainer.shap_values(X_test)
                
                results['logistic_regression'] = {
                    'model': lr_model,
                    'predictions': lr_pred,
                    'probabilities': lr_proba,
                    'performance': lr_performance,
                    'feature_importance': lr_importance,
                    'shap_values': lr_shap_values,
                    'feature_names': all_feature_names
                }
                
                self.models['logistic_regression'] = lr_model
                self.feature_names['logistic_regression'] = all_feature_names
                self.performance['logistic_regression'] = lr_performance
                self.feature_importance['logistic_regression'] = lr_importance
                self.shap_values['logistic_regression'] = lr_shap_values
                
                logger.info("Logistic Regression model completed")
            except Exception as e:
                logger.error(f"Error running Logistic Regression: {str(e)}")
            
            # Gradient Boosting
            try:
                gb_model = GradientBoostingClassifier(
                    n_estimators=100,
                    max_depth=6,
                    learning_rate=0.1,
                    subsample=0.8,
                    random_state=self.random_state
                )
                
                gb_model.fit(X_train, y_train)
                gb_pred = gb_model.predict(X_test)
                gb_proba = gb_model.predict_proba(X_test)[:, 1]
                
                # Calculate performance metrics
                gb_performance = self._calculate_performance_metrics(y_test, gb_pred, gb_proba)
                
                # Get feature importance
                gb_importance = pd.DataFrame({
                    'feature': all_feature_names,
                    'importance': gb_model.feature_importances_
                }).sort_values('importance', ascending=False)
                
                # Calculate SHAP values
                explainer = shap.TreeExplainer(gb_model)
                gb_shap_values = explainer.shap_values(X_test)
                
                results['gradient_boosting'] = {
                    'model': gb_model,
                    'predictions': gb_pred,
                    'probabilities': gb_proba,
                    'performance': gb_performance,
                    'feature_importance': gb_importance,
                    'shap_values': gb_shap_values,
                    'feature_names': all_feature_names
                }
                
                self.models['gradient_boosting'] = gb_model
                self.feature_names['gradient_boosting'] = all_feature_names
                self.performance['gradient_boosting'] = gb_performance
                self.feature_importance['gradient_boosting'] = gb_importance
                self.shap_values['gradient_boosting'] = gb_shap_values
                
                logger.info("Gradient Boosting model completed")
            except Exception as e:
                logger.error(f"Error running Gradient Boosting: {str(e)}")
            
            # Neural Network
            try:
                nn_model = MLPClassifier(
                    hidden_layer_sizes=(100, 50),
                    activation='relu',
                    solver='adam',
                    alpha=0.0001,
                    batch_size=32,
                    learning_rate='adaptive',
                    max_iter=200,
                    random_state=self.random_state
                )
                
                nn_model.fit(X_train, y_train)
                nn_pred = nn_model.predict(X_test)
                nn_proba = nn_model.predict_proba(X_test)[:, 1]
                
                # Calculate performance metrics
                nn_performance = self._calculate_performance_metrics(y_test, nn_pred, nn_proba)
                
                # For neural networks, we don't have direct feature importance
                # Use permutation importance instead
                from sklearn.inspection import permutation_importance
                
                perm_importance = permutation_importance(
                    nn_model, X_test, y_test, n_repeats=5, random_state=self.random_state
                )
                
                nn_importance = pd.DataFrame({
                    'feature': all_feature_names,
                    'importance': perm_importance.importances_mean
                }).sort_values('importance', ascending=False)
                
                # Calculate SHAP values (using KernelExplainer for black-box models)
                explainer = shap.KernelExplainer(nn_model.predict_proba, X_train[:100])  # Use subset for speed
                nn_shap_values = explainer.shap_values(X_test[:100])  # Use subset for speed
                
                results['neural_network'] = {
                    'model': nn_model,
                    'predictions': nn_pred,
                    'probabilities': nn_proba,
                    'performance': nn_performance,
                    'feature_importance': nn_importance,
                    'shap_values': nn_shap_values,
                    'feature_names': all_feature_names
                }
                
                self.models['neural_network'] = nn_model
                self.feature_names['neural_network'] = all_feature_names
                self.performance['neural_network'] = nn_performance
                self.feature_importance['neural_network'] = nn_importance
                self.shap_values['neural_network'] = nn_shap_values
                
                logger.info("Neural Network model completed")
            except Exception as e:
                logger.error(f"Error running Neural Network: {str(e)}")
            
            # Ensemble model (combine predictions from all models)
            try:
                # Get predictions from all models
                all_predictions = []
                all_probabilities = []
                
                for model_name in results:
                    all_predictions.append(results[model_name]['predictions'])
                    all_probabilities.append(results[model_name]['probabilities'])
                
                # Majority voting for predictions
                ensemble_pred = np.array(all_predictions).mean(axis=0) > 0.5
                
                # Average probabilities
                ensemble_proba = np.array(all_probabilities).mean(axis=0)
                
                # Calculate performance metrics
                ensemble_performance = self._calculate_performance_metrics(y_test, ensemble_pred, ensemble_proba)
                
                # For ensemble, average feature importances
                ensemble_importance = pd.DataFrame({
                    'feature': all_feature_names,
                    'importance': np.zeros(len(all_feature_names))
                })
                
                for model_name in results:
                    if model_name in self.feature_importance:
                        model_importance = self.feature_importance[model_name]
                        for i, feature in enumerate(all_feature_names):
                            if feature in model_importance['feature'].values:
                                idx = model_importance[model_importance['feature'] == feature].index[0]
                                ensemble_importance.loc[i, 'importance'] += model_importance.loc[idx, 'importance']
                
                # Normalize importance
                ensemble_importance['importance'] = ensemble_importance['importance'] / len(results)
                ensemble_importance = ensemble_importance.sort_values('importance', ascending=False)
                
                results['ensemble'] = {
                    'predictions': ensemble_pred,
                    'probabilities': ensemble_proba,
                    'performance': ensemble_performance,
                    'feature_importance': ensemble_importance,
                    'feature_names': all_feature_names
                }
                
                self.performance['ensemble'] = ensemble_performance
                self.feature_importance['ensemble'] = ensemble_importance
                
                logger.info("Ensemble model completed")
            except Exception as e:
                logger.error(f"Error running Ensemble model: {str(e)}")
            
            self.fitted = True
            return results
            
        except Exception as e:
            logger.error(f"Error running supervised models: {str(e)}")
            raise
    
    def _clean_data(self, X):
        """
        Clean data by handling infinity, NaN, and extreme values
        
        Args:
            X (DataFrame): Input data
            
        Returns:
            DataFrame: Cleaned data
        """
        try:
            # Replace infinity with NaN
            X = X.replace([np.inf, -np.inf], np.nan)
            
            # Replace extremely large values with a more reasonable maximum
            for col in X.columns:
                if X[col].dtype in ['float64', 'int64']:
                    # Calculate 99th percentile as a reasonable maximum
                    percentile_99 = np.nanpercentile(X[col], 99)
                    if not np.isnan(percentile_99):
                        # Cap values at 10 times the 99th percentile
                        max_val = percentile_99 * 10
                        X[col] = np.where(X[col] > max_val, max_val, X[col])
                        
                        # Similarly, handle extremely negative values
                        percentile_1 = np.nanpercentile(X[col], 1)
                        if not np.isnan(percentile_1):
                            min_val = percentile_1 * 10
                            X[col] = np.where(X[col] < min_val, min_val, X[col])
            
            return X
            
        except Exception as e:
            logger.error(f"Error cleaning data: {str(e)}")
            return X
    
    def _calculate_performance_metrics(self, y_true, y_pred, y_proba):
        """
        Calculate performance metrics for a model
        
        Args:
            y_true (array): True labels
            y_pred (array): Predicted labels
            y_proba (array): Predicted probabilities
            
        Returns:
            dict: Performance metrics
        """
        try:
            # Basic metrics
            accuracy = accuracy_score(y_true, y_pred)
            precision = precision_score(y_true, y_pred, zero_division=0)
            recall = recall_score(y_true, y_pred, zero_division=0)
            f1 = f1_score(y_true, y_pred, zero_division=0)
            
            # ROC AUC
            roc_auc = roc_auc_score(y_true, y_proba)
            
            # Average precision
            avg_precision = average_precision_score(y_true, y_proba)
            
            # Confusion matrix
            cm = confusion_matrix(y_true, y_pred)
            
            # Classification report
            class_report = classification_report(y_true, y_pred, output_dict=True)
            
            # Precision-Recall curve
            precision_curve, recall_curve, _ = precision_recall_curve(y_true, y_proba)
            
            # ROC curve
            fpr, tpr, _ = roc_curve(y_true, y_proba)
            
            return {
                'accuracy': accuracy,
                'precision': precision,
                'recall': recall,
                'f1': f1,
                'roc_auc': roc_auc,
                'avg_precision': avg_precision,
                'confusion_matrix': cm,
                'classification_report': class_report,
                'precision_curve': precision_curve,
                'recall_curve': recall_curve,
                'fpr': fpr,
                'tpr': tpr
            }
            
        except Exception as e:
            logger.error(f"Error calculating performance metrics: {str(e)}")
            raise
    
    def predict(self, df, model_name):
        """
        Make predictions using a fitted model
        
        Args:
            df (DataFrame): Input data
            model_name (str): Name of the model to use
            
        Returns:
            dict: Predictions and probabilities
        """
        if not self.fitted:
            raise ValueError("Models not fitted. Call run_models first.")
        
        if model_name not in self.models and model_name != 'ensemble':
            raise ValueError(f"Model {model_name} not found.")
        
        try:
            # Get feature names for this model
            if model_name == 'ensemble':
                # For ensemble, use all feature names
                feature_names = []
                for name in self.feature_names:
                    feature_names.extend(self.feature_names[name])
                feature_names = list(set(feature_names))  # Remove duplicates
            else:
                feature_names = self.feature_names[model_name]
            
            # Prepare data
            X = df[feature_names].copy()
            
            # Clean the data
            X = self._clean_data(X)
            
            # Handle categorical features
            for col in X.columns:
                if col in self.label_encoders:
                    le = self.label_encoders[col]
                    # Handle unseen categories
                    X[col] = X[col].astype(str).map(
                        lambda x: le.transform([x])[0] if x in le.classes_ else -1
                    )
            
            # Scale numeric features if scaler exists
            if 'numeric' in self.scalers:
                numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()
                if numeric_features:
                    X[numeric_features] = self.scalers['numeric'].transform(X[numeric_features])
            
            # Make predictions
            if model_name == 'ensemble':
                # Get predictions from all models
                all_predictions = []
                all_probabilities = []
                
                for name in self.models:
                    model = self.models[name]
                    pred = model.predict(X)
                    proba = model.predict_proba(X)[:, 1]
                    all_predictions.append(pred)
                    all_probabilities.append(proba)
                
                # Majority voting for predictions
                predictions = np.array(all_predictions).mean(axis=0) > 0.5
                
                # Average probabilities
                probabilities = np.array(all_probabilities).mean(axis=0)
                
            else:
                # Get model
                model = self.models[model_name]
                
                # Make predictions
                predictions = model.predict(X)
                probabilities = model.predict_proba(X)[:, 1]
            
            return {
                'predictions': predictions,
                'probabilities': probabilities
            }
            
        except Exception as e:
            logger.error(f"Error making predictions with {model_name}: {str(e)}")
            raise
    
    def get_feature_importance(self, model_name):
        """
        Get feature importance for a model
        
        Args:
            model_name (str): Name of the model
            
        Returns:
            DataFrame: Feature importance
        """
        if not self.fitted:
            raise ValueError("Models not fitted. Call run_models first.")
        
        if model_name not in self.feature_importance:
            raise ValueError(f"Feature importance for {model_name} not found.")
        
        return self.feature_importance[model_name]
    
    def get_performance(self, model_name):
        """
        Get performance metrics for a model
        
        Args:
            model_name (str): Name of the model
            
        Returns:
            dict: Performance metrics
        """
        if not self.fitted:
            raise ValueError("Models not fitted. Call run_models first.")
        
        if model_name not in self.performance:
            raise ValueError(f"Performance for {model_name} not found.")
        
        return self.performance[model_name]
    
    def get_shap_values(self, model_name):
        """
        Get SHAP values for a model
        
        Args:
            model_name (str): Name of the model
            
        Returns:
            array: SHAP values
        """
        if not self.fitted:
            raise ValueError("Models not fitted. Call run_models first.")
        
        if model_name not in self.shap_values:
            raise ValueError(f"SHAP values for {model_name} not found.")
        
        return self.shap_values[model_name]
    
    def plot_feature_importance(self, model_name, top_n=20):
        """
        Plot feature importance for a model
        
        Args:
            model_name (str): Name of the model
            top_n (int): Number of top features to show
        """
        try:
            # Get feature importance
            importance_df = self.get_feature_importance(model_name)
            
            # Get top features
            top_features = importance_df.head(top_n)
            
            # Create plot
            plt.figure(figsize=(10, 8))
            sns.barplot(x='importance', y='feature', data=top_features)
            plt.title(f'Top {top_n} Feature Importance - {model_name}')
            plt.tight_layout()
            
            return plt.gcf()
            
        except Exception as e:
            logger.error(f"Error plotting feature importance for {model_name}: {str(e)}")
            raise
    
    def plot_confusion_matrix(self, model_name):
        """
        Plot confusion matrix for a model
        
        Args:
            model_name (str): Name of the model
        """
        try:
            # Get performance metrics
            performance = self.get_performance(model_name)
            cm = performance['confusion_matrix']
            
            # Create plot
            plt.figure(figsize=(8, 6))
            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                       xticklabels=['Not Fraud', 'Fraud'], 
                       yticklabels=['Not Fraud', 'Fraud'])
            plt.title(f'Confusion Matrix - {model_name}')
            plt.ylabel('Actual')
            plt.xlabel('Predicted')
            plt.tight_layout()
            
            return plt.gcf()
            
        except Exception as e:
            logger.error(f"Error plotting confusion matrix for {model_name}: {str(e)}")
            raise
    
    def plot_roc_curve(self, model_name):
        """
        Plot ROC curve for a model
        
        Args:
            model_name (str): Name of the model
        """
        try:
            # Get performance metrics
            performance = self.get_performance(model_name)
            fpr = performance['fpr']
            tpr = performance['tpr']
            roc_auc = performance['roc_auc']
            
            # Create plot
            plt.figure(figsize=(8, 6))
            plt.plot(fpr, tpr, label=f'{model_name} (AUC = {roc_auc:.3f})')
            plt.plot([0, 1], [0, 1], 'k--')
            plt.xlim([0.0, 1.0])
            plt.ylim([0.0, 1.05])
            plt.xlabel('False Positive Rate')
            plt.ylabel('True Positive Rate')
            plt.title(f'ROC Curve - {model_name}')
            plt.legend(loc="lower right")
            plt.tight_layout()
            
            return plt.gcf()
            
        except Exception as e:
            logger.error(f"Error plotting ROC curve for {model_name}: {str(e)}")
            raise
    
    def plot_precision_recall_curve(self, model_name):
        """
        Plot precision-recall curve for a model
        
        Args:
            model_name (str): Name of the model
        """
        try:
            # Get performance metrics
            performance = self.get_performance(model_name)
            precision_curve = performance['precision_curve']
            recall_curve = performance['recall_curve']
            avg_precision = performance['avg_precision']
            
            # Create plot
            plt.figure(figsize=(8, 6))
            plt.plot(recall_curve, precision_curve, label=f'{model_name} (AP = {avg_precision:.3f})')
            plt.xlim([0.0, 1.0])
            plt.ylim([0.0, 1.05])
            plt.xlabel('Recall')
            plt.ylabel('Precision')
            plt.title(f'Precision-Recall Curve - {model_name}')
            plt.legend(loc="lower left")
            plt.tight_layout()
            
            return plt.gcf()
            
        except Exception as e:
            logger.error(f"Error plotting precision-recall curve for {model_name}: {str(e)}")
            raise

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\models\.ipynb_checkpoints\unsupervised-checkpoint.py ===
"""
Unsupervised Models Module
Implements unsupervised learning models for fraud detection
"""
import pandas as pd
import numpy as np
from sklearn.ensemble import IsolationForest
from sklearn.neighbors import LocalOutlierFactor
from sklearn.svm import OneClassSVM
from sklearn.cluster import DBSCAN, KMeans
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.impute import SimpleImputer
from sklearn.metrics import silhouette_score
import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.callbacks import EarlyStopping
import warnings
import logging
from typing import Dict, List, Tuple, Union
warnings.filterwarnings('ignore')
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class UnsupervisedModels:
    """
    Class for unsupervised fraud detection models
    Implements Isolation Forest, Local Outlier Factor, Autoencoders, etc.
    """
    
    def __init__(self, contamination=0.01, random_state=42):
        """
        Initialize UnsupervisedModels
        
        Args:
            contamination (float): Expected proportion of outliers
            random_state (int): Random seed
        """
        self.contamination = contamination
        self.random_state = random_state
        self.models = {}
        self.feature_names = {}
        self.scalers = {}
        self.imputers = {}
        self.fitted = False
        
    def run_models(self, df):
        """
        Run all unsupervised models
        
        Args:
            df (DataFrame): Input data
            
        Returns:
            dict: Model results
        """
        try:
            # Get numeric columns
            numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
            
            if len(numeric_cols) < 2:
                logger.warning("Not enough numeric columns for unsupervised models")
                return {}
            
            # Prepare data
            X = df[numeric_cols].copy()
            
            # Clean the data - handle infinity and very large values
            X = self._clean_data(X)
            
            # Scale data
            X_scaled, scaler, imputer = self._scale_data(X)
            
            # Store scaler and imputer
            self.scalers['global'] = scaler
            self.imputers['global'] = imputer
            
            # Run different models
            results = {}
            
            # Isolation Forest
            try:
                if_model = IsolationForest(
                    contamination=self.contamination,
                    random_state=self.random_state,
                    n_jobs=-1,
                    max_samples='auto'
                )
                if_predictions = if_model.fit_predict(X_scaled)
                if_scores = if_model.decision_function(X_scaled)
                
                # Normalize scores to 0-1 range
                if_scores_normalized = self._normalize_scores(if_scores)
                
                results['isolation_forest'] = {
                    'predictions': if_predictions,
                    'scores': if_scores_normalized,
                    'model': if_model,
                    'feature_names': numeric_cols
                }
                
                self.models['isolation_forest'] = if_model
                self.feature_names['isolation_forest'] = numeric_cols
                
                logger.info("Isolation Forest model completed")
            except Exception as e:
                logger.error(f"Error running Isolation Forest: {str(e)}")
            
            # Local Outlier Factor
            try:
                lof_model = LocalOutlierFactor(
                    contamination=self.contamination,
                    n_neighbors=min(20, len(X_scaled) - 1),  # Ensure n_neighbors < n_samples
                    novelty=True,
                    n_jobs=-1
                )
                lof_predictions = lof_model.fit_predict(X_scaled)
                lof_scores = lof_model.decision_function(X_scaled)
                
                # Normalize scores to 0-1 range
                lof_scores_normalized = self._normalize_scores(lof_scores)
                
                results['local_outlier_factor'] = {
                    'predictions': lof_predictions,
                    'scores': lof_scores_normalized,
                    'model': lof_model,
                    'feature_names': numeric_cols
                }
                
                self.models['local_outlier_factor'] = lof_model
                self.feature_names['local_outlier_factor'] = numeric_cols
                
                logger.info("Local Outlier Factor model completed")
            except Exception as e:
                logger.error(f"Error running Local Outlier Factor: {str(e)}")
            
            # One-Class SVM
            try:
                ocsvm_model = OneClassSVM(
                    nu=self.contamination,
                    kernel='rbf',
                    gamma='scale'
                )
                ocsvm_predictions = ocsvm_model.fit_predict(X_scaled)
                ocsvm_scores = ocsvm_model.decision_function(X_scaled)
                
                # Normalize scores to 0-1 range
                ocsvm_scores_normalized = self._normalize_scores(ocsvm_scores)
                
                results['one_class_svm'] = {
                    'predictions': ocsvm_predictions,
                    'scores': ocsvm_scores_normalized,
                    'model': ocsvm_model,
                    'feature_names': numeric_cols
                }
                
                self.models['one_class_svm'] = ocsvm_model
                self.feature_names['one_class_svm'] = numeric_cols
                
                logger.info("One-Class SVM model completed")
            except Exception as e:
                logger.error(f"Error running One-Class SVM: {str(e)}")
            
            # DBSCAN Clustering
            try:
                # Use a subset of data for DBSCAN if it's too large
                if len(X_scaled) > 10000:
                    X_subset = X_scaled[np.random.choice(len(X_scaled), 10000, replace=False)]
                else:
                    X_subset = X_scaled
                
                dbscan_model = DBSCAN(eps=0.5, min_samples=5)
                dbscan_labels = dbscan_model.fit_predict(X_subset)
                
                # Convert to outlier predictions (-1 is outlier in DBSCAN)
                dbscan_predictions = np.where(dbscan_labels == -1, -1, 1)
                
                # Calculate distance to nearest core point as anomaly score
                dbscan_scores = np.zeros(len(X_subset))
                for i in range(len(X_subset)):
                    if dbscan_labels[i] == -1:  # Outlier
                        # Find distance to nearest core point
                        core_points = np.where(dbscan_labels != -1)[0]
                        if len(core_points) > 0:
                            distances = np.linalg.norm(X_subset[i] - X_subset[core_points], axis=1)
                            dbscan_scores[i] = distances.min()
                        else:
                            dbscan_scores[i] = np.max(np.linalg.norm(X_subset - X_subset.mean(axis=0), axis=1))
                
                # Normalize scores to 0-1 range
                dbscan_scores_normalized = self._normalize_scores(dbscan_scores)
                
                # For the full dataset, use the model to predict
                if len(X_scaled) > 10000:
                    full_predictions = dbscan_model.fit_predict(X_scaled)
                    full_predictions = np.where(full_predictions == -1, -1, 1)
                    
                    # Calculate scores for full dataset
                    full_scores = np.zeros(len(X_scaled))
                    for i in range(len(X_scaled)):
                        if full_predictions[i] == -1:  # Outlier
                            # Find distance to nearest core point
                            core_points = np.where(full_predictions != -1)[0]
                            if len(core_points) > 0:
                                distances = np.linalg.norm(X_scaled[i] - X_scaled[core_points], axis=1)
                                full_scores[i] = distances.min()
                            else:
                                full_scores[i] = np.max(np.linalg.norm(X_scaled - X_scaled.mean(axis=0), axis=1))
                    
                    # Normalize scores
                    full_scores_normalized = self._normalize_scores(full_scores)
                else:
                    full_predictions = dbscan_predictions
                    full_scores_normalized = dbscan_scores_normalized
                
                results['dbscan'] = {
                    'predictions': full_predictions,
                    'scores': full_scores_normalized,
                    'model': dbscan_model,
                    'feature_names': numeric_cols
                }
                
                self.models['dbscan'] = dbscan_model
                self.feature_names['dbscan'] = numeric_cols
                
                logger.info("DBSCAN model completed")
            except Exception as e:
                logger.error(f"Error running DBSCAN: {str(e)}")
            
            # K-Means Clustering
            try:
                # Determine optimal number of clusters
                silhouette_scores = []
                k_range = range(2, min(11, len(X_scaled) // 2))
                
                for k in k_range:
                    kmeans = KMeans(n_clusters=k, random_state=self.random_state, n_init=10)
                    labels = kmeans.fit_predict(X_scaled)
                    silhouette_scores.append(silhouette_score(X_scaled, labels))
                
                # Find optimal k
                optimal_k = k_range[np.argmax(silhouette_scores)]
                
                # Fit K-Means with optimal k
                kmeans_model = KMeans(n_clusters=optimal_k, random_state=self.random_state, n_init=10)
                kmeans_labels = kmeans_model.fit_predict(X_scaled)
                
                # Calculate distance to nearest cluster center as anomaly score
                distances = kmeans_model.transform(X_scaled)
                min_distances = distances.min(axis=1)
                
                # Normalize scores to 0-1 range
                kmeans_scores_normalized = self._normalize_scores(min_distances)
                
                # Convert to outlier predictions (top contamination% are outliers)
                threshold = np.percentile(kmeans_scores_normalized, (1 - self.contamination) * 100)
                kmeans_predictions = np.where(kmeans_scores_normalized > threshold, -1, 1)
                
                results['kmeans'] = {
                    'predictions': kmeans_predictions,
                    'scores': kmeans_scores_normalized,
                    'model': kmeans_model,
                    'feature_names': numeric_cols,
                    'optimal_k': optimal_k,
                    'silhouette_scores': silhouette_scores
                }
                
                self.models['kmeans'] = kmeans_model
                self.feature_names['kmeans'] = numeric_cols
                
                logger.info(f"K-Means model completed with {optimal_k} clusters")
            except Exception as e:
                logger.error(f"Error running K-Means: {str(e)}")
            
            # Autoencoder
            try:
                # Build autoencoder model
                input_dim = X_scaled.shape[1]
                encoding_dim = max(2, input_dim // 2)  # At least 2 dimensions
                
                autoencoder = self._build_autoencoder(input_dim, encoding_dim)
                
                # Train autoencoder
                early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
                
                history = autoencoder.fit(
                    X_scaled, X_scaled,
                    epochs=50,
                    batch_size=32,
                    validation_split=0.2,
                    callbacks=[early_stopping],
                    verbose=0
                )
                
                # Get reconstruction errors
                reconstructions = autoencoder.predict(X_scaled)
                mse = np.mean(np.power(X_scaled - reconstructions, 2), axis=1)
                
                # Handle any infinity or NaN values in MSE
                mse = np.nan_to_num(mse)
                mse = np.clip(mse, 0, np.finfo(np.float64).max)  # Clip to valid range
                
                # Normalize scores to 0-1 range
                mse_normalized = self._normalize_scores(mse)
                
                # Convert to outlier predictions (top contamination% are outliers)
                threshold = np.percentile(mse_normalized, (1 - self.contamination) * 100)
                autoencoder_predictions = np.where(mse_normalized > threshold, -1, 1)
                
                results['autoencoder'] = {
                    'predictions': autoencoder_predictions,
                    'scores': mse_normalized,
                    'model': autoencoder,
                    'feature_names': numeric_cols,
                    'history': history.history
                }
                
                self.models['autoencoder'] = autoencoder
                self.feature_names['autoencoder'] = numeric_cols
                
                logger.info("Autoencoder model completed")
            except Exception as e:
                logger.error(f"Error running Autoencoder: {str(e)}")
            
            # PCA-based anomaly detection
            try:
                # Fit PCA
                pca = PCA(n_components=min(10, X_scaled.shape[1] - 1), random_state=self.random_state)
                pca_transformed = pca.fit_transform(X_scaled)
                
                # Calculate reconstruction error
                X_reconstructed = pca.inverse_transform(pca_transformed)
                pca_errors = np.mean(np.power(X_scaled - X_reconstructed, 2), axis=1)
                
                # Handle any infinity or NaN values in errors
                pca_errors = np.nan_to_num(pca_errors)
                pca_errors = np.clip(pca_errors, 0, np.finfo(np.float64).max)  # Clip to valid range
                
                # Normalize scores to 0-1 range
                pca_errors_normalized = self._normalize_scores(pca_errors)
                
                # Convert to outlier predictions (top contamination% are outliers)
                threshold = np.percentile(pca_errors_normalized, (1 - self.contamination) * 100)
                pca_predictions = np.where(pca_errors_normalized > threshold, -1, 1)
                
                results['pca'] = {
                    'predictions': pca_predictions,
                    'scores': pca_errors_normalized,
                    'model': pca,
                    'feature_names': numeric_cols,
                    'explained_variance': pca.explained_variance_ratio_
                }
                
                self.models['pca'] = pca
                self.feature_names['pca'] = numeric_cols
                
                logger.info("PCA-based anomaly detection completed")
            except Exception as e:
                logger.error(f"Error running PCA-based anomaly detection: {str(e)}")
            
            self.fitted = True
            return results
            
        except Exception as e:
            logger.error(f"Error running unsupervised models: {str(e)}")
            raise
    
    def _clean_data(self, X):
        """
        Clean data by handling infinity, NaN, and extreme values
        
        Args:
            X (DataFrame): Input data
            
        Returns:
            DataFrame: Cleaned data
        """
        try:
            # Replace infinity with NaN
            X = X.replace([np.inf, -np.inf], np.nan)
            
            # Replace extremely large values with a more reasonable maximum
            for col in X.columns:
                if X[col].dtype in ['float64', 'int64']:
                    # Calculate 99th percentile as a reasonable maximum
                    percentile_99 = np.nanpercentile(X[col], 99)
                    if not np.isnan(percentile_99):
                        # Cap values at 10 times the 99th percentile
                        max_val = percentile_99 * 10
                        X[col] = np.where(X[col] > max_val, max_val, X[col])
                        
                        # Similarly, handle extremely negative values
                        percentile_1 = np.nanpercentile(X[col], 1)
                        if not np.isnan(percentile_1):
                            min_val = percentile_1 * 10
                            X[col] = np.where(X[col] < min_val, min_val, X[col])
            
            return X
            
        except Exception as e:
            logger.error(f"Error cleaning data: {str(e)}")
            return X
    
    def _scale_data(self, X):
        """
        Scale data using RobustScaler (more resistant to outliers)
        
        Args:
            X (DataFrame): Input data
            
        Returns:
            tuple: (scaled data, scaler, imputer)
        """
        try:
            # First impute missing values
            imputer = SimpleImputer(strategy='median')
            X_imputed = imputer.fit_transform(X)
            
            # Use RobustScaler instead of StandardScaler to handle outliers better
            scaler = RobustScaler()
            X_scaled = scaler.fit_transform(X_imputed)
            
            # Check for any remaining infinity or NaN values
            X_scaled = np.nan_to_num(X_scaled)
            X_scaled = np.clip(X_scaled, -1e10, 1e10)  # Clip to reasonable range
            
            return X_scaled, scaler, imputer
            
        except Exception as e:
            logger.error(f"Error scaling data: {str(e)}")
            # Fallback to simple normalization
            X_normalized = (X - X.min()) / (X.max() - X.min())
            return X_normalized.values, None, None
    
    def _normalize_scores(self, scores):
        """
        Normalize scores to 0-1 range
        
        Args:
            scores (array): Input scores
            
        Returns:
            array: Normalized scores
        """
        try:
            # Handle NaN and infinity
            scores = np.nan_to_num(scores)
            scores = np.clip(scores, -1e10, 1e10)  # Clip to reasonable range
            
            # Min-max normalization
            min_score = scores.min()
            max_score = scores.max()
            
            if max_score > min_score:
                normalized_scores = (scores - min_score) / (max_score - min_score)
            else:
                normalized_scores = np.zeros_like(scores)
            
            return normalized_scores
            
        except Exception as e:
            logger.error(f"Error normalizing scores: {str(e)}")
            return np.zeros_like(scores)
    
    def _build_autoencoder(self, input_dim, encoding_dim):
        """
        Build autoencoder model
        
        Args:
            input_dim (int): Input dimension
            encoding_dim (int): Encoding dimension
            
        Returns:
            Model: Autoencoder model
        """
        try:
            # Encoder
            input_layer = layers.Input(shape=(input_dim,))
            encoded = layers.Dense(encoding_dim * 2, activation='relu')(input_layer)
            encoded = layers.Dense(encoding_dim, activation='relu')(encoded)
            
            # Decoder
            decoded = layers.Dense(encoding_dim * 2, activation='relu')(encoded)
            decoded = layers.Dense(input_dim, activation='linear')(decoded)
            
            # Autoencoder model
            autoencoder = models.Model(input_layer, decoded)
            
            # Compile model
            autoencoder.compile(optimizer='adam', loss='mean_squared_error')
            
            return autoencoder
            
        except Exception as e:
            logger.error(f"Error building autoencoder: {str(e)}")
            raise
    
    def predict(self, df, model_name):
        """
        Make predictions using a fitted model
        
        Args:
            df (DataFrame): Input data
            model_name (str): Name of the model to use
            
        Returns:
            dict: Predictions and scores
        """
        if not self.fitted:
            raise ValueError("Models not fitted. Call run_models first.")
        
        if model_name not in self.models:
            raise ValueError(f"Model {model_name} not found.")
        
        try:
            # Get feature names for this model
            feature_names = self.feature_names[model_name]
            
            # Prepare data
            X = df[feature_names].copy()
            
            # Clean the data
            X = self._clean_data(X)
            
            # Scale data using fitted scaler and imputer
            if 'global' in self.scalers and 'global' in self.imputers:
                X_imputed = self.imputers['global'].transform(X)
                X_scaled = self.scalers['global'].transform(X_imputed)
            else:
                # Fallback to simple normalization
                X_scaled = (X - X.min()) / (X.max() - X.min())
                X_scaled = np.nan_to_num(X_scaled)
                X_scaled = np.clip(X_scaled, -1e10, 1e10)
            
            # Get model
            model = self.models[model_name]
            
            # Make predictions
            if model_name == 'isolation_forest':
                predictions = model.predict(X_scaled)
                scores = model.decision_function(X_scaled)
                # Normalize scores to 0-1 range
                scores_normalized = self._normalize_scores(scores)
                
            elif model_name == 'local_outlier_factor':
                predictions = model.predict(X_scaled)
                scores = model.decision_function(X_scaled)
                # Normalize scores to 0-1 range
                scores_normalized = self._normalize_scores(scores)
                
            elif model_name == 'one_class_svm':
                predictions = model.predict(X_scaled)
                scores = model.decision_function(X_scaled)
                # Normalize scores to 0-1 range
                scores_normalized = self._normalize_scores(scores)
                
            elif model_name == 'dbscan':
                predictions = model.fit_predict(X_scaled)
                # Convert to outlier predictions (-1 is outlier in DBSCAN)
                predictions = np.where(predictions == -1, -1, 1)
                
                # Calculate distance to nearest core point as anomaly score
                scores = np.zeros(len(X_scaled))
                for i in range(len(X_scaled)):
                    if predictions[i] == -1:  # Outlier
                        # Find distance to nearest core point
                        core_points = np.where(predictions != -1)[0]
                        if len(core_points) > 0:
                            distances = np.linalg.norm(X_scaled[i] - X_scaled[core_points], axis=1)
                            scores[i] = distances.min()
                        else:
                            scores[i] = np.max(np.linalg.norm(X_scaled - X_scaled.mean(axis=0), axis=1))
                
                # Normalize scores to 0-1 range
                scores_normalized = self._normalize_scores(scores)
                
            elif model_name == 'kmeans':
                # Transform data
                distances = model.transform(X_scaled)
                min_distances = distances.min(axis=1)
                
                # Normalize scores to 0-1 range
                scores_normalized = self._normalize_scores(min_distances)
                
                # Convert to outlier predictions (top contamination% are outliers)
                threshold = np.percentile(scores_normalized, (1 - self.contamination) * 100)
                predictions = np.where(scores_normalized > threshold, -1, 1)
                
            elif model_name == 'autoencoder':
                # Get reconstructions
                reconstructions = model.predict(X_scaled)
                mse = np.mean(np.power(X_scaled - reconstructions, 2), axis=1)
                
                # Handle any infinity or NaN values in MSE
                mse = np.nan_to_num(mse)
                mse = np.clip(mse, 0, np.finfo(np.float64).max)
                
                # Normalize scores to 0-1 range
                scores_normalized = self._normalize_scores(mse)
                
                # Convert to outlier predictions (top contamination% are outliers)
                threshold = np.percentile(scores_normalized, (1 - self.contamination) * 100)
                predictions = np.where(scores_normalized > threshold, -1, 1)
                
            elif model_name == 'pca':
                # Transform data
                pca_transformed = model.transform(X_scaled)
                
                # Calculate reconstruction error
                X_reconstructed = model.inverse_transform(pca_transformed)
                mse = np.mean(np.power(X_scaled - X_reconstructed, 2), axis=1)
                
                # Handle any infinity or NaN values in errors
                mse = np.nan_to_num(mse)
                mse = np.clip(mse, 0, np.finfo(np.float64).max)
                
                # Normalize scores to 0-1 range
                scores_normalized = self._normalize_scores(mse)
                
                # Convert to outlier predictions (top contamination% are outliers)
                threshold = np.percentile(scores_normalized, (1 - self.contamination) * 100)
                predictions = np.where(scores_normalized > threshold, -1, 1)
                
            else:
                raise ValueError(f"Unknown model: {model_name}")
            
            return {
                'predictions': predictions,
                'scores': scores_normalized
            }
            
        except Exception as e:
            logger.error(f"Error making predictions with {model_name}: {str(e)}")
            raise
    
    def get_feature_importance(self, model_name):
        """
        Get feature importance for a model
        
        Args:
            model_name (str): Name of the model
            
        Returns:
            DataFrame: Feature importance
        """
        if not self.fitted:
            raise ValueError("Models not fitted. Call run_models first.")
        
        if model_name not in self.models:
            raise ValueError(f"Model {model_name} not found.")
        
        try:
            # Get feature names
            feature_names = self.feature_names[model_name]
            
            if model_name == 'isolation_forest':
                # Get feature importance from the model
                importance = self.models[model_name].feature_importances_
                
                # Create DataFrame
                importance_df = pd.DataFrame({
                    'feature': feature_names,
                    'importance': importance
                }).sort_values('importance', ascending=False)
                
                return importance_df
                
            elif model_name == 'pca':
                # Get explained variance ratio
                explained_variance = self.models[model_name].explained_variance_ratio_
                
                # Create DataFrame
                importance_df = pd.DataFrame({
                    'feature': [f'PC{i+1}' for i in range(len(explained_variance))],
                    'importance': explained_variance
                }).sort_values('importance', ascending=False)
                
                return importance_df
                
            else:
                # For other models, return equal importance
                importance = np.ones(len(feature_names)) / len(feature_names)
                
                # Create DataFrame
                importance_df = pd.DataFrame({
                    'feature': feature_names,
                    'importance': importance
                }).sort_values('importance', ascending=False)
                
                return importance_df
                
        except Exception as e:
            logger.error(f"Error getting feature importance for {model_name}: {str(e)}")
            raise

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\models\.ipynb_checkpoints\__init__-checkpoint.py ===

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\models\__pycache__\rule_based.cpython-39.pyc ===
a
    èöhám  „                   @   s∏   d Z ddlZddlZddlZddlmZmZ ddlZddl	Z	ddl
Z
ddlZddlmZmZmZmZmZ ddlmZmZmZmZmZ e
†d° ejejdç e†e°ZG dd	Ñ d	ÉZdS )
z@
Rule-based Models Module
Implements rule-based fraud detection
È    N)⁄datetime⁄	timedelta)⁄Dict⁄List⁄Tuple⁄Union⁄Callable)⁄is_api_available⁄get_demo_sanctions_data⁄get_demo_tax_compliance_data⁄get_demo_bank_verification_data⁄#get_demo_identity_verification_data⁄ignore)⁄levelc                   @   s  e Zd ZdZdEddÑZddÑ Zdd	Ñ Zd
dÑ ZddÑ ZddÑ Z	dFddÑZ
dGddÑZddÑ ZdHddÑZdIddÑZdJdd ÑZd!d"Ñ ZdKd#d$ÑZd%d&Ñ ZdLd(d)ÑZd*d+Ñ ZdMd-d.ÑZdNd/d0ÑZd1d2Ñ Zd3d4Ñ Zd5d6Ñ Zd7d8Ñ ZdOd;d<ÑZd=d>Ñ Zd?d@Ñ ZdAdBÑ ZdCdDÑ ZdS )P⁄
RuleEnginez]
    Class for rule-based fraud detection
    Implements configurable rules with weights
    NÁffffffÊ?c                 C   sL   || _ i | _i | _i | _d| _i | _|r@tj†|°r@| †	|° n| †
°  dS )zæ
        Initialize RuleEngine
        
        Args:
            config_path (str, optional): Path to configuration file
            threshold (float): Threshold for rule violation
        FN)⁄	threshold⁄rules⁄rule_weights⁄rule_descriptions⁄fitted⁄api_available⁄os⁄path⁄exists⁄_load_config⁄_load_default_rules)⁄self⁄config_pathr   © r   ˙gC:\Users\Tranquil Abyss\fraud-detection-platform\app\..\src\fraud_detection_engine\models\rule_based.py⁄__init__   s    zRuleEngine.__init__c              
   C   s  zŒt |dÉè}t†|°}W d  É n1 s,0    Y  d|v rî|d †° D ]H\}}|†dd°rJ| †|°| j|< |†dd°| j|< |†dd	°| j|< qJt	d
Ét	dÉt	dÉt	dÉt	dÉdú| _
t†d| j
õ ù° W nB têy } z(t†dt|Éõ ù° | †°  W Y d}~n
d}~0 0 dS )zÑ
        Load configuration from YAML file
        
        Args:
            config_path (str): Path to configuration file
        ⁄rNr   ⁄enabledT⁄weightÁ      ?⁄description⁄ ⁄	sanctions⁄tax_compliance⁄bank_verification⁄identity_verification⁄geolocation©r(   r)   r*   r+   r,   ˙API availability: zError loading configuration: )⁄open⁄yaml⁄	safe_load⁄items⁄get⁄_create_rule_functionr   r   r   r	   r   ⁄logger⁄info⁄	Exception⁄error⁄strr   )r   r   ⁄f⁄config⁄	rule_name⁄rule_config⁄er   r   r    r   1   s&    (˚zRuleEngine._load_configc              
   C   s¿  êz|| j | jd< d| jd< d| jd< | j| jd< d| jd< d| jd< | j| jd< d| jd< d| jd< | j| jd	< d
| jd	< d| jd	< | j| jd< d| jd< d| jd< | j| jd< d| jd< d| jd< | j	| jd< d| jd< d| jd< | j
| jd< d| jd< d| jd< | j| jd< d| jd< d| jd< | j| jd< d| jd< d| jd< | j| jd< d| jd< d| jd< | j| jd< d
| jd< d| jd< | j| jd< d| jd< d| jd< | j| jd < d| jd < d!| jd < | j| jd"< d| jd"< d#| jd"< | j| jd$< d| jd$< d%| jd$< | j| jd&< d| jd&< d'| jd&< | j| jd(< d| jd(< d)| jd(< td*Étd$Étd&Étd(Étd+Éd,ú| _t†d-| jõ ù° W n< têy∫ } z"t†d.t|Éõ ù° Ç W Y d/}~n
d/}~0 0 d/S )0z$
        Load default rules
        Zhigh_amountg333333”?z$Transaction amount exceeds thresholdZunusual_amount_for_sendergöôôôôô…?z Amount is unusual for the senderZunusual_amount_for_receiverz"Amount is unusual for the receiver⁄round_amountgöôôôôôπ?z(Transaction amount is suspiciously round⁄high_frequency_senderz&High transaction frequency from sender⁄high_frequency_receiverz&High transaction frequency to receiver⁄rapid_successiongöôôôôôŸ?z)Multiple transactions in rapid succession⁄cross_borderz)Transaction crosses international borders⁄high_risk_countryg      ‡?z&Transaction involves high-risk country⁄unusual_location_for_senderz,Transaction from unusual location for sender⁄unusual_hourz Transaction during unusual hours⁄weekendzTransaction on weekend⁄
new_senderz!First transaction from new sender⁄new_receiverz!First transaction to new receiverZsanctions_checkz(Transaction involves sanctioned entitiesr)   z+Transaction involves non-compliant entitiesr*   z-Transaction involves unverified bank accountsr+   z*Transaction involves unverified identitiesr(   r,   r-   r.   zError loading default rules: N)⁄_high_amount_ruler   r   r   ⁄_unusual_amount_for_sender_rule⁄!_unusual_amount_for_receiver_rule⁄_round_amount_rule⁄_high_frequency_sender_rule⁄_high_frequency_receiver_rule⁄_rapid_succession_rule⁄_cross_border_rule⁄_high_risk_country_rule⁄!_unusual_location_for_sender_rule⁄_unusual_hour_rule⁄_weekend_rule⁄_new_sender_rule⁄_new_receiver_rule⁄_sanctions_check_rule⁄_tax_compliance_rule⁄_bank_verification_rule⁄_identity_verification_ruler	   r   r5   r6   r7   r8   r9   )r   r>   r   r   r    r   S   sÇ    



































˚zRuleEngine._load_default_rulesc                    s>  |† d°}|dkr*|† dd°âáfddÑS |dkrZ|† dd	°â|† d
d°âáááfddÑS |dkrä|† dd	°â|† d
d°âáááfddÑS |dkr™|† dd°âáfddÑS |dkr⁄|† dd°â|† dd°âáááfddÑS |dkêr|† dd°â|† dd°âáááfddÑS |dkêr>|† dd°â|† d
d	°âáááfddÑS |dkêrTáfddÑS |dkêr||† d g d!¢°â á áfd"dÑS |d#kêríáfd$dÑS |d%kêrƒ|† d&d'°â|† d(d°âáááfd)dÑS |d*kêr⁄áfd+dÑS |d,kêr˛|† dd-°âááfd.dÑS |d/kêr"|† dd-°âááfd0dÑS t†d1|õ ù° d2dÑ S d3S )4z«
        Create a rule function from configuration
        
        Args:
            rule_config (dict): Rule configuration
            
        Returns:
            function: Rule function
        ⁄typeZamount_thresholdr   È'  c                    s   | † dd°à kS )N⁄amountr   ©r3   ©⁄row©r   r   r    ⁄<lambda>√   Û    z2RuleEngine._create_rule_function.<locals>.<lambda>Zsender_amount_outlier⁄std_multiplierÈ   ⁄min_transactionsÈ   c                    s   à† | àà °S ©N)Z_is_sender_amount_outlierr`   ©rg   r   re   r   r    rc   »   rd   Zreceiver_amount_outlierc                    s   à† | àà °S ri   )Z_is_receiver_amount_outlierr`   rj   r   r    rc   Õ   rd   r?   ÈË  c                    s$   | † dd°à ko"| † dd°d dkS )Nr^   r   rk   r_   r`   rb   r   r    rc   —   rd   r@   ⁄time_window⁄1H⁄max_transactionsÈ
   c                    s   à† | àà °S ri   )Z_is_high_frequency_senderr`   ©rn   r   rl   r   r    rc   ÷   rd   rA   c                    s   à† | àà °S ri   )Z_is_high_frequency_receiverr`   rp   r   r    rc   €   rd   rB   ⁄5Mc                    s   à† | àà °S ri   )Z_is_rapid_successionr`   )rg   r   rl   r   r    rc   ‡   rd   rC   c                    s
   à † | °S ri   )Z_is_cross_borderr`   ©r   r   r    rc   „   rd   rD   ⁄	countries©zNorth Korea⁄Iran⁄Syria⁄Cubac                    s   à† | à °S ri   )Z_is_high_risk_countryr`   )rs   r   r   r    rc   Á   rd   rE   c                    s
   à † | °S ri   )Z_is_unusual_location_for_senderr`   rr   r   r    rc   Í   rd   rF   ⁄
start_hourÈ   ⁄end_hourc                    s   à† | àà °S ri   )Z_is_unusual_hourr`   )rz   r   rx   r   r    rc   Ô   rd   rG   c                    s
   à † | °S ri   )Z_is_weekendr`   rr   r   r    rc   Ú   rd   rH   ⁄7Dc                    s   à † | à°S ri   )Z_is_new_senderr`   ©r   rl   r   r    rc   ˆ   rd   rI   c                    s   à † | à°S ri   )Z_is_new_receiverr`   r|   r   r    rc   ˙   rd   zUnknown rule type: c                 S   s   dS )NFr   r`   r   r   r    rc   ˛   rd   N)r3   r5   ⁄warning)r   r=   ⁄	rule_typer   )	rs   rz   rn   rg   r   rx   re   r   rl   r    r4   µ   s`    










z RuleEngine._create_rule_functionc                    s∂  êzrd|j v r:tjj†|d °s:|†° }t†|d °|d< d|j v rN|†d°}i }i âi }àj†	° D ê]2\}}zºg }|†
° D ]h\}}z||É}	|†|	° W q| ty‚ }
 z0t†d|õ dt|
Éõ ù° |†d° W Y d}
~
q|d}
~
0 0 q||||< àj†|d°âáfddÑ|D Éà|< d	dÑ t|ÉD É||< W qd têyñ }
 zRt†d|õ d
t|
Éõ ù° dgt|É ||< dgt|É à|< g ||< W Y d}
~
qdd}
~
0 0 qdg }tt|ÉÉD ](â tá áfddÑàD ÉÉ}|†|° êq™tàj†° ÉâáfddÑ|D É}áfddÑ|D É}g }tt|ÉÉD ]8â g }|D ]}à || v êr"|†|° êq"|†|° êqdà_|à||||àjàjàjdú	W S  têy∞ }
 z"t†dt|
Éõ ù° Ç W Y d}
~
n
d}
~
0 0 dS )z≠
        Apply all rules to the dataframe
        
        Args:
            df (DataFrame): Input data
            
        Returns:
            dict: Rule results
        ⁄	timestampzError applying rule z	 to row: FNr%   c                    s   g | ]}|rà nd ëqS )r   r   )⁄.0⁄result)r$   r   r    ⁄
<listcomp>)  rd   z*RuleEngine.apply_rules.<locals>.<listcomp>c                 S   s   g | ]\}}|r|ëqS r   r   )rÄ   ⁄irÅ   r   r   r    rÇ   ,  rd   ˙: r   c                 3   s   | ]}à| à  V  qd S ri   r   )rÄ   r<   )rÉ   ⁄rule_scoresr   r    ⁄	<genexpr>7  rd   z)RuleEngine.apply_rules.<locals>.<genexpr>c                    s   g | ]}|à  ëqS r   r   ©rÄ   ⁄score)⁄max_possible_scorer   r    rÇ   <  rd   c                    s   g | ]}|à j këqS r   rb   rá   rr   r   r    rÇ   ?  rd   T)	⁄rule_resultsrÖ   ⁄total_scores⁄normalized_scores⁄rule_violations⁄violated_rule_namesr   r   r   zError applying rules: )⁄columns⁄pd⁄api⁄types⁄is_datetime64_any_dtype⁄copy⁄to_datetime⁄sort_valuesr   r2   ⁄iterrows⁄appendr7   r5   r}   r9   r   r3   ⁄	enumerater8   ⁄len⁄range⁄sum⁄valuesr   r   )r   ⁄dfrä   ⁄violated_rulesr<   ⁄	rule_func⁄results⁄_ra   rÅ   r>   rã   rà   rå   rç   ré   ⁄namesr   )rÉ   râ   rÖ   r   r$   r    ⁄apply_rules   sp    


" ˜zRuleEngine.apply_rulesc                 C   s   |† dd°dkS )z#Check if transaction amount is highr^   r   r]   r_   ©r   ra   r   r   r    rJ   ]  s    zRuleEngine._high_amount_rulerf   rh   c                 C   s   dS )z)Check if amount is unusual for the senderFr   ©r   ra   re   rg   r   r   r    rK   a  s    z*RuleEngine._unusual_amount_for_sender_rulec                 C   s   dS )z+Check if amount is unusual for the receiverFr   r¶   r   r   r    rL   g  s    z,RuleEngine._unusual_amount_for_receiver_rulec                 C   s    |† dd°}|dko|d dkS )z1Check if transaction amount is suspiciously roundr^   r   rk   r_   )r   ra   r^   r   r   r    rM   m  s    zRuleEngine._round_amount_rulerm   ro   c                 C   s   dS )z.Check if sender has high transaction frequencyFr   ©r   ra   rl   rn   r   r   r    rN   r  s    z&RuleEngine._high_frequency_sender_rulec                 C   s   dS )z0Check if receiver has high transaction frequencyFr   rß   r   r   r    rO   x  s    z(RuleEngine._high_frequency_receiver_rulerq   c                 C   s   dS )z<Check if there are multiple transactions in rapid successionFr   )r   ra   rl   rg   r   r   r    rP   ~  s    z!RuleEngine._rapid_succession_rulec                 C   sP   |† dd°}|† dd°}|r |s$dS |†d°d †° }|†d°d †° }||kS )z2Check if transaction crosses international borders⁄sender_locationr'   ⁄receiver_locationF˙,r   )r3   ⁄split⁄strip)r   ra   r®   r©   ⁄sender_country⁄receiver_countryr   r   r    rQ   Ñ  s    zRuleEngine._cross_border_rulec                 C   sL   |du rg d¢}|† dd°}|† dd°}|D ]}||v s@||v r, dS q,dS )z/Check if transaction involves high-risk countryNrt   r®   r'   r©   TFr_   )r   ra   rs   r®   r©   ⁄countryr   r   r    rR   í  s    z"RuleEngine._high_risk_country_rulec                 C   s   dS )z8Check if transaction is from unusual location for senderFr   r•   r   r   r    rS   °  s    z,RuleEngine._unusual_location_for_sender_rulery   c                 C   sh   |† d°}|du rdS tjj†|°s.t†|°}|j}||krL||kpJ||k S ||  ko^|k S   S dS )z,Check if transaction is during unusual hoursr   NF)r3   rê   rë   rí   rì   rï   ⁄hour)r   ra   rx   rz   r   r∞   r   r   r    rT   ß  s    

zRuleEngine._unusual_hour_rulec                 C   s8   |† d°}|du rdS tjj†|°s.t†|°}|jdkS )z"Check if transaction is on weekendr   NFrh   )r3   rê   rë   rí   rì   rï   ⁄	dayofweek)r   ra   r   r   r   r    rU   ∏  s    

zRuleEngine._weekend_ruler{   c                 C   s   dS )z2Check if this is the first transaction from senderFr   ©r   ra   rl   r   r   r    rV   ƒ  s    zRuleEngine._new_sender_rulec                 C   s   dS )z2Check if this is the first transaction to receiverFr   r≤   r   r   r    rW      s    zRuleEngine._new_receiver_rulec              
   C   s¨   zl| j †dd°sdtÉ }|†dd°}|†dd°}|d jj|ddç†° p\|d jj|ddç†° }|W S W dS W n: ty¶ } z"t†dt|Éõ ù° W Y d	}~dS d	}~0 0 d	S )
z1Check if transaction involves sanctioned entitiesr(   F⁄	sender_idr'   ⁄receiver_id⁄	entity_id)⁄nazError in sanctions check: N)	r   r3   r
   r9   ⁄contains⁄anyr7   r5   r}   )r   ra   Zsanctions_datar≥   r¥   Zis_sanctionedr>   r   r   r    rX   –  s    ˛
z RuleEngine._sanctions_check_rulec           	   
   C   s∂   zv| j †dd°sntÉ }|†dd°}|†dd°}d}|†° D ],\}}|d ||fv r:|d dkr:d	} qhq:|W S W dS W n: ty∞ } z"t†d
t|Éõ ù° W Y d}~dS d}~0 0 dS )z#Check if entities are tax compliantr)   Fr≥   r'   r¥   rµ   ⁄compliance_statuszNon-compliantTzError in tax compliance check: N)r   r3   r   ró   r7   r5   r}   r9   )	r   ra   Ztax_datar≥   r¥   Zis_non_compliantr¢   ⁄entityr>   r   r   r    rY   Í  s    
zRuleEngine._tax_compliance_rulec           	   
   C   s∂   zv| j †dd°sntÉ }|†dd°}|†dd°}d}|†° D ],\}}|d ||fv r:|d dkr:d	} qhq:|W S W dS W n: ty∞ } z"t†d
t|Éõ ù° W Y d}~dS d}~0 0 dS )z#Check if bank accounts are verifiedr*   Fr≥   r'   r¥   ⁄account_number⁄verification_status˙Not VerifiedTz"Error in bank verification check: N)r   r3   r   ró   r7   r5   r}   r9   )	r   ra   Z	bank_datar≥   r¥   ⁄is_unverifiedr¢   ⁄accountr>   r   r   r    rZ     s    
z"RuleEngine._bank_verification_rulec           	   
   C   s∂   zv| j †dd°sntÉ }|†dd°}|†dd°}d}|†° D ],\}}|d ||fv r:|d dkr:d	} qhq:|W S W dS W n: ty∞ } z"t†d
t|Éõ ù° W Y d}~dS d}~0 0 dS )z Check if identities are verifiedr+   Fr≥   r'   r¥   ⁄	id_numberrº   rΩ   Tz&Error in identity verification check: N)r   r3   r   ró   r7   r5   r}   r9   )	r   ra   Zidentity_datar≥   r¥   ræ   r¢   ⁄identityr>   r   r   r    r[   "  s    
z&RuleEngine._identity_verification_ruler%   r'   c                 C   s2   || j |< || j|< || j|< t†d|õ ù° dS )z˛
        Add a custom rule
        
        Args:
            rule_name (str): Name of the rule
            rule_func (function): Rule function
            weight (float): Weight of the rule
            description (str): Description of the rule
        zAdded rule: N)r   r   r   r5   r6   )r   r<   r†   r$   r&   r   r   r    ⁄add_rule>  s    



zRuleEngine.add_rulec                 C   sH   || j v r4| j |= | j|= | j|= t†d|õ ù° nt†d|õ ù° dS )zn
        Remove a rule
        
        Args:
            rule_name (str): Name of the rule to remove
        zRemoved rule: ˙Rule not found: N)r   r   r   r5   r6   r}   )r   r<   r   r   r    ⁄remove_ruleM  s    
zRuleEngine.remove_rulec                 C   s@   || j v r,|| j |< t†d|õ d|õ ù° nt†d|õ ù° dS )zô
        Update the weight of a rule
        
        Args:
            rule_name (str): Name of the rule
            weight (float): New weight
        zUpdated weight for rule rÑ   r√   N)r   r5   r6   r}   )r   r<   r$   r   r   r    ⁄update_rule_weight\  s    

zRuleEngine.update_rule_weightc                 C   s   t | j†° É| j| jdúS )z]
        Get all rules
        
        Returns:
            dict: Rules information
        )r   ⁄weights⁄descriptions)⁄listr   ⁄keysr   r   rr   r   r   r    ⁄	get_rulesj  s    ˝zRuleEngine.get_rulesc              
   C   s¬   zÇdi i}| j D ]$}d| j| | j| dú|d |< qt|dÉè }tj||ddç W d  É n1 sf0    Y  t†d|õ ù° W n: tyº } z"t†	d	t
|Éõ ù° Ç W Y d}~n
d}~0 0 dS )
zÖ
        Save current configuration to file
        
        Args:
            config_path (str): Path to save configuration
        r   T)r#   r$   r&   ⁄wF)⁄default_flow_styleNzConfiguration saved to zError saving configuration: )r   r   r   r/   r0   ⁄dumpr5   r6   r7   r8   r9   )r   r   r;   r<   r:   r>   r   r   r    ⁄save_configw  s    ˇ
˝.zRuleEngine.save_config)Nr   )rf   rh   )rf   rh   )rm   ro   )rm   ro   )rq   rf   )N)ry   rh   )r{   )r{   )r%   r'   ) ⁄__name__⁄
__module__⁄__qualname__⁄__doc__r!   r   r   r4   r§   rJ   rK   rL   rM   rN   rO   rP   rQ   rR   rS   rT   rU   rV   rW   rX   rY   rZ   r[   r¬   rƒ   r≈   r    rŒ   r   r   r   r    r      s:   
"bK]









r   )r“   ⁄pandasrê   ⁄numpy⁄np⁄rer   r   r0   r   ⁄warnings⁄logging⁄typingr   r   r   r   r   ⁄&fraud_detection_engine.utils.api_utilsr	   r
   r   r   r   ⁄filterwarnings⁄basicConfig⁄INFO⁄	getLoggerrœ   r5   r   r   r   r   r    ⁄<module>   s   



=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\models\__pycache__\supervised.cpython-39.pyc ===
a
    èöh·ë  „                   @   sÑ  d Z ddlZddlZddlmZmZmZm	Z	 ddl
mZmZmZ ddlmZ ddlmZ ddlmZ ddlmZ dd	lmZmZmZmZmZmZmZmZmZm Z  dd
l!m"Z"m#Z#m$Z$ ddl%m&Z&m'Z'm(Z( ddl)m*Z*m+Z+m,Z, ddl-m.Z.m/Z/ ddl0m1Z1m2Z2 ddl3Z4ddl5Z6ddl7Z7ddl8m9Z: ddl;Z<ddl=Z=ddl>Z>ddl?m@Z@mAZAmBZBmCZC e=†Dd° e>jEe>jFdç e>†GeH°ZIG ddÑ dÉZJdS )zT
Supervised Models Module
Implements supervised learning models for fraud detection
È    N)⁄train_test_split⁄cross_val_score⁄GridSearchCV⁄StratifiedKFold)⁄RandomForestClassifier⁄GradientBoostingClassifier⁄ExtraTreesClassifier)⁄LogisticRegression)⁄SVC)⁄MLPClassifier)⁄
GaussianNB)
⁄accuracy_score⁄precision_score⁄recall_score⁄f1_score⁄roc_auc_score⁄confusion_matrix⁄classification_report⁄precision_recall_curve⁄average_precision_score⁄	roc_curve)⁄StandardScaler⁄LabelEncoder⁄RobustScaler)⁄SelectKBest⁄	f_classif⁄RFE)⁄SMOTE⁄ADASYN⁄BorderlineSMOTE)⁄RandomUnderSampler⁄
TomekLinks)⁄SMOTEENN⁄
SMOTETomek)⁄Dict⁄List⁄Tuple⁄Union⁄ignore)⁄levelc                   @   st   e Zd ZdZdddÑZddÑ Zd	d
Ñ ZddÑ ZddÑ ZddÑ Z	ddÑ Z
ddÑ Zd ddÑZddÑ ZddÑ ZddÑ ZdS )!⁄SupervisedModelszv
    Class for supervised fraud detection models
    Implements Random Forest, XGBoost, Logistic Regression, etc.
    Áöôôôôô…?È*   Tc                 C   sR   || _ || _|| _i | _i | _i | _i | _i | _i | _i | _	i | _
i | _d| _dS )zı
        Initialize SupervisedModels
        
        Args:
            test_size (float): Proportion of data for testing
            random_state (int): Random seed
            handle_imbalance (bool): Whether to handle class imbalance
        FN)⁄	test_size⁄random_state⁄handle_imbalance⁄models⁄feature_names⁄scalers⁄label_encodersZfeature_selectors⁄
resamplers⁄performance⁄feature_importance⁄shap_values⁄fitted)⁄selfr-   r.   r/   © r:   ˙gC:\Users\Tranquil Abyss\fraud-detection-platform\app\..\src\fraud_detection_engine\models\supervised.py⁄__init__)   s    	zSupervisedModels.__init__c           Q         s‘  êzêd|j vr†t†d° |jtjgdçj †° }t|Édkrñ|| †d°}t	É }|†
|°}t†|°jddç}t†|d°}||k†t°}|†° }||d< qÆtdÉÇn|d †t°}g d	¢â á fd
dÑ|j D É}	||	 jtjgdçj †° }
||	 jddgdçj †° }||	 †° }| †|°}|D ]:}||j v êrtÉ }|†
|| †t°°||< || j|< êqt||| j| j|dç\}}}}|
êr¨tÉ }|†
||
 °||
< |†||
 °||
< || jd< | jêrË|†° t|É dk êrËt| jdçt| jdçt | jdçt!| jdçt"| jdçdú}d}d}|†#° D ]∞\}}zd|†$||°\}}t%d| jddç}t&|||dddç}|†'° }||kêr||}|}|| j(d< || }}W n@ t)êyæ } z&t†d|õ dt|Éõ ù° W Y d}~n
d}~0 0 êq|êrﬁt†*d|õ dù° n
t†d° |	}i }z⁄t%d d!d"d#d$| jdd%ç}|†+||° |†,|°} |†-|°ddÖdf }!| †.|| |!°}"t/†0||j1d&ú°j2d'd(d)ç}#t3†4|°}$|$†5|°}%|| |!|"|#|%|d*ú|d+< || j6d+< || j7d+< |"| j8d+< |#| j9d+< |%| j5d+< t†*d,° W n: t)êy } z t†:d-t|Éõ ù° W Y d}~n
d}~0 0 z¸t;j<d d.dd/d/t||dk Ét||dk É | jd(d0d1ç	}&|&†+||° |&†,|°}'|&†-|°ddÖdf }(| †.||'|(°})t/†0||&j1d&ú°j2d'd(d)ç}*t3†4|&°}$|$†5|°}+|&|'|(|)|*|+|d*ú|d2< |&| j6d2< || j7d2< |)| j8d2< |*| j9d2< |+| j5d2< t†*d3° W n: t)êy< } z t†:d4t|Éõ ù° W Y d}~n
d}~0 0 z¯t=j>d d.dd/d/t||dk Ét||dk É | jd5ç},|,†+||° |,†,|°}-|,†-|°ddÖdf }.| †.||-|.°}/t/†0||,j1d&ú°j2d'd(d)ç}0t3†4|,°}$|$†5|°}1|,|-|.|/|0|1|d*ú|d6< |,| j6d6< || j7d6< |/| j8d6< |0| j9d6< |1| j5d6< t†*d7° W n: t)êyp } z t†:d8t|Éõ ù° W Y d}~n
d}~0 0 z‡t?d9d$| jd:d;ç}2|2†+||° |2†,|°}3|2†-|°ddÖdf }4| †.||3|4°}5t/†0|t†|2j@d °d&ú°j2d'd(d)ç}6t3†A|2|°}$|$†5|°}7|2|3|4|5|6|7|d*ú|d<< |2| j6d<< || j7d<< |5| j8d<< |6| j9d<< |7| j5d<< t†*d=° W n: t)êyå } z t†:d>t|Éõ ù° W Y d}~n
d}~0 0 z÷tBd d.dd/| jd?ç}8|8†+||° |8†,|°}9|8†-|°ddÖdf }:| †.||9|:°};t/†0||8j1d&ú°j2d'd(d)ç}<t3†4|8°}$|$†5|°}=|8|9|:|;|<|=|d*ú|d@< |8| j6d@< || j7d@< |;| j8d@< |<| j9d@< |=| j5d@< t†*dA° W n: t)êyû } z t†:dBt|Éõ ù° W Y d}~n
d}~0 0 êztCdCdDdEdFdGdHdI| jdJç}>|>†+||° |>†,|°}?|>†-|°ddÖdf }@| †.||?|@°}AddKlDmE}B |B|>||d"| jdLç}Ct/†0||CjFd&ú°j2d'd(d)ç}Dt3†G|>j-|dd Ö °}$|$†5|dd Ö °}E|>|?|@|A|D|E|d*ú|dM< |>| j6dM< || j7dM< |A| j8dM< |D| j9dM< |E| j5dM< t†*dN° W n: t)ê	yÏ } z t†:dOt|Éõ ù° W Y d}~n
d}~0 0 êz\g }Fg }G|D ]*}H|F†H||H dP ° |G†H||H dQ ° ê	q˛t†I|F°j'ddçdRk}It†I|G°j'ddç}J| †.||I|J°}Kt/†0|t†Jt|É°d&ú°}L|D ]v}H|H| j9v ê
r~| j9|H }MtK|ÉD ]P\}N}O|O|MdS jLv ê
r†|M|MdS |Ok jMd }P|LjN|Nd'f  |MjN|Pd'f 7  < ê
q†ê
q~|Ld' t|É |Ld'< |Lj2d'd(d)ç}L|I|J|K|L|dTú|dU< |K| j8dU< |L| j9dU< t†*dV° W n: t)êyÜ } z t†:dWt|Éõ ù° W Y d}~n
d}~0 0 dX| _O|W S  t)êyŒ } z"t†:dYt|Éõ ù° Ç W Y d}~n
d}~0 0 dS )Zzß
        Run all supervised models
        
        Args:
            df (DataFrame): Input data
            
        Returns:
            dict: Model results
        ⁄
fraud_flagzENo fraud_flag column found. Using synthetic labels for demonstration.©⁄includer   È   ©⁄axisÈ_   z6No numeric columns found for creating synthetic labels)r=   ⁄transaction_id⁄	sender_id⁄receiver_idc                    s   g | ]}|à vr|ëqS r:   r:   )⁄.0⁄col©⁄exclude_colsr:   r;   ⁄
<listcomp>g   Û    z/SupervisedModels.run_models.<locals>.<listcomp>⁄object⁄category)r-   r.   ⁄stratify⁄numericgöôôôôôπ?)r.   )ZsmoteZadasynZborderline_smoteZ	smote_ennZsmote_tomekNÈ2   Èˇˇˇˇ)⁄n_estimatorsr.   ⁄n_jobsÈ   ⁄f1)⁄cv⁄scoring⁄bestzError with ˙: zUsing z for handling class imbalancez#No suitable resampling method foundÈd   È
   È   È   ⁄balanced)rS   ⁄	max_depth⁄min_samples_split⁄min_samples_leaf⁄class_weightr.   rT   )⁄feature⁄
importancere   F)⁄	ascending)⁄model⁄predictions⁄probabilitiesr5   r6   r7   r1   Zrandom_forestzRandom Forest model completedzError running Random Forest: È   göôôôôôÈ?Zlogloss)	rS   r`   ⁄learning_rate⁄	subsample⁄colsample_bytree⁄scale_pos_weightr.   Zuse_label_encoderZeval_metric⁄xgboostzXGBoost model completedzError running XGBoost: )rS   r`   rk   rl   rm   rn   r.   ⁄lightgbmzLightGBM model completedzError running LightGBM: Á      ?iË  )⁄Crc   r.   ⁄max_iter⁄logistic_regressionz#Logistic Regression model completedz#Error running Logistic Regression: )rS   r`   rk   rl   r.   ⁄gradient_boostingz!Gradient Boosting model completedz!Error running Gradient Boosting: )r[   rQ   ⁄relu⁄adamg-CÎ‚6?È    ⁄adaptiveÈ»   )Zhidden_layer_sizes⁄
activation⁄solver⁄alpha⁄
batch_sizerk   rs   r.   )⁄permutation_importance)⁄	n_repeatsr.   ⁄neural_networkzNeural Network model completedzError running Neural Network: rh   ri   Á      ‡?rd   )rh   ri   r5   r6   r1   ⁄ensemblezEnsemble model completedzError running Ensemble model: Tz!Error running supervised models: )P⁄columns⁄logger⁄warning⁄select_dtypes⁄np⁄number⁄tolist⁄len⁄fillnar   ⁄fit_transform⁄abs⁄max⁄
percentile⁄astype⁄int⁄copy⁄
ValueError⁄_clean_datar   ⁄strr3   r   r-   r.   r   ⁄	transformr2   r/   ⁄sumr   r   r   r"   r#   ⁄itemsZfit_resampler   r   ⁄meanr4   ⁄	Exception⁄info⁄fit⁄predict⁄predict_proba⁄_calculate_performance_metrics⁄pd⁄	DataFrame⁄feature_importances_⁄sort_values⁄shapZTreeExplainerr7   r0   r1   r5   r6   ⁄error⁄xgbZXGBClassifier⁄lgbZLGBMClassifierr	   ⁄coef_ZLinearExplainerr   r   Zsklearn.inspectionr   Zimportances_meanZKernelExplainer⁄append⁄array⁄zeros⁄	enumerate⁄values⁄index⁄locr8   )Qr9   ⁄df⁄numeric_cols⁄X⁄scaler⁄X_scaled⁄z_scores⁄	threshold⁄y⁄feature_cols⁄numeric_features⁄categorical_featuresrH   ⁄le⁄X_train⁄X_test⁄y_train⁄y_testZresampling_methodsZbest_method⁄
best_score⁄method_name⁄	resamplerZX_resampledZy_resampled⁄rf⁄scores⁄	avg_score⁄e⁄all_feature_names⁄resultsZrf_modelZrf_predZrf_probaZrf_performanceZrf_importanceZ	explainerZrf_shap_valuesZ	xgb_modelZxgb_predZ	xgb_probaZxgb_performanceZxgb_importanceZxgb_shap_valuesZ	lgb_modelZlgb_predZ	lgb_probaZlgb_performanceZlgb_importanceZlgb_shap_valuesZlr_modelZlr_predZlr_probaZlr_performanceZlr_importanceZlr_shap_valuesZgb_modelZgb_predZgb_probaZgb_performanceZgb_importanceZgb_shap_valuesZnn_modelZnn_predZnn_probaZnn_performancer   Zperm_importanceZnn_importanceZnn_shap_values⁄all_predictions⁄all_probabilities⁄
model_nameZensemble_predZensemble_probaZensemble_performanceZensemble_importanceZmodel_importance⁄ird   ⁄idxr:   rI   r;   ⁄
run_models@   sí   






ˇ





˚	

4
˘

˛˝

˘






*˜
˛˝

˘






*˘

˛˝

˘






*¸
˛˝
˘






*˚
˛˝

˘






*¯
ˇ˛˝˘






*˛
(˚


*zSupervisedModels.run_modelsc              
   C   s¸   z∏|† tjtj gtj°}|jD ]í}|| jdv r t†|| d°}t†|°s |d }t†|| |k||| °||< t†|| d°}t†|°s |d }t†|| |k ||| °||< q |W S  t	ê yˆ } z$t
†dt|Éõ ù° |W  Y d}~S d}~0 0 dS )z…
        Clean data by handling infinity, NaN, and extreme values
        
        Args:
            X (DataFrame): Input data
            
        Returns:
            DataFrame: Cleaned data
        )⁄float64⁄int64Èc   r\   r@   zError cleaning data: N)⁄replacerà   ⁄inf⁄nanrÑ   ⁄dtype⁄nanpercentile⁄isnan⁄whererõ   rÖ   r¶   rñ   )r9   r≥   rH   ⁄percentile_99⁄max_val⁄percentile_1⁄min_valr«   r:   r:   r;   rï     s     



 zSupervisedModels._clean_datac                 C   s‡   z†t ||É}t||ddç}t||ddç}t||ddç}t||É}t||É}	t||É}
t||ddç}t||É\}}}t	||É\}}}||||||	|
|||||dúW S  t
y⁄ } z"t†dt|Éõ ù° Ç W Y d}~n
d}~0 0 dS )a   
        Calculate performance metrics for a model
        
        Args:
            y_true (array): True labels
            y_pred (array): Predicted labels
            y_proba (array): Predicted probabilities
            
        Returns:
            dict: Performance metrics
        r   )⁄zero_divisionT)⁄output_dict)⁄accuracy⁄	precision⁄recallrV   ⁄roc_auc⁄avg_precisionr   r   ⁄precision_curve⁄recall_curve⁄fpr⁄tprz'Error calculating performance metrics: N)r   r   r   r   r   r   r   r   r   r   rõ   rÖ   r¶   rñ   )r9   ⁄y_true⁄y_pred⁄y_probar‡   r·   r‚   rV   r„   r‰   ⁄cmZclass_reportrÂ   rÊ   ⁄_rÁ   rË   r«   r:   r:   r;   r†   =  s6    



Ùz/SupervisedModels._calculate_performance_metricsc              
      s  | j stdÉÇ|| jvr0|dkr0td|õ dùÉÇêzî|dkrjg }| jD ]}|†| j| ° qFtt|ÉÉ}n
| j| }|| †° }| †|°}|j	D ]8}|| j
v rê| j
| â || †t°†á fddÑ°||< qêd| jv êr
|jtjgdçj	†° }|êr
| jd †|| °||< |dkêríg }g }	| jD ]D}| j| }
|
†|°}|
†|°d	d	Öd
f }|†|° |	†|° êq"t†|°jddçdk}t†|	°jddç}n*| j| }
|
†|°}|
†|°d	d	Öd
f }||dúW S  têy } z(t†d|õ dt|Éõ ù° Ç W Y d	}~n
d	}~0 0 d	S )z˙
        Make predictions using a fitted model
        
        Args:
            df (DataFrame): Input data
            model_name (str): Name of the model to use
            
        Returns:
            dict: Predictions and probabilities
        ˙)Models not fitted. Call run_models first.rÉ   zModel ˙ not found.c                    s   | à j v rà †| g°d S dS )Nr   rR   )⁄classes_ró   )⁄x©rº   r:   r;   ⁄<lambda>ù  rL   z*SupervisedModels.predict.<locals>.<lambda>rP   r>   Nr@   r   rA   rÇ   )rh   ri   zError making predictions with rZ   )r8   rî   r0   r1   ⁄extend⁄list⁄setrì   rï   rÑ   r3   rë   rñ   ⁄mapr2   rá   rà   râ   rä   ró   rû   rü   r™   r´   rö   rõ   rÖ   r¶   )r9   r±   rÃ   r1   ⁄namer≥   rH   r∫   r    rÀ   rg   ⁄pred⁄probarh   ri   r«   r:   rÚ   r;   rû   u  sV    






ˇ







˛zSupervisedModels.predictc                 C   s2   | j stdÉÇ|| jvr(td|õ dùÉÇ| j| S )z√
        Get feature importance for a model
        
        Args:
            model_name (str): Name of the model
            
        Returns:
            DataFrame: Feature importance
        rÓ   zFeature importance for rÔ   )r8   rî   r6   ©r9   rÃ   r:   r:   r;   ⁄get_feature_importance   s
    

z'SupervisedModels.get_feature_importancec                 C   s2   | j stdÉÇ|| jvr(td|õ dùÉÇ| j| S )z¿
        Get performance metrics for a model
        
        Args:
            model_name (str): Name of the model
            
        Returns:
            dict: Performance metrics
        rÓ   zPerformance for rÔ   )r8   rî   r5   r˚   r:   r:   r;   ⁄get_performance‹  s
    

z SupervisedModels.get_performancec                 C   s2   | j stdÉÇ|| jvr(td|õ dùÉÇ| j| S )z±
        Get SHAP values for a model
        
        Args:
            model_name (str): Name of the model
            
        Returns:
            array: SHAP values
        rÓ   zSHAP values for rÔ   )r8   rî   r7   r˚   r:   r:   r;   ⁄get_shap_valuesÓ  s
    

z SupervisedModels.get_shap_valuesÈ   c              
   C   sû   zX| † |°}|†|°}tjddç tjdd|dç t†d|õ d|õ ù° t†°  t†° W S  t	yò } z(t
†d|õ d	t|Éõ ù° Ç W Y d
}~n
d
}~0 0 d
S )z¥
        Plot feature importance for a model
        
        Args:
            model_name (str): Name of the model
            top_n (int): Number of top features to show
        )r\   È   ©⁄figsizere   rd   )rÒ   r∏   ⁄datazTop z Feature Importance - z&Error plotting feature importance for rZ   N)r¸   ⁄head⁄plt⁄figure⁄snsZbarplot⁄title⁄tight_layout⁄gcfrõ   rÖ   r¶   rñ   )r9   rÃ   ⁄top_n⁄importance_df⁄top_featuresr«   r:   r:   r;   ⁄plot_feature_importance   s    


z(SupervisedModels.plot_feature_importancec              
   C   s∏   zr| † |°}|d }tjddç tj|dddddgddgd	ç t†d
|õ ù° t†d° t†d° t†°  t†	° W S  t
y≤ } z(t†d|õ dt|Éõ ù° Ç W Y d}~n
d}~0 0 dS )zz
        Plot confusion matrix for a model
        
        Args:
            model_name (str): Name of the model
        r   ©r   rj   r  T⁄d⁄Bluesz	Not Fraud⁄Fraud)⁄annot⁄fmt⁄cmap⁄xticklabels⁄yticklabelszConfusion Matrix - ⁄Actual⁄	Predictedz$Error plotting confusion matrix for rZ   N)r˝   r  r  r  ⁄heatmapr  ⁄ylabel⁄xlabelr	  r
  rõ   rÖ   r¶   rñ   )r9   rÃ   r5   rÏ   r«   r:   r:   r;   ⁄plot_confusion_matrix  s     
˛


z&SupervisedModels.plot_confusion_matrixc              
   C   s  z¿| † |°}|d }|d }|d }tjddç tj|||õ d|dõdùd	ç t†d
dgd
dgd° t†ddg° t†ddg° t†d° t†d° t†d|õ ù° tj	ddç t†
°  t†° W S  têy } z(t†d|õ dt|Éõ ù° Ç W Y d}~n
d}~0 0 dS )zs
        Plot ROC curve for a model
        
        Args:
            model_name (str): Name of the model
        rÁ   rË   r„   r  r  z (AUC = ˙.3f˙)©⁄labelr   r@   zk--Á        rq   ÁÕÃÃÃÃÃ?zFalse Positive RatezTrue Positive RatezROC Curve - zlower right©r∞   zError plotting ROC curve for rZ   N©r˝   r  r  ⁄plot⁄xlim⁄ylimr  r  r  ⁄legendr	  r
  rõ   rÖ   r¶   rñ   )r9   rÃ   r5   rÁ   rË   r„   r«   r:   r:   r;   ⁄plot_roc_curve7  s&    



zSupervisedModels.plot_roc_curvec              
   C   s   z™| † |°}|d }|d }|d }tjddç tj|||õ d|dõdùd	ç t†d
dg° t†d
dg° t†d° t†d° t†d|õ ù° tj	ddç t†
°  t†° W S  tyÍ } z(t†d|õ dt|Éõ ù° Ç W Y d}~n
d}~0 0 dS )zÄ
        Plot precision-recall curve for a model
        
        Args:
            model_name (str): Name of the model
        rÂ   rÊ   r‰   r  r  z (AP = r  r  r   r"  rq   r#  ⁄Recall⁄	PrecisionzPrecision-Recall Curve - z
lower leftr$  z*Error plotting precision-recall curve for rZ   Nr%  )r9   rÃ   r5   rÂ   rÊ   r‰   r«   r:   r:   r;   ⁄plot_precision_recall_curveW  s$    



z,SupervisedModels.plot_precision_recall_curveN)r+   r,   T)rˇ   )⁄__name__⁄
__module__⁄__qualname__⁄__doc__r<   rœ   rï   r†   rû   r¸   r˝   r˛   r  r  r*  r-  r:   r:   r:   r;   r*   #   s    
   \$8U
 r*   )Kr1  ⁄pandasr°   ⁄numpyrà   Zsklearn.model_selectionr   r   r   r   ⁄sklearn.ensembler   r   r   ⁄sklearn.linear_modelr	   Zsklearn.svmr
   Zsklearn.neural_networkr   Zsklearn.naive_bayesr   Zsklearn.metricsr   r   r   r   r   r   r   r   r   r   ⁄sklearn.preprocessingr   r   r   Zsklearn.feature_selectionr   r   r   Zimblearn.over_samplingr   r   r   Zimblearn.under_samplingr    r!   Zimblearn.combiner"   r#   ro   rß   rp   r®   r•   ⁄matplotlib.pyplot⁄pyplotr  ⁄seabornr  ⁄warnings⁄logging⁄typingr$   r%   r&   r'   ⁄filterwarnings⁄basicConfig⁄INFO⁄	getLoggerr.  rÖ   r*   r:   r:   r:   r;   ⁄<module>   s4   0



=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\models\__pycache__\unsupervised.cpython-39.pyc ===
a
    èöh9t  „                   @   s  d Z ddlZddlZddlmZ ddlmZ ddl	m
Z
 ddlmZmZ ddlmZ ddlmZ dd	lmZmZ dd
lmZ ddlmZ ddlZddlmZmZ ddlmZ ddl Z ddl!Z!ddl"m#Z#m$Z$m%Z%m&Z& e †'d° e!j(e!j)dç e!†*e+°Z,G ddÑ dÉZ-dS )zX
Unsupervised Models Module
Implements unsupervised learning models for fraud detection
È    N)⁄IsolationForest)⁄LocalOutlierFactor)⁄OneClassSVM)⁄DBSCAN⁄KMeans)⁄PCA)⁄TSNE)⁄StandardScaler⁄RobustScaler)⁄SimpleImputer)⁄silhouette_score)⁄layers⁄models)⁄EarlyStopping)⁄Dict⁄List⁄Tuple⁄Union⁄ignore)⁄levelc                   @   sR   e Zd ZdZdddÑZddÑ Zdd	Ñ Zd
dÑ ZddÑ ZddÑ Z	ddÑ Z
ddÑ ZdS )⁄UnsupervisedModelszÅ
    Class for unsupervised fraud detection models
    Implements Isolation Forest, Local Outlier Factor, Autoencoders, etc.
    Á{ÆG·zÑ?È*   c                 C   s.   || _ || _i | _i | _i | _i | _d| _dS )zµ
        Initialize UnsupervisedModels
        
        Args:
            contamination (float): Expected proportion of outliers
            random_state (int): Random seed
        FN)⁄contamination⁄random_stater   ⁄feature_names⁄scalers⁄imputers⁄fitted)⁄selfr   r   © r    ˙iC:\Users\Tranquil Abyss\fraud-detection-platform\app\..\src\fraud_detection_engine\models\unsupervised.py⁄__init__    s    zUnsupervisedModels.__init__c           <   
   C   sä  êzF|j tjgdçj†° }t|Édk r6t†d° i W S || †° }| †	|°}| †
|°\}}}|| jd< || jd< i }zft| j| jdddç}|†|°}	|†|°}
| †|
°}|	|||dú|d	< || jd	< || jd	< t†d
° W n: têy } z t†dt|Éõ ù° W Y d}~n
d}~0 0 zrt| jtdt|Éd Édddç}|†|°}|†|°}| †|°}||||dú|d< || jd< || jd< t†d° W n: têy¬ } z t†dt|Éõ ù° W Y d}~n
d}~0 0 zbt| jdddç}|†|°}|†|°}| †|°}||||dú|d< || jd< || jd< t†d° W n: têy` } z t†dt|Éõ ù° W Y d}~n
d}~0 0 êzt|Édkêrê|tjjt|Édddç }n|}tdddç}|†|°}t†|dkdd°}t† t|É°}t!t|ÉÉD ]Ñ}|| dkêr÷t†|dk°d  }t|Éd kêr2tj"j#|| ||  dd!ç}|†° ||< n&t†$tj"j#||j%d d!ç dd!ç°||< êq÷| †|°}t|Édkêr<|†|°}t†|dkdd°}t† t|É°}t!t|ÉÉD ]Ñ}|| dkêr™t†|dk°d  }t|Éd kêrtj"j#|| ||  dd!ç}|†° ||< n&t†$tj"j#||j%d d!ç dd!ç°||< êq™| †|°} n|}|} || ||dú|d"< || jd"< || jd"< t†d#° W n: têy∞ } z t†d$t|Éõ ù° W Y d}~n
d}~0 0 êzg }!t!dtd%t|Éd ÉÉ}"|"D ]0}#t&|#| jd&d'ç}$|$†|°}%|!†'t(||%É° êq÷|"t†)|!° }&t&|&| jd&d'ç}'|'†|°}(|'†*|°}|jdd!ç})| †|)°}*t†+|*d| j d( °}+t†|*|+kdd°},|,|*|'||&|!d)ú|d*< |'| jd*< || jd*< t†d+|&õ d,ù° W n: têy } z t†d-t|Éõ ù° W Y d}~n
d}~0 0 z¸|j,d }-t$d|-d É}.| †-|-|.°}/t.d.ddd/ç}0|/j/||d0d1d2|0gd d3ç}1|/†0|°}2tj%t†1||2 d°dd!ç}3t†2|3°}3t†3|3d t†4tj5°j$°}3| †|3°}4t†+|4d| j d( °}+t†|4|+kdd°}5|5|4|/||1j6d4ú|d5< |/| jd5< || jd5< t†d6° W n: têy( } z t†d7t|Éõ ù° W Y d}~n
d}~0 0 zÿt7td&|j,d d É| jd8ç}6|6†8|°}7|6†9|7°}8tj%t†1||8 d°dd!ç}9t†2|9°}9t†3|9d t†4tj5°j$°}9| †|9°}:t†+|:d| j d( °}+t†|:|+kdd°};|;|:|6||6j:d9ú|d:< |6| jd:< || jd:< t†d;° W n: têy< } z t†d<t|Éõ ù° W Y d}~n
d}~0 0 d| _;|W S  têyÑ } z"t†d=t|Éõ ù° Ç W Y d}~n
d}~0 0 dS )>z©
        Run all unsupervised models
        
        Args:
            df (DataFrame): Input data
            
        Returns:
            dict: Model results
        )⁄includeÈ   z2Not enough numeric columns for unsupervised models⁄globalÈˇˇˇˇ⁄auto)r   r   ⁄n_jobsZmax_samples)⁄predictions⁄scores⁄modelr   ⁄isolation_forestz Isolation Forest model completedz Error running Isolation Forest: NÈ   È   T)r   Zn_neighborsZnoveltyr(   ⁄local_outlier_factorz$Local Outlier Factor model completedz$Error running Local Outlier Factor: ⁄rbf⁄scale)⁄nu⁄kernel⁄gamma⁄one_class_svmzOne-Class SVM model completedzError running One-Class SVM: i'  F)⁄replaceg      ‡?È   )⁄eps⁄min_samplesr   ©⁄axis⁄dbscanzDBSCAN model completedzError running DBSCAN: È   È
   )⁄
n_clustersr   ⁄n_initÈd   )r)   r*   r+   r   ⁄	optimal_k⁄silhouette_scores⁄kmeanszK-Means model completed with z	 clusterszError running K-Means: Zval_loss)⁄monitor⁄patienceZrestore_best_weightsÈ2   È    göôôôôô…?)⁄epochs⁄
batch_sizeZvalidation_split⁄	callbacks⁄verbose)r)   r*   r+   r   ⁄history⁄autoencoderzAutoencoder model completedzError running Autoencoder: )⁄n_componentsr   )r)   r*   r+   r   ⁄explained_variance⁄pcaz%PCA-based anomaly detection completedz+Error running PCA-based anomaly detection: z#Error running unsupervised models: )<⁄select_dtypes⁄np⁄number⁄columns⁄tolist⁄len⁄logger⁄warning⁄copy⁄_clean_data⁄_scale_datar   r   r   r   r   ⁄fit_predict⁄decision_function⁄_normalize_scoresr   r   ⁄info⁄	Exception⁄error⁄strr   ⁄minr   ⁄random⁄choicer   ⁄where⁄zeros⁄range⁄linalg⁄norm⁄max⁄meanr   ⁄appendr   ⁄argmax⁄	transform⁄
percentile⁄shape⁄_build_autoencoderr   ⁄fit⁄predict⁄power⁄
nan_to_num⁄clip⁄finfo⁄float64rM   r   ⁄fit_transform⁄inverse_transform⁄explained_variance_ratio_r   )<r   ⁄df⁄numeric_cols⁄X⁄X_scaled⁄scaler⁄imputer⁄resultsZif_modelZif_predictionsZ	if_scoresZif_scores_normalized⁄eZ	lof_modelZlof_predictionsZ
lof_scoresZlof_scores_normalizedZocsvm_modelZocsvm_predictionsZocsvm_scoresZocsvm_scores_normalized⁄X_subsetZdbscan_modelZdbscan_labelsZdbscan_predictionsZdbscan_scores⁄i⁄core_points⁄	distancesZdbscan_scores_normalizedZfull_predictionsZfull_scoresZfull_scores_normalizedrC   Zk_range⁄krD   ⁄labelsrB   Zkmeans_modelZkmeans_labels⁄min_distancesZkmeans_scores_normalized⁄	thresholdZkmeans_predictions⁄	input_dim⁄encoding_dimrN   ⁄early_stoppingrM   ⁄reconstructions⁄mseZmse_normalizedZautoencoder_predictionsrQ   ⁄pca_transformed⁄X_reconstructedZ
pca_errorsZpca_errors_normalizedZpca_predictionsr    r    r!   ⁄
run_models0   sÑ   




¸


¸


*¸


¸


*˝


¸


*
*

*¸


*



˙
	

*
˙



˚


*



˚


*zUnsupervisedModels.run_modelsc              
   C   s¸   z∏|† tjtj gtj°}|jD ]í}|| jdv r t†|| d°}t†|°s |d }t†|| |k||| °||< t†|| d°}t†|°s |d }t†|| |k ||| °||< q |W S  t	ê yˆ } z$t
†dt|Éõ ù° |W  Y d}~S d}~0 0 dS )z…
        Clean data by handling infinity, NaN, and extreme values
        
        Args:
            X (DataFrame): Input data
            
        Returns:
            DataFrame: Cleaned data
        )rz   ⁄int64Èc   r>   r.   zError cleaning data: N)r6   rS   ⁄inf⁄nanrU   ⁄dtype⁄nanpercentile⁄isnanrg   ra   rX   rb   rc   )r   rÄ   ⁄colZpercentile_99⁄max_valZpercentile_1⁄min_valrÖ   r    r    r!   r[   q  s     



 zUnsupervisedModels._clean_datac              
   C   sÆ   zHt ddç}|†|°}tÉ }|†|°}t†|°}t†|dd°}|||fW S  ty® } zHt†dt	|Éõ ù° ||†
°  |†° |†
°   }|jddfW  Y d}~S d}~0 0 dS )zŸ
        Scale data using RobustScaler (more resistant to outliers)
        
        Args:
            X (DataFrame): Input data
            
        Returns:
            tuple: (scaled data, scaler, imputer)
        ⁄median)⁄strategyÁ    _†¬Á    _†BzError scaling data: N)r   r{   r
   rS   rw   rx   ra   rX   rb   rc   rd   rl   ⁄values)r   rÄ   rÉ   ⁄	X_imputedrÇ   rÅ   rÖ   ⁄X_normalizedr    r    r!   r\   ï  s    




zUnsupervisedModels._scale_datac              
   C   sö   zRt †|°}t †|dd°}|†° }|†° }||krD|| ||  }n
t †|°}|W S  tyî } z*t†dt	|Éõ ù° t †|°W  Y d}~S d}~0 0 dS )z≤
        Normalize scores to 0-1 range
        
        Args:
            scores (array): Input scores
            
        Returns:
            array: Normalized scores
        r¢   r£   zError normalizing scores: N)
rS   rw   rx   rd   rl   ⁄
zeros_likera   rX   rb   rc   )r   r*   ⁄	min_score⁄	max_scoreZnormalized_scoresrÖ   r    r    r!   r_   ¥  s    


z$UnsupervisedModels._normalize_scoresc              
   C   sæ   z~t j|fdç}t j|d ddç|É}t j|ddç|É}t j|d ddç|É}t j|ddç|É}t†||°}|jdddç |W S  ty∏ } z"t†d	t	|Éõ ù° Ç W Y d
}~n
d
}~0 0 d
S )z„
        Build autoencoder model
        
        Args:
            input_dim (int): Input dimension
            encoding_dim (int): Encoding dimension
            
        Returns:
            Model: Autoencoder model
        )rr   r$   Zrelu)⁄
activation⁄linear⁄adam⁄mean_squared_error)⁄	optimizer⁄losszError building autoencoder: N)
r   ⁄InputZDenser   ⁄Model⁄compilera   rX   rb   rc   )r   ré   rè   Zinput_layer⁄encoded⁄decodedrN   rÖ   r    r    r!   rs   “  s    z%UnsupervisedModels._build_autoencoderc              
   C   sﬁ  | j stdÉÇ|| jvr(td|õ dùÉÇêzl| j| }|| †° }| †|°}d| jv rÇd| jv rÇ| jd †|°}| jd †|°}n4||†	°  |†
° |†	°   }t†|°}t†|dd°}| j| }|dkrÍ|†|°}|†|°}	| †|	°}
ên¢|dkêr|†|°}|†|°}	| †|	°}
ênv|d	kêrB|†|°}|†|°}	| †|	°}
ênJ|d
kêr|†|°}t†|dkdd°}t†t|É°}	tt|ÉÉD ]Ñ}|| dkêrÇt†|dk°d }t|Édkêrﬁtjj|| ||  ddç}|†	° |	|< n&t†
tjj||jddç ddç°|	|< êqÇ| †|	°}
ênv|dkêrl|†|°}|j	ddç}| †|°}
t†|
d| j d °}t†|
|kdd°}ên |dkêr|†|°}tjt†|| d°ddç}t†|°}t†|dt†tj°j
°}| †|°}
t†|
d| j d °}t†|
|kdd°}nú|dkêr~|†|°}|†|°}tjt†|| d°ddç}t†|°}t†|dt†tj°j
°}| †|°}
t†|
d| j d °}t†|
|kdd°}ntd|õ ùÉÇ||
dúW S  têyÿ } z(t †!d|õ dt"|Éõ ù° Ç W Y d}~n
d}~0 0 dS )zÛ
        Make predictions using a fitted model
        
        Args:
            df (DataFrame): Input data
            model_name (str): Name of the model to use
            
        Returns:
            dict: Predictions and scores
        ˙)Models not fitted. Call run_models first.˙Model ˙ not found.r%   r¢   r£   r,   r/   r5   r<   r&   r.   r   r:   rD   rA   rN   r$   rQ   zUnknown model: )r)   r*   zError making predictions with ˙: N)#r   ⁄
ValueErrorr   r   rZ   r[   r   r   rp   rd   rl   rS   rw   rx   ru   r^   r_   r]   rg   rh   rW   ri   rj   rk   rm   rq   r   rv   ry   rz   r|   ra   rX   rb   rc   )r   r~   ⁄
model_namer   rÄ   r•   rÅ   r+   r)   r*   Zscores_normalizedrá   rà   râ   rå   rç   rë   rí   rì   rî   rÖ   r    r    r!   ru   Û  sä    














*











˛zUnsupervisedModels.predictc              
   C   s.  | j stdÉÇ|| jvr(td|õ dùÉÇzæ| j| }|dkrh| j| j}t†||dú°jdddç}|W S |d	krÆ| j| j}t†d
dÑ t	t
|ÉÉD É|dú°jdddç}|W S t†t
|É°t
|É }t†||dú°jdddç}|W S W nB têy( } z(t†d|õ dt|Éõ ù° Ç W Y d}~n
d}~0 0 dS )z√
        Get feature importance for a model
        
        Args:
            model_name (str): Name of the model
            
        Returns:
            DataFrame: Feature importance
        rµ   r∂   r∑   r,   )⁄feature⁄
importancerº   F)⁄	ascendingrQ   c                 S   s   g | ]}d |d õ ùëqS )⁄PCr.   r    )⁄.0rá   r    r    r!   ⁄
<listcomp>£  Û    z=UnsupervisedModels.get_feature_importance.<locals>.<listcomp>z%Error getting feature importance for r∏   N)r   rπ   r   r   Zfeature_importances_⁄pd⁄	DataFrame⁄sort_valuesr}   ri   rW   rS   ⁄onesra   rX   rb   rc   )r   r∫   r   rº   Zimportance_dfrP   rÖ   r    r    r!   ⁄get_feature_importance}  sF    


˛˝˛˝˛˝
z)UnsupervisedModels.get_feature_importanceN)r   r   )⁄__name__⁄
__module__⁄__qualname__⁄__doc__r"   rï   r[   r\   r_   rs   ru   r∆   r    r    r    r!   r      s   
  C$! r   ).r    ⁄pandasr¬   ⁄numpyrS   Zsklearn.ensembler   Zsklearn.neighborsr   Zsklearn.svmr   Zsklearn.clusterr   r   ⁄sklearn.decompositionr   Zsklearn.manifoldr   ⁄sklearn.preprocessingr	   r
   Zsklearn.imputer   Zsklearn.metricsr   ⁄
tensorflow⁄tfZtensorflow.kerasr   r   Ztensorflow.keras.callbacksr   ⁄warnings⁄logging⁄typingr   r   r   r   ⁄filterwarnings⁄basicConfig⁄INFO⁄	getLoggerr«   rX   r   r    r    r    r!   ⁄<module>   s*   



=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\models\__pycache__\__init__.cpython-39.pyc ===
a
    èöh    „                   @   s   d S )N© r   r   r   ˙eC:\Users\Tranquil Abyss\fraud-detection-platform\app\..\src\fraud_detection_engine\models\__init__.py⁄<module>   Û    

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\reporting\pdf_generator.py ===
"""
PDF Generator Module
Implements PDF report generation for fraud detection results
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import os
import io
import base64
from reportlab.lib.pagesizes import letter, A4
from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Table, TableStyle, Image, PageBreak
from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
from reportlab.lib.units import inch
from reportlab.lib import colors
from reportlab.lib.enums import TA_CENTER, TA_LEFT, TA_RIGHT
from reportlab.graphics.shapes import Drawing
from reportlab.graphics.charts.lineplots import LinePlot
from reportlab.graphics.charts.barcharts import VerticalBarChart
from reportlab.graphics.widgets import markers
import warnings
import logging
from typing import Dict, List, Tuple, Union

warnings.filterwarnings('ignore')
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class PDFGenerator:
    """
    Class for generating PDF reports for fraud detection results
    Implements professional audit report generation
    """
    
    def __init__(self, output_dir='../reports/generated'):
        """
        Initialize PDFGenerator
        
        Args:
            output_dir (str): Output directory for reports
        """
        self.output_dir = output_dir
        self.styles = getSampleStyleSheet()
        self.setup_custom_styles()
        
        # Create output directory if it doesn't exist
        os.makedirs(output_dir, exist_ok=True)
    
    def setup_custom_styles(self):
        """Setup custom styles for the report"""
        # Title style
        self.styles.add(ParagraphStyle(
            name='CustomTitle',
            parent=self.styles['Title'],
            fontSize=24,
            spaceAfter=30,
            alignment=TA_CENTER,
            textColor=colors.darkblue
        ))
        
        # Subtitle style
        self.styles.add(ParagraphStyle(
            name='CustomSubtitle',
            parent=self.styles['Heading1'],
            fontSize=16,
            spaceAfter=12,
            alignment=TA_CENTER,
            textColor=colors.darkblue
        ))
        
        # Section heading style
        self.styles.add(ParagraphStyle(
            name='SectionHeading',
            parent=self.styles['Heading2'],
            fontSize=14,
            spaceAfter=12,
            textColor=colors.darkblue
        ))
        
        # Subsection heading style
        self.styles.add(ParagraphStyle(
            name='SubsectionHeading',
            parent=self.styles['Heading3'],
            fontSize=12,
            spaceAfter=6,
            textColor=colors.darkblue
        ))
        
        # Body style
        self.styles.add(ParagraphStyle(
            name='Body',
            parent=self.styles['Normal'],
            fontSize=10,
            spaceAfter=6,
            alignment=TA_LEFT
        ))
        
        # Footer style
        self.styles.add(ParagraphStyle(
            name='Footer',
            parent=self.styles['Normal'],
            fontSize=8,
            alignment=TA_CENTER,
            textColor=colors.grey
        ))
    
    def generate_executive_summary(self, df, risk_scores, top_fraud, include_charts=True, include_recommendations=True):
        """
        Generate executive summary report
        
        Args:
            df (DataFrame): Transaction data
            risk_scores (DataFrame): Risk scores
            top_fraud (DataFrame): Top fraudulent transactions
            include_charts (bool): Whether to include charts
            include_recommendations (bool): Whether to include recommendations
            
        Returns:
            str: Path to generated PDF
        """
        try:
            # Create filename
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"executive_summary_{timestamp}.pdf"
            filepath = os.path.join(self.output_dir, filename)
            
            # Create document
            doc = SimpleDocTemplate(filepath, pagesize=A4)
            story = []
            
            # Add title
            story.append(Paragraph("Fraud Detection Executive Summary", self.styles['CustomTitle']))
            story.append(Spacer(1, 12))
            
            # Add subtitle with date
            report_date = datetime.now().strftime("%B %d, %Y")
            story.append(Paragraph(f"Report Date: {report_date}", self.styles['CustomSubtitle']))
            story.append(PageBreak())
            
            # Add summary statistics
            story.append(Paragraph("Summary Statistics", self.styles['SectionHeading']))
            
            # Calculate statistics
            total_transactions = len(df)
            fraud_transactions = risk_scores['is_fraud'].sum()
            fraud_percentage = (fraud_transactions / total_transactions) * 100
            total_amount = df['amount'].sum() if 'amount' in df.columns else 0
            fraud_amount = df.loc[risk_scores['is_fraud'], 'amount'].sum() if 'amount' in df.columns else 0
            
            # Create statistics table
            stats_data = [
                ['Metric', 'Value'],
                ['Total Transactions', f"{total_transactions:,}"],
                ['Fraudulent Transactions', f"{fraud_transactions:,}"],
                ['Fraud Percentage', f"{fraud_percentage:.2f}%"],
                ['Total Amount', f"${total_amount:,.2f}"],
                ['Fraud Amount', f"${fraud_amount:,.2f}"]
            ]
            
            stats_table = Table(stats_data, colWidths=[3*inch, 2*inch])
            stats_table.setStyle(TableStyle([
                ('BACKGROUND', (0, 0), (-1, 0), colors.darkblue),
                ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),
                ('ALIGN', (0, 0), (-1, -1), 'CENTER'),
                ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),
                ('FONTSIZE', (0, 0), (-1, 0), 12),
                ('BOTTOMPADDING', (0, 0), (-1, 0), 12),
                ('BACKGROUND', (0, 1), (-1, -1), colors.beige),
                ('GRID', (0, 0), (-1, -1), 1, colors.black)
            ]))
            
            story.append(stats_table)
            story.append(Spacer(1, 12))
            
            # Add risk distribution chart if requested
            if include_charts:
                story.append(Paragraph("Risk Distribution", self.styles['SectionHeading']))
                
                # Create risk distribution chart
                plt.figure(figsize=(8, 6))
                sns.histplot(risk_scores['risk_score'], bins=50, kde=True)
                plt.title('Distribution of Risk Scores')
                plt.xlabel('Risk Score')
                plt.ylabel('Frequency')
                
                # Convert plot to image
                img_buffer = io.BytesIO()
                plt.savefig(img_buffer, format='png', dpi=300, bbox_inches='tight')
                img_buffer.seek(0)
                
                # Add image to report
                risk_dist_img = Image(img_buffer, width=6*inch, height=4*inch)
                story.append(risk_dist_img)
                story.append(Spacer(1, 12))
                
                plt.close()
            
            # Add top fraudulent transactions
            story.append(Paragraph("Top Fraudulent Transactions", self.styles['SectionHeading']))
            
            # Create table for top fraud transactions
            if len(top_fraud) > 0:
                # Prepare data
                fraud_data = []
                fraud_data.append(['Transaction ID', 'Amount', 'Risk Score', 'Date'])
                
                for _, row in top_fraud.head(10).iterrows():
                    transaction_id = row.get('transaction_id', 'N/A')
                    amount = f"${row.get('amount', 0):,.2f}"
                    risk_score = f"{row.get('risk_score', 0):.3f}"
                    date = row.get('timestamp', 'N/A')
                    
                    if isinstance(date, pd.Timestamp):
                        date = date.strftime('%Y-%m-%d')
                    
                    fraud_data.append([transaction_id, amount, risk_score, date])
                
                fraud_table = Table(fraud_data, colWidths=[1.5*inch, 1*inch, 1*inch, 1.5*inch])
                fraud_table.setStyle(TableStyle([
                    ('BACKGROUND', (0, 0), (-1, 0), colors.darkblue),
                    ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),
                    ('ALIGN', (0, 0), (-1, -1), 'CENTER'),
                    ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),
                    ('FONTSIZE', (0, 0), (-1, 0), 10),
                    ('BOTTOMPADDING', (0, 0), (-1, 0), 12),
                    ('BACKGROUND', (0, 1), (-1, -1), colors.beige),
                    ('GRID', (0, 0), (-1, -1), 1, colors.black)
                ]))
                
                story.append(fraud_table)
                story.append(Spacer(1, 12))
            
            # Add recommendations if requested
            if include_recommendations:
                story.append(Paragraph("Recommendations", self.styles['SectionHeading']))
                
                recommendations = [
                    "1. Immediately review all high-risk transactions (risk score > 0.8).",
                    "2. Implement additional verification steps for transactions over $10,000.",
                    "3. Monitor patterns in fraudulent transactions to identify potential fraud rings.",
                    "4. Update fraud detection rules based on recent fraud patterns.",
                    "5. Conduct regular audits of the fraud detection system."
                ]
                
                for rec in recommendations:
                    story.append(Paragraph(rec, self.styles['Body']))
                
                story.append(Spacer(1, 12))
            
            # Add footer
            footer_text = f"Generated by Fraud Detection System on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
            story.append(Paragraph(footer_text, self.styles['Footer']))
            
            # Build PDF
            doc.build(story)
            
            logger.info(f"Executive summary report generated: {filepath}")
            return filepath
            
        except Exception as e:
            logger.error(f"Error generating executive summary: {str(e)}")
            raise
    
    def generate_detailed_fraud_analysis(self, df, risk_scores, top_fraud, explanations, 
                                        include_charts=True, include_explanations=True, 
                                        include_recommendations=True):
        """
        Generate detailed fraud analysis report
        
        Args:
            df (DataFrame): Transaction data
            risk_scores (DataFrame): Risk scores
            top_fraud (DataFrame): Top fraudulent transactions
            explanations (dict): Transaction explanations
            include_charts (bool): Whether to include charts
            include_explanations (bool): Whether to include explanations
            include_recommendations (bool): Whether to include recommendations
            
        Returns:
            str: Path to generated PDF
        """
        try:
            # Create filename
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"detailed_fraud_analysis_{timestamp}.pdf"
            filepath = os.path.join(self.output_dir, filename)
            
            # Create document
            doc = SimpleDocTemplate(filepath, pagesize=A4)
            story = []
            
            # Add title
            story.append(Paragraph("Detailed Fraud Analysis Report", self.styles['CustomTitle']))
            story.append(Spacer(1, 12))
            
            # Add subtitle with date
            report_date = datetime.now().strftime("%B %d, %Y")
            story.append(Paragraph(f"Report Date: {report_date}", self.styles['CustomSubtitle']))
            story.append(PageBreak())
            
            # Add table of contents
            story.append(Paragraph("Table of Contents", self.styles['SectionHeading']))
            
            toc_items = [
                "1. Executive Summary",
                "2. Methodology",
                "3. Analysis Results",
                "4. Detailed Transaction Analysis",
                "5. Risk Factors",
                "6. Recommendations",
                "7. Appendix"
            ]
            
            for item in toc_items:
                story.append(Paragraph(item, self.styles['Body']))
            
            story.append(PageBreak())
            
            # Add executive summary
            story.append(Paragraph("1. Executive Summary", self.styles['SectionHeading']))
            
            # Calculate statistics
            total_transactions = len(df)
            fraud_transactions = risk_scores['is_fraud'].sum()
            fraud_percentage = (fraud_transactions / total_transactions) * 100
            
            summary_text = f"""
            This report presents a detailed analysis of fraudulent transactions detected by the fraud detection system.
            The analysis identified {fraud_transactions:,} fraudulent transactions out of {total_transactions:,} total transactions,
            representing {fraud_percentage:.2f}% of all transactions. The total value of fraudulent transactions 
            amounts to ${df.loc[risk_scores['is_fraud'], 'amount'].sum():,.2f} if amount data is available.
            """
            
            story.append(Paragraph(summary_text, self.styles['Body']))
            story.append(Spacer(1, 12))
            
            # Add methodology
            story.append(Paragraph("2. Methodology", self.styles['SectionHeading']))
            
            methodology_text = """
            The fraud detection system employs a multi-layered approach combining unsupervised learning, 
            supervised learning, and rule-based methods to identify potentially fraudulent transactions. 
            The system analyzes various features including transaction amounts, frequency patterns, 
            geographic locations, temporal patterns, and behavioral characteristics to assign risk scores 
            to each transaction.
            """
            
            story.append(Paragraph(methodology_text, self.styles['Body']))
            story.append(Spacer(1, 12))
            
            # Add analysis results
            story.append(Paragraph("3. Analysis Results", self.styles['SectionHeading']))
            
            # Add risk distribution chart if requested
            if include_charts:
                story.append(Paragraph("3.1 Risk Distribution", self.styles['SubsectionHeading']))
                
                # Create risk distribution chart
                plt.figure(figsize=(8, 6))
                sns.histplot(risk_scores['risk_score'], bins=50, kde=True)
                plt.title('Distribution of Risk Scores')
                plt.xlabel('Risk Score')
                plt.ylabel('Frequency')
                
                # Convert plot to image
                img_buffer = io.BytesIO()
                plt.savefig(img_buffer, format='png', dpi=300, bbox_inches='tight')
                img_buffer.seek(0)
                
                # Add image to report
                risk_dist_img = Image(img_buffer, width=6*inch, height=4*inch)
                story.append(risk_dist_img)
                story.append(Spacer(1, 12))
                
                plt.close()
            
            # Add fraud by time chart if requested
            if include_charts and 'timestamp' in df.columns:
                story.append(Paragraph("3.2 Fraud by Time", self.styles['SubsectionHeading']))
                
                # Create fraud by time chart
                df_with_risk = df.copy()
                df_with_risk['is_fraud'] = risk_scores['is_fraud']
                df_with_risk['timestamp'] = pd.to_datetime(df_with_risk['timestamp'])
                df_with_risk['date'] = df_with_risk['timestamp'].dt.date
                
                fraud_by_date = df_with_risk.groupby('date')['is_fraud'].agg(['sum', 'count']).reset_index()
                fraud_by_date['fraud_rate'] = fraud_by_date['sum'] / fraud_by_date['count']
                
                plt.figure(figsize=(10, 6))
                plt.subplot(2, 1, 1)
                plt.plot(fraud_by_date['date'], fraud_by_date['sum'], marker='o')
                plt.title('Fraud Count by Date')
                plt.ylabel('Count')
                plt.xticks(rotation=45)
                
                plt.subplot(2, 1, 2)
                plt.plot(fraud_by_date['date'], fraud_by_date['fraud_rate'], marker='o', color='red')
                plt.title('Fraud Rate by Date')
                plt.ylabel('Rate')
                plt.xlabel('Date')
                plt.xticks(rotation=45)
                
                plt.tight_layout()
                
                # Convert plot to image
                img_buffer = io.BytesIO()
                plt.savefig(img_buffer, format='png', dpi=300, bbox_inches='tight')
                img_buffer.seek(0)
                
                # Add image to report
                fraud_time_img = Image(img_buffer, width=6*inch, height=5*inch)
                story.append(fraud_time_img)
                story.append(Spacer(1, 12))
                
                plt.close()
            
            story.append(PageBreak())
            
            # Add detailed transaction analysis
            story.append(Paragraph("4. Detailed Transaction Analysis", self.styles['SectionHeading']))
            
            # Add top fraudulent transactions
            story.append(Paragraph("4.1 Top Fraudulent Transactions", self.styles['SubsectionHeading']))
            
            # Create table for top fraud transactions
            if len(top_fraud) > 0:
                # Prepare data
                fraud_data = []
                fraud_data.append(['Transaction ID', 'Amount', 'Risk Score', 'Date', 'Risk Level'])
                
                for _, row in top_fraud.head(20).iterrows():
                    transaction_id = row.get('transaction_id', 'N/A')
                    amount = f"${row.get('amount', 0):,.2f}"
                    risk_score = f"{row.get('risk_score', 0):.3f}"
                    date = row.get('timestamp', 'N/A')
                    risk_level = row.get('risk_level', 'N/A')
                    
                    if isinstance(date, pd.Timestamp):
                        date = date.strftime('%Y-%m-%d')
                    
                    fraud_data.append([transaction_id, amount, risk_score, date, risk_level])
                
                fraud_table = Table(fraud_data, colWidths=[1.2*inch, 0.8*inch, 0.8*inch, 1*inch, 0.8*inch])
                fraud_table.setStyle(TableStyle([
                    ('BACKGROUND', (0, 0), (-1, 0), colors.darkblue),
                    ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),
                    ('ALIGN', (0, 0), (-1, -1), 'CENTER'),
                    ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),
                    ('FONTSIZE', (0, 0), (-1, 0), 8),
                    ('BOTTOMPADDING', (0, 0), (-1, 0), 12),
                    ('BACKGROUND', (0, 1), (-1, -1), colors.beige),
                    ('GRID', (0, 0), (-1, -1), 1, colors.black)
                ]))
                
                story.append(fraud_table)
                story.append(Spacer(1, 12))
            
            # Add detailed explanations if requested
            if include_explanations and explanations:
                story.append(Paragraph("4.2 Transaction Explanations", self.styles['SubsectionHeading']))
                
                # Add explanations for top 5 fraudulent transactions
                for i, (idx, explanation) in enumerate(list(explanations.items())[:5]):
                    story.append(Paragraph(f"Transaction {i+1}: {explanation.get('transaction_id', idx)}", 
                                          self.styles['SubsectionHeading']))
                    
                    # Add explanation text
                    explanation_text = explanation.get('text_explanation', 'No explanation available.')
                    story.append(Paragraph(explanation_text, self.styles['Body']))
                    
                    # Add top factors
                    if explanation.get('top_factors'):
                        story.append(Paragraph("Top Contributing Factors:", self.styles['Body']))
                        for factor, contribution in explanation['top_factors'][:3]:
                            story.append(Paragraph(f"- {factor}: {contribution:.3f}", self.styles['Body']))
                    
                    # Add rule violations
                    if explanation.get('rule_violations'):
                        story.append(Paragraph("Rule Violations:", self.styles['Body']))
                        for rule in explanation['rule_violations']:
                            story.append(Paragraph(f"- {rule}", self.styles['Body']))
                    
                    story.append(Spacer(1, 12))
            
            story.append(PageBreak())
            
            # Add risk factors
            story.append(Paragraph("5. Risk Factors", self.styles['SectionHeading']))
            
            # Add common risk factors
            risk_factors = [
                "1. High Transaction Amount: Transactions significantly above average for the sender or receiver.",
                "2. Unusual Geographic Patterns: Transactions from or to high-risk locations.",
                "3. Temporal Anomalies: Transactions at unusual times or in rapid succession.",
                "4. Behavioral Deviations: Transactions that deviate from established patterns.",
                "5. Network Anomalies: Transactions involving suspicious networks of entities."
            ]
            
            for factor in risk_factors:
                story.append(Paragraph(factor, self.styles['Body']))
            
            story.append(Spacer(1, 12))
            
            # Add recommendations
            story.append(Paragraph("6. Recommendations", self.styles['SectionHeading']))
            
            recommendations = [
                "1. Immediate Actions:",
                "   - Block all high-risk transactions (risk score > 0.9).",
                "   - Contact customers associated with high-risk transactions for verification.",
                "   - Flag accounts involved in multiple suspicious transactions for review.",
                "",
                "2. System Improvements:",
                "   - Update fraud detection rules based on recent fraud patterns.",
                "   - Implement additional verification steps for high-value transactions.",
                "   - Enhance monitoring of cross-border transactions.",
                "",
                "3. Process Enhancements:",
                "   - Conduct regular audits of the fraud detection system.",
                "   - Provide training to staff on identifying fraud indicators.",
                "   - Establish clear procedures for handling suspected fraud.",
                "",
                "4. Long-term Strategies:",
                "   - Develop machine learning models to adapt to evolving fraud patterns.",
                "   - Implement real-time fraud detection capabilities.",
                "   - Collaborate with industry partners to share fraud intelligence."
            ]
            
            for rec in recommendations:
                story.append(Paragraph(rec, self.styles['Body']))
            
            story.append(Spacer(1, 12))
            
            # Add appendix
            story.append(Paragraph("7. Appendix", self.styles['SectionHeading']))
            
            # Add methodology details
            story.append(Paragraph("7.1 Methodology Details", self.styles['SubsectionHeading']))
            
            methodology_details = """
            The fraud detection system utilizes a combination of the following techniques:
            
            - Unsupervised Learning: Isolation Forest, Local Outlier Factor, Autoencoders
            - Supervised Learning: Random Forest, XGBoost, Neural Networks
            - Rule-based Detection: Configurable rules for known fraud patterns
            - Feature Engineering: Statistical, graph-based, NLP, and time-series features
            
            Each transaction is assigned a risk score between 0 and 1, with higher scores indicating 
            greater likelihood of fraud. Transactions with scores above the threshold (typically 0.5) 
            are flagged for review.
            """
            
            story.append(Paragraph(methodology_details, self.styles['Body']))
            story.append(Spacer(1, 12))
            
            # Add footer
            footer_text = f"Generated by Fraud Detection System on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
            story.append(Paragraph(footer_text, self.styles['Footer']))
            
            # Build PDF
            doc.build(story)
            
            logger.info(f"Detailed fraud analysis report generated: {filepath}")
            return filepath
            
        except Exception as e:
            logger.error(f"Error generating detailed fraud analysis: {str(e)}")
            raise
    
    def generate_technical_report(self, df, risk_scores, model_results, include_charts=True):
        """
        Generate technical report
        
        Args:
            df (DataFrame): Transaction data
            risk_scores (DataFrame): Risk scores
            model_results (dict): Model results
            include_charts (bool): Whether to include charts
            
        Returns:
            str: Path to generated PDF
        """
        try:
            # Create filename
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"technical_report_{timestamp}.pdf"
            filepath = os.path.join(self.output_dir, filename)
            
            # Create document
            doc = SimpleDocTemplate(filepath, pagesize=A4)
            story = []
            
            # Add title
            story.append(Paragraph("Fraud Detection Technical Report", self.styles['CustomTitle']))
            story.append(Spacer(1, 12))
            
            # Add subtitle with date
            report_date = datetime.now().strftime("%B %d, %Y")
            story.append(Paragraph(f"Report Date: {report_date}", self.styles['CustomSubtitle']))
            story.append(PageBreak())
            
            # Add system overview
            story.append(Paragraph("1. System Overview", self.styles['SectionHeading']))
            
            overview_text = """
            The fraud detection system is designed to identify potentially fraudulent transactions 
            using a combination of machine learning techniques and rule-based methods. The system 
            processes transaction data in real-time, extracting various features and applying multiple 
            detection algorithms to generate risk scores for each transaction.
            """
            
            story.append(Paragraph(overview_text, self.styles['Body']))
            story.append(Spacer(1, 12))
            
            # Add data processing
            story.append(Paragraph("2. Data Processing", self.styles['SectionHeading']))
            
            data_processing_text = """
            The system processes transaction data through the following stages:
            
            1. Data Ingestion: Transactions are loaded from various sources (CSV, Excel, databases).
            2. Data Preprocessing: Missing values are handled, data types are converted, and basic 
               validation is performed.
            3. Feature Engineering: Statistical, graph-based, NLP, and time-series features are extracted.
            4. Model Application: Multiple models are applied to generate risk scores.
            5. Risk Aggregation: Scores from different models are combined to produce a final risk score.
            """
            
            story.append(Paragraph(data_processing_text, self.styles['Body']))
            story.append(Spacer(1, 12))
            
            # Add model performance
            story.append(Paragraph("3. Model Performance", self.styles['SectionHeading']))
            
            # Add unsupervised model performance
            if 'unsupervised' in model_results:
                story.append(Paragraph("3.1 Unsupervised Models", self.styles['SubsectionHeading']))
                
                unsupervised_text = f"""
                The system employs {len(model_results['unsupervised'])} unsupervised learning models:
                
                """
                
                for model_name in model_results['unsupervised']:
                    unsupervised_text += f"- {model_name}\n"
                
                story.append(Paragraph(unsupervised_text, self.styles['Body']))
                story.append(Spacer(1, 6))
            
            # Add supervised model performance
            if 'supervised' in model_results:
                story.append(Paragraph("3.2 Supervised Models", self.styles['SubsectionHeading']))
                
                supervised_text = f"""
                The system employs {len(model_results['supervised'])} supervised learning models:
                
                """
                
                for model_name in model_results['supervised']:
                    if 'performance' in model_results['supervised'][model_name]:
                        perf = model_results['supervised'][model_name]['performance']
                        auc = perf.get('roc_auc', 0)
                        supervised_text += f"- {model_name}: AUC = {auc:.3f}\n"
                    else:
                        supervised_text += f"- {model_name}\n"
                
                story.append(Paragraph(supervised_text, self.styles['Body']))
                story.append(Spacer(1, 6))
            
            # Add rule-based model performance
            if 'rule' in model_results:
                story.append(Paragraph("3.3 Rule-based Models", self.styles['SubsectionHeading']))
                
                rule_text = f"""
                The system employs {len(model_results['rule'].get('rules', {}))} rule-based detectors:
                
                """
                
                for rule_name in model_results['rule'].get('rules', {}):
                    rule_text += f"- {rule_name}\n"
                
                story.append(Paragraph(rule_text, self.styles['Body']))
                story.append(Spacer(1, 6))
            
            # Add feature importance if available
            if 'supervised' in model_results:
                story.append(Paragraph("3.4 Feature Importance", self.styles['SubsectionHeading']))
                
                # Get feature importance from Random Forest if available
                if 'random_forest' in model_results['supervised']:
                    rf_data = model_results['supervised']['random_forest']
                    if 'feature_importance' in rf_data:
                        importance_df = rf_data['feature_importance']
                        
                        # Create table for top features
                        feature_data = []
                        feature_data.append(['Feature', 'Importance'])
                        
                        for _, row in importance_df.head(10).iterrows():
                            feature_data.append([row['feature'], f"{row['importance']:.3f}"])
                        
                        feature_table = Table(feature_data, colWidths=[3*inch, 1.5*inch])
                        feature_table.setStyle(TableStyle([
                            ('BACKGROUND', (0, 0), (-1, 0), colors.darkblue),
                            ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),
                            ('ALIGN', (0, 0), (-1, -1), 'CENTER'),
                            ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),
                            ('FONTSIZE', (0, 0), (-1, 0), 10),
                            ('BOTTOMPADDING', (0, 0), (-1, 0), 12),
                            ('BACKGROUND', (0, 1), (-1, -1), colors.beige),
                            ('GRID', (0, 0), (-1, -1), 1, colors.black)
                        ]))
                        
                        story.append(feature_table)
                        story.append(Spacer(1, 6))
            
            # Add system architecture
            story.append(Paragraph("4. System Architecture", self.styles['SectionHeading']))
            
            architecture_text = """
            The fraud detection system is built with a modular architecture consisting of the following components:
            
            1. Data Ingestion Layer: Handles data loading from various sources.
            2. Feature Engineering Layer: Extracts features for fraud detection.
            3. Model Layer: Applies various detection algorithms.
            4. Risk Aggregation Layer: Combines model outputs.
            5. Reporting Layer: Generates reports and visualizations.
            6. API Layer: Provides interfaces for external systems.
            """
            
            story.append(Paragraph(architecture_text, self.styles['Body']))
            story.append(Spacer(1, 12))
            
            # Add performance metrics
            story.append(Paragraph("5. Performance Metrics", self.styles['SectionHeading']))
            
            # Calculate performance metrics
            total_transactions = len(df)
            fraud_transactions = risk_scores['is_fraud'].sum()
            fraud_percentage = (fraud_transactions / total_transactions) * 100
            
            metrics_data = [
                ['Metric', 'Value'],
                ['Total Transactions', f"{total_transactions:,}"],
                ['Fraudulent Transactions', f"{fraud_transactions:,}"],
                ['Fraud Percentage', f"{fraud_percentage:.2f}%"],
                ['Processing Time', f"{len(df) * 0.001:.2f} seconds (estimated)"]
            ]
            
            metrics_table = Table(metrics_data, colWidths=[3*inch, 2*inch])
            metrics_table.setStyle(TableStyle([
                ('BACKGROUND', (0, 0), (-1, 0), colors.darkblue),
                ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),
                ('ALIGN', (0, 0), (-1, -1), 'CENTER'),
                ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),
                ('FONTSIZE', (0, 0), (-1, 0), 12),
                ('BOTTOMPADDING', (0, 0), (-1, 0), 12),
                ('BACKGROUND', (0, 1), (-1, -1), colors.beige),
                ('GRID', (0, 0), (-1, -1), 1, colors.black)
            ]))
            
            story.append(metrics_table)
            story.append(Spacer(1, 12))
            
            # Add footer
            footer_text = f"Generated by Fraud Detection System on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
            story.append(Paragraph(footer_text, self.styles['Footer']))
            
            # Build PDF
            doc.build(story)
            
            logger.info(f"Technical report generated: {filepath}")
            return filepath
            
        except Exception as e:
            logger.error(f"Error generating technical report: {str(e)}")
            raise
    
    def generate_custom_report(self, df, risk_scores, top_fraud, explanations, model_results, 
                             include_charts=True, include_explanations=True, 
                             include_recommendations=True):
        """
        Generate custom report with user-specified sections
        
        Args:
            df (DataFrame): Transaction data
            risk_scores (DataFrame): Risk scores
            top_fraud (DataFrame): Top fraudulent transactions
            explanations (dict): Transaction explanations
            model_results (dict): Model results
            include_charts (bool): Whether to include charts
            include_explanations (bool): Whether to include explanations
            include_recommendations (bool): Whether to include recommendations
            
        Returns:
            str: Path to generated PDF
        """
        try:
            # Create filename
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"custom_report_{timestamp}.pdf"
            filepath = os.path.join(self.output_dir, filename)
            
            # Create document
            doc = SimpleDocTemplate(filepath, pagesize=A4)
            story = []
            
            # Add title
            story.append(Paragraph("Custom Fraud Detection Report", self.styles['CustomTitle']))
            story.append(Spacer(1, 12))
            
            # Add subtitle with date
            report_date = datetime.now().strftime("%B %d, %Y")
            story.append(Paragraph(f"Report Date: {report_date}", self.styles['CustomSubtitle']))
            story.append(PageBreak())
            
            # Add summary statistics
            story.append(Paragraph("Summary Statistics", self.styles['SectionHeading']))
            
            # Calculate statistics
            total_transactions = len(df)
            fraud_transactions = risk_scores['is_fraud'].sum()
            fraud_percentage = (fraud_transactions / total_transactions) * 100
            
            stats_text = f"""
            Total Transactions: {total_transactions:,}
            Fraudulent Transactions: {fraud_transactions:,}
            Fraud Percentage: {fraud_percentage:.2f}%
            """
            
            story.append(Paragraph(stats_text, self.styles['Body']))
            story.append(Spacer(1, 12))
            
            # Add risk distribution chart if requested
            if include_charts:
                story.append(Paragraph("Risk Distribution", self.styles['SectionHeading']))
                
                # Create risk distribution chart
                plt.figure(figsize=(8, 6))
                sns.histplot(risk_scores['risk_score'], bins=50, kde=True)
                plt.title('Distribution of Risk Scores')
                plt.xlabel('Risk Score')
                plt.ylabel('Frequency')
                
                # Convert plot to image
                img_buffer = io.BytesIO()
                plt.savefig(img_buffer, format='png', dpi=300, bbox_inches='tight')
                img_buffer.seek(0)
                
                # Add image to report
                risk_dist_img = Image(img_buffer, width=6*inch, height=4*inch)
                story.append(risk_dist_img)
                story.append(Spacer(1, 12))
                
                plt.close()
            
            # Add top fraudulent transactions
            story.append(Paragraph("Top Fraudulent Transactions", self.styles['SectionHeading']))
            
            # Create table for top fraud transactions
            if len(top_fraud) > 0:
                # Prepare data
                fraud_data = []
                fraud_data.append(['Transaction ID', 'Amount', 'Risk Score'])
                
                for _, row in top_fraud.head(10).iterrows():
                    transaction_id = row.get('transaction_id', 'N/A')
                    amount = f"${row.get('amount', 0):,.2f}"
                    risk_score = f"{row.get('risk_score', 0):.3f}"
                    
                    fraud_data.append([transaction_id, amount, risk_score])
                
                fraud_table = Table(fraud_data, colWidths=[2*inch, 1.5*inch, 1.5*inch])
                fraud_table.setStyle(TableStyle([
                    ('BACKGROUND', (0, 0), (-1, 0), colors.darkblue),
                    ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),
                    ('ALIGN', (0, 0), (-1, -1), 'CENTER'),
                    ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),
                    ('FONTSIZE', (0, 0), (-1, 0), 12),
                    ('BOTTOMPADDING', (0, 0), (-1, 0), 12),
                    ('BACKGROUND', (0, 1), (-1, -1), colors.beige),
                    ('GRID', (0, 0), (-1, -1), 1, colors.black)
                ]))
                
                story.append(fraud_table)
                story.append(Spacer(1, 12))
            
            # Add detailed explanations if requested
            if include_explanations and explanations:
                story.append(Paragraph("Transaction Explanations", self.styles['SectionHeading']))
                
                # Add explanations for top 3 fraudulent transactions
                for i, (idx, explanation) in enumerate(list(explanations.items())[:3]):
                    story.append(Paragraph(f"Transaction {i+1}", self.styles['SubsectionHeading']))
                    
                    # Add explanation text
                    explanation_text = explanation.get('text_explanation', 'No explanation available.')
                    story.append(Paragraph(explanation_text, self.styles['Body']))
                    story.append(Spacer(1, 6))
            
            # Add recommendations if requested
            if include_recommendations:
                story.append(Paragraph("Recommendations", self.styles['SectionHeading']))
                
                recommendations = [
                    "1. Review all high-risk transactions immediately.",
                    "2. Implement additional verification for large transactions.",
                    "3. Monitor patterns in fraudulent activity.",
                    "4. Update detection rules regularly.",
                    "5. Conduct periodic system audits."
                ]
                
                for rec in recommendations:
                    story.append(Paragraph(rec, self.styles['Body']))
                
                story.append(Spacer(1, 12))
            
            # Add footer
            footer_text = f"Generated by Fraud Detection System on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
            story.append(Paragraph(footer_text, self.styles['Footer']))
            
            # Build PDF
            doc.build(story)
            
            logger.info(f"Custom report generated: {filepath}")
            return filepath
            
        except Exception as e:
            logger.error(f"Error generating custom report: {str(e)}")
            raise

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\reporting\__init__.py ===

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\reporting\.ipynb_checkpoints\pdf_generator-checkpoint.py ===
"""
PDF Generator Module
Implements PDF report generation for fraud detection results
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import os
import io
import base64
from reportlab.lib.pagesizes import letter, A4
from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Table, TableStyle, Image, PageBreak
from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
from reportlab.lib.units import inch
from reportlab.lib import colors
from reportlab.lib.enums import TA_CENTER, TA_LEFT, TA_RIGHT
from reportlab.graphics.shapes import Drawing
from reportlab.graphics.charts.lineplots import LinePlot
from reportlab.graphics.charts.barcharts import VerticalBarChart
from reportlab.graphics.widgets import markers
import warnings
import logging
from typing import Dict, List, Tuple, Union

warnings.filterwarnings('ignore')
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class PDFGenerator:
    """
    Class for generating PDF reports for fraud detection results
    Implements professional audit report generation
    """
    
    def __init__(self, output_dir='../reports/generated'):
        """
        Initialize PDFGenerator
        
        Args:
            output_dir (str): Output directory for reports
        """
        self.output_dir = output_dir
        self.styles = getSampleStyleSheet()
        self.setup_custom_styles()
        
        # Create output directory if it doesn't exist
        os.makedirs(output_dir, exist_ok=True)
    
    def setup_custom_styles(self):
        """Setup custom styles for the report"""
        # Title style
        self.styles.add(ParagraphStyle(
            name='CustomTitle',
            parent=self.styles['Title'],
            fontSize=24,
            spaceAfter=30,
            alignment=TA_CENTER,
            textColor=colors.darkblue
        ))
        
        # Subtitle style
        self.styles.add(ParagraphStyle(
            name='CustomSubtitle',
            parent=self.styles['Heading1'],
            fontSize=16,
            spaceAfter=12,
            alignment=TA_CENTER,
            textColor=colors.darkblue
        ))
        
        # Section heading style
        self.styles.add(ParagraphStyle(
            name='SectionHeading',
            parent=self.styles['Heading2'],
            fontSize=14,
            spaceAfter=12,
            textColor=colors.darkblue
        ))
        
        # Subsection heading style
        self.styles.add(ParagraphStyle(
            name='SubsectionHeading',
            parent=self.styles['Heading3'],
            fontSize=12,
            spaceAfter=6,
            textColor=colors.darkblue
        ))
        
        # Body style
        self.styles.add(ParagraphStyle(
            name='Body',
            parent=self.styles['Normal'],
            fontSize=10,
            spaceAfter=6,
            alignment=TA_LEFT
        ))
        
        # Footer style
        self.styles.add(ParagraphStyle(
            name='Footer',
            parent=self.styles['Normal'],
            fontSize=8,
            alignment=TA_CENTER,
            textColor=colors.grey
        ))
    
    def generate_executive_summary(self, df, risk_scores, top_fraud, include_charts=True, include_recommendations=True):
        """
        Generate executive summary report
        
        Args:
            df (DataFrame): Transaction data
            risk_scores (DataFrame): Risk scores
            top_fraud (DataFrame): Top fraudulent transactions
            include_charts (bool): Whether to include charts
            include_recommendations (bool): Whether to include recommendations
            
        Returns:
            str: Path to generated PDF
        """
        try:
            # Create filename
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"executive_summary_{timestamp}.pdf"
            filepath = os.path.join(self.output_dir, filename)
            
            # Create document
            doc = SimpleDocTemplate(filepath, pagesize=A4)
            story = []
            
            # Add title
            story.append(Paragraph("Fraud Detection Executive Summary", self.styles['CustomTitle']))
            story.append(Spacer(1, 12))
            
            # Add subtitle with date
            report_date = datetime.now().strftime("%B %d, %Y")
            story.append(Paragraph(f"Report Date: {report_date}", self.styles['CustomSubtitle']))
            story.append(PageBreak())
            
            # Add summary statistics
            story.append(Paragraph("Summary Statistics", self.styles['SectionHeading']))
            
            # Calculate statistics
            total_transactions = len(df)
            fraud_transactions = risk_scores['is_fraud'].sum()
            fraud_percentage = (fraud_transactions / total_transactions) * 100
            total_amount = df['amount'].sum() if 'amount' in df.columns else 0
            fraud_amount = df.loc[risk_scores['is_fraud'], 'amount'].sum() if 'amount' in df.columns else 0
            
            # Create statistics table
            stats_data = [
                ['Metric', 'Value'],
                ['Total Transactions', f"{total_transactions:,}"],
                ['Fraudulent Transactions', f"{fraud_transactions:,}"],
                ['Fraud Percentage', f"{fraud_percentage:.2f}%"],
                ['Total Amount', f"${total_amount:,.2f}"],
                ['Fraud Amount', f"${fraud_amount:,.2f}"]
            ]
            
            stats_table = Table(stats_data, colWidths=[3*inch, 2*inch])
            stats_table.setStyle(TableStyle([
                ('BACKGROUND', (0, 0), (-1, 0), colors.darkblue),
                ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),
                ('ALIGN', (0, 0), (-1, -1), 'CENTER'),
                ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),
                ('FONTSIZE', (0, 0), (-1, 0), 12),
                ('BOTTOMPADDING', (0, 0), (-1, 0), 12),
                ('BACKGROUND', (0, 1), (-1, -1), colors.beige),
                ('GRID', (0, 0), (-1, -1), 1, colors.black)
            ]))
            
            story.append(stats_table)
            story.append(Spacer(1, 12))
            
            # Add risk distribution chart if requested
            if include_charts:
                story.append(Paragraph("Risk Distribution", self.styles['SectionHeading']))
                
                # Create risk distribution chart
                plt.figure(figsize=(8, 6))
                sns.histplot(risk_scores['risk_score'], bins=50, kde=True)
                plt.title('Distribution of Risk Scores')
                plt.xlabel('Risk Score')
                plt.ylabel('Frequency')
                
                # Convert plot to image
                img_buffer = io.BytesIO()
                plt.savefig(img_buffer, format='png', dpi=300, bbox_inches='tight')
                img_buffer.seek(0)
                
                # Add image to report
                risk_dist_img = Image(img_buffer, width=6*inch, height=4*inch)
                story.append(risk_dist_img)
                story.append(Spacer(1, 12))
                
                plt.close()
            
            # Add top fraudulent transactions
            story.append(Paragraph("Top Fraudulent Transactions", self.styles['SectionHeading']))
            
            # Create table for top fraud transactions
            if len(top_fraud) > 0:
                # Prepare data
                fraud_data = []
                fraud_data.append(['Transaction ID', 'Amount', 'Risk Score', 'Date'])
                
                for _, row in top_fraud.head(10).iterrows():
                    transaction_id = row.get('transaction_id', 'N/A')
                    amount = f"${row.get('amount', 0):,.2f}"
                    risk_score = f"{row.get('risk_score', 0):.3f}"
                    date = row.get('timestamp', 'N/A')
                    
                    if isinstance(date, pd.Timestamp):
                        date = date.strftime('%Y-%m-%d')
                    
                    fraud_data.append([transaction_id, amount, risk_score, date])
                
                fraud_table = Table(fraud_data, colWidths=[1.5*inch, 1*inch, 1*inch, 1.5*inch])
                fraud_table.setStyle(TableStyle([
                    ('BACKGROUND', (0, 0), (-1, 0), colors.darkblue),
                    ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),
                    ('ALIGN', (0, 0), (-1, -1), 'CENTER'),
                    ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),
                    ('FONTSIZE', (0, 0), (-1, 0), 10),
                    ('BOTTOMPADDING', (0, 0), (-1, 0), 12),
                    ('BACKGROUND', (0, 1), (-1, -1), colors.beige),
                    ('GRID', (0, 0), (-1, -1), 1, colors.black)
                ]))
                
                story.append(fraud_table)
                story.append(Spacer(1, 12))
            
            # Add recommendations if requested
            if include_recommendations:
                story.append(Paragraph("Recommendations", self.styles['SectionHeading']))
                
                recommendations = [
                    "1. Immediately review all high-risk transactions (risk score > 0.8).",
                    "2. Implement additional verification steps for transactions over $10,000.",
                    "3. Monitor patterns in fraudulent transactions to identify potential fraud rings.",
                    "4. Update fraud detection rules based on recent fraud patterns.",
                    "5. Conduct regular audits of the fraud detection system."
                ]
                
                for rec in recommendations:
                    story.append(Paragraph(rec, self.styles['Body']))
                
                story.append(Spacer(1, 12))
            
            # Add footer
            footer_text = f"Generated by Fraud Detection System on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
            story.append(Paragraph(footer_text, self.styles['Footer']))
            
            # Build PDF
            doc.build(story)
            
            logger.info(f"Executive summary report generated: {filepath}")
            return filepath
            
        except Exception as e:
            logger.error(f"Error generating executive summary: {str(e)}")
            raise
    
    def generate_detailed_fraud_analysis(self, df, risk_scores, top_fraud, explanations, 
                                        include_charts=True, include_explanations=True, 
                                        include_recommendations=True):
        """
        Generate detailed fraud analysis report
        
        Args:
            df (DataFrame): Transaction data
            risk_scores (DataFrame): Risk scores
            top_fraud (DataFrame): Top fraudulent transactions
            explanations (dict): Transaction explanations
            include_charts (bool): Whether to include charts
            include_explanations (bool): Whether to include explanations
            include_recommendations (bool): Whether to include recommendations
            
        Returns:
            str: Path to generated PDF
        """
        try:
            # Create filename
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"detailed_fraud_analysis_{timestamp}.pdf"
            filepath = os.path.join(self.output_dir, filename)
            
            # Create document
            doc = SimpleDocTemplate(filepath, pagesize=A4)
            story = []
            
            # Add title
            story.append(Paragraph("Detailed Fraud Analysis Report", self.styles['CustomTitle']))
            story.append(Spacer(1, 12))
            
            # Add subtitle with date
            report_date = datetime.now().strftime("%B %d, %Y")
            story.append(Paragraph(f"Report Date: {report_date}", self.styles['CustomSubtitle']))
            story.append(PageBreak())
            
            # Add table of contents
            story.append(Paragraph("Table of Contents", self.styles['SectionHeading']))
            
            toc_items = [
                "1. Executive Summary",
                "2. Methodology",
                "3. Analysis Results",
                "4. Detailed Transaction Analysis",
                "5. Risk Factors",
                "6. Recommendations",
                "7. Appendix"
            ]
            
            for item in toc_items:
                story.append(Paragraph(item, self.styles['Body']))
            
            story.append(PageBreak())
            
            # Add executive summary
            story.append(Paragraph("1. Executive Summary", self.styles['SectionHeading']))
            
            # Calculate statistics
            total_transactions = len(df)
            fraud_transactions = risk_scores['is_fraud'].sum()
            fraud_percentage = (fraud_transactions / total_transactions) * 100
            
            summary_text = f"""
            This report presents a detailed analysis of fraudulent transactions detected by the fraud detection system.
            The analysis identified {fraud_transactions:,} fraudulent transactions out of {total_transactions:,} total transactions,
            representing {fraud_percentage:.2f}% of all transactions. The total value of fraudulent transactions 
            amounts to ${df.loc[risk_scores['is_fraud'], 'amount'].sum():,.2f} if amount data is available.
            """
            
            story.append(Paragraph(summary_text, self.styles['Body']))
            story.append(Spacer(1, 12))
            
            # Add methodology
            story.append(Paragraph("2. Methodology", self.styles['SectionHeading']))
            
            methodology_text = """
            The fraud detection system employs a multi-layered approach combining unsupervised learning, 
            supervised learning, and rule-based methods to identify potentially fraudulent transactions. 
            The system analyzes various features including transaction amounts, frequency patterns, 
            geographic locations, temporal patterns, and behavioral characteristics to assign risk scores 
            to each transaction.
            """
            
            story.append(Paragraph(methodology_text, self.styles['Body']))
            story.append(Spacer(1, 12))
            
            # Add analysis results
            story.append(Paragraph("3. Analysis Results", self.styles['SectionHeading']))
            
            # Add risk distribution chart if requested
            if include_charts:
                story.append(Paragraph("3.1 Risk Distribution", self.styles['SubsectionHeading']))
                
                # Create risk distribution chart
                plt.figure(figsize=(8, 6))
                sns.histplot(risk_scores['risk_score'], bins=50, kde=True)
                plt.title('Distribution of Risk Scores')
                plt.xlabel('Risk Score')
                plt.ylabel('Frequency')
                
                # Convert plot to image
                img_buffer = io.BytesIO()
                plt.savefig(img_buffer, format='png', dpi=300, bbox_inches='tight')
                img_buffer.seek(0)
                
                # Add image to report
                risk_dist_img = Image(img_buffer, width=6*inch, height=4*inch)
                story.append(risk_dist_img)
                story.append(Spacer(1, 12))
                
                plt.close()
            
            # Add fraud by time chart if requested
            if include_charts and 'timestamp' in df.columns:
                story.append(Paragraph("3.2 Fraud by Time", self.styles['SubsectionHeading']))
                
                # Create fraud by time chart
                df_with_risk = df.copy()
                df_with_risk['is_fraud'] = risk_scores['is_fraud']
                df_with_risk['timestamp'] = pd.to_datetime(df_with_risk['timestamp'])
                df_with_risk['date'] = df_with_risk['timestamp'].dt.date
                
                fraud_by_date = df_with_risk.groupby('date')['is_fraud'].agg(['sum', 'count']).reset_index()
                fraud_by_date['fraud_rate'] = fraud_by_date['sum'] / fraud_by_date['count']
                
                plt.figure(figsize=(10, 6))
                plt.subplot(2, 1, 1)
                plt.plot(fraud_by_date['date'], fraud_by_date['sum'], marker='o')
                plt.title('Fraud Count by Date')
                plt.ylabel('Count')
                plt.xticks(rotation=45)
                
                plt.subplot(2, 1, 2)
                plt.plot(fraud_by_date['date'], fraud_by_date['fraud_rate'], marker='o', color='red')
                plt.title('Fraud Rate by Date')
                plt.ylabel('Rate')
                plt.xlabel('Date')
                plt.xticks(rotation=45)
                
                plt.tight_layout()
                
                # Convert plot to image
                img_buffer = io.BytesIO()
                plt.savefig(img_buffer, format='png', dpi=300, bbox_inches='tight')
                img_buffer.seek(0)
                
                # Add image to report
                fraud_time_img = Image(img_buffer, width=6*inch, height=5*inch)
                story.append(fraud_time_img)
                story.append(Spacer(1, 12))
                
                plt.close()
            
            story.append(PageBreak())
            
            # Add detailed transaction analysis
            story.append(Paragraph("4. Detailed Transaction Analysis", self.styles['SectionHeading']))
            
            # Add top fraudulent transactions
            story.append(Paragraph("4.1 Top Fraudulent Transactions", self.styles['SubsectionHeading']))
            
            # Create table for top fraud transactions
            if len(top_fraud) > 0:
                # Prepare data
                fraud_data = []
                fraud_data.append(['Transaction ID', 'Amount', 'Risk Score', 'Date', 'Risk Level'])
                
                for _, row in top_fraud.head(20).iterrows():
                    transaction_id = row.get('transaction_id', 'N/A')
                    amount = f"${row.get('amount', 0):,.2f}"
                    risk_score = f"{row.get('risk_score', 0):.3f}"
                    date = row.get('timestamp', 'N/A')
                    risk_level = row.get('risk_level', 'N/A')
                    
                    if isinstance(date, pd.Timestamp):
                        date = date.strftime('%Y-%m-%d')
                    
                    fraud_data.append([transaction_id, amount, risk_score, date, risk_level])
                
                fraud_table = Table(fraud_data, colWidths=[1.2*inch, 0.8*inch, 0.8*inch, 1*inch, 0.8*inch])
                fraud_table.setStyle(TableStyle([
                    ('BACKGROUND', (0, 0), (-1, 0), colors.darkblue),
                    ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),
                    ('ALIGN', (0, 0), (-1, -1), 'CENTER'),
                    ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),
                    ('FONTSIZE', (0, 0), (-1, 0), 8),
                    ('BOTTOMPADDING', (0, 0), (-1, 0), 12),
                    ('BACKGROUND', (0, 1), (-1, -1), colors.beige),
                    ('GRID', (0, 0), (-1, -1), 1, colors.black)
                ]))
                
                story.append(fraud_table)
                story.append(Spacer(1, 12))
            
            # Add detailed explanations if requested
            if include_explanations and explanations:
                story.append(Paragraph("4.2 Transaction Explanations", self.styles['SubsectionHeading']))
                
                # Add explanations for top 5 fraudulent transactions
                for i, (idx, explanation) in enumerate(list(explanations.items())[:5]):
                    story.append(Paragraph(f"Transaction {i+1}: {explanation.get('transaction_id', idx)}", 
                                          self.styles['SubsectionHeading']))
                    
                    # Add explanation text
                    explanation_text = explanation.get('text_explanation', 'No explanation available.')
                    story.append(Paragraph(explanation_text, self.styles['Body']))
                    
                    # Add top factors
                    if explanation.get('top_factors'):
                        story.append(Paragraph("Top Contributing Factors:", self.styles['Body']))
                        for factor, contribution in explanation['top_factors'][:3]:
                            story.append(Paragraph(f"- {factor}: {contribution:.3f}", self.styles['Body']))
                    
                    # Add rule violations
                    if explanation.get('rule_violations'):
                        story.append(Paragraph("Rule Violations:", self.styles['Body']))
                        for rule in explanation['rule_violations']:
                            story.append(Paragraph(f"- {rule}", self.styles['Body']))
                    
                    story.append(Spacer(1, 12))
            
            story.append(PageBreak())
            
            # Add risk factors
            story.append(Paragraph("5. Risk Factors", self.styles['SectionHeading']))
            
            # Add common risk factors
            risk_factors = [
                "1. High Transaction Amount: Transactions significantly above average for the sender or receiver.",
                "2. Unusual Geographic Patterns: Transactions from or to high-risk locations.",
                "3. Temporal Anomalies: Transactions at unusual times or in rapid succession.",
                "4. Behavioral Deviations: Transactions that deviate from established patterns.",
                "5. Network Anomalies: Transactions involving suspicious networks of entities."
            ]
            
            for factor in risk_factors:
                story.append(Paragraph(factor, self.styles['Body']))
            
            story.append(Spacer(1, 12))
            
            # Add recommendations
            story.append(Paragraph("6. Recommendations", self.styles['SectionHeading']))
            
            recommendations = [
                "1. Immediate Actions:",
                "   - Block all high-risk transactions (risk score > 0.9).",
                "   - Contact customers associated with high-risk transactions for verification.",
                "   - Flag accounts involved in multiple suspicious transactions for review.",
                "",
                "2. System Improvements:",
                "   - Update fraud detection rules based on recent fraud patterns.",
                "   - Implement additional verification steps for high-value transactions.",
                "   - Enhance monitoring of cross-border transactions.",
                "",
                "3. Process Enhancements:",
                "   - Conduct regular audits of the fraud detection system.",
                "   - Provide training to staff on identifying fraud indicators.",
                "   - Establish clear procedures for handling suspected fraud.",
                "",
                "4. Long-term Strategies:",
                "   - Develop machine learning models to adapt to evolving fraud patterns.",
                "   - Implement real-time fraud detection capabilities.",
                "   - Collaborate with industry partners to share fraud intelligence."
            ]
            
            for rec in recommendations:
                story.append(Paragraph(rec, self.styles['Body']))
            
            story.append(Spacer(1, 12))
            
            # Add appendix
            story.append(Paragraph("7. Appendix", self.styles['SectionHeading']))
            
            # Add methodology details
            story.append(Paragraph("7.1 Methodology Details", self.styles['SubsectionHeading']))
            
            methodology_details = """
            The fraud detection system utilizes a combination of the following techniques:
            
            - Unsupervised Learning: Isolation Forest, Local Outlier Factor, Autoencoders
            - Supervised Learning: Random Forest, XGBoost, Neural Networks
            - Rule-based Detection: Configurable rules for known fraud patterns
            - Feature Engineering: Statistical, graph-based, NLP, and time-series features
            
            Each transaction is assigned a risk score between 0 and 1, with higher scores indicating 
            greater likelihood of fraud. Transactions with scores above the threshold (typically 0.5) 
            are flagged for review.
            """
            
            story.append(Paragraph(methodology_details, self.styles['Body']))
            story.append(Spacer(1, 12))
            
            # Add footer
            footer_text = f"Generated by Fraud Detection System on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
            story.append(Paragraph(footer_text, self.styles['Footer']))
            
            # Build PDF
            doc.build(story)
            
            logger.info(f"Detailed fraud analysis report generated: {filepath}")
            return filepath
            
        except Exception as e:
            logger.error(f"Error generating detailed fraud analysis: {str(e)}")
            raise
    
    def generate_technical_report(self, df, risk_scores, model_results, include_charts=True):
        """
        Generate technical report
        
        Args:
            df (DataFrame): Transaction data
            risk_scores (DataFrame): Risk scores
            model_results (dict): Model results
            include_charts (bool): Whether to include charts
            
        Returns:
            str: Path to generated PDF
        """
        try:
            # Create filename
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"technical_report_{timestamp}.pdf"
            filepath = os.path.join(self.output_dir, filename)
            
            # Create document
            doc = SimpleDocTemplate(filepath, pagesize=A4)
            story = []
            
            # Add title
            story.append(Paragraph("Fraud Detection Technical Report", self.styles['CustomTitle']))
            story.append(Spacer(1, 12))
            
            # Add subtitle with date
            report_date = datetime.now().strftime("%B %d, %Y")
            story.append(Paragraph(f"Report Date: {report_date}", self.styles['CustomSubtitle']))
            story.append(PageBreak())
            
            # Add system overview
            story.append(Paragraph("1. System Overview", self.styles['SectionHeading']))
            
            overview_text = """
            The fraud detection system is designed to identify potentially fraudulent transactions 
            using a combination of machine learning techniques and rule-based methods. The system 
            processes transaction data in real-time, extracting various features and applying multiple 
            detection algorithms to generate risk scores for each transaction.
            """
            
            story.append(Paragraph(overview_text, self.styles['Body']))
            story.append(Spacer(1, 12))
            
            # Add data processing
            story.append(Paragraph("2. Data Processing", self.styles['SectionHeading']))
            
            data_processing_text = """
            The system processes transaction data through the following stages:
            
            1. Data Ingestion: Transactions are loaded from various sources (CSV, Excel, databases).
            2. Data Preprocessing: Missing values are handled, data types are converted, and basic 
               validation is performed.
            3. Feature Engineering: Statistical, graph-based, NLP, and time-series features are extracted.
            4. Model Application: Multiple models are applied to generate risk scores.
            5. Risk Aggregation: Scores from different models are combined to produce a final risk score.
            """
            
            story.append(Paragraph(data_processing_text, self.styles['Body']))
            story.append(Spacer(1, 12))
            
            # Add model performance
            story.append(Paragraph("3. Model Performance", self.styles['SectionHeading']))
            
            # Add unsupervised model performance
            if 'unsupervised' in model_results:
                story.append(Paragraph("3.1 Unsupervised Models", self.styles['SubsectionHeading']))
                
                unsupervised_text = f"""
                The system employs {len(model_results['unsupervised'])} unsupervised learning models:
                
                """
                
                for model_name in model_results['unsupervised']:
                    unsupervised_text += f"- {model_name}\n"
                
                story.append(Paragraph(unsupervised_text, self.styles['Body']))
                story.append(Spacer(1, 6))
            
            # Add supervised model performance
            if 'supervised' in model_results:
                story.append(Paragraph("3.2 Supervised Models", self.styles['SubsectionHeading']))
                
                supervised_text = f"""
                The system employs {len(model_results['supervised'])} supervised learning models:
                
                """
                
                for model_name in model_results['supervised']:
                    if 'performance' in model_results['supervised'][model_name]:
                        perf = model_results['supervised'][model_name]['performance']
                        auc = perf.get('roc_auc', 0)
                        supervised_text += f"- {model_name}: AUC = {auc:.3f}\n"
                    else:
                        supervised_text += f"- {model_name}\n"
                
                story.append(Paragraph(supervised_text, self.styles['Body']))
                story.append(Spacer(1, 6))
            
            # Add rule-based model performance
            if 'rule' in model_results:
                story.append(Paragraph("3.3 Rule-based Models", self.styles['SubsectionHeading']))
                
                rule_text = f"""
                The system employs {len(model_results['rule'].get('rules', {}))} rule-based detectors:
                
                """
                
                for rule_name in model_results['rule'].get('rules', {}):
                    rule_text += f"- {rule_name}\n"
                
                story.append(Paragraph(rule_text, self.styles['Body']))
                story.append(Spacer(1, 6))
            
            # Add feature importance if available
            if 'supervised' in model_results:
                story.append(Paragraph("3.4 Feature Importance", self.styles['SubsectionHeading']))
                
                # Get feature importance from Random Forest if available
                if 'random_forest' in model_results['supervised']:
                    rf_data = model_results['supervised']['random_forest']
                    if 'feature_importance' in rf_data:
                        importance_df = rf_data['feature_importance']
                        
                        # Create table for top features
                        feature_data = []
                        feature_data.append(['Feature', 'Importance'])
                        
                        for _, row in importance_df.head(10).iterrows():
                            feature_data.append([row['feature'], f"{row['importance']:.3f}"])
                        
                        feature_table = Table(feature_data, colWidths=[3*inch, 1.5*inch])
                        feature_table.setStyle(TableStyle([
                            ('BACKGROUND', (0, 0), (-1, 0), colors.darkblue),
                            ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),
                            ('ALIGN', (0, 0), (-1, -1), 'CENTER'),
                            ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),
                            ('FONTSIZE', (0, 0), (-1, 0), 10),
                            ('BOTTOMPADDING', (0, 0), (-1, 0), 12),
                            ('BACKGROUND', (0, 1), (-1, -1), colors.beige),
                            ('GRID', (0, 0), (-1, -1), 1, colors.black)
                        ]))
                        
                        story.append(feature_table)
                        story.append(Spacer(1, 6))
            
            # Add system architecture
            story.append(Paragraph("4. System Architecture", self.styles['SectionHeading']))
            
            architecture_text = """
            The fraud detection system is built with a modular architecture consisting of the following components:
            
            1. Data Ingestion Layer: Handles data loading from various sources.
            2. Feature Engineering Layer: Extracts features for fraud detection.
            3. Model Layer: Applies various detection algorithms.
            4. Risk Aggregation Layer: Combines model outputs.
            5. Reporting Layer: Generates reports and visualizations.
            6. API Layer: Provides interfaces for external systems.
            """
            
            story.append(Paragraph(architecture_text, self.styles['Body']))
            story.append(Spacer(1, 12))
            
            # Add performance metrics
            story.append(Paragraph("5. Performance Metrics", self.styles['SectionHeading']))
            
            # Calculate performance metrics
            total_transactions = len(df)
            fraud_transactions = risk_scores['is_fraud'].sum()
            fraud_percentage = (fraud_transactions / total_transactions) * 100
            
            metrics_data = [
                ['Metric', 'Value'],
                ['Total Transactions', f"{total_transactions:,}"],
                ['Fraudulent Transactions', f"{fraud_transactions:,}"],
                ['Fraud Percentage', f"{fraud_percentage:.2f}%"],
                ['Processing Time', f"{len(df) * 0.001:.2f} seconds (estimated)"]
            ]
            
            metrics_table = Table(metrics_data, colWidths=[3*inch, 2*inch])
            metrics_table.setStyle(TableStyle([
                ('BACKGROUND', (0, 0), (-1, 0), colors.darkblue),
                ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),
                ('ALIGN', (0, 0), (-1, -1), 'CENTER'),
                ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),
                ('FONTSIZE', (0, 0), (-1, 0), 12),
                ('BOTTOMPADDING', (0, 0), (-1, 0), 12),
                ('BACKGROUND', (0, 1), (-1, -1), colors.beige),
                ('GRID', (0, 0), (-1, -1), 1, colors.black)
            ]))
            
            story.append(metrics_table)
            story.append(Spacer(1, 12))
            
            # Add footer
            footer_text = f"Generated by Fraud Detection System on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
            story.append(Paragraph(footer_text, self.styles['Footer']))
            
            # Build PDF
            doc.build(story)
            
            logger.info(f"Technical report generated: {filepath}")
            return filepath
            
        except Exception as e:
            logger.error(f"Error generating technical report: {str(e)}")
            raise
    
    def generate_custom_report(self, df, risk_scores, top_fraud, explanations, model_results, 
                             include_charts=True, include_explanations=True, 
                             include_recommendations=True):
        """
        Generate custom report with user-specified sections
        
        Args:
            df (DataFrame): Transaction data
            risk_scores (DataFrame): Risk scores
            top_fraud (DataFrame): Top fraudulent transactions
            explanations (dict): Transaction explanations
            model_results (dict): Model results
            include_charts (bool): Whether to include charts
            include_explanations (bool): Whether to include explanations
            include_recommendations (bool): Whether to include recommendations
            
        Returns:
            str: Path to generated PDF
        """
        try:
            # Create filename
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"custom_report_{timestamp}.pdf"
            filepath = os.path.join(self.output_dir, filename)
            
            # Create document
            doc = SimpleDocTemplate(filepath, pagesize=A4)
            story = []
            
            # Add title
            story.append(Paragraph("Custom Fraud Detection Report", self.styles['CustomTitle']))
            story.append(Spacer(1, 12))
            
            # Add subtitle with date
            report_date = datetime.now().strftime("%B %d, %Y")
            story.append(Paragraph(f"Report Date: {report_date}", self.styles['CustomSubtitle']))
            story.append(PageBreak())
            
            # Add summary statistics
            story.append(Paragraph("Summary Statistics", self.styles['SectionHeading']))
            
            # Calculate statistics
            total_transactions = len(df)
            fraud_transactions = risk_scores['is_fraud'].sum()
            fraud_percentage = (fraud_transactions / total_transactions) * 100
            
            stats_text = f"""
            Total Transactions: {total_transactions:,}
            Fraudulent Transactions: {fraud_transactions:,}
            Fraud Percentage: {fraud_percentage:.2f}%
            """
            
            story.append(Paragraph(stats_text, self.styles['Body']))
            story.append(Spacer(1, 12))
            
            # Add risk distribution chart if requested
            if include_charts:
                story.append(Paragraph("Risk Distribution", self.styles['SectionHeading']))
                
                # Create risk distribution chart
                plt.figure(figsize=(8, 6))
                sns.histplot(risk_scores['risk_score'], bins=50, kde=True)
                plt.title('Distribution of Risk Scores')
                plt.xlabel('Risk Score')
                plt.ylabel('Frequency')
                
                # Convert plot to image
                img_buffer = io.BytesIO()
                plt.savefig(img_buffer, format='png', dpi=300, bbox_inches='tight')
                img_buffer.seek(0)
                
                # Add image to report
                risk_dist_img = Image(img_buffer, width=6*inch, height=4*inch)
                story.append(risk_dist_img)
                story.append(Spacer(1, 12))
                
                plt.close()
            
            # Add top fraudulent transactions
            story.append(Paragraph("Top Fraudulent Transactions", self.styles['SectionHeading']))
            
            # Create table for top fraud transactions
            if len(top_fraud) > 0:
                # Prepare data
                fraud_data = []
                fraud_data.append(['Transaction ID', 'Amount', 'Risk Score'])
                
                for _, row in top_fraud.head(10).iterrows():
                    transaction_id = row.get('transaction_id', 'N/A')
                    amount = f"${row.get('amount', 0):,.2f}"
                    risk_score = f"{row.get('risk_score', 0):.3f}"
                    
                    fraud_data.append([transaction_id, amount, risk_score])
                
                fraud_table = Table(fraud_data, colWidths=[2*inch, 1.5*inch, 1.5*inch])
                fraud_table.setStyle(TableStyle([
                    ('BACKGROUND', (0, 0), (-1, 0), colors.darkblue),
                    ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),
                    ('ALIGN', (0, 0), (-1, -1), 'CENTER'),
                    ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),
                    ('FONTSIZE', (0, 0), (-1, 0), 12),
                    ('BOTTOMPADDING', (0, 0), (-1, 0), 12),
                    ('BACKGROUND', (0, 1), (-1, -1), colors.beige),
                    ('GRID', (0, 0), (-1, -1), 1, colors.black)
                ]))
                
                story.append(fraud_table)
                story.append(Spacer(1, 12))
            
            # Add detailed explanations if requested
            if include_explanations and explanations:
                story.append(Paragraph("Transaction Explanations", self.styles['SectionHeading']))
                
                # Add explanations for top 3 fraudulent transactions
                for i, (idx, explanation) in enumerate(list(explanations.items())[:3]):
                    story.append(Paragraph(f"Transaction {i+1}", self.styles['SubsectionHeading']))
                    
                    # Add explanation text
                    explanation_text = explanation.get('text_explanation', 'No explanation available.')
                    story.append(Paragraph(explanation_text, self.styles['Body']))
                    story.append(Spacer(1, 6))
            
            # Add recommendations if requested
            if include_recommendations:
                story.append(Paragraph("Recommendations", self.styles['SectionHeading']))
                
                recommendations = [
                    "1. Review all high-risk transactions immediately.",
                    "2. Implement additional verification for large transactions.",
                    "3. Monitor patterns in fraudulent activity.",
                    "4. Update detection rules regularly.",
                    "5. Conduct periodic system audits."
                ]
                
                for rec in recommendations:
                    story.append(Paragraph(rec, self.styles['Body']))
                
                story.append(Spacer(1, 12))
            
            # Add footer
            footer_text = f"Generated by Fraud Detection System on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
            story.append(Paragraph(footer_text, self.styles['Footer']))
            
            # Build PDF
            doc.build(story)
            
            logger.info(f"Custom report generated: {filepath}")
            return filepath
            
        except Exception as e:
            logger.error(f"Error generating custom report: {str(e)}")
            raise

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\reporting\.ipynb_checkpoints\__init__-checkpoint.py ===

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\reporting\__pycache__\pdf_generator.cpython-39.pyc ===
a
    èöhñ´  „                   @   sH  d Z ddlZddlZddlmZ ddlZ	ddl
m
Z
 ddlZddlZddlZddlmZmZ ddlmZmZmZmZmZmZmZ ddlmZmZ ddlmZ ddlmZ dd	l m!Z!m"Z"m#Z# dd
l$m%Z% ddl&m'Z' ddl(m)Z) ddl*m+Z+ ddl,Z,ddl-Z-ddl.m/Z/m0Z0m1Z1m2Z2 e,†3d° e-j4e-j5dç e-†6e7°Z8G ddÑ dÉZ9dS )zS
PDF Generator Module
Implements PDF report generation for fraud detection results
È    N)⁄datetime)⁄letter⁄A4)⁄SimpleDocTemplate⁄	Paragraph⁄Spacer⁄Table⁄
TableStyle⁄Image⁄	PageBreak)⁄getSampleStyleSheet⁄ParagraphStyle)⁄inch)⁄colors)⁄	TA_CENTER⁄TA_LEFT⁄TA_RIGHT)⁄Drawing)⁄LinePlot)⁄VerticalBarChart)⁄markers)⁄Dict⁄List⁄Tuple⁄Union⁄ignore)⁄levelc                   @   sJ   e Zd ZdZdddÑZddÑ Zddd	ÑZdd
dÑZdddÑZdddÑZ	dS )⁄PDFGeneratorzz
    Class for generating PDF reports for fraud detection results
    Implements professional audit report generation
    ˙../reports/generatedc                 C   s(   || _ tÉ | _| †°  tj|ddç dS )z{
        Initialize PDFGenerator
        
        Args:
            output_dir (str): Output directory for reports
        T)⁄exist_okN)⁄
output_dirr   ⁄styles⁄setup_custom_styles⁄os⁄makedirs)⁄selfr    © r&   ˙mC:\Users\Tranquil Abyss\fraud-detection-platform\app\..\src\fraud_detection_engine\reporting\pdf_generator.py⁄__init__&   s    zPDFGenerator.__init__c              
   C   s“   | j †td| j d ddttjdç° | j †td| j d dd	ttjdç° | j †td
| j d dd	tjdç° | j †td| j d d	dtjdç° | j †td| j d ddtdç° | j †td| j d dttjdç° dS )z"Setup custom styles for the report⁄CustomTitle⁄TitleÈ   È   )⁄name⁄parent⁄fontSize⁄
spaceAfter⁄	alignment⁄	textColor⁄CustomSubtitleZHeading1È   È   ⁄SectionHeadingZHeading2È   )r-   r.   r/   r0   r2   ⁄SubsectionHeadingZHeading3È   ⁄Body⁄NormalÈ
   )r-   r.   r/   r0   r1   ⁄FooterÈ   )r-   r.   r/   r1   r2   N)r!   ⁄addr   r   r   ⁄darkbluer   ⁄grey)r%   r&   r&   r'   r"   4   sX    ˙
˙
˚	˚	˚	˚z PDFGenerator.setup_custom_stylesTc           !      C   s§  êz`t †° †d°}d|õ dù}tj†| j|°}t|tdç}	g }
|
†	t
d| jd É° |
†	tddÉ° t †° †d	°}|
†	t
d
|õ ù| jd É° |
†	tÉ ° |
†	t
d| jd É° t|É}|d †° }|| d }d|jv rÊ|d †° nd}d|jv êr|j|d df †° nd}ddgd|dõgd|dõgd|dõdùgdd|dõùgdd|dõùgg}t|dt dt gd ç}|†td!d"d#tjfd$d"d#tjfd%d&d'd(d!d)d*tjfd+d"d*dtjfgÉ° |
†	|° |
†	tddÉ° |êrà|
†	t
d,| jd É° tjd-d.ç tj|d/ d0d1d2ç t†d3° t† d4° t†!d5° t"†#° }tj$|d6d7d8d9ç |†%d° t&|d:t d;t d<ç}|
†	|° |
†	tddÉ° t†'°  |
†	t
d=| jd É° t|Édkêrƒg }|†	g d>¢° |†(d?°†)° D ]p\}}|†*d@dA°}d|†*dd°dõù}|†*d/d°dBõ}|†*dCdA°}t+|t,j-Éêr(|†dD°}|†	||||g° êqÃt|dEt dt dt dEt gd ç}|†td!d"d#tjfd$d"d#tjfd%d&dFd(d!d)d*tjfd+d"d*dtjfgÉ° |
†	|° |
†	tddÉ° |êr|
†	t
dG| jd É° g dH¢}|D ]}|
†	t
|| jdI É° êqÏ|
†	tddÉ° dJt †° †dK°õ ù}|
†	t
|| jdL É° |	†.|
° t/†0dM|õ ù° |W S  t1êyû }  z"t/†2dNt3| Éõ ù° Ç W Y dO} ~ n
dO} ~ 0 0 dOS )Pa∏  
        Generate executive summary report
        
        Args:
            df (DataFrame): Transaction data
            risk_scores (DataFrame): Risk scores
            top_fraud (DataFrame): Top fraudulent transactions
            include_charts (bool): Whether to include charts
            include_recommendations (bool): Whether to include recommendations
            
        Returns:
            str: Path to generated PDF
        ˙%Y%m%d_%H%M%SZexecutive_summary_˙.pdf©Zpagesizez!Fraud Detection Executive Summaryr)   È   r5   ˙	%B %d, %Y˙Report Date: r3   ˙Summary Statisticsr6   ⁄is_fraudÈd   ⁄amountr   ⁄Metric⁄Value˙Total Transactions˙,˙Fraudulent Transactions˙Fraud Percentage˙.2f˙%zTotal Amount˙$˙,.2fzFraud AmountÈ   È   ©⁄	colWidths⁄
BACKGROUND©r   r   ©Èˇˇˇˇr   ⁄	TEXTCOLOR©ZALIGNr[   ©r]   r]   ⁄CENTER©ZFONTNAMEr[   r\   zHelvetica-Bold©⁄FONTSIZEr[   r\   r5   ©ZBOTTOMPADDINGr[   r\   r5   ©r   rE   r`   ⁄GRID˙Risk Distribution©r>   r9   ©⁄figsize⁄
risk_scoreÈ2   T©⁄bins⁄kde˙Distribution of Risk Scores˙
Risk Score⁄	Frequency⁄pngÈ,  ⁄tight©⁄format⁄dpi⁄bbox_inchesr9   È   ©⁄width⁄height˙Top Fraudulent Transactions)˙Transaction ID⁄Amountrr   ⁄Dater<   ⁄transaction_id˙N/A˙.3f⁄	timestamp˙%Y-%m-%dÁ      ¯?©rd   r[   r\   r<   ⁄Recommendations)zD1. Immediately review all high-risk transactions (risk score > 0.8).zI2. Implement additional verification steps for transactions over $10,000.zQ3. Monitor patterns in fraudulent transactions to identify potential fraud rings.z?4. Update fraud detection rules based on recent fraud patterns.z85. Conduct regular audits of the fraud detection system.r:   ˙'Generated by Fraud Detection System on ˙%Y-%m-%d %H:%M:%Sr=   z$Executive summary report generated: z$Error generating executive summary: N)4r   ⁄now⁄strftimer#   ⁄path⁄joinr    r   r   ⁄appendr   r!   r   r   ⁄len⁄sum⁄columns⁄locr   r   ⁄setStyler	   r   r@   ⁄
whitesmoke⁄beige⁄black⁄plt⁄figure⁄sns⁄histplot⁄title⁄xlabel⁄ylabel⁄io⁄BytesIO⁄savefig⁄seekr
   ⁄close⁄head⁄iterrows⁄get⁄
isinstance⁄pd⁄	Timestamp⁄build⁄logger⁄info⁄	Exception⁄error⁄str)!r%   ⁄df⁄risk_scores⁄	top_fraud⁄include_charts⁄include_recommendationsrÜ   ⁄filename⁄filepath⁄doc⁄story⁄report_date⁄total_transactions⁄fraud_transactions⁄fraud_percentage⁄total_amountZfraud_amountZ
stats_dataZstats_table⁄
img_buffer⁄risk_dist_img⁄
fraud_data⁄_⁄rowrÉ   rK   rl   ⁄date⁄fraud_table⁄recommendations⁄rec⁄footer_text⁄er&   r&   r'   ⁄generate_executive_summaryn   s¥    &

˙	¯






$¯

z'PDFGenerator.generate_executive_summaryc           0      C   st  êz0t †° †d°}d|õ dù}	tj†| j|	°}
t|
tdç}g }|†	t
d| jd É° |†	tddÉ° t †° †d	°}|†	t
d
|õ ù| jd É° |†	tÉ ° |†	t
d| jd É° g d¢}|D ]}|†	t
|| jd É° qº|†	tÉ ° |†	t
d| jd É° t|É}|d †° }|| d }d|dõd|dõd|dõd|j|d df †° dõdù	}|†	t
|| jd É° |†	tddÉ° |†	t
d| jd É° d}|†	t
|| jd É° |†	tddÉ° |†	t
d| jd É° |êrÑ|†	t
d| jd  É° tjd!d"ç tj|d# d$d%d&ç t†d'° t†d(° t†d)° t†° }tj|d*d+d,d-ç |†d.° t|d/t d0t d1ç}|†	|° |†	tddÉ° t†°  |êrd2|jv êr|†	t
d3| jd  É° |† ° }|d |d< t!†"|d2 °|d2< |d2 j#j$|d4< |†%d4°d †&d5d6g°†'° }|d5 |d6  |d7< tjd8d"ç t†(d9dd° tj)|d4 |d5 d:d;ç t†d<° t†d=° tj*d>d?ç t†(d9dd9° tj)|d4 |d7 d:d@dAç t†dB° t†dC° t†dD° tj*d>d?ç t†+°  t†° }tj|d*d+d,d-ç |†d.° t|d/t dEt d1ç}|†	|° |†	tddÉ° t†°  |†	tÉ ° |†	t
dF| jd É° |†	t
dG| jd  É° t|Éd.kêråg }|†	g dH¢° |†,dI°†-° D ]~\}}|†.dJdK°}dL|†.dd.°dõù}|†.d#d.°dMõ}|†.d2dK°} |†.dNdK°}!t/| t!j0ÉêrË| †dO°} |†	|||| |!g° êqÄt1|dPt dQt dQt dt dQt gdRç}"|"†2t3dSdTdUt4j5fdVdTdUt4j6fdWdXdYdZdSd[d\t4j7fd]dTd\dt4j8fgÉ° |†	|"° |†	tddÉ° |êrË|êrË|†	t
d^| jd  É° t9t:|†;° Éd_dEÖ ÉD ê]\}#\}$}%|†	t
d`|#d õ da|%†.dJ|$°õ ù| jd  É° |%†.dbdc°}&|†	t
|&| jd É° |%†.dd°êrÜ|†	t
de| jd É° |%dd d_dfÖ D ].\}'}(|†	t
dg|'õ da|(dMõù| jd É° êqV|%†.dh°êr‘|†	t
di| jd É° |%dh D ]"})|†	t
dg|)õ ù| jd É° êq∞|†	tddÉ° êq∆|†	tÉ ° |†	t
dj| jd É° g dk¢}*|*D ]}'|†	t
|'| jd É° êq|†	tddÉ° |†	t
dl| jd É° g dm¢}+|+D ]},|†	t
|,| jd É° êqf|†	tddÉ° |†	t
dn| jd É° |†	t
do| jd  É° dp}-|†	t
|-| jd É° |†	tddÉ° dqt †° †dr°õ ù}.|†	t
|.| jds É° |†<|° t=†>dt|
õ ù° |
W S  t?êyn }/ z"t=†@dutA|/Éõ ù° Ç W Y d_}/~/n
d_}/~/0 0 d_S )vaA  
        Generate detailed fraud analysis report
        
        Args:
            df (DataFrame): Transaction data
            risk_scores (DataFrame): Risk scores
            top_fraud (DataFrame): Top fraudulent transactions
            explanations (dict): Transaction explanations
            include_charts (bool): Whether to include charts
            include_explanations (bool): Whether to include explanations
            include_recommendations (bool): Whether to include recommendations
            
        Returns:
            str: Path to generated PDF
        rB   Zdetailed_fraud_analysis_rC   rD   zDetailed Fraud Analysis Reportr)   rE   r5   rF   rG   r3   zTable of Contentsr6   )˙1. Executive Summary˙2. Methodology˙3. Analysis Results˙ 4. Detailed Transaction Analysis˙5. Risk Factors˙6. Recommendations˙7. Appendixr:   rÃ   rI   rJ   zù
            This report presents a detailed analysis of fraudulent transactions detected by the fraud detection system.
            The analysis identified rO   z  fraudulent transactions out of z. total transactions,
            representing rR   z[% of all transactions. The total value of fraudulent transactions 
            amounts to $rK   rU   z* if amount data is available.
            rÕ   a“  
            The fraud detection system employs a multi-layered approach combining unsupervised learning, 
            supervised learning, and rule-based methods to identify potentially fraudulent transactions. 
            The system analyzes various features including transaction amounts, frequency patterns, 
            geographic locations, temporal patterns, and behavioral characteristics to assign risk scores 
            to each transaction.
            rŒ   z3.1 Risk Distributionr8   ri   rj   rl   rm   Trn   rq   rr   rs   rt   ru   rv   rw   r   r9   r{   r|   rÜ   z3.2 Fraud by Timer≈   rì   ⁄count⁄
fraud_rate)r<   r9   rW   ⁄o)⁄markerzFraud Count by Date⁄CountÈ-   )⁄rotation⁄red)r÷   ⁄colorzFraud Rate by DateZRaterÇ   È   rœ   z4.1 Top Fraudulent Transactions)rÄ   rÅ   rr   rÇ   z
Risk LevelÈ   rÉ   rÑ   rT   rÖ   ⁄
risk_levelrá   g333333Û?göôôôôôÈ?rX   rZ   r[   r\   r^   r_   rb   )rd   r[   r\   r>   re   rf   r`   rg   z4.2 Transaction ExplanationsN˙Transaction z: ⁄text_explanation˙No explanation available.⁄top_factorszTop Contributing Factors:rV   ˙- ⁄rule_violationszRule Violations:r–   )z`1. High Transaction Amount: Transactions significantly above average for the sender or receiver.zL2. Unusual Geographic Patterns: Transactions from or to high-risk locations.zL3. Temporal Anomalies: Transactions at unusual times or in rapid succession.zN4. Behavioral Deviations: Transactions that deviate from established patterns.zM5. Network Anomalies: Transactions involving suspicious networks of entities.r—   )z1. Immediate Actions:z9   - Block all high-risk transactions (risk score > 0.9).zO   - Contact customers associated with high-risk transactions for verification.zK   - Flag accounts involved in multiple suspicious transactions for review.⁄ z2. System Improvements:zA   - Update fraud detection rules based on recent fraud patterns.zI   - Implement additional verification steps for high-value transactions.z5   - Enhance monitoring of cross-border transactions.rÂ   z3. Process Enhancements:z:   - Conduct regular audits of the fraud detection system.z?   - Provide training to staff on identifying fraud indicators.z=   - Establish clear procedures for handling suspected fraud.rÂ   z4. Long-term Strategies:zI   - Develop machine learning models to adapt to evolving fraud patterns.z6   - Implement real-time fraud detection capabilities.zD   - Collaborate with industry partners to share fraud intelligence.r“   z7.1 Methodology Detailsa√  
            The fraud detection system utilizes a combination of the following techniques:
            
            - Unsupervised Learning: Isolation Forest, Local Outlier Factor, Autoencoders
            - Supervised Learning: Random Forest, XGBoost, Neural Networks
            - Rule-based Detection: Configurable rules for known fraud patterns
            - Feature Engineering: Statistical, graph-based, NLP, and time-series features
            
            Each transaction is assigned a risk score between 0 and 1, with higher scores indicating 
            greater likelihood of fraud. Transactions with scores above the threshold (typically 0.5) 
            are flagged for review.
            rã   rå   r=   z*Detailed fraud analysis report generated: z*Error generating detailed fraud analysis: )Br   rç   ré   r#   rè   rê   r    r   r   rë   r   r!   r   r   rí   rì   rï   rö   rõ   rú   rù   rû   rü   r†   r°   r¢   r£   r§   r
   r   r•   rî   ⁄copyr™   ⁄to_datetime⁄dtr≈   ⁄groupby⁄agg⁄reset_index⁄subplot⁄plot⁄xticks⁄tight_layoutr¶   rß   r®   r©   r´   r   rñ   r	   r   r@   ró   rò   rô   ⁄	enumerate⁄list⁄itemsr¨   r≠   rÆ   rØ   r∞   r±   )0r%   r≤   r≥   r¥   ⁄explanationsrµ   ⁄include_explanationsr∂   rÜ   r∑   r∏   rπ   r∫   rª   Z	toc_items⁄itemrº   rΩ   ræ   Zsummary_textZmethodology_textr¿   r¡   Zdf_with_risk⁄fraud_by_dateZfraud_time_imgr¬   r√   rƒ   rÉ   rK   rl   r≈   rﬁ   r∆   ⁄i⁄idx⁄explanation⁄explanation_text⁄factor⁄contribution⁄ruleZrisk_factorsr«   r»   Zmethodology_detailsr…   r    r&   r&   r'   ⁄ generate_detailed_fraud_analysis  s&   
˛˛˝¸













*¯
& ˇ( 
z-PDFGenerator.generate_detailed_fraud_analysisc           "      C   sæ  êzzt †° †d°}d|õ dù}tj†| j|°}t|tdç}g }	|	†	t
d| jd É° |	†	tddÉ° t †° †d	°}
|	†	t
d
|
õ ù| jd É° |	†	tÉ ° |	†	t
d| jd É° d}|	†	t
|| jd É° |	†	tddÉ° |	†	t
d| jd É° d}|	†	t
|| jd É° |	†	tddÉ° |	†	t
d| jd É° d|v êr™|	†	t
d| jd É° dt|d Éõ dù}|d D ]}|d|õ dù7 }êql|	†	t
|| jd É° |	†	tddÉ° d|v êrl|	†	t
d| jd É° dt|d Éõ dù}|d D ]^}d|d | v êr2|d | d }|†dd °}|d|õ d!|d"õdù7 }n|d|õ dù7 }êqÊ|	†	t
|| jd É° |	†	tddÉ° d#|v êrˆ|	†	t
d$| jd É° dt|d# †d%i °Éõ d&ù}|d# †d%i °D ]}|d|õ dù7 }êq∏|	†	t
|| jd É° |	†	tddÉ° d|v êr|	†	t
d'| jd É° d(|d v êr|d d( }d)|v êr|d) }g }|†	d*d+g° |†d,°†° D ]$\}}|†	|d- |d. d"õg° êqbt|d/t d0t gd1ç}|†td2d3d4tjfd5d3d4tjfd6d7d8d9d2d:d;tjfd<d3d;dtjfgÉ° |	†	|° |	†	tddÉ° |	†	t
d=| jd É° d>}|	†	t
|| jd É° |	†	tddÉ° |	†	t
d?| jd É° t|É}|d@ †° }|| dA }dBdCgdD|dEõgdF|dEõgdG|dHõdIùgdJt|ÉdK dHõdLùgg}t|d/t dMt gd1ç}|†td2d3d4tjfd5d3d4tjfd6d7dNd9d2d:d;tjfd<d3d;dtjfgÉ° |	†	|° |	†	tddÉ° dOt †° †dP°õ ù} |	†	t
| | jdQ É° |†|	° t†dR|õ ù° |W S  têy∏ }! z"t† dSt!|!Éõ ù° Ç W Y dT}!~!n
dT}!~!0 0 dTS )UaR  
        Generate technical report
        
        Args:
            df (DataFrame): Transaction data
            risk_scores (DataFrame): Risk scores
            model_results (dict): Model results
            include_charts (bool): Whether to include charts
            
        Returns:
            str: Path to generated PDF
        rB   Ztechnical_report_rC   rD   z Fraud Detection Technical Reportr)   rE   r5   rF   rG   r3   z1. System Overviewr6   aã  
            The fraud detection system is designed to identify potentially fraudulent transactions 
            using a combination of machine learning techniques and rule-based methods. The system 
            processes transaction data in real-time, extracting various features and applying multiple 
            detection algorithms to generate risk scores for each transaction.
            r:   z2. Data Processingaá  
            The system processes transaction data through the following stages:
            
            1. Data Ingestion: Transactions are loaded from various sources (CSV, Excel, databases).
            2. Data Preprocessing: Missing values are handled, data types are converted, and basic 
               validation is performed.
            3. Feature Engineering: Statistical, graph-based, NLP, and time-series features are extracted.
            4. Model Application: Multiple models are applied to generate risk scores.
            5. Risk Aggregation: Scores from different models are combined to produce a final risk score.
            z3. Model Performance⁄unsupervisedz3.1 Unsupervised Modelsr8   z$
                The system employs z@ unsupervised learning models:
                
                r„   ⁄
r9   ⁄
supervisedz3.2 Supervised Modelsz> supervised learning models:
                
                ⁄performance⁄roc_aucr   z: AUC = rÖ   r˝   z3.3 Rule-based Models⁄rulesz8 rule-based detectors:
                
                z3.4 Feature Importance⁄random_forest⁄feature_importance⁄FeatureZ
Importancer<   ⁄feature⁄
importancerV   rà   rX   rZ   r[   r\   r^   r_   rb   râ   re   rf   r`   rg   z4. System Architecturea:  
            The fraud detection system is built with a modular architecture consisting of the following components:
            
            1. Data Ingestion Layer: Handles data loading from various sources.
            2. Feature Engineering Layer: Extracts features for fraud detection.
            3. Model Layer: Applies various detection algorithms.
            4. Risk Aggregation Layer: Combines model outputs.
            5. Reporting Layer: Generates reports and visualizations.
            6. API Layer: Provides interfaces for external systems.
            z5. Performance MetricsrI   rJ   rL   rM   rN   rO   rP   rQ   rR   rS   zProcessing Timeg¸©Ò“MbP?z seconds (estimated)rW   rc   rã   rå   r=   zTechnical report generated: z#Error generating technical report: N)"r   rç   ré   r#   rè   rê   r    r   r   rë   r   r!   r   r   rí   r®   r¶   rß   r   r   rñ   r	   r   r@   ró   rò   rô   rì   r¨   r≠   rÆ   rØ   r∞   r±   )"r%   r≤   r≥   ⁄model_resultsrµ   rÜ   r∑   r∏   rπ   r∫   rª   Zoverview_textZdata_processing_textZunsupervised_text⁄
model_nameZsupervised_textZperf⁄aucZ	rule_text⁄	rule_nameZrf_data⁄importance_dfZfeature_datar√   rƒ   Zfeature_tableZarchitecture_textrº   rΩ   ræ   Zmetrics_dataZmetrics_tabler…   r    r&   r&   r'   ⁄generate_technical_report>  s⁄    

ˇ

ˇ
ˇ

¯


˚¯

z&PDFGenerator.generate_technical_reportc	           $      C   sV  êzt †° †d°}	d|	õ dù}
tj†| j|
°}t|tdç}g }|†	t
d| jd É° |†	tddÉ° t †° †d	°}|†	t
d
|õ ù| jd É° |†	tÉ ° |†	t
d| jd É° t|É}|d †° }|| d }d|dõd|dõd|dõdù}|†	t
|| jd É° |†	tddÉ° |êr |†	t
d| jd É° tjddç tj|d dddç t†d° t†d° t†d ° t†° }tj|d!d"d#d$ç |†d%° t|d&t d't d(ç}|†	|° |†	tddÉ° t†°  |†	t
d)| jd É° t|Éd%kêr⁄g }|†	g d*¢° |†d+°†° D ]J\}}|† d,d-°}d.|† d/d%°d0õù}|† dd%°d1õ}|†	|||g° êqt!|d2t d3t d3t gd4ç}|†"t#d5d6d7t$j%fd8d6d7t$j&fd9d:d;d<d5d=d>t$j'fd?d6d>dt$j(fgÉ° |†	|° |†	tddÉ° |êrv|êrv|†	t
d@| jd É° t)t*|†+° ÉdAdBÖ ÉD ]`\}\}}|†	t
dC|d õ ù| jdD É° |† dEdF°}|†	t
|| jd É° |†	tdd&É° êq|êrÃ|†	t
dG| jd É° g dH¢} | D ]}!|†	t
|!| jd É° êqû|†	tddÉ° dIt †° †dJ°õ ù}"|†	t
|"| jdK É° |†,|° t-†.dL|õ ù° |W S  t/êyP }# z"t-†0dMt1|#Éõ ù° Ç W Y dA}#~#n
dA}#~#0 0 dAS )Na}  
        Generate custom report with user-specified sections
        
        Args:
            df (DataFrame): Transaction data
            risk_scores (DataFrame): Risk scores
            top_fraud (DataFrame): Top fraudulent transactions
            explanations (dict): Transaction explanations
            model_results (dict): Model results
            include_charts (bool): Whether to include charts
            include_explanations (bool): Whether to include explanations
            include_recommendations (bool): Whether to include recommendations
            
        Returns:
            str: Path to generated PDF
        rB   Zcustom_report_rC   rD   zCustom Fraud Detection Reportr)   rE   r5   rF   rG   r3   rH   r6   rI   rJ   z!
            Total Transactions: rO   z&
            Fraudulent Transactions: z
            Fraud Percentage: rR   z%
            r:   rh   ri   rj   rl   rm   Trn   rq   rr   rs   rt   ru   rv   rw   r   r9   r{   r|   r   )rÄ   rÅ   rr   r<   rÉ   rÑ   rT   rK   rU   rÖ   rW   rà   rX   rZ   r[   r\   r^   r_   rb   rc   re   rf   r`   rg   zTransaction ExplanationsNrV   rﬂ   r8   r‡   r·   rä   )z11. Review all high-risk transactions immediately.z<2. Implement additional verification for large transactions.z+3. Monitor patterns in fraudulent activity.z$4. Update detection rules regularly.z"5. Conduct periodic system audits.rã   rå   r=   zCustom report generated: z Error generating custom report: )2r   rç   ré   r#   rè   rê   r    r   r   rë   r   r!   r   r   rí   rì   rö   rõ   rú   rù   rû   rü   r†   r°   r¢   r£   r§   r
   r   r•   r¶   rß   r®   r   rñ   r	   r   r@   ró   rò   rô   r   rÒ   rÚ   r¨   r≠   rÆ   rØ   r∞   r±   )$r%   r≤   r≥   r¥   rÛ   r
  rµ   rÙ   r∂   rÜ   r∑   r∏   rπ   r∫   rª   rº   rΩ   ræ   Z
stats_textr¿   r¡   r¬   r√   rƒ   rÉ   rK   rl   r∆   r˜   r¯   r˘   r˙   r«   r»   r…   r    r&   r&   r'   ⁄generate_custom_report  s¢    ˇ˛˝





¯
$ 
z#PDFGenerator.generate_custom_reportN)r   )TT)TTT)T)TTT)
⁄__name__⁄
__module__⁄__qualname__⁄__doc__r(   r"   rÀ   r˛   r  r  r&   r&   r&   r'   r       s   
:
   ˛
  5
 S  ˛r   ):r  ⁄pandasr™   ⁄numpy⁄np⁄matplotlib.pyplot⁄pyplotrö   ⁄seabornrú   r   r#   r°   ⁄base64Zreportlab.lib.pagesizesr   r   Zreportlab.platypusr   r   r   r   r	   r
   r   Zreportlab.lib.stylesr   r   Zreportlab.lib.unitsr   Zreportlab.libr   Zreportlab.lib.enumsr   r   r   Zreportlab.graphics.shapesr   Z#reportlab.graphics.charts.lineplotsr   Z#reportlab.graphics.charts.barchartsr   Zreportlab.graphics.widgetsr   ⁄warnings⁄logging⁄typingr   r   r   r   ⁄filterwarnings⁄basicConfig⁄INFO⁄	getLoggerr  r≠   r   r&   r&   r&   r'   ⁄<module>   s2   $



=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\reporting\__pycache__\__init__.cpython-39.pyc ===
a
    èöh    „                   @   s   d S )N© r   r   r   ˙hC:\Users\Tranquil Abyss\fraud-detection-platform\app\..\src\fraud_detection_engine\reporting\__init__.py⁄<module>   Û    

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\utils\api_utils.py ===
"""
API Utilities Module
Handles checking API availability and providing demo data
"""
import yaml
import os
import logging
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
logger = logging.getLogger(__name__)

def is_api_available(service_name):
    """
    Check if an API service is available
    
    Args:
        service_name (str): Name of the service (e.g., 'gemini', 'openai', 'news_api')
        
    Returns:
        bool: True if API is available, False otherwise
    """
    try:
        # Get the directory where this script is located
        script_dir = os.path.dirname(os.path.abspath(__file__))
        config_path = os.path.join(script_dir, '..', '..', 'config', 'api_keys.yml')
        
        with open(config_path, 'r') as f:
            config = yaml.safe_load(f)
        
        api_key = config.get(service_name, {}).get('api_key', '')
        
        # Check if key is available and not a placeholder
        return api_key and api_key not in ["YOUR_" + service_name.upper() + "_API_KEY", "NOT_AVAILABLE"]
    except Exception as e:
        logger.warning(f"Error checking API availability for {service_name}: {str(e)}")
        return False

def get_demo_sanctions_data():
    """
    Get demo sanctions data for testing
    
    Returns:
        DataFrame: Demo sanctions data
    """
    try:
        # Create demo data
        demo_data = pd.DataFrame({
            'entity_id': ['ENT001', 'ENT002', 'ENT003'],
            'name': ['Demo Entity 1', 'Demo Entity 2', 'Demo Entity 3'],
            'country': ['North Korea', 'Iran', 'Syria'],
            'list_type': ['Sanctions List', 'Sanctions List', 'Sanctions List'],
            'added_date': ['2023-01-01', '2023-02-01', '2023-03-01']
        })
        
        logger.info("Using demo sanctions data")
        return demo_data
    except Exception as e:
        logger.error(f"Error creating demo sanctions data: {str(e)}")
        return pd.DataFrame()

def get_demo_tax_compliance_data():
    """
    Get demo tax compliance data for testing
    
    Returns:
        DataFrame: Demo tax compliance data
    """
    try:
        # Create demo data
        demo_data = pd.DataFrame({
            'tax_id': ['TAX001', 'TAX002', 'TAX003'],
            'entity_name': ['Demo Corp 1', 'Demo Corp 2', 'Demo Corp 3'],
            'compliance_status': ['Compliant', 'Non-compliant', 'Under Review'],
            'last_checked': ['2023-01-01', '2023-02-01', '2023-03-01']
        })
        
        logger.info("Using demo tax compliance data")
        return demo_data
    except Exception as e:
        logger.error(f"Error creating demo tax compliance data: {str(e)}")
        return pd.DataFrame()

def get_demo_bank_verification_data():
    """
    Get demo bank verification data for testing
    
    Returns:
        DataFrame: Demo bank verification data
    """
    try:
        # Create demo data
        demo_data = pd.DataFrame({
            'account_number': ['ACC001', 'ACC002', 'ACC003'],
            'bank_name': ['Demo Bank 1', 'Demo Bank 2', 'Demo Bank 3'],
            'verification_status': ['Verified', 'Not Verified', 'Pending'],
            'last_verified': ['2023-01-01', '2023-02-01', '2023-03-01']
        })
        
        logger.info("Using demo bank verification data")
        return demo_data
    except Exception as e:
        logger.error(f"Error creating demo bank verification data: {str(e)}")
        return pd.DataFrame()

def get_demo_identity_verification_data():
    """
    Get demo identity verification data for testing
    
    Returns:
        DataFrame: Demo identity verification data
    """
    try:
        # Create demo data
        demo_data = pd.DataFrame({
            'id_number': ['ID001', 'ID002', 'ID003'],
            'name': ['Demo Person 1', 'Demo Person 2', 'Demo Person 3'],
            'verification_status': ['Verified', 'Not Verified', 'Pending'],
            'last_verified': ['2023-01-01', '2023-02-01', '2023-03-01']
        })
        
        logger.info("Using demo identity verification data")
        return demo_data
    except Exception as e:
        logger.error(f"Error creating demo identity verification data: {str(e)}")
        return pd.DataFrame()

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\utils\__init__.py ===

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\utils\.ipynb_checkpoints\api_utils-checkpoint.py ===
"""
API Utilities Module
Handles checking API availability and providing demo data
"""
import yaml
import os
import logging
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
logger = logging.getLogger(__name__)

def is_api_available(service_name):
    """
    Check if an API service is available
    
    Args:
        service_name (str): Name of the service (e.g., 'gemini', 'openai', 'news_api')
        
    Returns:
        bool: True if API is available, False otherwise
    """
    try:
        # Get the directory where this script is located
        script_dir = os.path.dirname(os.path.abspath(__file__))
        config_path = os.path.join(script_dir, '..', '..', 'config', 'api_keys.yml')
        
        with open(config_path, 'r') as f:
            config = yaml.safe_load(f)
        
        api_key = config.get(service_name, {}).get('api_key', '')
        
        # Check if key is available and not a placeholder
        return api_key and api_key not in ["YOUR_" + service_name.upper() + "_API_KEY", "NOT_AVAILABLE"]
    except Exception as e:
        logger.warning(f"Error checking API availability for {service_name}: {str(e)}")
        return False

def get_demo_sanctions_data():
    """
    Get demo sanctions data for testing
    
    Returns:
        DataFrame: Demo sanctions data
    """
    try:
        # Create demo data
        demo_data = pd.DataFrame({
            'entity_id': ['ENT001', 'ENT002', 'ENT003'],
            'name': ['Demo Entity 1', 'Demo Entity 2', 'Demo Entity 3'],
            'country': ['North Korea', 'Iran', 'Syria'],
            'list_type': ['Sanctions List', 'Sanctions List', 'Sanctions List'],
            'added_date': ['2023-01-01', '2023-02-01', '2023-03-01']
        })
        
        logger.info("Using demo sanctions data")
        return demo_data
    except Exception as e:
        logger.error(f"Error creating demo sanctions data: {str(e)}")
        return pd.DataFrame()

def get_demo_tax_compliance_data():
    """
    Get demo tax compliance data for testing
    
    Returns:
        DataFrame: Demo tax compliance data
    """
    try:
        # Create demo data
        demo_data = pd.DataFrame({
            'tax_id': ['TAX001', 'TAX002', 'TAX003'],
            'entity_name': ['Demo Corp 1', 'Demo Corp 2', 'Demo Corp 3'],
            'compliance_status': ['Compliant', 'Non-compliant', 'Under Review'],
            'last_checked': ['2023-01-01', '2023-02-01', '2023-03-01']
        })
        
        logger.info("Using demo tax compliance data")
        return demo_data
    except Exception as e:
        logger.error(f"Error creating demo tax compliance data: {str(e)}")
        return pd.DataFrame()

def get_demo_bank_verification_data():
    """
    Get demo bank verification data for testing
    
    Returns:
        DataFrame: Demo bank verification data
    """
    try:
        # Create demo data
        demo_data = pd.DataFrame({
            'account_number': ['ACC001', 'ACC002', 'ACC003'],
            'bank_name': ['Demo Bank 1', 'Demo Bank 2', 'Demo Bank 3'],
            'verification_status': ['Verified', 'Not Verified', 'Pending'],
            'last_verified': ['2023-01-01', '2023-02-01', '2023-03-01']
        })
        
        logger.info("Using demo bank verification data")
        return demo_data
    except Exception as e:
        logger.error(f"Error creating demo bank verification data: {str(e)}")
        return pd.DataFrame()

def get_demo_identity_verification_data():
    """
    Get demo identity verification data for testing
    
    Returns:
        DataFrame: Demo identity verification data
    """
    try:
        # Create demo data
        demo_data = pd.DataFrame({
            'id_number': ['ID001', 'ID002', 'ID003'],
            'name': ['Demo Person 1', 'Demo Person 2', 'Demo Person 3'],
            'verification_status': ['Verified', 'Not Verified', 'Pending'],
            'last_verified': ['2023-01-01', '2023-02-01', '2023-03-01']
        })
        
        logger.info("Using demo identity verification data")
        return demo_data
    except Exception as e:
        logger.error(f"Error creating demo identity verification data: {str(e)}")
        return pd.DataFrame()

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\utils\.ipynb_checkpoints\__init__-checkpoint.py ===

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\utils\__pycache__\api_utils.cpython-39.pyc ===
a
    èöh  „                   @   sr   d Z ddlZddlZddlZddlZddlZddlmZm	Z	 e†
e°ZddÑ ZddÑ Zdd	Ñ Zd
dÑ ZddÑ ZdS )zP
API Utilities Module
Handles checking API availability and providing demo data
È    N)⁄datetime⁄	timedeltac              
   C   s‘   zét j†t j†t°°}t j†|dddd°}t|dÉè}t†|°}W d  É n1 sT0    Y  |†	| i °†	dd°}|oå|d| †
°  d	 d
fvW S  tyŒ } z(t†d| õ dt|Éõ ù° W Y d}~dS d}~0 0 dS )z‚
    Check if an API service is available
    
    Args:
        service_name (str): Name of the service (e.g., 'gemini', 'openai', 'news_api')
        
    Returns:
        bool: True if API is available, False otherwise
    z..⁄configzapi_keys.yml⁄rN⁄api_key⁄ ZYOUR_Z_API_KEYZNOT_AVAILABLEz$Error checking API availability for z: F)⁄os⁄path⁄dirname⁄abspath⁄__file__⁄join⁄open⁄yaml⁄	safe_load⁄get⁄upper⁄	Exception⁄logger⁄warning⁄str)⁄service_name⁄
script_dir⁄config_path⁄fr   r   ⁄e© r   ˙eC:\Users\Tranquil Abyss\fraud-detection-platform\app\..\src\fraud_detection_engine\utils\api_utils.py⁄is_api_available   s    
(r   c               
   C   sÄ   z:t †g d¢g d¢g d¢g d¢g d¢dú°} t†d° | W S  tyz } z(t†dt|Éõ ù° t †° W  Y d	}~S d	}~0 0 d	S )
zf
    Get demo sanctions data for testing
    
    Returns:
        DataFrame: Demo sanctions data
    )ZENT001ZENT002ZENT003)zDemo Entity 1zDemo Entity 2zDemo Entity 3)zNorth Korea⁄IranZSyria)˙Sanctions Listr    r    ©z
2023-01-01z
2023-02-01z
2023-03-01)Z	entity_id⁄name⁄country⁄	list_typeZ
added_datezUsing demo sanctions dataz$Error creating demo sanctions data: N©⁄pd⁄	DataFramer   ⁄infor   ⁄errorr   ©Z	demo_datar   r   r   r   ⁄get_demo_sanctions_data'   s    ˚
r+   c               
   C   sz   z4t †g d¢g d¢g d¢g d¢dú°} t†d° | W S  tyt } z(t†dt|Éõ ù° t †° W  Y d}~S d}~0 0 dS )	zp
    Get demo tax compliance data for testing
    
    Returns:
        DataFrame: Demo tax compliance data
    )ZTAX001ZTAX002ZTAX003)zDemo Corp 1zDemo Corp 2zDemo Corp 3)Z	CompliantzNon-compliantzUnder Reviewr!   )Ztax_idZentity_nameZcompliance_statusZlast_checkedzUsing demo tax compliance dataz)Error creating demo tax compliance data: Nr%   r*   r   r   r   ⁄get_demo_tax_compliance_data>   s    ¸
r,   c               
   C   sz   z4t †g d¢g d¢g d¢g d¢dú°} t†d° | W S  tyt } z(t†dt|Éõ ù° t †° W  Y d}~S d}~0 0 dS )	zv
    Get demo bank verification data for testing
    
    Returns:
        DataFrame: Demo bank verification data
    )ZACC001ZACC002ZACC003)zDemo Bank 1zDemo Bank 2zDemo Bank 3©ZVerifiedzNot Verified⁄Pendingr!   )Zaccount_numberZ	bank_name⁄verification_status⁄last_verifiedz!Using demo bank verification dataz,Error creating demo bank verification data: Nr%   r*   r   r   r   ⁄get_demo_bank_verification_dataT   s    ¸
r1   c               
   C   sz   z4t †g d¢g d¢g d¢g d¢dú°} t†d° | W S  tyt } z(t†dt|Éõ ù° t †° W  Y d}~S d}~0 0 dS )	z~
    Get demo identity verification data for testing
    
    Returns:
        DataFrame: Demo identity verification data
    )ZID001ZID002ZID003)zDemo Person 1zDemo Person 2zDemo Person 3r-   r!   )Z	id_numberr"   r/   r0   z%Using demo identity verification dataz0Error creating demo identity verification data: Nr%   r*   r   r   r   ⁄#get_demo_identity_verification_dataj   s    ¸
r2   )⁄__doc__r   r   ⁄logging⁄pandasr&   ⁄numpy⁄npr   r   ⁄	getLogger⁄__name__r   r   r+   r,   r1   r2   r   r   r   r   ⁄<module>   s   


=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\utils\__pycache__\__init__.cpython-39.pyc ===
a
    èöh    „                   @   s   d S )N© r   r   r   ˙dC:\Users\Tranquil Abyss\fraud-detection-platform\app\..\src\fraud_detection_engine\utils\__init__.py⁄<module>   Û    

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\__pycache__\__init__.cpython-313.pyc ===
Û
    cÄöh    „                   Û   ï g )N© r   Û    ⁄^C:\Users\Tranquil Abyss\fraud-detection-platform\app\..\src\fraud_detection_engine\__init__.py⁄<module>r      s   Òr   

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\src\fraud_detection_engine\__pycache__\__init__.cpython-39.pyc ===
a
    èöh    „                   @   s   d S )N© r   r   r   ˙^C:\Users\Tranquil Abyss\fraud-detection-platform\app\..\src\fraud_detection_engine\__init__.py⁄<module>   Û    

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\tests\test_features.py ===
"""
Test cases for feature engineering module
"""

import pytest
import pandas as pd
import numpy as np
import os
import sys

# Add src to path
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'src'))

from fraud_detection_engine.features.statistical_features import StatisticalFeatures
from fraud_detection_engine.features.graph_features import GraphFeatures
from fraud_detection_engine.features.nlp_features import NLPFeatures
from fraud_detection_engine.features.timeseries_features import TimeSeriesFeatures

class TestStatisticalFeatures:
    """Test cases for StatisticalFeatures class"""
    
    def setup_method(self):
        """Setup test data"""
        self.feature_extractor = StatisticalFeatures()
        
        # Create test data
        np.random.seed(42)
        self.test_data = pd.DataFrame({
            'transaction_id': range(1, 101),
            'timestamp': pd.date_range('2023-01-01', periods=100, freq='D'),
            'amount': np.random.lognormal(5, 1, 100),
            'sender_id': [f'sender_{i % 10}' for i in range(100)],
            'receiver_id': [f'receiver_{i % 10}' for i in range(100)]
        })
    
    def test_extract_features(self):
        """Test feature extraction"""
        result_df = self.feature_extractor.extract_features(self.test_data)
        
        # Check that features are added
        original_cols = set(self.test_data.columns)
        result_cols = set(result_df.columns)
        new_cols = result_cols - original_cols
        
        assert len(new_cols) > 0
        
        # Check specific features
        assert 'amount_zscore' in result_df.columns
        assert 'amount_mad_zscore' in result_df.columns
        assert 'amount_percentile_rank' in result_df.columns
        assert 'benford_chi_square' in result_df.columns
    
    def test_benford_features(self):
        """Test Benford's Law features"""
        result_df = self.feature_extractor._extract_benford_features(self.test_data)
        
        assert 'benford_chi_square' in result_df.columns
        assert 'benford_p_value' in result_df.columns
        
        # Check that deviation features are added
        for i in range(1, 6):
            assert f'benford_deviation_{i}' in result_df.columns
    
    def test_zscore_features(self):
        """Test Z-score features"""
        result_df = self.feature_extractor._extract_zscore_features(self.test_data)
        
        assert 'amount_zscore' in result_df.columns
        assert 'amount_zscore_outlier' in result_df.columns
        assert 'sender_amount_zscore' in result_df.columns
        assert 'receiver_amount_zscore' in result_df.columns
    
    def test_mad_features(self):
        """Test MAD features"""
        result_df = self.feature_extractor._extract_mad_features(self.test_data)
        
        assert 'amount_mad_zscore' in result_df.columns
        assert 'amount_mad_outlier' in result_df.columns
        assert 'sender_amount_mad_zscore' in result_df.columns
        assert 'receiver_amount_mad_zscore' in result_df.columns
    
    def test_fit_transform(self):
        """Test fit_transform method"""
        result_df = self.feature_extractor.fit_transform(self.test_data)
        
        # Check that features are added
        original_cols = set(self.test_data.columns)
        result_cols = set(result_df.columns)
        new_cols = result_cols - original_cols
        
        assert len(new_cols) > 0
        assert self.feature_extractor.fitted == True
    
    def test_transform(self):
        """Test transform method"""
        # First fit the extractor
        self.feature_extractor.fit_transform(self.test_data)
        
        # Create new data
        new_data = pd.DataFrame({
            'transaction_id': range(101, 111),
            'timestamp': pd.date_range('2023-04-11', periods=10, freq='D'),
            'amount': np.random.lognormal(5, 1, 10),
            'sender_id': [f'sender_{i}' for i in range(10)],
            'receiver_id': [f'receiver_{i}' for i in range(10)]
        })
        
        # Transform new data
        result_df = self.feature_extractor.transform(new_data)
        
        # Check that features are added
        original_cols = set(new_data.columns)
        result_cols = set(result_df.columns)
        new_cols = result_cols - original_cols
        
        assert len(new_cols) > 0

class TestGraphFeatures:
    """Test cases for GraphFeatures class"""
    
    def setup_method(self):
        """Setup test data"""
        self.feature_extractor = GraphFeatures()
        
        # Create test data
        self.test_data = pd.DataFrame({
            'transaction_id': range(1, 21),
            'timestamp': pd.date_range('2023-01-01', periods=20, freq='D'),
            'amount': np.random.uniform(100, 1000, 20),
            'sender_id': [f'sender_{i % 5}' for i in range(20)],
            'receiver_id': [f'receiver_{i % 5}' for i in range(20)]
        })
    
    def test_extract_features(self):
        """Test feature extraction"""
        result_df = self.feature_extractor.extract_features(self.test_data)
        
        # Check that features are added
        original_cols = set(self.test_data.columns)
        result_cols = set(result_df.columns)
        new_cols = result_cols - original_cols
        
        assert len(new_cols) > 0
        
        # Check specific features
        assert 'sender_degree_centrality' in result_df.columns
        assert 'receiver_degree_centrality' in result_df.columns
        assert 'sender_clustering_coefficient' in result_df.columns
        assert 'receiver_clustering_coefficient' in result_df.columns
    
    def test_build_graphs(self):
        """Test graph building"""
        self.feature_extractor._build_graphs(self.test_data)
        
        # Check that graphs are built
        assert self.feature_extractor.graph is not None
        assert self.feature_extractor.sender_graph is not None
        assert self.feature_extractor.receiver_graph is not None
        assert self.feature_extractor.bipartite_graph is not None
    
    def test_centrality_features(self):
        """Test centrality features"""
        result_df = self.feature_extractor._extract_centrality_features(self.test_data)
        
        assert 'sender_degree_centrality' in result_df.columns
        assert 'receiver_degree_centrality' in result_df.columns
        assert 'sender_betweenness_centrality' in result_df.columns
        assert 'receiver_betweenness_centrality' in result_df.columns
        assert 'sender_pagerank' in result_df.columns
        assert 'receiver_pagerank' in result_df.columns
    
    def test_clustering_features(self):
        """Test clustering features"""
        result_df = self.feature_extractor._extract_clustering_features(self.test_data)
        
        assert 'sender_clustering_coefficient' in result_df.columns
        assert 'receiver_clustering_coefficient' in result_df.columns
        assert 'sender_avg_neighbor_degree' in result_df.columns
        assert 'receiver_avg_neighbor_degree' in result_df.columns

class TestNLPFeatures:
    """Test cases for NLPFeatures class"""
    
    def setup_method(self):
        """Setup test data"""
        self.feature_extractor = NLPFeatures()
        
        # Create test data
        self.test_data = pd.DataFrame({
            'transaction_id': range(1, 11),
            'timestamp': pd.date_range('2023-01-01', periods=10, freq='D'),
            'amount': np.random.uniform(100, 1000, 10),
            'sender_id': [f'sender_{i}' for i in range(10)],
            'receiver_id': [f'receiver_{i}' for i in range(10)],
            'description': [
                'Payment for services',
                'URGENT: Send money immediately',
                'Transfer funds',
                'Suspicious transaction',
                'Regular payment',
                'HURRY: Quick transfer needed',
                'Business expense',
                'CONFIDENTIAL: Private transfer',
                'Normal transaction',
                'Send $1000 ASAP'
            ],
            'notes': [
                'This is a normal transaction',
                'Please send money urgently',
                'Business related transfer',
                'This looks suspicious',
                'Regular monthly payment',
                'Need this done quickly',
                'Office supplies',
                'Keep this confidential',
                'Standard transaction',
                'Emergency funds needed'
            ]
        })
    
    def test_extract_features(self):
        """Test feature extraction"""
        result_df = self.feature_extractor.extract_features(self.test_data)
        
        # Check that features are added
        original_cols = set(self.test_data.columns)
        result_cols = set(result_df.columns)
        new_cols = result_cols - original_cols
        
        assert len(new_cols) > 0
        
        # Check specific features
        assert 'description_char_count' in result_df.columns
        assert 'description_sentiment_compound' in result_df.columns
        assert 'description_fraud_keyword_count' in result_df.columns
        assert 'description_money_pattern_count' in result_df.columns
    
    def test_basic_text_features(self):
        """Test basic text features"""
        result_df = self.feature_extractor._extract_basic_text_features(self.test_data)
        
        assert 'description_char_count' in result_df.columns
        assert 'description_word_count' in result_df.columns
        assert 'description_sentence_count' in result_df.columns
        assert 'description_avg_word_length' in result_df.columns
        assert 'description_punctuation_count' in result_df.columns
    
    def test_sentiment_features(self):
        """Test sentiment features"""
        result_df = self.feature_extractor._extract_sentiment_features(self.test_data)
        
        assert 'description_sentiment_neg' in result_df.columns
        assert 'description_sentiment_neu' in result_df.columns
        assert 'description_sentiment_pos' in result_df.columns
        assert 'description_sentiment_compound' in result_df.columns
        assert 'description_textblob_polarity' in result_df.columns
        assert 'description_textblob_subjectivity' in result_df.columns
    
    def test_keyword_features(self):
        """Test keyword features"""
        result_df = self.feature_extractor._extract_keyword_features(self.test_data)
        
        assert 'description_fraud_keyword_count' in result_df.columns
        assert 'description_has_fraud_keywords' in result_df.columns
        assert 'description_urgency_keyword_count' in result_df.columns
        assert 'description_secrecy_keyword_count' in result_df.columns
        assert 'description_money_keyword_count' in result_df.columns

class TestTimeSeriesFeatures:
    """Test cases for TimeSeriesFeatures class"""
    
    def setup_method(self):
        """Setup test data"""
        self.feature_extractor = TimeSeriesFeatures()
        
        # Create test data with timestamps
        np.random.seed(42)
        timestamps = pd.date_range('2023-01-01', periods=100, freq='H')
        
        self.test_data = pd.DataFrame({
            'transaction_id': range(1, 101),
            'timestamp': timestamps,
            'amount': np.random.lognormal(5, 1, 100),
            'sender_id': [f'sender_{i % 10}' for i in range(100)],
            'receiver_id': [f'receiver_{i % 10}' for i in range(100)]
        })
    
    def test_extract_features(self):
        """Test feature extraction"""
        result_df = self.feature_extractor.extract_features(self.test_data)
        
        # Check that features are added
        original_cols = set(self.test_data.columns)
        result_cols = set(result_df.columns)
        new_cols = result_cols - original_cols
        
        assert len(new_cols) > 0
        
        # Check specific features
        assert 'hour' in result_df.columns
        assert 'dayofweek' in result_df.columns
        assert 'is_weekend' in result_df.columns
        assert 'is_business_hours' in result_df.columns
        assert 'transaction_frequency_1H' in result_df.columns
    
    def test_temporal_features(self):
        """Test temporal features"""
        result_df = self.feature_extractor._extract_temporal_features(self.test_data)
        
        assert 'hour' in result_df.columns
        assert 'day' in result_df.columns
        assert 'month' in result_df.columns
        assert 'year' in result_df.columns
        assert 'dayofweek' in result_df.columns
        assert 'is_weekend' in result_df.columns
        assert 'is_business_hours' in result_df.columns
    
    def test_frequency_features(self):
        """Test frequency features"""
        result_df = self.feature_extractor._extract_frequency_features(self.test_data)
        
        assert 'transaction_frequency_1H' in result_df.columns
        assert 'transaction_frequency_6H' in result_df.columns
        assert 'transaction_frequency_24H' in result_df.columns
        assert 'sender_frequency_1H' in result_df.columns
        assert 'receiver_frequency_1H' in result_df.columns
    
    def test_burstiness_features(self):
        """Test burstiness features"""
        result_df = self.feature_extractor._extract_burstiness_features(self.test_data)
        
        assert 'burstiness_coefficient' in result_df.columns
        assert 'local_burstiness_10' in result_df.columns
        assert 'is_in_burst' in result_df.columns
        assert 'burst_duration' in result_df.columns

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\tests\test_ingestion.py ===
"""
Test cases for data ingestion module
"""

import pytest
import pandas as pd
import numpy as np
import os
import tempfile
import sys

# Add src to path
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'src'))

from fraud_detection_engine.ingestion.data_loader import DataLoader
from fraud_detection_engine.ingestion.column_mapper import ColumnMapper

class TestDataLoader:
    """Test cases for DataLoader class"""
    
    def setup_method(self):
        """Setup test data"""
        self.loader = DataLoader()
        
        # Create test data
        self.test_data = pd.DataFrame({
            'transaction_id': range(1, 101),
            'timestamp': pd.date_range('2023-01-01', periods=100, freq='D'),
            'amount': np.random.uniform(10, 1000, 100),
            'sender_id': [f'sender_{i}' for i in range(1, 101)],
            'receiver_id': [f'receiver_{i}' for i in range(1, 101)],
            'currency': ['USD'] * 100,
            'description': [f'Transaction {i}' for i in range(1, 101)]
        })
        
        # Create temporary CSV file
        self.temp_csv = tempfile.NamedTemporaryFile(suffix='.csv', delete=False)
        self.test_data.to_csv(self.temp_csv.name, index=False)
        self.temp_csv.close()
    
    def teardown_method(self):
        """Clean up test data"""
        os.unlink(self.temp_csv.name)
    
    def test_load_csv(self):
        """Test loading CSV file"""
        df = self.loader.load_data(self.temp_csv.name)
        
        assert len(df) == 100
        assert 'transaction_id' in df.columns
        assert 'timestamp' in df.columns
        assert 'amount' in df.columns
        assert 'sender_id' in df.columns
        assert 'receiver_id' in df.columns
    
    def test_preprocess_data(self):
        """Test data preprocessing"""
        # Create data with missing values
        test_data_with_missing = self.test_data.copy()
        test_data_with_missing.loc[0, 'amount'] = np.nan
        test_data_with_missing.loc[1, 'sender_id'] = np.nan
        
        # Preprocess data
        processed_data = self.loader._preprocess_data(test_data_with_missing)
        
        # Check that missing values are handled
        assert not processed_data['amount'].isna().any()
        assert not processed_data['sender_id'].isna().any()
        
        # Check that timestamp is converted to datetime
        assert pd.api.types.is_datetime64_any_dtype(processed_data['timestamp'])
    
    def test_validate_data(self):
        """Test data validation"""
        validation_result = self.loader.validate_data()
        
        assert validation_result['valid'] == True
        assert len(validation_result['errors']) == 0
    
    def test_get_data_info(self):
        """Test getting data information"""
        self.loader.load_data(self.temp_csv.name)
        info = self.loader.get_data_info()
        
        assert 'shape' in info
        assert 'columns' in info
        assert 'dtypes' in info
        assert 'missing_values' in info
        assert 'memory_usage' in info
        assert 'numeric_stats' in info
        assert 'categorical_stats' in info
    
    def test_get_sample(self):
        """Test getting data sample"""
        self.loader.load_data(self.temp_csv.name)
        sample = self.loader.get_sample(n=5)
        
        assert len(sample) == 5
    
    def test_save_data(self):
        """Test saving data"""
        self.loader.load_data(self.temp_csv.name)
        
        # Create temporary file for saving
        temp_save = tempfile.NamedTemporaryFile(suffix='.csv', delete=False)
        temp_save.close()
        
        try:
            self.loader.save_data(temp_save.name)
            
            # Load saved data and verify
            loaded_data = pd.read_csv(temp_save.name)
            assert len(loaded_data) == 100
        finally:
            os.unlink(temp_save.name)

class TestColumnMapper:
    """Test cases for ColumnMapper class"""
    
    def setup_method(self):
        """Setup test data"""
        self.mapper = ColumnMapper()
        
        # Test user columns
        self.user_columns = [
            'tx_id', 'trans_date', 'amt', 'from_id', 'to_id',
            'sender_location', 'receiver_location', 'tx_type'
        ]
    
    def test_get_expected_columns(self):
        """Test getting expected columns"""
        expected_columns = self.mapper.get_expected_columns()
        
        assert 'transaction_id' in expected_columns
        assert 'timestamp' in expected_columns
        assert 'amount' in expected_columns
        assert 'sender_id' in expected_columns
        assert 'receiver_id' in expected_columns
    
    def test_auto_map_columns(self):
        """Test automatic column mapping"""
        mapping = self.mapper.auto_map_columns(self.user_columns)
        
        assert 'tx_id' in mapping
        assert mapping['tx_id'] == 'transaction_id'
        
        assert 'trans_date' in mapping
        assert mapping['trans_date'] == 'timestamp'
        
        assert 'amt' in mapping
        assert mapping['amt'] == 'amount'
        
        assert 'from_id' in mapping
        assert mapping['from_id'] == 'sender_id'
        
        assert 'to_id' in mapping
        assert mapping['to_id'] == 'receiver_id'
    
    def test_apply_mapping(self):
        """Test applying column mapping"""
        # Create test dataframe
        test_df = pd.DataFrame({
            'tx_id': range(1, 6),
            'trans_date': pd.date_range('2023-01-01', periods=5),
            'amt': [100, 200, 300, 400, 500],
            'from_id': ['s1', 's2', 's3', 's4', 's5'],
            'to_id': ['r1', 'r2', 'r3', 'r4', 'r5']
        })
        
        # Get mapping
        mapping = self.mapper.auto_map_columns(test_df.columns.tolist())
        
        # Apply mapping
        mapped_df = self.mapper.apply_mapping(test_df, mapping)
        
        # Check that mapped columns exist
        assert 'transaction_id' in mapped_df.columns
        assert 'timestamp' in mapped_df.columns
        assert 'amount' in mapped_df.columns
        assert 'sender_id' in mapped_df.columns
        assert 'receiver_id' in mapped_df.columns
    
    def test_validate_mapping(self):
        """Test validating mapping"""
        mapping = self.mapper.auto_map_columns(self.user_columns)
        validation = self.mapper.validate_mapping(mapping)
        
        assert validation['valid'] == True
        assert len(validation['errors']) == 0
    
    def test_get_mapping_suggestions(self):
        """Test getting mapping suggestions"""
        suggestions = self.mapper.get_mapping_suggestions(self.user_columns)
        
        assert 'tx_id' in suggestions
        assert len(suggestions['tx_id']) > 0
        
        # Check that top suggestion is correct
        top_suggestion = suggestions['tx_id'][0]
        assert top_suggestion['column'] == 'transaction_id'
        assert top_suggestion['confidence'] > 0.8

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\tests\test_models.py ===
"""
Test cases for models module
"""

import pytest
import pandas as pd
import numpy as np
import os
import sys

# Add src to path
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'src'))

from fraud_detection_engine.models.unsupervised import UnsupervisedModels
from fraud_detection_engine.models.supervised import SupervisedModels
from fraud_detection_engine.models.rule_based import RuleEngine

class TestUnsupervisedModels:
    """Test cases for UnsupervisedModels class"""
    
    def setup_method(self):
        """Setup test data"""
        self.models = UnsupervisedModels(contamination=0.1)
        
        # Create test data
        np.random.seed(42)
        self.test_data = pd.DataFrame({
            'feature1': np.random.normal(0, 1, 100),
            'feature2': np.random.normal(0, 1, 100),
            'feature3': np.random.normal(0, 1, 100),
            'feature4': np.random.normal(0, 1, 100),
            'feature5': np.random.normal(0, 1, 100)
        })
        
        # Add some outliers
        self.test_data.loc[0:4, 'feature1'] = 10  # Outliers
        self.test_data.loc[5:9, 'feature2'] = -10  # Outliers
    
    def test_run_models(self):
        """Test running models"""
        results = self.models.run_models(self.test_data)
        
        # Check that results are returned
        assert isinstance(results, dict)
        assert len(results) > 0
        
        # Check that each model has required keys
        for model_name, model_results in results.items():
            assert 'predictions' in model_results
            assert 'scores' in model_results
            assert 'model' in model_results
            assert 'feature_names' in model_results
    
    def test_isolation_forest(self):
        """Test Isolation Forest model"""
        results = self.models.run_models(self.test_data)
        
        if 'isolation_forest' in results:
            model_results = results['isolation_forest']
            
            # Check predictions
            predictions = model_results['predictions']
            assert len(predictions) == len(self.test_data)
            assert set(predictions) == {-1, 1}  # -1 for outliers, 1 for inliers
            
            # Check scores
            scores = model_results['scores']
            assert len(scores) == len(self.test_data)
            assert all(0 <= score <= 1 for score in scores)  # Normalized scores
    
    def test_local_outlier_factor(self):
        """Test Local Outlier Factor model"""
        results = self.models.run_models(self.test_data)
        
        if 'local_outlier_factor' in results:
            model_results = results['local_outlier_factor']
            
            # Check predictions
            predictions = model_results['predictions']
            assert len(predictions) == len(self.test_data)
            assert set(predictions) == {-1, 1}  # -1 for outliers, 1 for inliers
            
            # Check scores
            scores = model_results['scores']
            assert len(scores) == len(self.test_data)
            assert all(0 <= score <= 1 for score in scores)  # Normalized scores
    
    def test_autoencoder(self):
        """Test Autoencoder model"""
        results = self.models.run_models(self.test_data)
        
        if 'autoencoder' in results:
            model_results = results['autoencoder']
            
            # Check predictions
            predictions = model_results['predictions']
            assert len(predictions) == len(self.test_data)
            assert set(predictions) == {-1, 1}  # -1 for outliers, 1 for inliers
            
            # Check scores
            scores = model_results['scores']
            assert len(scores) == len(self.test_data)
            assert all(0 <= score <= 1 for score in scores)  # Normalized scores
    
    def test_predict(self):
        """Test making predictions"""
        # First fit models
        self.models.run_models(self.test_data)
        
        # Create new data
        new_data = pd.DataFrame({
            'feature1': np.random.normal(0, 1, 10),
            'feature2': np.random.normal(0, 1, 10),
            'feature3': np.random.normal(0, 1, 10),
            'feature4': np.random.normal(0, 1, 10),
            'feature5': np.random.normal(0, 1, 10)
        })
        
        # Test prediction with Isolation Forest
        if 'isolation_forest' in self.models.models:
            result = self.models.predict(new_data, 'isolation_forest')
            
            assert 'predictions' in result
            assert 'scores' in result
            assert len(result['predictions']) == len(new_data)
            assert len(result['scores']) == len(new_data)
    
    def test_get_feature_importance(self):
        """Test getting feature importance"""
        # First fit models
        self.models.run_models(self.test_data)
        
        # Test with Isolation Forest
        if 'isolation_forest' in self.models.models:
            importance = self.models.get_feature_importance('isolation_forest')
            
            assert isinstance(importance, pd.DataFrame)
            assert 'feature' in importance.columns
            assert 'importance' in importance.columns
            assert len(importance) == len(self.test_data.columns)

class TestSupervisedModels:
    """Test cases for SupervisedModels class"""
    
    def setup_method(self):
        """Setup test data"""
        self.models = SupervisedModels(test_size=0.3, random_state=42)
        
        # Create test data
        np.random.seed(42)
        self.test_data = pd.DataFrame({
            'feature1': np.random.normal(0, 1, 100),
            'feature2': np.random.normal(0, 1, 100),
            'feature3': np.random.normal(0, 1, 100),
            'feature4': np.random.normal(0, 1, 100),
            'feature5': np.random.normal(0, 1, 100),
            'fraud_flag': np.random.choice([0, 1], 100, p=[0.9, 0.1])  # Imbalanced classes
        })
    
    def test_run_models(self):
        """Test running models"""
        results = self.models.run_models(self.test_data)
        
        # Check that results are returned
        assert isinstance(results, dict)
        assert len(results) > 0
        
        # Check that each model has required keys
        for model_name, model_results in results.items():
            assert 'model' in model_results
            assert 'performance' in model_results
            assert 'feature_importance' in model_results
            assert 'feature_names' in model_results
    
    def test_random_forest(self):
        """Test Random Forest model"""
        results = self.models.run_models(self.test_data)
        
        if 'random_forest' in results:
            model_results = results['random_forest']
            
            # Check model
            model = model_results['model']
            assert model is not None
            
            # Check performance
            performance = model_results['performance']
            assert 'accuracy' in performance
            assert 'precision' in performance
            assert 'recall' in performance
            assert 'f1' in performance
            assert 'roc_auc' in performance
            
            # Check feature importance
            importance = model_results['feature_importance']
            assert isinstance(importance, pd.DataFrame)
            assert 'feature' in importance.columns
            assert 'importance' in importance.columns
    
    def test_xgboost(self):
        """Test XGBoost model"""
        results = self.models.run_models(self.test_data)
        
        if 'xgboost' in results:
            model_results = results['xgboost']
            
            # Check model
            model = model_results['model']
            assert model is not None
            
            # Check performance
            performance = model_results['performance']
            assert 'accuracy' in performance
            assert 'precision' in performance
            assert 'recall' in performance
            assert 'f1' in performance
            assert 'roc_auc' in performance
            
            # Check feature importance
            importance = model_results['feature_importance']
            assert isinstance(importance, pd.DataFrame)
            assert 'feature' in importance.columns
            assert 'importance' in importance.columns
    
    def test_predict(self):
        """Test making predictions"""
        # First fit models
        self.models.run_models(self.test_data)
        
        # Create new data
        new_data = pd.DataFrame({
            'feature1': np.random.normal(0, 1, 10),
            'feature2': np.random.normal(0, 1, 10),
            'feature3': np.random.normal(0, 1, 10),
            'feature4': np.random.normal(0, 1, 10),
            'feature5': np.random.normal(0, 1, 10)
        })
        
        # Test prediction with Random Forest
        if 'random_forest' in self.models.models:
            result = self.models.predict(new_data, 'random_forest')
            
            assert 'predictions' in result
            assert 'probabilities' in result
            assert len(result['predictions']) == len(new_data)
            assert len(result['probabilities']) == len(new_data)
    
    def test_get_feature_importance(self):
        """Test getting feature importance"""
        # First fit models
        self.models.run_models(self.test_data)
        
        # Test with Random Forest
        if 'random_forest' in self.models.models:
            importance = self.models.get_feature_importance('random_forest')
            
            assert isinstance(importance, pd.DataFrame)
            assert 'feature' in importance.columns
            assert 'importance' in importance.columns
    
    def test_get_performance(self):
        """Test getting performance metrics"""
        # First fit models
        self.models.run_models(self.test_data)
        
        # Test with Random Forest
        if 'random_forest' in self.models.models:
            performance = self.models.get_performance('random_forest')
            
            assert isinstance(performance, dict)
            assert 'accuracy' in performance
            assert 'precision' in performance
            assert 'recall' in performance
            assert 'f1' in performance
            assert 'roc_auc' in performance

class TestRuleEngine:
    """Test cases for RuleEngine class"""
    
    def setup_method(self):
        """Setup test data"""
        self.rule_engine = RuleEngine(threshold=0.5)
        
        # Create test data
        self.test_data = pd.DataFrame({
            'transaction_id': range(1, 11),
            'timestamp': pd.date_range('2023-01-01', periods=10, freq='D'),
            'amount': [100, 200, 10000, 5000, 15000, 300, 400, 12000, 8000, 20000],
            'sender_id': [f'sender_{i}' for i in range(10)],
            'receiver_id': [f'receiver_{i}' for i in range(10)],
            'sender_location': ['USA'] * 5 + ['Canada'] * 5,
            'receiver_location': ['USA'] * 8 + ['North Korea'] * 2,
            'description': [
                'Normal payment',
                'Regular transfer',
                'Large amount',
                'Big transaction',
                'Huge payment',
                'Small amount',
                'Normal transfer',
                'Large transaction',
                'Big payment',
                'Massive amount'
            ]
        })
    
    def test_apply_rules(self):
        """Test applying rules"""
        results = self.rule_engine.apply_rules(self.test_data)
        
        # Check that results are returned
        assert isinstance(results, dict)
        
        # Check required keys
        assert 'rule_results' in results
        assert 'rule_scores' in results
        assert 'total_scores' in results
        assert 'normalized_scores' in results
        assert 'rule_violations' in results
        assert 'violated_rule_names' in results
        assert 'rules' in results
        assert 'rule_weights' in results
        assert 'rule_descriptions' in results
    
    def test_high_amount_rule(self):
        """Test high amount rule"""
        # Apply rules
        results = self.rule_engine.apply_rules(self.test_data)
        
        # Check rule results
        if 'high_amount' in results['rule_results']:
            rule_results = results['rule_results']['high_amount']
            
            # Should flag transactions with amount > 10000
            expected_flags = [amount > 10000 for amount in self.test_data['amount']]
            assert rule_results == expected_flags
    
    def test_cross_border_rule(self):
        """Test cross border rule"""
        # Apply rules
        results = self.rule_engine.apply_rules(self.test_data)
        
        # Check rule results
        if 'cross_border' in results['rule_results']:
            rule_results = results['rule_results']['cross_border']
            
            # Should flag transactions with different sender and receiver countries
            expected_flags = []
            for i in range(len(self.test_data)):
                sender_country = self.test_data.loc[i, 'sender_location'].split(',')[0]
                receiver_country = self.test_data.loc[i, 'receiver_location'].split(',')[0]
                expected_flags.append(sender_country != receiver_country)
            
            assert rule_results == expected_flags
    
    def test_high_risk_country_rule(self):
        """Test high risk country rule"""
        # Apply rules
        results = self.rule_engine.apply_rules(self.test_data)
        
        # Check rule results
        if 'high_risk_country' in results['rule_results']:
            rule_results = results['rule_results']['high_risk_country']
            
            # Should flag transactions involving North Korea
            expected_flags = ['North Korea' in location for location in self.test_data['receiver_location']]
            assert rule_results == expected_flags
    
    def test_rule_violations(self):
        """Test rule violations"""
        # Apply rules
        results = self.rule_engine.apply_rules(self.test_data)
        
        # Check that violations are detected
        violations = results['rule_violations']
        assert len(violations) == len(self.test_data)
        
        # Check that some transactions are flagged
        assert any(violations)  # At least one transaction should be flagged
    
    def test_add_rule(self):
        """Test adding custom rule"""
        # Add custom rule
        def custom_rule(row):
            return row.get('amount', 0) > 5000
        
        self.rule_engine.add_rule('custom_rule', custom_rule, weight=0.5, description='Custom rule')
        
        # Check that rule is added
        assert 'custom_rule' in self.rule_engine.rules
        assert self.rule_engine.rule_weights['custom_rule'] == 0.5
        assert self.rule_engine.rule_descriptions['custom_rule'] == 'Custom rule'
    
    def test_remove_rule(self):
        """Test removing rule"""
        # Remove a rule
        if 'high_amount' in self.rule_engine.rules:
            self.rule_engine.remove_rule('high_amount')
            
            # Check that rule is removed
            assert 'high_amount' not in self.rule_engine.rules
            assert 'high_amount' not in self.rule_engine.rule_weights
            assert 'high_amount' not in self.rule_engine.rule_descriptions
    
    def test_update_rule_weight(self):
        """Test updating rule weight"""
        # Update rule weight
        self.rule_engine.update_rule_weight('high_amount', 0.5)
        
        # Check that weight is updated
        assert self.rule_engine.rule_weights['high_amount'] == 0.5
    
    def test_get_rules(self):
        """Test getting rules"""
        rules_info = self.rule_engine.get_rules()
        
        # Check structure
        assert 'rules' in rules_info
        assert 'weights' in rules_info
        assert 'descriptions' in rules_info
        
        # Check that rules are returned
        assert len(rules_info['rules']) > 0
        assert len(rules_info['weights']) > 0
        assert len(rules_info['descriptions']) > 0

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\tests\.ipynb_checkpoints\test_features-checkpoint.py ===
"""
Test cases for feature engineering module
"""

import pytest
import pandas as pd
import numpy as np
import os
import sys

# Add src to path
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'src'))

from fraud_detection_engine.features.statistical_features import StatisticalFeatures
from fraud_detection_engine.features.graph_features import GraphFeatures
from fraud_detection_engine.features.nlp_features import NLPFeatures
from fraud_detection_engine.features.timeseries_features import TimeSeriesFeatures

class TestStatisticalFeatures:
    """Test cases for StatisticalFeatures class"""
    
    def setup_method(self):
        """Setup test data"""
        self.feature_extractor = StatisticalFeatures()
        
        # Create test data
        np.random.seed(42)
        self.test_data = pd.DataFrame({
            'transaction_id': range(1, 101),
            'timestamp': pd.date_range('2023-01-01', periods=100, freq='D'),
            'amount': np.random.lognormal(5, 1, 100),
            'sender_id': [f'sender_{i % 10}' for i in range(100)],
            'receiver_id': [f'receiver_{i % 10}' for i in range(100)]
        })
    
    def test_extract_features(self):
        """Test feature extraction"""
        result_df = self.feature_extractor.extract_features(self.test_data)
        
        # Check that features are added
        original_cols = set(self.test_data.columns)
        result_cols = set(result_df.columns)
        new_cols = result_cols - original_cols
        
        assert len(new_cols) > 0
        
        # Check specific features
        assert 'amount_zscore' in result_df.columns
        assert 'amount_mad_zscore' in result_df.columns
        assert 'amount_percentile_rank' in result_df.columns
        assert 'benford_chi_square' in result_df.columns
    
    def test_benford_features(self):
        """Test Benford's Law features"""
        result_df = self.feature_extractor._extract_benford_features(self.test_data)
        
        assert 'benford_chi_square' in result_df.columns
        assert 'benford_p_value' in result_df.columns
        
        # Check that deviation features are added
        for i in range(1, 6):
            assert f'benford_deviation_{i}' in result_df.columns
    
    def test_zscore_features(self):
        """Test Z-score features"""
        result_df = self.feature_extractor._extract_zscore_features(self.test_data)
        
        assert 'amount_zscore' in result_df.columns
        assert 'amount_zscore_outlier' in result_df.columns
        assert 'sender_amount_zscore' in result_df.columns
        assert 'receiver_amount_zscore' in result_df.columns
    
    def test_mad_features(self):
        """Test MAD features"""
        result_df = self.feature_extractor._extract_mad_features(self.test_data)
        
        assert 'amount_mad_zscore' in result_df.columns
        assert 'amount_mad_outlier' in result_df.columns
        assert 'sender_amount_mad_zscore' in result_df.columns
        assert 'receiver_amount_mad_zscore' in result_df.columns
    
    def test_fit_transform(self):
        """Test fit_transform method"""
        result_df = self.feature_extractor.fit_transform(self.test_data)
        
        # Check that features are added
        original_cols = set(self.test_data.columns)
        result_cols = set(result_df.columns)
        new_cols = result_cols - original_cols
        
        assert len(new_cols) > 0
        assert self.feature_extractor.fitted == True
    
    def test_transform(self):
        """Test transform method"""
        # First fit the extractor
        self.feature_extractor.fit_transform(self.test_data)
        
        # Create new data
        new_data = pd.DataFrame({
            'transaction_id': range(101, 111),
            'timestamp': pd.date_range('2023-04-11', periods=10, freq='D'),
            'amount': np.random.lognormal(5, 1, 10),
            'sender_id': [f'sender_{i}' for i in range(10)],
            'receiver_id': [f'receiver_{i}' for i in range(10)]
        })
        
        # Transform new data
        result_df = self.feature_extractor.transform(new_data)
        
        # Check that features are added
        original_cols = set(new_data.columns)
        result_cols = set(result_df.columns)
        new_cols = result_cols - original_cols
        
        assert len(new_cols) > 0

class TestGraphFeatures:
    """Test cases for GraphFeatures class"""
    
    def setup_method(self):
        """Setup test data"""
        self.feature_extractor = GraphFeatures()
        
        # Create test data
        self.test_data = pd.DataFrame({
            'transaction_id': range(1, 21),
            'timestamp': pd.date_range('2023-01-01', periods=20, freq='D'),
            'amount': np.random.uniform(100, 1000, 20),
            'sender_id': [f'sender_{i % 5}' for i in range(20)],
            'receiver_id': [f'receiver_{i % 5}' for i in range(20)]
        })
    
    def test_extract_features(self):
        """Test feature extraction"""
        result_df = self.feature_extractor.extract_features(self.test_data)
        
        # Check that features are added
        original_cols = set(self.test_data.columns)
        result_cols = set(result_df.columns)
        new_cols = result_cols - original_cols
        
        assert len(new_cols) > 0
        
        # Check specific features
        assert 'sender_degree_centrality' in result_df.columns
        assert 'receiver_degree_centrality' in result_df.columns
        assert 'sender_clustering_coefficient' in result_df.columns
        assert 'receiver_clustering_coefficient' in result_df.columns
    
    def test_build_graphs(self):
        """Test graph building"""
        self.feature_extractor._build_graphs(self.test_data)
        
        # Check that graphs are built
        assert self.feature_extractor.graph is not None
        assert self.feature_extractor.sender_graph is not None
        assert self.feature_extractor.receiver_graph is not None
        assert self.feature_extractor.bipartite_graph is not None
    
    def test_centrality_features(self):
        """Test centrality features"""
        result_df = self.feature_extractor._extract_centrality_features(self.test_data)
        
        assert 'sender_degree_centrality' in result_df.columns
        assert 'receiver_degree_centrality' in result_df.columns
        assert 'sender_betweenness_centrality' in result_df.columns
        assert 'receiver_betweenness_centrality' in result_df.columns
        assert 'sender_pagerank' in result_df.columns
        assert 'receiver_pagerank' in result_df.columns
    
    def test_clustering_features(self):
        """Test clustering features"""
        result_df = self.feature_extractor._extract_clustering_features(self.test_data)
        
        assert 'sender_clustering_coefficient' in result_df.columns
        assert 'receiver_clustering_coefficient' in result_df.columns
        assert 'sender_avg_neighbor_degree' in result_df.columns
        assert 'receiver_avg_neighbor_degree' in result_df.columns

class TestNLPFeatures:
    """Test cases for NLPFeatures class"""
    
    def setup_method(self):
        """Setup test data"""
        self.feature_extractor = NLPFeatures()
        
        # Create test data
        self.test_data = pd.DataFrame({
            'transaction_id': range(1, 11),
            'timestamp': pd.date_range('2023-01-01', periods=10, freq='D'),
            'amount': np.random.uniform(100, 1000, 10),
            'sender_id': [f'sender_{i}' for i in range(10)],
            'receiver_id': [f'receiver_{i}' for i in range(10)],
            'description': [
                'Payment for services',
                'URGENT: Send money immediately',
                'Transfer funds',
                'Suspicious transaction',
                'Regular payment',
                'HURRY: Quick transfer needed',
                'Business expense',
                'CONFIDENTIAL: Private transfer',
                'Normal transaction',
                'Send $1000 ASAP'
            ],
            'notes': [
                'This is a normal transaction',
                'Please send money urgently',
                'Business related transfer',
                'This looks suspicious',
                'Regular monthly payment',
                'Need this done quickly',
                'Office supplies',
                'Keep this confidential',
                'Standard transaction',
                'Emergency funds needed'
            ]
        })
    
    def test_extract_features(self):
        """Test feature extraction"""
        result_df = self.feature_extractor.extract_features(self.test_data)
        
        # Check that features are added
        original_cols = set(self.test_data.columns)
        result_cols = set(result_df.columns)
        new_cols = result_cols - original_cols
        
        assert len(new_cols) > 0
        
        # Check specific features
        assert 'description_char_count' in result_df.columns
        assert 'description_sentiment_compound' in result_df.columns
        assert 'description_fraud_keyword_count' in result_df.columns
        assert 'description_money_pattern_count' in result_df.columns
    
    def test_basic_text_features(self):
        """Test basic text features"""
        result_df = self.feature_extractor._extract_basic_text_features(self.test_data)
        
        assert 'description_char_count' in result_df.columns
        assert 'description_word_count' in result_df.columns
        assert 'description_sentence_count' in result_df.columns
        assert 'description_avg_word_length' in result_df.columns
        assert 'description_punctuation_count' in result_df.columns
    
    def test_sentiment_features(self):
        """Test sentiment features"""
        result_df = self.feature_extractor._extract_sentiment_features(self.test_data)
        
        assert 'description_sentiment_neg' in result_df.columns
        assert 'description_sentiment_neu' in result_df.columns
        assert 'description_sentiment_pos' in result_df.columns
        assert 'description_sentiment_compound' in result_df.columns
        assert 'description_textblob_polarity' in result_df.columns
        assert 'description_textblob_subjectivity' in result_df.columns
    
    def test_keyword_features(self):
        """Test keyword features"""
        result_df = self.feature_extractor._extract_keyword_features(self.test_data)
        
        assert 'description_fraud_keyword_count' in result_df.columns
        assert 'description_has_fraud_keywords' in result_df.columns
        assert 'description_urgency_keyword_count' in result_df.columns
        assert 'description_secrecy_keyword_count' in result_df.columns
        assert 'description_money_keyword_count' in result_df.columns

class TestTimeSeriesFeatures:
    """Test cases for TimeSeriesFeatures class"""
    
    def setup_method(self):
        """Setup test data"""
        self.feature_extractor = TimeSeriesFeatures()
        
        # Create test data with timestamps
        np.random.seed(42)
        timestamps = pd.date_range('2023-01-01', periods=100, freq='H')
        
        self.test_data = pd.DataFrame({
            'transaction_id': range(1, 101),
            'timestamp': timestamps,
            'amount': np.random.lognormal(5, 1, 100),
            'sender_id': [f'sender_{i % 10}' for i in range(100)],
            'receiver_id': [f'receiver_{i % 10}' for i in range(100)]
        })
    
    def test_extract_features(self):
        """Test feature extraction"""
        result_df = self.feature_extractor.extract_features(self.test_data)
        
        # Check that features are added
        original_cols = set(self.test_data.columns)
        result_cols = set(result_df.columns)
        new_cols = result_cols - original_cols
        
        assert len(new_cols) > 0
        
        # Check specific features
        assert 'hour' in result_df.columns
        assert 'dayofweek' in result_df.columns
        assert 'is_weekend' in result_df.columns
        assert 'is_business_hours' in result_df.columns
        assert 'transaction_frequency_1H' in result_df.columns
    
    def test_temporal_features(self):
        """Test temporal features"""
        result_df = self.feature_extractor._extract_temporal_features(self.test_data)
        
        assert 'hour' in result_df.columns
        assert 'day' in result_df.columns
        assert 'month' in result_df.columns
        assert 'year' in result_df.columns
        assert 'dayofweek' in result_df.columns
        assert 'is_weekend' in result_df.columns
        assert 'is_business_hours' in result_df.columns
    
    def test_frequency_features(self):
        """Test frequency features"""
        result_df = self.feature_extractor._extract_frequency_features(self.test_data)
        
        assert 'transaction_frequency_1H' in result_df.columns
        assert 'transaction_frequency_6H' in result_df.columns
        assert 'transaction_frequency_24H' in result_df.columns
        assert 'sender_frequency_1H' in result_df.columns
        assert 'receiver_frequency_1H' in result_df.columns
    
    def test_burstiness_features(self):
        """Test burstiness features"""
        result_df = self.feature_extractor._extract_burstiness_features(self.test_data)
        
        assert 'burstiness_coefficient' in result_df.columns
        assert 'local_burstiness_10' in result_df.columns
        assert 'is_in_burst' in result_df.columns
        assert 'burst_duration' in result_df.columns

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\tests\.ipynb_checkpoints\test_ingestion-checkpoint.py ===
"""
Test cases for data ingestion module
"""

import pytest
import pandas as pd
import numpy as np
import os
import tempfile
import sys

# Add src to path
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'src'))

from fraud_detection_engine.ingestion.data_loader import DataLoader
from fraud_detection_engine.ingestion.column_mapper import ColumnMapper

class TestDataLoader:
    """Test cases for DataLoader class"""
    
    def setup_method(self):
        """Setup test data"""
        self.loader = DataLoader()
        
        # Create test data
        self.test_data = pd.DataFrame({
            'transaction_id': range(1, 101),
            'timestamp': pd.date_range('2023-01-01', periods=100, freq='D'),
            'amount': np.random.uniform(10, 1000, 100),
            'sender_id': [f'sender_{i}' for i in range(1, 101)],
            'receiver_id': [f'receiver_{i}' for i in range(1, 101)],
            'currency': ['USD'] * 100,
            'description': [f'Transaction {i}' for i in range(1, 101)]
        })
        
        # Create temporary CSV file
        self.temp_csv = tempfile.NamedTemporaryFile(suffix='.csv', delete=False)
        self.test_data.to_csv(self.temp_csv.name, index=False)
        self.temp_csv.close()
    
    def teardown_method(self):
        """Clean up test data"""
        os.unlink(self.temp_csv.name)
    
    def test_load_csv(self):
        """Test loading CSV file"""
        df = self.loader.load_data(self.temp_csv.name)
        
        assert len(df) == 100
        assert 'transaction_id' in df.columns
        assert 'timestamp' in df.columns
        assert 'amount' in df.columns
        assert 'sender_id' in df.columns
        assert 'receiver_id' in df.columns
    
    def test_preprocess_data(self):
        """Test data preprocessing"""
        # Create data with missing values
        test_data_with_missing = self.test_data.copy()
        test_data_with_missing.loc[0, 'amount'] = np.nan
        test_data_with_missing.loc[1, 'sender_id'] = np.nan
        
        # Preprocess data
        processed_data = self.loader._preprocess_data(test_data_with_missing)
        
        # Check that missing values are handled
        assert not processed_data['amount'].isna().any()
        assert not processed_data['sender_id'].isna().any()
        
        # Check that timestamp is converted to datetime
        assert pd.api.types.is_datetime64_any_dtype(processed_data['timestamp'])
    
    def test_validate_data(self):
        """Test data validation"""
        validation_result = self.loader.validate_data()
        
        assert validation_result['valid'] == True
        assert len(validation_result['errors']) == 0
    
    def test_get_data_info(self):
        """Test getting data information"""
        self.loader.load_data(self.temp_csv.name)
        info = self.loader.get_data_info()
        
        assert 'shape' in info
        assert 'columns' in info
        assert 'dtypes' in info
        assert 'missing_values' in info
        assert 'memory_usage' in info
        assert 'numeric_stats' in info
        assert 'categorical_stats' in info
    
    def test_get_sample(self):
        """Test getting data sample"""
        self.loader.load_data(self.temp_csv.name)
        sample = self.loader.get_sample(n=5)
        
        assert len(sample) == 5
    
    def test_save_data(self):
        """Test saving data"""
        self.loader.load_data(self.temp_csv.name)
        
        # Create temporary file for saving
        temp_save = tempfile.NamedTemporaryFile(suffix='.csv', delete=False)
        temp_save.close()
        
        try:
            self.loader.save_data(temp_save.name)
            
            # Load saved data and verify
            loaded_data = pd.read_csv(temp_save.name)
            assert len(loaded_data) == 100
        finally:
            os.unlink(temp_save.name)

class TestColumnMapper:
    """Test cases for ColumnMapper class"""
    
    def setup_method(self):
        """Setup test data"""
        self.mapper = ColumnMapper()
        
        # Test user columns
        self.user_columns = [
            'tx_id', 'trans_date', 'amt', 'from_id', 'to_id',
            'sender_location', 'receiver_location', 'tx_type'
        ]
    
    def test_get_expected_columns(self):
        """Test getting expected columns"""
        expected_columns = self.mapper.get_expected_columns()
        
        assert 'transaction_id' in expected_columns
        assert 'timestamp' in expected_columns
        assert 'amount' in expected_columns
        assert 'sender_id' in expected_columns
        assert 'receiver_id' in expected_columns
    
    def test_auto_map_columns(self):
        """Test automatic column mapping"""
        mapping = self.mapper.auto_map_columns(self.user_columns)
        
        assert 'tx_id' in mapping
        assert mapping['tx_id'] == 'transaction_id'
        
        assert 'trans_date' in mapping
        assert mapping['trans_date'] == 'timestamp'
        
        assert 'amt' in mapping
        assert mapping['amt'] == 'amount'
        
        assert 'from_id' in mapping
        assert mapping['from_id'] == 'sender_id'
        
        assert 'to_id' in mapping
        assert mapping['to_id'] == 'receiver_id'
    
    def test_apply_mapping(self):
        """Test applying column mapping"""
        # Create test dataframe
        test_df = pd.DataFrame({
            'tx_id': range(1, 6),
            'trans_date': pd.date_range('2023-01-01', periods=5),
            'amt': [100, 200, 300, 400, 500],
            'from_id': ['s1', 's2', 's3', 's4', 's5'],
            'to_id': ['r1', 'r2', 'r3', 'r4', 'r5']
        })
        
        # Get mapping
        mapping = self.mapper.auto_map_columns(test_df.columns.tolist())
        
        # Apply mapping
        mapped_df = self.mapper.apply_mapping(test_df, mapping)
        
        # Check that mapped columns exist
        assert 'transaction_id' in mapped_df.columns
        assert 'timestamp' in mapped_df.columns
        assert 'amount' in mapped_df.columns
        assert 'sender_id' in mapped_df.columns
        assert 'receiver_id' in mapped_df.columns
    
    def test_validate_mapping(self):
        """Test validating mapping"""
        mapping = self.mapper.auto_map_columns(self.user_columns)
        validation = self.mapper.validate_mapping(mapping)
        
        assert validation['valid'] == True
        assert len(validation['errors']) == 0
    
    def test_get_mapping_suggestions(self):
        """Test getting mapping suggestions"""
        suggestions = self.mapper.get_mapping_suggestions(self.user_columns)
        
        assert 'tx_id' in suggestions
        assert len(suggestions['tx_id']) > 0
        
        # Check that top suggestion is correct
        top_suggestion = suggestions['tx_id'][0]
        assert top_suggestion['column'] == 'transaction_id'
        assert top_suggestion['confidence'] > 0.8

=== File: C:\Users\Tranquil Abyss\fraud-detection-platform\tests\.ipynb_checkpoints\test_models-checkpoint.py ===
"""
Test cases for models module
"""

import pytest
import pandas as pd
import numpy as np
import os
import sys

# Add src to path
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'src'))

from fraud_detection_engine.models.unsupervised import UnsupervisedModels
from fraud_detection_engine.models.supervised import SupervisedModels
from fraud_detection_engine.models.rule_based import RuleEngine

class TestUnsupervisedModels:
    """Test cases for UnsupervisedModels class"""
    
    def setup_method(self):
        """Setup test data"""
        self.models = UnsupervisedModels(contamination=0.1)
        
        # Create test data
        np.random.seed(42)
        self.test_data = pd.DataFrame({
            'feature1': np.random.normal(0, 1, 100),
            'feature2': np.random.normal(0, 1, 100),
            'feature3': np.random.normal(0, 1, 100),
            'feature4': np.random.normal(0, 1, 100),
            'feature5': np.random.normal(0, 1, 100)
        })
        
        # Add some outliers
        self.test_data.loc[0:4, 'feature1'] = 10  # Outliers
        self.test_data.loc[5:9, 'feature2'] = -10  # Outliers
    
    def test_run_models(self):
        """Test running models"""
        results = self.models.run_models(self.test_data)
        
        # Check that results are returned
        assert isinstance(results, dict)
        assert len(results) > 0
        
        # Check that each model has required keys
        for model_name, model_results in results.items():
            assert 'predictions' in model_results
            assert 'scores' in model_results
            assert 'model' in model_results
            assert 'feature_names' in model_results
    
    def test_isolation_forest(self):
        """Test Isolation Forest model"""
        results = self.models.run_models(self.test_data)
        
        if 'isolation_forest' in results:
            model_results = results['isolation_forest']
            
            # Check predictions
            predictions = model_results['predictions']
            assert len(predictions) == len(self.test_data)
            assert set(predictions) == {-1, 1}  # -1 for outliers, 1 for inliers
            
            # Check scores
            scores = model_results['scores']
            assert len(scores) == len(self.test_data)
            assert all(0 <= score <= 1 for score in scores)  # Normalized scores
    
    def test_local_outlier_factor(self):
        """Test Local Outlier Factor model"""
        results = self.models.run_models(self.test_data)
        
        if 'local_outlier_factor' in results:
            model_results = results['local_outlier_factor']
            
            # Check predictions
            predictions = model_results['predictions']
            assert len(predictions) == len(self.test_data)
            assert set(predictions) == {-1, 1}  # -1 for outliers, 1 for inliers
            
            # Check scores
            scores = model_results['scores']
            assert len(scores) == len(self.test_data)
            assert all(0 <= score <= 1 for score in scores)  # Normalized scores
    
    def test_autoencoder(self):
        """Test Autoencoder model"""
        results = self.models.run_models(self.test_data)
        
        if 'autoencoder' in results:
            model_results = results['autoencoder']
            
            # Check predictions
            predictions = model_results['predictions']
            assert len(predictions) == len(self.test_data)
            assert set(predictions) == {-1, 1}  # -1 for outliers, 1 for inliers
            
            # Check scores
            scores = model_results['scores']
            assert len(scores) == len(self.test_data)
            assert all(0 <= score <= 1 for score in scores)  # Normalized scores
    
    def test_predict(self):
        """Test making predictions"""
        # First fit models
        self.models.run_models(self.test_data)
        
        # Create new data
        new_data = pd.DataFrame({
            'feature1': np.random.normal(0, 1, 10),
            'feature2': np.random.normal(0, 1, 10),
            'feature3': np.random.normal(0, 1, 10),
            'feature4': np.random.normal(0, 1, 10),
            'feature5': np.random.normal(0, 1, 10)
        })
        
        # Test prediction with Isolation Forest
        if 'isolation_forest' in self.models.models:
            result = self.models.predict(new_data, 'isolation_forest')
            
            assert 'predictions' in result
            assert 'scores' in result
            assert len(result['predictions']) == len(new_data)
            assert len(result['scores']) == len(new_data)
    
    def test_get_feature_importance(self):
        """Test getting feature importance"""
        # First fit models
        self.models.run_models(self.test_data)
        
        # Test with Isolation Forest
        if 'isolation_forest' in self.models.models:
            importance = self.models.get_feature_importance('isolation_forest')
            
            assert isinstance(importance, pd.DataFrame)
            assert 'feature' in importance.columns
            assert 'importance' in importance.columns
            assert len(importance) == len(self.test_data.columns)

class TestSupervisedModels:
    """Test cases for SupervisedModels class"""
    
    def setup_method(self):
        """Setup test data"""
        self.models = SupervisedModels(test_size=0.3, random_state=42)
        
        # Create test data
        np.random.seed(42)
        self.test_data = pd.DataFrame({
            'feature1': np.random.normal(0, 1, 100),
            'feature2': np.random.normal(0, 1, 100),
            'feature3': np.random.normal(0, 1, 100),
            'feature4': np.random.normal(0, 1, 100),
            'feature5': np.random.normal(0, 1, 100),
            'fraud_flag': np.random.choice([0, 1], 100, p=[0.9, 0.1])  # Imbalanced classes
        })
    
    def test_run_models(self):
        """Test running models"""
        results = self.models.run_models(self.test_data)
        
        # Check that results are returned
        assert isinstance(results, dict)
        assert len(results) > 0
        
        # Check that each model has required keys
        for model_name, model_results in results.items():
            assert 'model' in model_results
            assert 'performance' in model_results
            assert 'feature_importance' in model_results
            assert 'feature_names' in model_results
    
    def test_random_forest(self):
        """Test Random Forest model"""
        results = self.models.run_models(self.test_data)
        
        if 'random_forest' in results:
            model_results = results['random_forest']
            
            # Check model
            model = model_results['model']
            assert model is not None
            
            # Check performance
            performance = model_results['performance']
            assert 'accuracy' in performance
            assert 'precision' in performance
            assert 'recall' in performance
            assert 'f1' in performance
            assert 'roc_auc' in performance
            
            # Check feature importance
            importance = model_results['feature_importance']
            assert isinstance(importance, pd.DataFrame)
            assert 'feature' in importance.columns
            assert 'importance' in importance.columns
    
    def test_xgboost(self):
        """Test XGBoost model"""
        results = self.models.run_models(self.test_data)
        
        if 'xgboost' in results:
            model_results = results['xgboost']
            
            # Check model
            model = model_results['model']
            assert model is not None
            
            # Check performance
            performance = model_results['performance']
            assert 'accuracy' in performance
            assert 'precision' in performance
            assert 'recall' in performance
            assert 'f1' in performance
            assert 'roc_auc' in performance
            
            # Check feature importance
            importance = model_results['feature_importance']
            assert isinstance(importance, pd.DataFrame)
            assert 'feature' in importance.columns
            assert 'importance' in importance.columns
    
    def test_predict(self):
        """Test making predictions"""
        # First fit models
        self.models.run_models(self.test_data)
        
        # Create new data
        new_data = pd.DataFrame({
            'feature1': np.random.normal(0, 1, 10),
            'feature2': np.random.normal(0, 1, 10),
            'feature3': np.random.normal(0, 1, 10),
            'feature4': np.random.normal(0, 1, 10),
            'feature5': np.random.normal(0, 1, 10)
        })
        
        # Test prediction with Random Forest
        if 'random_forest' in self.models.models:
            result = self.models.predict(new_data, 'random_forest')
            
            assert 'predictions' in result
            assert 'probabilities' in result
            assert len(result['predictions']) == len(new_data)
            assert len(result['probabilities']) == len(new_data)
    
    def test_get_feature_importance(self):
        """Test getting feature importance"""
        # First fit models
        self.models.run_models(self.test_data)
        
        # Test with Random Forest
        if 'random_forest' in self.models.models:
            importance = self.models.get_feature_importance('random_forest')
            
            assert isinstance(importance, pd.DataFrame)
            assert 'feature' in importance.columns
            assert 'importance' in importance.columns
    
    def test_get_performance(self):
        """Test getting performance metrics"""
        # First fit models
        self.models.run_models(self.test_data)
        
        # Test with Random Forest
        if 'random_forest' in self.models.models:
            performance = self.models.get_performance('random_forest')
            
            assert isinstance(performance, dict)
            assert 'accuracy' in performance
            assert 'precision' in performance
            assert 'recall' in performance
            assert 'f1' in performance
            assert 'roc_auc' in performance

class TestRuleEngine:
    """Test cases for RuleEngine class"""
    
    def setup_method(self):
        """Setup test data"""
        self.rule_engine = RuleEngine(threshold=0.5)
        
        # Create test data
        self.test_data = pd.DataFrame({
            'transaction_id': range(1, 11),
            'timestamp': pd.date_range('2023-01-01', periods=10, freq='D'),
            'amount': [100, 200, 10000, 5000, 15000, 300, 400, 12000, 8000, 20000],
            'sender_id': [f'sender_{i}' for i in range(10)],
            'receiver_id': [f'receiver_{i}' for i in range(10)],
            'sender_location': ['USA'] * 5 + ['Canada'] * 5,
            'receiver_location': ['USA'] * 8 + ['North Korea'] * 2,
            'description': [
                'Normal payment',
                'Regular transfer',
                'Large amount',
                'Big transaction',
                'Huge payment',
                'Small amount',
                'Normal transfer',
                'Large transaction',
                'Big payment',
                'Massive amount'
            ]
        })
    
    def test_apply_rules(self):
        """Test applying rules"""
        results = self.rule_engine.apply_rules(self.test_data)
        
        # Check that results are returned
        assert isinstance(results, dict)
        
        # Check required keys
        assert 'rule_results' in results
        assert 'rule_scores' in results
        assert 'total_scores' in results
        assert 'normalized_scores' in results
        assert 'rule_violations' in results
        assert 'violated_rule_names' in results
        assert 'rules' in results
        assert 'rule_weights' in results
        assert 'rule_descriptions' in results
    
    def test_high_amount_rule(self):
        """Test high amount rule"""
        # Apply rules
        results = self.rule_engine.apply_rules(self.test_data)
        
        # Check rule results
        if 'high_amount' in results['rule_results']:
            rule_results = results['rule_results']['high_amount']
            
            # Should flag transactions with amount > 10000
            expected_flags = [amount > 10000 for amount in self.test_data['amount']]
            assert rule_results == expected_flags
    
    def test_cross_border_rule(self):
        """Test cross border rule"""
        # Apply rules
        results = self.rule_engine.apply_rules(self.test_data)
        
        # Check rule results
        if 'cross_border' in results['rule_results']:
            rule_results = results['rule_results']['cross_border']
            
            # Should flag transactions with different sender and receiver countries
            expected_flags = []
            for i in range(len(self.test_data)):
                sender_country = self.test_data.loc[i, 'sender_location'].split(',')[0]
                receiver_country = self.test_data.loc[i, 'receiver_location'].split(',')[0]
                expected_flags.append(sender_country != receiver_country)
            
            assert rule_results == expected_flags
    
    def test_high_risk_country_rule(self):
        """Test high risk country rule"""
        # Apply rules
        results = self.rule_engine.apply_rules(self.test_data)
        
        # Check rule results
        if 'high_risk_country' in results['rule_results']:
            rule_results = results['rule_results']['high_risk_country']
            
            # Should flag transactions involving North Korea
            expected_flags = ['North Korea' in location for location in self.test_data['receiver_location']]
            assert rule_results == expected_flags
    
    def test_rule_violations(self):
        """Test rule violations"""
        # Apply rules
        results = self.rule_engine.apply_rules(self.test_data)
        
        # Check that violations are detected
        violations = results['rule_violations']
        assert len(violations) == len(self.test_data)
        
        # Check that some transactions are flagged
        assert any(violations)  # At least one transaction should be flagged
    
    def test_add_rule(self):
        """Test adding custom rule"""
        # Add custom rule
        def custom_rule(row):
            return row.get('amount', 0) > 5000
        
        self.rule_engine.add_rule('custom_rule', custom_rule, weight=0.5, description='Custom rule')
        
        # Check that rule is added
        assert 'custom_rule' in self.rule_engine.rules
        assert self.rule_engine.rule_weights['custom_rule'] == 0.5
        assert self.rule_engine.rule_descriptions['custom_rule'] == 'Custom rule'
    
    def test_remove_rule(self):
        """Test removing rule"""
        # Remove a rule
        if 'high_amount' in self.rule_engine.rules:
            self.rule_engine.remove_rule('high_amount')
            
            # Check that rule is removed
            assert 'high_amount' not in self.rule_engine.rules
            assert 'high_amount' not in self.rule_engine.rule_weights
            assert 'high_amount' not in self.rule_engine.rule_descriptions
    
    def test_update_rule_weight(self):
        """Test updating rule weight"""
        # Update rule weight
        self.rule_engine.update_rule_weight('high_amount', 0.5)
        
        # Check that weight is updated
        assert self.rule_engine.rule_weights['high_amount'] == 0.5
    
    def test_get_rules(self):
        """Test getting rules"""
        rules_info = self.rule_engine.get_rules()
        
        # Check structure
        assert 'rules' in rules_info
        assert 'weights' in rules_info
        assert 'descriptions' in rules_info
        
        # Check that rules are returned
        assert len(rules_info['rules']) > 0
        assert len(rules_info['weights']) > 0
        assert len(rules_info['descriptions']) > 0
